@article{monotonic_numeric_representations,
  title={Monotonic Representation of Numeric Properties in Language Models},
  author={Heinzerling, Benjamin and Inui, Kentaro},
  journal={arXiv preprint arXiv:2403.10381},
  year={2024}
}

@article{othello_neel_linear,
  title={Emergent linear representations in world models of self-supervised sequence models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2309.00941},
  year={2023}
}

@article{linear_representation_hypothesis,
  title={The linear representation hypothesis and the geometry of large language models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  journal={arXiv preprint arXiv:2311.03658},
  year={2023}
}

@article{origins_of_linear_representations,
  title={On the Origins of Linear Representations in Large Language Models},
  author={Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon and Veitch, Victor},
  journal={arXiv preprint arXiv:2403.03867},
  year={2024}
}



@article{dictionary_monosemanticity_anthropic,
   title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
   author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
   year={2023},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@article{linear_truth_sam,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{original_othello_paper,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@article{space_time,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@article{gpt2_ioi,
  title={Interpretability in the wild: a circuit for indirect object identification in gpt-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={\url{https://transformer-circuits.pub/2022/toy_model/index.html}}
}

@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{ziming_grokking,
  title={Towards understanding grokking: An effective theory of representation learning},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34651--34663},
  year={2022}
}

@article{neel_grokking,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{clock_and_pizza,
  title={The clock and the pizza: Two stories in mechanistic explanation of neural networks},
  author={Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{successor_heads,
  title={Successor heads: Recurring, interpretable attention heads in the wild},
  author={Gould, Rhys and Ong, Euan and Ogden, George and Conmy, Arthur},
  journal={arXiv preprint arXiv:2312.09230},
  year={2023}
}

@inproceedings{mechanistic_interp_arithmetic,
  title={A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis},
  author={Stolfo, Alessandro and Belinkov, Yonatan and Sachan, Mrinmaya},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7035--7052},
  year={2023}
}

@article{gpt2_greater_than,
  title={How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author={Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{word2vec,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@inproceedings{king_and_queen,
  title={Linguistic regularities in continuous space word representations},
  author={Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
  pages={746--751},
  year={2013}
}

@article{other_sae_paper,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@article{marks2024sparse,
  title={Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models},
  author={Marks, Samuel and Rager, Can and Michaud, Eric J and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  journal={arXiv preprint arXiv:2403.19647},
  year={2024}
}

@article{acdc_circuits,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16318--16352},
  year={2023}
}

@incollection{Johnson1984,
  author    = {William B. Johnson and Joram Lindenstrauss},
  title     = {Extensions of Lipschitz mappings into a Hilbert space},
  booktitle = {Conference in modern analysis and probability (New Haven, Conn., 1982)},
  publisher = {American Mathematical Society},
  address   = {Providence, RI},
  volume    = {26},
  series    = {Contemporary Mathematics},
  pages     = {189--206},
  year      = {1984},
  doi       = {10.1090/conm/026/737400},
  isbn      = {0-8218-5030-X},
  mrnumber  = {0737400},
  s2cid     = {117819162}
}

@article{sparks_of_agi,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{human_eval,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@techreport{claude_3_tech_report,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  year={2024},
  institution={Anthropic}
}

@article{gemini_tech_report,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{gpt_4_tech_report,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{michaud2024opening,
  title={Opening the AI black box: program synthesis via mechanistic interpretability},
  author={Michaud, Eric J and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukeli{\'c}, Mateja and Tegmark, Max},
  journal={arXiv preprint arXiv:2402.05110},
  year={2024}
}

@misc{bloom2024gpt2residualsaes,
   title = {Open Source Sparse Autoencoders for all Residual Stream Layers of GPT2 Small},
   author = {Joseph Bloom},
   year = {2024},
   howpublished = {\url{https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream}},
}

@article{mistral_7b,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@misc{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@misc{grokmodelcard,
  title={Grok-1 Model Card},
  author={xAI},
  year={2024},
  url = {https://x.ai/blog/grok/model-card}
}

https://x.ai/blog/grok/model-card

@article{gpt_2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@article{best_practices_activation_patching,
  title={Towards best practices of activation patching in language models: Metrics and methods},
  author={Zhang, Fred and Nanda, Neel},
  journal={arXiv preprint arXiv:2309.16042},
  year={2023}
}

@misc{belief_state_geoemtry,
   title = {Transformers Represent Belief State Geometry in their Residual Stream},
   author = {Shai, Adam and Riechers, Paul and Teixeira, Lucas and Oldenziel, Alexander and Marzen, Sarah},
   year = {2024},
   howpublished = {\url{https://www.alignmentforum.org/posts/gTZ2SxesbHckJ3CkF/transformers-represent-belief-state-geometry-in-their}},
}


@article{nonlinear_features_in_context,
  title={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape},
  author={Kim, Juno and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2402.01258},
  year={2024}
}

@article{polytope_lens,
  title={Interpreting neural networks through the polytope lens},
  author={Black, Sid and Sharkey, Lee and Grinsztajn, Leo and Winsor, Eric and Braun, Dan and Merizian, Jacob and Parker, Kip and Guevara, Carlos Ram{\'o}n and Millidge, Beren and Alfour, Gabriel and others},
  journal={arXiv preprint arXiv:2211.12312},
  year={2022}
}

@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}

@MISC {bill_johnson_mathoverflow,
    TITLE = {Almost orthogonal vectors},
    AUTHOR = {Bill Johnson (https://mathoverflow.net/users/2554/bill-johnson)},
    HOWPUBLISHED = {MathOverflow},
    NOTE = {URL:https://mathoverflow.net/q/24873 (version: 2010-05-16)},
    EPRINT = {https://mathoverflow.net/q/24873},
    URL = {https://mathoverflow.net/q/24873}
}

@article{syed2023attribution,
  title={Attribution Patching Outperforms Automated Circuit Discovery},
  author={Syed, Aaquib and Rager, Can and Conmy, Arthur},
  journal={arXiv preprint arXiv:2310.10348},
  year={2023}
}

@misc{high21s,
author = "Nicholas J. Higham",
title = "Singular Value Inequalities",
year = 2021,
month = may,
howpublished ="\url{https://nhigham.com/2021/05/04/singular-value-inequalities/}"
}

@article{gershgorin1931uber,
  title = {\"Uber die Abgrenzung der Eigenwerte einer Matrix},
  author = {Gershgorin, Semyon Aranovich},
  journal = {Izvestiya Rossi\u\i skoi akademii nauk. Seriya matematicheskaya},
  number = {6},
  pages = {749--754},
  year = {1931},
  publisher = {Rossijskaya akademiya nauk, Matematicheskij institut im. V.A. Steklova Rossi\u\i skaya~…}
}

@article{circles_theory,
  title={Feature emergence via margin maximization: case studies in algebraic tasks},
  author={Morwani, Depen and Edelman, Benjamin L and Oncescu, Costin-Andrei and Zhao, Rosie and Kakade, Sham},
  journal={arXiv preprint arXiv:2311.07568},
  year={2023}
}

@article{platonic,
  title={The Platonic Representation Hypothesis},
  author={Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  journal={arXiv preprint arXiv:2405.07987},
  year={2024}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{manifesto_two,
  title={Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems},
  author={Dalrymple, David and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and others},
  journal={arXiv preprint arXiv:2405.06624},
  year={2024}
}

@article{manifesto_one,
  title={Provably safe systems: the only path to controllable agi},
  author={Tegmark, Max and Omohundro, Steve},
  journal={arXiv preprint arXiv:2309.01933},
  year={2023}
}

@inproceedings{lattanzi2020framework,
  title={A framework for parallelizing hierarchical clustering methods},
  author={Lattanzi, Silvio and Lavastida, Thomas and Lu, Kefu and Moseley, Benjamin},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, W{\"u}rzburg, Germany, September 16--20, 2019, Proceedings, Part I},
  pages={73--89},
  year={2020},
  organization={Springer}
}

@misc{gao2024scalingevaluatingsparseautoencoders,
      title={Scaling and evaluating sparse autoencoders}, 
      author={Leo Gao and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
      year={2024},
      eprint={2406.04093},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04093}, 
}

 @article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }



@misc{claude3modelcard,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  year={2024},
  url = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{engels2024not,
  title={Not All Language Model Features Are Linear},
  author={Engels, Joshua and Liao, Isaac and Michaud, Eric J and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{raina2009large,
  title={Large-scale deep unsupervised learning using graphics processors},
  author={Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={873--880},
  year={2009}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@article{chen2020slide,
  title={Slide: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems},
  author={Chen, Beidi and Medini, Tharun and Farwell, James and Tai, Charlie and Shrivastava, Anshumali and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={291--306},
  year={2020}
}