{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**1. Abstract:**\nThis paper introduces AnySkin, a versatile, easily replaceable tactile sensor designed to address key challenges in robotic touch sensing: versatility, replaceability, and data reusability. The sensor builds on the ReSkin framework by decoupling the sensing mechanism from the interaction surface and introducing a self-aligned, self-adhering attachment mechanism. AnySkin provides a streamlined fabrication process, enhances slip detection, generalizes across new sensor instances, and maintains consistent performance in learned policies even with sensor replacements. It is open-sourced, with additional resources available at https://any-skin.github.io/.\n\n**2. Introduction:**\nIn the context of robotics, touch sensing significantly lags behind vision and proprioception despite being crucial for biological movement and control. The paper explores reasons for this disparity, highlighting the high cost, lack of convenience, and inconsistency of touch sensors. In contrast to cameras, which are cheap, easy to use, and offer consistent data, touch sensors are often expensive and custom-built, leading to significant variability. This results in challenges when transferring models across different sensors. AnySkin addresses these issues by providing a cheap, convenient, and consistent touch sensor. It separates the sensing mechanism from the interaction surface, enhancing durability and replaceability. The sensor is evaluated on various tasks and compared with existing sensors like DIGIT and ReSkin to demonstrate its efficacy.\n\n**3. Related Work:**\nExisting tactile sensors utilize various modalities:\n- **Capacitative sensors**: Highly sensitive but struggle with modeling shear force and are prone to breakage.\n- **Resistive sensors**: Durable and simple but offer low spatial resolution.\n- **Optical sensors**: High-resolution but limited by the physical constraints of cameras.\n- **Magnetic sensors (ReSkin and uSkin)**: Address many limitations with adaptability and the ability to capture shear forces.\nThe paper advances the field by improving on ReSkin's design with a focus on enhancing versatility, consistency, and replaceability. It emphasizes the need for tactile sensors to be as ubiquitous and scalable as vision sensors, making AnySkin a significant step forward.\n\n**4. Method:**\nAnySkin consists of the following components:\n- **Magnetizing skins post-curing** using a pulse magnetizer for stronger magnetic fields.\n- **Physical separation** between magnetic elastomer and magnetometer circuit.\n- **Finer magnetic particles** to ensure a uniform particle distribution.\n- **Self-aligning design** to minimize variability in positioning and ensure consistent signal.\nThe sensor fabrication follows a similar general outline to ReSkin: mixing magnetic particles with elastomer, curing in molds, and then magnetizing. The skin shapes are designed using a two-part mold with alignment guides, adjustable through an open-source CAD tool for ease of customization.\n\n**Fabrication Process**:\n1. **Mold Design**: Two-part molds with alignment guides ensure self-adherence without adhesives.\n2. **Elastomer Composition**: Mixture includes Smooth-On\u2019s DragonSkin 10 Slow elastomer and Magnequench MQFP-15-7 particles.\n3. **Magnetization**: Post-curing with a pulse magnetizer ensures stronger and more uniform magnetic fields.\n4. **Magnetic Elastomer Fabrication**: The mix is degassed, poured into molds, cured for 16 hours, and magnetized.\n\n**5. Experimental Setup:**\nExperiments were conducted using the Kinova Jaco arm, Onrobot RG-2 gripper, and various robotic platforms (xArm, Franka, Leap hand). An array of precise robotic tasks such as plug insertion, card swiping, and USB insertion were used to test the sensor's performance. Data collected for these tasks were used to train behavior cloning models using vision and tactile data, processed using an LSTM for slip detection and the BAKU transformer architecture for learned policies.\n\n**6. Results and Analysis:**\n**Quantitative Evaluation**: \n- AnySkin showed increased signal strength and reduced variability with the use of finer particles and pulse magnetizing.\n- Performance during slip detection was high (92% accuracy on unseen objects).\n- Replacement time was significantly lower (12s) compared to ReSkin (both adhesive and screws) and DIGIT sensors.\n\n**Policy Generalization**: \n- Policies trained with AnySkin maintained high performance (~13% drop) on sensor replacement compared to ReSkin (~43% drop) and DIGIT (low initial and post-replacement performance).\n\n**7. Discussion:**\n**Implications**:\n- The reduced variability and increased signal strength of AnySkin are central to its superior performance.\n- It enables scalable data collection and consistent model performance across sensor replacements, addressing a major bottleneck in tactile sensing.\n- The physical separation and self-aligning design are crucial innovations.\n\n**Limitations**:\n- Susceptibility to magnetic and ferromagnetic interference.\n- The need for further standardization in the fabrication process to enhance consistency.\n\n**8. Conclusion:**\nAnySkin offers an effective, scalable solution for tactile sensing in robotics, providing high versatility, easy replaceability, and consistent data reusability. Future research directions include integrating noise reduction techniques, standardizing fabrication procedures, and exploring broader applications in complex robotic tasks. The advancement marks a step closer to making tactile sensors as ubiquitous as visual sensors in robotic systems."}, {"role": "assistant", "content": "AnySkin: Plug-and-play Skin Sensing for Robotic Touch\nRaunaq Bhirangi1,2,\u2217, Venkatesh Pattabiraman1, Enes Erciyes1, Yifeng Cao3, Tess Hellebrekers4, Lerrel Pinto1\nhttps://any-skin.github.io\nLEAP Hand Dobb-E stick\n FrankaCompatible with a variety of robots Swappable for learned policies\nAnySkin\nReSkin\nDIGIT\n0 20 40 60 80\nTraining\nNew Skin\nSuccess Rate, in %\nEasy to assemble\nAnySkin\nFingertip\nReSkin circuit\nFig. 1: We present AnySkin, a skin sensor made for robotic touch that is easy to assemble, compatible with different robot end-effectors\nand generalizes to new skin instances. AnySkin senses contact through distortions in magnetic field generated by magnetized iron particles\nin the sensing surface. The flexible surface is physically separated from its electronics, which allows for easy replacability when damaged.\nAbstract \u2014 While tactile sensing is widely accepted as an\nimportant and useful sensing modality, its use pales in compar-\nison to other sensory modalities like vision and proprioception.\nAnySkin addresses the critical challenges that impede the use of\ntactile sensing \u2013 versatility, replaceability, and data reusability.\nBuilding on the simplistic design of ReSkin, and decoupling\nthe sensing electronics from the sensing interface, AnySkin\nsimplifies integration making it as straightforward as putting\non a phone case and connecting a charger. Furthermore,\nAnySkin is the first uncalibrated tactile-sensor with cross-\ninstance generalizability of learned manipulation policies. To\nsummarize, this work makes three key contributions: first, we\nintroduce a streamlined fabrication process and a design tool\nfor creating an adhesive-free, durable and easily replaceable\nmagnetic tactile sensor; second, we characterize slip detection\nand policy learning with the AnySkin sensor; and third, we\ndemonstrate zero-shot generalization of models trained on one\ninstance of AnySkin to new instances, and compare it with\npopular existing tactile solutions like DIGIT and ReSkin.\nI. I NTRODUCTION\nTouch sensing is widely recognized as a crucial modality\nfor biological movement and control [1], [2]. Unlike vision,\nsound, or proprioception, touch provides sensing at the\npoint of contact, allowing agents to perceive and reason\nabout forces and pressure. However, a closer examination\nof robotics literature reveals a different narrative. Prominent\n1New York University, New York, NY , USA\n2Carnegie Mellon University, Pittsburgh, PA, USA\n3Columbia University, New York, NY , USA\n4Meta AI Research, USA\n\u2217Correspondence to raunaqbhirangi@nyu.eduworks and current state-of-the-art in robot learning primarily\nutilize vision sensing in conjunction with proprioception to\ntrain manipulation skills [3], [4], often ignoring touch. If\ntouch is indeed vital from a biological perspective, why does\nit remain a second-class citizen in sensorimotor control?\nTo address this question, let\u2019s examine what made cam-\neras ubiquitous in robotics. Three key factors are at play:\ncost, convenience, and consistency. Cameras are relatively\ninexpensive (under $20), easy to integrate on a wide variety\nof robot platforms (e.g. multi-view, depth, ego-centric), and\nallow for models trained on them (e.g. object detection,\nsegmentation) to easily transfer to images captured with\nnew cameras. In contrast, touch sensors are often costly\ndue to expensive fabrication processes [5] (e.g., pressure-\nbased sensors) or the need for high-end components [6]\n(e.g., Biotac). They are inconvenient to use on different\nrobot platforms, being custom-built for specific robot end-\neffectors and constrained form factors requiring extensive\nadaptation for different shapes [7], [8]. Finally, touch sensors\nare inconsistent. Due to boutique fabrication, sensor profiles\ncan vary significantly even when produced through the same\nprocess [9], [10]. This inconsistency poses a challenge when\ntransferring tactile-based models across different instances of\nthe same sensor. This transfer is particularly critical for touch\nsensors due to their persistent need for replacement. Soft\nsensing interfaces \u2013 imperative to touch sensors to preserve\ngrasp stability \u2013 wear out faster than hard interfaces, leading\nto more frequent replacements.\nIn this work we present AnySkin, a new touch sensor thatarXiv:2409.08276v1  [cs.RO]  12 Sep 2024is cheap, convenient to use and has consistent response across\ndifferent sensor instances. AnySkin builds on ReSkin [9], a\nmagnetic-field based touch sensor, by improving its fabrica-\ntion, separating the sensing mechanism from the interaction\nsurface, and developing a new self-adhering, self-aligning\nattachment mechanism. This allows AnySkin to (a) have\nstronger magnetic fields, which significantly improves its\nsensor response, (b) be easy to fabricate for arbitrary surface\nshapes, which allows easy use on different end-effectors, (c)\nbe easy to replace the sensor without adversely affecting the\ndata collection process or the efficacy of models trained on\nprevious sensors (Fig. 1).\nWe run a suite of experiments to understand the efficacy of\nAnySkin vis-a-viz other prominent touch sensors. Our main\nfindings can be summarized below:\n1) AnySkin can readily be used on a variety of robots\nincluding xArm, Franka, and the four-fingered Leap\nhand (See fabrication details in Section IV).\n2) AnySkin is compatible with ML techniques for slip\ndetection and visuo-tactile policy learning for precise\ntasks such as inserting USBs (See learning details in\nSection V).\n3) AnySkin takes an average of 12 seconds to replace\nand can be reused after replacement (See replacement\nstudy in Section V-C).\n4) Models trained on one AnySkin transfer zero-shot to\na different AnySkin with only a 13% reduction in\nperformance on a plug insertion task compared to the\n43% drop in performance with ReSkin [9] sensors.\nAnySkin is fully open-sourced. Videos of fabrication,\nattachment, and robot policies are best viewed on our project\nwebsite: https://any-skin.github.io/ .\nII. R ELATED WORK\nA. Tactile sensing\nExisting literature on tactile sensing explores a wide range\nof modalities, each with their own set of advantages and\nlimitations. Capacitative sensors [11], [12] sense contact\nthrough changes in capacitance, offering high sensitivity.\nHowever, they struggle to model shear force and are prone\nto breakage due to direct electrical connections between the\ncircuitry and elastomer. Resistive sensors [5], [13], [14] are\nsimple and durable, but tend to provide spatially discrete\nsensing with low spatial resolution. Optical sensors [8],\n[15], [16] capture high-resolution contact information using\ncameras to track the deformation of an elastomer, but often\npose hard, stringent limits on the sensor form factor, due\nto the physical constraints on the camera field of view.\nThis complicates integration for a wide range of applications\nand significantly increases the effort required to sensorize\nsurfaces of different shapes and sizes.\nMagnetic tactile sensors [17], [18] largely overcome these\nlimitations due to three salient advantages: (a) separation of\nthe sensing electronics from the sensing interface to improve\nrobustness (b) compatibility with different form factors, and\n(c) an ability to capture shear forces [9]. Two prominentclasses of magnetic sensors in robotics right now - ReSkin [9]\nand uSkin [18] - use elastomeric sensing interfaces with mag-\nnetic microparticles and macro-sized magnets respectively. In\nthis work, we build on ReSkin sensors due to their lower cost\nand ease of fabrication.\nB. Replaceability for Tactile Sensors\nRecent developments in rapid prototyping and elastomer\ntechnology have spurred a substantial rise in the number\nof robotic tactile sensors. Most tactile solutions rely on\nsoft sensing interfaces to enable stable conformal contact\nwith objects in the environment. Soft interfaces are prone\nto frequent wear and tear from contact-rich interactions, but\ndiscussions on replaceability for tactile sensors remain few\nand far between. There are two main factors to consider\nwhen evaluating replaceability: (a) the physical ease of\nreplacing the sensory interface, and (b) signal consistency\nwhen replacing a worn out instance with a new instance. The\nformer is far more frequently discussed [6], [16], [9], [19]\nand resolved by simply separating the sensing interface \u2013\ngenerally the damage-prone soft elastomer \u2013 from the sensing\nelectronics which last much longer. The latter, however,\nis much less discussed. Prior work in tactile sensing has\nfound significant variation across different instances of the\nsame tactile sensor [9], [10]. Higher susceptibility to wear\ncombined with a lack of generalizability across instances\nseverely restricts the scale of data explored in most existing\nwork on tactile learning [20], [21]. Signal consistency across\ninstances is central to building scalable and generalizable\ntactile models, and making tactile sensing a ubiquitous\npresence in robot learning. In Section V, we quantitatively\ndemonstrate the improved consistency of AnySkin signal\nover ReSkin, and present a direct replaceability comparison\nwith the DIGIT and ReSkin sensors through policy learning\nexperiments.\nIII. A NYSKIN: COMPONENTS\nAnySkin builds on ReSkin [9], a tactile skin composed\nof a soft magnetized skin coupled with magnetometer-based\nsensing circuitry. By detecting distortions in magnetic fields,\nReSkin measures skin deformations caused by normal and\nshear forces [17], [9]. Its adaptability enables integration\nacross various applications, from robotic hands [20] to arm\nsleeves and even dog shoes. AnySkin uses the same 5-\nmagnetometer circuitry as ReSkin, while introducing key de-\nsign and fabrication changes to the skin to improve durability,\nrepeatability, and replaceability.\n\u2022Magnetizing skins post-curing using a pulse magnetizer\n\u2022Introducing physical separation between magnetic elas-\ntomer and magnetometer circuit\n\u2022Utilizing finer magnetic particles to achieve a more\nuniform particle distribution\n\u2022Implementing a self-aligning design for reduced vari-\nability in the positioning of elastomers and circuitry\nWhile some of these additions have been used in isolation in\nprior work [20], [22], there has been little discussion on their\neffect on sensor response. In this section, we elaborate on theOutlet cavity\nInlet cavity\nAlignment guides\nSmooth-On \nDragonSkin 10 Slow\nMagnequench \nMQFP-15-7 (25\u03bcm)+\nPulse Magnetizer\n AnySkin(a) Fabrication of AnySkin\n (b) Top and bottom surfaces of skins\nFig. 2: (a) AnySkin is made by mixing Smooth-On DragonSkin 10 Slow and MQFP-15-7(25 \u00b5m) magnetic particles in a 1:1:2 ratio, and\ncuring it in the two-part molds shown above. Cured skins are magnetized using a pulse magnetizer. (b) Skins made with MQP-15-7(-80\nmesh) and MQFP-15-7(25 \u00b5m) particles. Note the concentration of particles at the surface of the former due to the larger particle size.\nrationale for each of these choices, followed by a quantitative\ncomparison of the sensor response in Section V-A.\nReSkin uses a grid of cube magnets during curing to\nmagnetize the elastomeric skins. While effective, this ap-\nproach has several drawbacks, such as producing skins with\nrelatively weak magnetic fields. As a result, although the\ndesign of ReSkin separates the sensing electronics and the\nsensing interface, adding physical distance between the skin\nand the sensors significantly weakens the signal, making it\ninfeasible in reality. In contact-rich tasks, where the sensing\nskin undergoes considerable strain, the absence of physical\nseparation leads to stress being transmitted directly to the\nelectronics, ultimately compromising their durability.\nAdditionally, applying a magnetic field during elastomer\ncuring increases variability in the signal response. Before\ncuring, magnetic particles are free to move through the liquid\nelastomer under the effect of the magnetic field. As a result,\nthe distribution of particles is influenced by the temporal\nevolution of the applied magnetic field, ie. how you move\nthe magnets into place, which can be difficult to control\nwhen fabricating. To circumvent these disadvantages, we\npropose using a pulse magnetizer to magnetize the skins\nafter curing in line with [20], [22] as shown in Fig. 2a. The\npulse magnetizer can apply a large enough magnetic field\nto magnetize the dipoles in the magnetic elastomer. Curing\noutside the influence of magnetic fields allows for a more\nuniform distribution of magnetic particles through the bulk\nof the sensor.\nHowever, simply changing the magnetization procedure\nresults in other problems. Curing outside the influence of a\nmagnetic field causes the particles to settle to the bottom\nof the sensing skin due to gravity as shown in Fig. 2b. This\nresults in lower durability as the skin begins to shed magnetic\nparticles during contact-rich interactions. To get around this\nproblem, we replace the magnetic particles with much finer\nparticles (details in Section IV-B). The smaller particles\noperate in a sufficiently low Reynolds number regime to\nallow the elastomer to cure before they can settle on one\nsurface of the elastomer.\nFinally, since ReSkin relies on magnetic field distortions to\nmeasure contact characteristics, sensor response is stronglyinfluenced by the relative positioning of the magnetic skin\nand the magnetometer circuitry (see Section V-A). This\nmeans that loss of adhesion, or peeling, over the life of the\nsensing skin can significantly affect the consistency of the\nsignal. Ideally, we would like the skin to stay adhered until\nit needs to be replaced due to wear and tear. Using screws to\nadhere the skin as suggested in [9] results in poor durability\ndue to a stress concentration at the screw-skin interface,\nespecially in tasks involving shear forces. Using an adhesive\nlike Silpoxy [16], [20] can be used to create sticker-like skins\nthat last relatively longer but still tend to peel under repetitive\ncyclic loading. With AnySkin, we eliminate the need for\nadhesives or fasteners by modifying the design of the skins\nto be self-adhering. Additionally, we also eliminate the need\nto manually align skin and circuitry, which can significantly\naffect signal consistency as demonstrated in Section V-A.\nIV. A NYSKIN: FABRICATION\nThe overall fabrication procedure follows the general\noutline of ReSkin: Magnetic particles and elastomer are\nmixed in specified ratios; the resulting mixture is poured\ninto the molds; cured skins are magnetized. The shape of\nthe fingertip-skin assembly is designed to be triangular as\nshown in Figure 1 to improve reachability. In this section,\nwe elaborate on the details of the fabrication procedure\nfor AnySkin, and key changes to the ReSkin fabrication\nprocedure that result in a new, upgraded sensor.\nA. Mold design\nThe shape of the magnetic skin is dictated by the molds\nused for curing. To create self-adhering skins as outlined in\nSection III, we present a two-part mold design as shown in\nFig. 2a. We choose a skin thickness of 2 mm following [9]\nwith a triangular shape for its advantageous form factor. All\nthe experiments presented in this paper use this triangular\nskin. We also open-source a mold design CAD tool that\ngenerates designs for the fingertip as well as 2-part molds\nfrom just a 2D drawing. Unlike tactile sensors that require\nsignificant engineering for changes in form factor [16], [8],\nAnySkin makes it effortless to diversify your tactile sensor.TABLE I: AnySkin\u2019s signal strength is comparable to ReSkin with a significantly lower variability across skin instances, while being\nphysically separated from the magnetometer electronics. Statistics computed over 5 samples of each type (PM: Pulse magnetizer, FP: finer\nparticles, SA: self-aligning)\nExperiment\nReSkin +PM +FP +SA (AnySkin)\nBxy BzBxy Bz Bxy Bz Bxy Bz\nAverage strength, in \u00b5T 1062 302 1818 5212 1602 5784 283 1265\nNormalized std. deviation across instances 0.54 0.87 0.34 0.12 0.21 0.15 0.12 0.10\nNormalized std. deviation across 1mm misalignments 1.38 1.43 0.25 0.11 0.18 0.07 Self-aligning\nB. Elastomer composition\nFor AnySkin, we mix magnetic microparticles and two-\npart polymer (Dragonskin 10 Slow; Smooth-On) in the\nsame 2:1:1 ratio as ReSkin, while using finer Magnequench\nMQFP-15-7(25 \u00b5m). These particles are about 100x smaller\nthan the MQP-15-7(-80 mesh) used with ReSkin, and do not\nsettle before curing, due to their lower Reynolds number.\nThis ensures that magnetic particles are more evenly dis-\ntributed through the volume of the skin, thereby improving\nconsistency of the signal.\nC. Magnetization\nReSkin is magnetized by sandwiching the magnetic elas-\ntomer mix between a grid of magnets while it is curing. This\nresults in higher variance in distribution of magnetic particles\nwithin the core of the skin based on the exact timing of\nsandwiching the skins. Drawing from D\u2019Manus [20], we use\na pulse magnetizer for magnetizing the skins after curing\nis complete. Separating the magnetizing process from the\ncuring process allows the skins to cure undisturbed and\nmaintain a more uniform distribution of magnetic particles.\nFurthermore, the magnetic field applied by the pulse mag-\nnetizer is far stronger than the sandwich of grid magnets.\nThis results in skins with stronger magnetic fields, which in\nturn enables larger separations between magnetic skin and\nmagnetometer circuitry.\nD. Magnetic elastomer fabrication\nThe final fabrication process follows similar steps as\nthe ReSkin fabrication process. The molds are first aligned\nusing the built-in alignment guides and clicked together. We\nuse plastic clamps to hold the parts together. The two-part\nelastomer compound is then mixed and degassed. This is\nfollowed by the addition of magnetic micro particles and\nanother round of mixing and degassing. Once degassing is\ncomplete, the magnetic elastomer mix is poured through the\nmold inlet as shown in Fig. 2 until it emerges at the outlet,\npausing as necessary to allow the mixture to flow through\nand fill the entire mold. The filled mold is then placed in a\nvacuum chamber and a pressure of 29mm of Hg/in is applied,\nagain pausing as necessary to prevent overflow as the liquid\nbubbles. This pressure is held for 10 minutes before releasing\nthe vacuum. The molds are allowed to rest for 16 hours,before prying them open and trimming excess material to\nreveal the fully cured AnySkin.\nV. E XPERIMENTS AND RESULTS\nIn this section, we perform extensive experiments to\ndemonstrate the capabilities of AnySkin as a tactile sensor,\nand within the context of policy learning. These experiments\nare designed to answer the following questions:\n\u2022How do the fabrication changes outlined in Section III\ninfluence signal characteristics?\n\u2022Can AnySkin sensors be used to detect slip?\n\u2022How does AnySkin\u2019s ease of replaceability compare\nwith other sensors like DIGIT and ReSkin?\n\u2022How does replacing AnySkin affect the performance of\nlearned policies, and how does this compare with other\nsensors like ReSkin and DIGIT?\nA. Comparison between ReSkin and AnySkin signal\nTo quantitatively demonstrate the effect of each of the\nfabrication changes listed in Section III towards improving\nthe consistency of AnySkin, we present the following set of\nexperiments analyzing the raw signal from the four different\nskins shown in Table I, tracking the progression from ReSkin\nto AnySkin:\n1) Effect of pulse magnetizer on signal strength: To un-\nderstand the effect of the pulse magnetizer on signal strength,\nwe take five instances of each skin type and measure the\nraw signal corresponding to each instance. We average the\nabsolute values across the three axes of the five magnetome-\nters, and report the results in Table I. We see a significant\nincrease in the raw magnetic field for both sets of pulse\nmagnetized skins. This increase allows us to add a physical\nseparation between sensing skin and the sensory electronics,\nwhich improves replaceability, as well as repeatability of the\nsignal as discussed below.\n2) Comparison of signal consistency across skins: To\ncompare signal consistency across the sensing skins, we\ncompute the standard deviation along each axis of the five\nmagnetometers across the five instance of each skin type.\nTo account for the larger signal strengths of the pulse\nmagnetized skins and allow for a fairer comparison, we\nnormalize the computed standard deviations by the mean\nabsolute values along xyandzaxes for each skin type.\nAggregated statistics for the different skins are presentedTraining objects Data collection setup\nTest objects\nFig. 3: Experimental setup used for slip detection experiments\nin Table I. Note that finer particles reduce the variability in\nsensory signal, which could be attributed to the more uniform\ndistribution of particles resulting from reduced settling under\nthe influence of gravity.\nFig. 4: Effect of circuit-skin alignment on signal consistency\n3) Effect of alignment on signal consistency: Finally, to\nunderstand the importance of the self-aligning design of\nAnySkin, we take a single skin (20mm \u00d720mm) of each\ntype except AnySkin, and collect magnetometer data from\nplacing it at a 1mm offset along each side as shown in Fig. 4.\nNormalized standard deviations are computed across the\naligned and misaligned variants across each skin, and results\nare reported in Table I. Across different skins, variability\nin sensor response from just misalignment is larger than the\ncross-instance variability seen with AnySkin, underlining the\nimportance of the self-aligning design of AnySkin.\nB. Slip Detection\nWe quantify AnySkin\u2019s ability to detect slip of grasped\nobjects through a controlled slip detection experiment. Our\nexperimental setup consists of a Kinova Jaco arm and a\nOnrobot RG-2 gripper with integrated AnySkin. An object,\nthat is held by a human operator, is grasped and liftedup slowly for 1 second. We use a set of 40 daily objects\nwith varying shapes, weights and materials. We collect 6\ntrajectories for each object by changing the grasping force,\nwidth and location. After the data collection is complete, a\nhuman annotator labels the sequence as slip or no-slip from\nthe corresponding videos. Data from 30 objects is used for\ntraining while 10 objects are used for evaluation. Some of\nthe objects used, along with the data collection setup are\nshown in Fig. 3.\nThe data collection frequency for tactile data is 100 Hz.\nWe subsample the signal by 15 along the temporal axis and\ntake the first difference. We use an LSTM [23] to train our\nslip prediction models. Our model is able to detect slip on\nunseen objects with 92% accuracy.\nC. Ease of replaceability\nWe compare the ease of replaceability of AnySkin against\nthe replaceability of other skins like DIGIT and ReSkin. For\nReSkin, we use two different methods of adhering skins to\nthe circuit board \u2013 screws [9] and adhesive stickers [20]. The\nskins are compared along two axes \u2013 replacement time and\nre-usability after replacement \u2013 through a user study with 10\nnon-expert users, and the results are presented in Table II\nTABLE II: Comparison of replaceability of different sensors\nSensor Time to replace, in s Reusable\nReSkin (adhesive) 82\u00b164 No\nReSkin (screws) 236\u00b164 Yes\nDIGIT 58\u00b122 Yes\nAnySkin 12\u00b15 Yes1\n123\n4\n1\n2\n3\n4\nUSB Insertion\nCard Swiping\nPlug InsertionFig. 5: Experimental setup and set of tasks used for evaluating skin replaceability in policy learning\nWe find that users found it significantly faster and easier\nto replace AnySkin than any of the other skins used for com-\nparison. Furthermore, eliminating adhesion allows replaced\nskins to be reused without needing the extra hours to reapply\nadhesion and allow it to cure.\nD. Replaceability in Policy Learning\nThe most important consequence of the signal consistency\nand replaceability of AnySkin outlined so far, is its ability\nto enable policy generalization across different instances of\nthe skin. In this section, we compare the cross-instance\ngeneralizability of policies trained on DIGIT, ReSkin and\nAnySkin on a plug insertion task that requires considerable\nprecision. We follow this up by demonstrating the cross-\ninstance generalizability of AnySkin across three more pre-\ncise manipulation tasks.\n1) Experimental Setup: For our policy learning experi-\nments, we train behavior cloning models for a set of precise\nmanipulation tasks. Our experimental setup is shown in\nFig. 5 and consists of an X-Arm 7DOF robot in a robot cage\nas shown in the image. The setup consists of four different\ncameras \u2013 three fixed to the cage and one egocentric camera\nattached to the robot\u2019s wrist. We attach an AnySkin sensor\nto one of the fingertips and a plain silicone tip to the other.\nA Meta Quest 3 and the accompanying joystick controller\nare used to teleoperate the robot using Open-Teach [24], an\nopen-source teleoperation framework.We demonstrate the replaceability of AnySkin on a set of\nthree contact-rich manipulation tasks shown in Fig. 5:\n\u2022Plug insertion : The robot starts with a plug grasped in\nthe gripper. The task requires the robot to move to the\nfirst socket on the socket strip and insert the plug. The\nlocation of the socket strip is randomized in a 20 cm \u00d7\n20 cm box on the table, and learned policies are tested\non socket locations unseen in the training data.\n\u2022Card swiping : The robot starts with a credit card\ngrasped in the gripper. It must move to the location\nof a credit card machine on the table and swipe the\ncredit card. The location of the credit card machine\nis randomized in a 40 cm \u00d715 cm box and angled\nin the range (\u221230\u25e6to30\u25e6)on the table, and learned\npolicies are tested on card machine locations unseen in\nthe training data.\n\u2022USB insertion : The robot starts with a USB cable\ngrasped in the gripper. It must move to the location\nof the USB port on the table and insert the cable. The\nlocation of the USB port is randomized in a 20 cm \u00d7\n20 cm box on the table, and learned policies are tested\non port locations unseen in the training data.\n2) Model Architecture and Training: Our policies are\ntrained using behavior cloning. For each task presented in\nthis section, we collect a set of 96 demonstration trajectories,\nwith data from the four cameras in addition to unchanged\ninstances of the respective tactile sensor(s) being used. TheBAKU [25] architecture is used as the policy architecture.\nBAKU tokenizes each input using a modality-specific en-\ncoder: image inputs from cameras and DIGITs are encoded\nusing ResNet-18 [26] encoders, while AnySkin and ReSkin\ninputs are encoded using an MLP encoder. An action token\nis appended to the set of encoded tokens before passing\nthe sequence through a transformer encoder, and the output\ncorresponding to the action token is used to predict actions.\nWe use action chunking [27] and predict the next 10 actions\nat every timestep. For every training setting, we train three\nseparate models corresponding to three different seeds, and\npresent aggregated statistics on 10 policy rollouts.\n3) Evaluating cross-instance generalizability: To investi-\ngate the replaceability of AnySkin in the context of policy\nlearning, we evaluate behavior cloning policies trained using\na single instance of AnySkin on a new instance. Note\nthat we use a different training and test skin for each of\nthe presented tasks to avoid over-indexing on specific skin\ninstances. Table III presents a comparison between policy\nperformance with the original and swapped skins for each of\nthe precise, contact-rich tasks outlined above. Additionally,\nwe train and evaluate a policy that only takes camera images\nas input to serve as a control experiment and verify that\nthe policies indeed rely on tactile data. We find that across\ntasks, performance drops by an average of just 15.6% and\nvisuotactile policies with swapped skins continue to do\nsignificantly better than purely visual policies. This result\ndemonstrates the strength and uniqueness of AnySkin as a\ntactile sensor for contact-rich manipulation.\n4) Comparison across sensors: To better contextualize\nthe significance of this result, we present a replaceability\ncomparison with DIGIT [16] and ReSkin [9] sensors. We\ncollect two additional datasets of 96 demonstration trajecto-\nries each for the plug insertion task with these sensors similar\nto AnySkin. Replaceability is evaluated by swapping skins\nas outlined in the previous section. Success rates from 10\nevaluations across three seeds for each setting are reported\nin Table III.\nTABLE III: Success rates (out of 10) for policies when swapping\nout tactile skins. All statistics computed over 3 training seeds\nTask Cameras only Cameras + Skin\nOriginal skin Swapped skin\nCross-instance generalization\nPlug Insertion 1.7\u00b10.6 6 .7\u00b11.5 5 .3\u00b12.5\nCard Swiping 2.0\u00b11.0 7 .0\u00b11.7 6 .3\u00b10.6\nUSB Insertion 1.7\u00b11.2 5 .7\u00b11.5 3 .0\u00b11.0\nComparison across sensors \u2013 Plug Insertion\nAnySkin 1.7\u00b10.6 6.7\u00b11.5 5 .3\u00b12.5\nReSkin 1.7\u00b11.2 6 .0\u00b11.7 1 .7\u00b11.2\nDIGIT 1.7\u00b11.5 2 .3\u00b10.6 1 .3\u00b10.6\nBased on these results, we find that visuotactile policies\ntrained with ReSkin and AnySkin have similar performance\non solving the plug insertion task. However, when the skin\nis replaced, the performance of the ReSkin policy falls\n43% to the same level as the camera-only policy, while the\nperformance of AnySkin policies only drops by 13%. This\n20406080100120Fig. 6: Difference between two DIGIT instances (left); Maximum\nchange in DIGIT signal across 96 demos (right)\nform of transferability is a significant boost to scaling efforts\nlike training large tactile models and real world deployment\nof models trained in the laboratory.\nAn unexpected result from these experiments was the\npoor success rate of policies trained using the DIGIT sensor\nwhich has been shown to be successful in other visuotactile\ntasks, perhaps on less precise ones, in prior work [10], [28].\nConsequently, we don\u2019t see a significant gap between the\npoor performance of the visuotactile policy on the original\nand swapped DIGIT skins. However, the high variability\nacross instances of the DIGIT sensors is previously docu-\nmented [10], and we find that a closer look at the DIGIT\nsignal from our plug insertion dataset indicates that even if\nit were possible to train more performant policies, they are\nunlikely to generalize across instances. Across the 96 demon-\nstration trajectories from the plug insertion, we compute the\nmaximum change in pixel values across channels and across\ntrajectories, and compare it against the pixel-wise differences\nbetween the original and a swapped instance of the DIGIT\nsensor in Fig. 6. Given that the maximum change in sensory\nmeasurement is comparable, and arguably smaller, than the\ndifference in signal between two instances, it is unlikely\nthat policies trained on one sensor will generalize to new\ninstances.\nVI. C ONCLUSION AND LIMITATIONS\nIn this paper, we present AnySkin, a new magnetic tactile\nsensor. AnySkin is versatile, self-adhering and improves on\nthe repeatability of sensor response across different instances\nof the skin. Furthermore, to the best of our knowledge,\nAnySkin is the first sensor to demonstrate zero-shot gen-\neralization of visuotactile policies to new instances of the\ntactile skin. As we work towards developing more capable\nand performant models utilizing tactile data, AnySkin is the\nfirst step towards ensuring that large datasets of tactile data\ncan be collected and effectively used for training useful,\ngeneralizable models. This work opens the door to a host\nof exciting applications of AnySkin. It could be incorporated\ninto a large-scale data collection tool such as UMI [29] or the\nStick [30], [31]. Future work could also investigate simple\ncalibration schemes or conditional learning frameworks to\ncompletely close the gap between training and swapped\nskin instances. Deeper investigations into standardizing the\nfabrication procedure could also help further improve signal\nconsistency.Despite its potential, AnySkin still inherits some of the\ndrawbacks of the ReSkin sensor, primary among them being\ninterference from magnetic and ferromagnetic objects in\nthe environment. Using machine learning approaches for\nnoise reduction in magnetic sensors or improving the skin\ndesign to incorporate a Faraday cage could help resolve these\ndifficulties, and take tactile sensing one step closer to being\na first-class citizen in robotics.\nACKNOWLEDGMENTS\nSpecial thanks for Krishna Bodduluri, Mike Lambeta, and\nteam from Meta AI Research for providing the DIGIT sen-\nsors for comparison. NYU authors are supported by grants\nfrom Honda, Hyundai, NSF award 2339096 and ONR awards\nN00014-21-1-2758 and N00014-22-1-2773. LP is supported\nby the Packard Fellowship.\nREFERENCES\n[1] J. Jenner and J. Stephens, \u201cCutaneous reflex responses and their central\nnervous pathways studied in man,\u201d The Journal of physiology , vol.\n333, no. 1, pp. 405\u2013419, 1982.\n[2] R. S. Johansson, \u201cSensory control of dexterous manipulation in\nhumans,\u201d in Hand and brain . Elsevier, 1996, pp. 381\u2013414.\n[3] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Ir-\npan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, et al. , \u201cOpen\nx-embodiment: Robotic learning datasets and rt-x models,\u201d arXiv\npreprint arXiv:2310.08864 , 2023.\n[4] C. Chi, S. Feng, Y . Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song,\n\u201cDiffusion policy: Visuomotor policy learning via action diffusion,\u201d\narXiv preprint arXiv:2303.04137 , 2023.\n[5] S. Sundaram, P. Kellnhofer, Y . Li, J.-Y . Zhu, A. Torralba, and W. Ma-\ntusik, \u201cLearning the signatures of the human grasp using a scalable\ntactile glove,\u201d Nature , vol. 569, no. 7758, pp. 698\u2013702, 2019.\n[6] N. Wettels, V . J. Santos, R. S. Johansson, and G. E. Loeb, \u201cBiomimetic\ntactile sensor array,\u201d Advanced robotics , vol. 22, no. 8, pp. 829\u2013849,\n2008.\n[7] S. Wang, Y . She, B. Romero, and E. Adelson, \u201cGelsight wedge:\nMeasuring high-resolution 3d contact geometry with a compact robot\nfinger,\u201d in 2021 IEEE International Conference on Robotics and\nAutomation (ICRA) . IEEE, 2021, pp. 6468\u20136475.\n[8] I. H. Taylor, S. Dong, and A. Rodriguez, \u201cGelslim 3.0: High-resolution\nmeasurement of shape, force and slip in a compact tactile-sensing\nfinger,\u201d in 2022 International Conference on Robotics and Automation\n(ICRA) . IEEE, 2022, pp. 10 781\u201310 787.\n[9] R. Bhirangi, T. Hellebrekers, C. Majidi, and A. Gupta, \u201cReskin:\nversatile, replaceable, lasting tactile skins,\u201d in 5th Annual Conference\non Robot Learning , 2021.\n[10] S. Suresh, H. Qi, T. Wu, T. Fan, L. Pineda, M. Lambeta, J. Malik,\nM. Kalakrishnan, R. Calandra, M. Kaess, et al. , \u201cNeural feels with\nneural fields: Visuo-tactile perception for in-hand manipulation,\u201d arXiv\npreprint arXiv:2312.13469 , 2023.\n[11] O. Glauser, D. Panozzo, O. Hilliges, and O. Sorkine-Hornung, \u201cDe-\nformation capture via soft and stretchable sensor arrays,\u201d ACM Trans-\nactions on Graphics (TOG) , vol. 38, no. 2, pp. 1\u201316, 2019.\n[12] T.-Y . Wu, L. Tan, Y . Zhang, T. Seyed, and X.-D. Yang, \u201cCapacitivo:\nContact-based object recognition on interactive fabrics using capacitive\nsensing,\u201d in Proceedings of the 33rd annual acm symposium on user\ninterface software and technology , 2020, pp. 649\u2013661.\n[13] T. Bhattacharjee, A. Jain, S. Vaish, M. D. Killpack, and C. C. Kemp,\n\u201cTactile sensing over articulated joints with stretchable sensors,\u201d in\n2013 World Haptics Conference (WHC) . IEEE, 2013, pp. 103\u2013108.\n[14] S. Stassi, V . Cauda, G. Canavese, and C. F. Pirri, \u201cFlexible tactile sens-\ning based on piezoresistive composites: A review,\u201d Sensors , vol. 14,\nno. 3, pp. 5296\u20135332, 2014.\n[15] W. Yuan, S. Dong, and E. H. Adelson, \u201cGelsight: High-resolution robot\ntactile sensors for estimating geometry and force,\u201d Sensors , vol. 17,\nno. 12, p. 2762, 2017.[16] M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Maloon, V . R. Most,\nD. Stroud, R. Santos, A. Byagowi, G. Kammerer, et al. , \u201cDigit: A\nnovel design for a low-cost compact high-resolution tactile sensor with\napplication to in-hand manipulation,\u201d IEEE Robotics and Automation\nLetters , vol. 5, no. 3, pp. 3838\u20133845, 2020.\n[17] T. Hellebrekers, O. Kroemer, and C. Majidi, \u201cSoft magnetic skin for\ncontinuous deformation sensing,\u201d Advanced Intelligent Systems , vol. 1,\nno. 4, p. 1900025, 2019.\n[18] T. P. Tomo, M. Regoli, A. Schmitz, L. Natale, H. Kristanto, S. Somlor,\nL. Jamone, G. Metta, and S. Sugano, \u201cA new silicone structure for\nuskin\u2014a soft, distributed, digital 3-axis skin sensor and its integration\non the humanoid robot icub,\u201d IEEE Robotics and Automation Letters ,\nvol. 3, no. 3, pp. 2584\u20132591, 2018.\n[19] \u201cGelsight mini,\u201d https://www.gelsight.com/gelsightmini/, 2023.\n[20] R. Bhirangi, A. DeFranco, J. Adkins, C. Majidi, A. Gupta, T. Helle-\nbrekers, and V . Kumar, \u201cAll the feels: A dexterous hand with large-area\ntactile sensing,\u201d IEEE Robotics and Automation Letters , 2023.\n[21] S. Funabashi, G. Yan, A. Geier, A. Schmitz, T. Ogata, and S. Sug-\nano, \u201cMorphology-specific convolutional neural networks for tactile\nobject recognition with a multi-fingered hand,\u201d in 2019 International\nConference on Robotics and Automation (ICRA) . IEEE, 2019, pp.\n57\u201363.\n[22] R. Bhirangi, C. Wang, V . Pattabiraman, C. Majidi, A. Gupta, T. Helle-\nbrekers, and L. Pinto, \u201cHierarchical state space models for continuous\nsequence-to-sequence modeling,\u201d in Forty-first International Confer-\nence on Machine Learning , 2024.\n[23] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\nComputation , vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[24] A. Iyer, Z. Peng, Y . Dai, I. Guzey, S. Haldar, S. Chintala, and\nL. Pinto, \u201cOpen teach: A versatile teleoperation system for robotic\nmanipulation,\u201d arXiv preprint arXiv:2403.07870 , 2024.\n[25] S. Haldar, Z. Peng, and L. Pinto, \u201cBaku: An efficient transformer for\nmulti-task policy learning,\u201d arXiv preprint arXiv:2406.07539 , 2024.\n[26] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2016, pp. 770\u2013778.\n[27] T. Z. Zhao, V . Kumar, S. Levine, and C. Finn, \u201cLearning fine-grained\nbimanual manipulation with low-cost hardware,\u201d arXiv preprint\narXiv:2304.13705 , 2023.\n[28] H. Qi, B. Yi, S. Suresh, M. Lambeta, Y . Ma, R. Calandra, and J. Malik,\n\u201cGeneral in-hand object rotation with vision and touch,\u201d in Conference\non Robot Learning . PMLR, 2023, pp. 2549\u20132564.\n[29] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake,\nand S. Song, \u201cUniversal manipulation interface: In-the-wild robot\nteaching without in-the-wild robots,\u201d arXiv preprint arXiv:2402.10329 ,\n2024.\n[30] N. M. M. Shafiullah, A. Rai, H. Etukuru, Y . Liu, I. Misra, S. Chin-\ntala, and L. Pinto, \u201cOn bringing robots home,\u201d arXiv preprint\narXiv:2311.16098 , 2023.\n[31] H. Etukuru, N. Naka, Z. Hu, S. Lee, J. Mehu, A. Edsinger, C. Paxton,\nS. Chintala, L. Pinto, and N. M. M. Shafiullah, \u201cRobot utility models:\nGeneral policies for zero-shot deployment in new environments,\u201d arXiv\npreprint arXiv:2311.16098 , 2023."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract:\nThe paper introduces Hand-Object Interaction Pretraining (HOP), a novel framework designed to learn general robot manipulation priors from 3D hand-object interaction trajectories derived from in-the-wild videos. By employing a shared 3D space for hand and object modeling and retargeting human actions to robot actions, the method generates task-agnostic sensorimotor trajectories for robots. Generative modeling of these trajectories results in a manipulation prior, making downstream task adaptation sample-efficient and generalizable when finetuned through reinforcement learning (RL) or behavior cloning (BC).\n\n### 2. Introduction:\nThe paper explores reusable sensorimotor representations to enhance robots' manipulation capabilities, akin to advancements in other AI domains, enabled by large, unsupervised data-driven training. Videos are touted as an abundant source reflecting diverse and socially acceptable behaviors. The paper addresses the challenge of learning from videos by lifting 3D hand-object interactions to generate robot trajectories and constructing a task-agnostic manipulation policy.\n\n**Context**: The need for versatile and robust sensorimotor representations in robotics.\n**Research Questions**:\n- How to efficiently learn manipulation priors from videos?\n- How to translate human actions into robot actions through 3D modeling?\n- Does the learned prior improve sample efficiency and robustness in downstream tasks?\n**Literature Review**:\n- Visual representations traditionally lack motor components, limiting task performance.\n- Focus on 3D estimations and retargeting methods.\n- Comparison to state-of-the-art works extracting sensory and motor information.\n\n### 3. Related Work:\nThe paper situates itself within the context of leveraging videos for robot learning, pretraining visual representations, and 3D reconstruction of human actions.\n\n**Key Advances**:\n- Traditional vision pretraining lacks motor information.\n- Importance of 3D estimation techniques for detailed sensory-motor translations.\n- Use of comprehensive video datasets to capture diverse activities.\n  \n**Gaps Addressed**:\n- The necessity for task-agnostic and generalizable manipulation priors.\n- Bridging the embodiment gap and adding physical plausibility in generated trajectories.\n\n### 4. Method:\nThe proposed method, HOP, entails several critical components:\n\n**3D Estimation Pipeline**:\n- Estimation of 3D hand-object poses from monocular videos using the MCC-HO framework, refined for temporal smoothing and without CAD model dependency for generalization.\n\n**Mapping to Robot Actions**:\n- Formulation as a non-linear optimization problem loss function:\n  \\[\n  \\min_{a[k]} \\left( \\frac{1}{2}\\|\\mathbf{x}_h[k] - \\mathbf{f}(a[k])\\|^2 + \\lambda\\|a[k] - \\varphi[k-1]\\|^2 \\right)\n  \\]\n  where \\( \\mathbf{f} \\) represents robot forward kinematics and \\( \\mathbf{x}_h[k] \\) are hand keypoints.\n\n**Simulator-in-the-Loop Retargeting**:\n- High fidelity simulation ensuring realistic robot actions.\n- Scene diversification by introducing dynamic scene elements, enhancing generalization of joint trajectories.\n\n**Generative Modeling of Trajectories**:\n- Pretraining policy \\(\\pi_b\\) as a causal transformer to capture trajectory distributions by:\n  \\[\n  L(\\tau; \\theta) = \\mathbb{E}_{t}\\left[\\|\\mathbf{a}[t-L:t] - \\pi_b(\\mathbf{o}[t-L:t])\\|_1\\right]\n  \\]\n- Pretraining to build robust manipulation prior capturing object affordances, intuitive physics, etc.\n\n**Downstream Finetuning**:\n- Finetuning involves either reinforcement learning (PPO) or behavioral cloning to customize \\(\\pi_b\\) for specific tasks.\n- Results demonstrate that pretrained policy expedites learning and enhances robustness and generalization.\n\n### 5. Experimental Setup:\n**Robot**: \n- 7-DoF xArm with a 16-DoF Allegro hand.\n- Proprioception from joint positions and visual sensing via Zed-2 stereo camera.\n\n**Simulation Environment**:\n- IsaacGym simulator with pointcloud or depth image visual inputs.\n- Simulated environments comprised of structured and randomized setups.\n\n**Datasets**:\n- DexYCB and 100 Days of Hands datasets used, totaling about 70,000 robot-object interaction trajectories from 450 video sequences.\n\n**Baselines and Pretraining**:\n- Comparison with visual pre-training methods: ImageNet (VIT-B), R3M, VIP, MVP, and Diffusion Policies.\n- Transformers pre-trained using AdamW optimizer, finetuning involves both PPO for reinforcement learning and behavior cloning techniques.\n\n### 6. Results and Analysis:\n**Evaluation Metrics**:\n- Success rates across different manipulation tasks.\n- Sample efficiency in terms of learning time and convergence.\n- Robustness and generalization in grasp perturbation and new object handling.\n\n**Significant Findings**:\n- Superior performance of HOP in multi-object tasks compared to traditional visual pretraining methods, especially visible in the complex \"Grasp & Lift\" task.\n- High efficiency and successful adaptation using HOP, showing robustness to disturbances, outperforming PPO, DAPG, and AMP baselines.\n- Retargeting object interactions from human videos introduces detailed affordances and physics not easily captured through vision-only methods.\n- HOP showcases significant improvement in sample efficiency and robustness, supporting generalization to previously unseen tasks and robust hand-object handling.\n\n### 7. Discussion:\n**Interpretation**:\n- Utility of generative modeling for manipulation prior.\n- Enhancement in fine-tuning efficacy and robustness to physical perturbations attributed to learned priors.\n- Comparison against pretraining methods reaffirms the importance of motor action components.\n\n**Limitations**:\n- Dependency on the quality of hand-object motion capture data.\n- Simplifications in physical parameters like object dynamics may limit generalization.\n\n### 8. Conclusion:\n**Key Contributions**:\n- Introduction of HOP, leveraging human videos for robot manipulation pretraining.\n- Effective task-agnostic manipulation priors improving sample efficiency and robustness.\n- Demonstrated significant benefits in finetuning, robustness to disruptions, and generalization in novel tasks.\n\n**Future Work**:\n- Exploration of more complex, multi-object interactions in pretraining corpora.\n- Advances in 3D reconstruction to include richer contextual and dynamic scenes."}, {"role": "assistant", "content": "Hand-Object Interaction Pretraining from Videos\nHimanshu Gaurav Singh\u2217Antonio Loquercio\u2217Carmelo Sferrazza Jane Wu\nHaozhi Qi Pieter Abbeel Jitendra Malik\nAbstract: We present an approach to learn general robot manipulation priors\nfrom 3D hand-object interaction trajectories. We build a framework to use in-\nthe-wild videos to generate sensorimotor robot trajectories. We do so by lifting\nboth the human hand and the manipulated object in a shared 3D space and retar-\ngeting human motions to robot actions. Generative modeling on this data gives\nus a task-agnostic base policy. This policy captures a general yet flexible manip-\nulation prior. We empirically demonstrate that finetuning this policy, with both\nreinforcement learning (RL) and behavior cloning (BC), enables sample-efficient\nadaptation to downstream tasks and simultaneously improves robustness and gen-\neralizability compared to prior approaches. Qualitative experiments are available\nat: https://hgaurav2k.github.io/hop/.\nKeywords: Learning from videos, dexterous manipulation.\n1 Introduction\nReusable sensorimotor representations have the potential to give robots access to the versatility\nof their sensorimotor apparatus, thereby enabling them to achieve a wide variety of goals. Simi-\nlar to advancements in other AI domains [1, 2], such representations are likely to be trained with\nunsupervised objectives on large datasets. In this work, we study the feasibility of training such\nrepresentations using human videos in the context of dexterous manipulation.\nUsing videos as a data engine comes with several advantages: (1) they are abundant; (2) they cover a\nwide range of skills that we want robots to master; and (3) they reflect natural or socially acceptable\nbehaviors that we want robots to emulate. However, training sensorimotor representations on videos\nis a challenging endeavor. First, videos only partially capture the nature of an agent\u2019s interaction with\ntheir surroundings. For instance, by looking at a person holding an object, it is almost impossible to\nestimate the force their fingers are exerting. In addition, the larger the embodiment gap between a\nhuman and a robot, the more their actions will differ to achieve the same objectives.\nThe difficulty of learning from videos led previous work to mostly focus on specific aspects of the\nproblem. One line of research focused on training visual representations with off-the-shelf self-\nsupervised vision algorithms on large vision datasets [3, 4, 5, 6, 7]. While simple and effective,\nsuch pretrained representations lack a motor component, making them less effective on downstream\ntasks [8]. Another line of work aims to extract both sensory and motor information from videos by\nestimating human motions in 3D [9, 10, 11, 12, 13]. However, these approaches require alignment\nbetween the training videos and the robot\u2019s downstream tasks, which compromises the generality of\nthe learned representations. Finally, recent works aim to use egocentric videos of human activities to\nlearn an explicit hand-object interaction prior in the form of a contact-pose prediction model [14, 15].\nWhile a contact-pose prior is potentially task-agnostic, useful information in hand-object trajectories\nextends beyond contact-poses, including but not limited to pre/post-contact trajectories, intuitive\nphysics of the interaction and human preferences.\n\u2217denotes equal contribution. All authors are affiliated to UC Berkeley.arXiv:2409.08273v1  [cs.RO]  12 Sep 2024Figure 1: Real world rollouts of the policy finetuned from HOP using less than 50 demonstrations. HOP\nenables sample-efficient downstream adaptaion by learning a general manipulation prior from human videos.\nIn this paper, we present an approach to capture a general manipulation prior from in-the-wild\nvideos. Such a prior is implicitly embedded in the weights of a causal transformer, pretrained with\na conditional distribution matching objective on sensorimotor robot trajectories. These trajectories\nare generated by mapping 3D hand-object interactions to the robot\u2019s embodiment via a physically\ngrounded simulator. The choice of an implicit prior, aligned with the current paradigm in vision and\nlanguage research, has the potential advantage of becoming more and more expressive as the quality\nand diversity of the data increases. The resulting prior can be quickly adapted to any manipulation\ntask either with reinforcement learning or behavioral cloning. After adaptation, the prior takes the\nform of an end-to-end policy mapping the robot\u2019s multi-modal sensory stream to low-level joint\ncommands. This policy can be directly executed on a physical robot (see Fig. 1).\nWe empirically study the advantages brought forward by pretraining with hand-object interactions in\nboth simulation and real-world experiments. The findings of this study indicate that our manipula-\ntion prior considerably speeds up skill acquisition compared to previous methods, even if such skills\nare not represented in the training videos. Additionally, it improves generalization and robustness to\ndisturbances in the downstream policy.\n2 Overview\nThe objective of Hand-Object interaction Pretraing (HOP) is to capture general hand-object inter-\naction priors from videos. In contrast to previous work, we do not assume a strict alignment of\nthe human\u2019s intent in the video and the downstream robot tasks. Our key intuition is that the ba-\nsic skills required for manipulation lie on a manifold whose axes are well covered by unstructured\nhuman-object interactions.\nWe extract sensorimotor information from videos by lifting the human hand and the manipulated\nobject in a shared 3D space. We then bring such 3D representations to a physics simulator, where\nwe map human motion to robot actions. There are several advantages to using a simulator as an\nintermediary between videos and robot sensorimotor trajectories: (i) we can add physics, inevitably\nlost in videos, back to the interactions; (ii) it enables the synthesis of large training datasets without\nputting the physical platform in danger; and (iii) we can add diversity to the data by randomizing\nthe simulation environment, e.g., varying the friction between the robot\u2019s joints, the scene\u2019s layout,\nand the object\u2019s location relative to the robot.\n2Figure 2: 3-D hand-object trajectories from in-the-wild human manipulation videos are re-targeted to a robot\nembodiment within a physics simulator, resulting in physically grounded robot data. General manipulation\npriors are learnt from this using generative modelling of trajectories. Such representation enables sample-\nefficient adaptation for new downstream tasks.\nWe generate a dataset of robot-object interactions D={\u03c41, \u03c42, . . . , \u03c4 N}where \u03c4=\n{(o[0],a[0]),(o[1],a[1]), . . . , (o[T],a[T])}are the observation-action pairs of a single sensorimo-\ntor trajectory. An observation o[k]\u2208Oat time k\u2208[0, . . . , T ]consists of visual scene information\n(a depth image or a pointcloud) and robot\u2019s joint angles \u03d5[k],i.e., its proprioception. The action\na[k]\u2208Aconsists of continuous joint angles, which are converted to joint torques with a low-level\nPD controller. We use Dto train a base policy \u03c0bon the unsupervised objective of next-action\nprediction from a history of sensory observations, i.e.,\u02c6a=\u03c0b(o[t:t\u2212L]), where Lis a fixed\ncontext length. We finetune \u03c0bto generate task-specific policies \u03c0teither optimizing a reward with\nreinforcement learning or a behavioral cloning objective on few task-specific demonstrations. The\nnext section presents each aspect of our method in detail.\n3 Method\n3.1 Lifting Hand-Object Interaction Videos to 3D\nRecovering the underlying 3D structure of hand-object interactions from in-the-wild monocular\nvideos is inherently ambiguous. To alleviate such ambiguity, previous work leveraged the insight\nthat the human hand can be used as an anchor for the 3D location and scale of the manipulated ob-\nject [16, 17, 18, 19, 20, 21]. Our setup to estimate hand-object interaction trajectories from videos\nbuilds upon recent advances in 3D vision. Our approach closely follows MCC-HO [16] with a few\nmodifications to adapt it to our use case.\nGiven a single RGB image and an estimate of the 3D hand geometry from HaMeR [22], MCC-\nHO jointly infers hand-object geometry as point clouds. To fine-tune the quality of the prediction,\nMCC-HO finetunes the object\u2019s pose by fitting it to a CAD model. However, this finetuning assumes\nknowledge of the object the human is interacting with. To increase generality, we wave this assump-\ntion and skip the CAD-based post-processing. This simplification comes at the cost of reduced\nreconstruction quality and temporal smoothness. While we find the first problem not critical for pre-\ntraining, we increase temporal smoothness by anchoring object reconstructions to time-smoothed\nhand detections [22]. In addition, we make the simplifying assumption that the camera from which\nthe video is collected is static. More details of our 3D estimation pipeline are provided in the ap-\npendix. The result of this pipeline is a sequence of 3D hand-object poses.\n3.2 Mapping 3D Human-Object Interactions to Robot-Object Interactions\nWe formulate a non-linear optimization problem to generate a sensorimotor trajectory \u03c4from a se-\nquence of 3D hand-object poses. At each step k, we find the action a[k]by optimizing the following\ncost function:\nmin\na[k]1\n2\u2225xh[k]\u2212f(a[k])\u22252+\u03bb\u2225a[k]\u2212\u03d5[k\u22121]\u22252s.t.a[k]\u2208A, (1)\n3where fis the robot\u2019s forward kinematics, and xh[k]are the 3D coordinates of a set of keypoints\non the human hand. The first term of Eq. 1 represents the difference between the robot\u2019s and the\nhuman keypoints as a function of the robot\u2019s desired joints a[k]. The second term is proportional to\nthe energy required to execute the action a[k], which we minimize to favor smoothness.\nWhile there are approaches that include the object dynamics in the optimization [23, 24, 25, 26],\nthey are challenging to apply to in-the-wild videos due to noise in object pose estimates. In addition,\nin-the-wild videos do not have reliable information about objects\u2019 physical properties, e.g., mass or\nfriction. Therefore we disregard the dynamics of the manipulated object and place it on every step at\nthe location observed in the video. While this can lead to physical implausibility in the object motion\nand possibly lack of force-closure grasps, the data quality does not deteriorate much in practice, as\nwe can still learn useful behaviors. We empirically show that without object-trajectories (Section\n5.3), the quality of the base policy decreases.\nThe optimization is performed independently on each timestep k, and the resulting actions a[k]\nare executed in a high-fidelity simulator to generate o[k]. We refer to this method of mapping hu-\nman motion to robot sensorimotor trajectories \u03c4assimulator-in-the-loop retargeting . The primary\nadvantage of this approach is the optimization in (1) can be conducted using a simplified forward\nkinematics f, reducing the computational burden. Despite this simplification, the actions are exe-\ncuted in a high-fidelity simulator, ensuring realistic behaviors and high-quality observations.\nWe randomize the simulated scene to increase data diversity. Specifically, we add obstacles like\ntables and walls to the scene and vary their positions relative to the robot. This allows us to add ran-\ndom constraints to this optimization problem, which increases the overall diversity in the extracted\njoint trajectories. This is particularly important for robots with kinematic redundancies, since they\nhave multiple joint position trajectories for the same end-effector trajectory. Note that this approach\nto retargeting differs from previous methods, disregarding other objects in the scene and optimizing\nactions via physics-based constraints, e.g., minimum jerk [12, 27, 9] or minimum velocity [13].\nThe quality of the resulting robot trajectories decreases as the difference between the environment\nwhere the video was collected, the simulated scene, and fgrows. However, given the non-convex\noptimization landscape of (1), we can obtain good trajectories by running the optimization multiple\ntimes with various initial positions and scene layouts. High-quality data is then obtained by pruning\nthe trajectories on metrics like collision with obstacles and the tracking error between the hand\u2019s and\nthe robot\u2019s keypoints.\n3.3 Robot Trajectory Pretraining\nThe resulting trajectory dataset Tcontains knowledge that could be valuable to any manipulation\ntasks. For instance, Thas information about object affordance, i.e., where and how to grasp; some\nintuitive (although rudimentary) physics, e.g., an object should be reached upon before being lifted;\nor wrist-hand coordination, i.e., the behavior of orienting and shaping the hand simultaneously while\nmoving the wrist to maximize efficiency [28].\nWe aim to incorporate this knowledge as useful behavioral priors into a policy \u03c0bthat can be\nfinetuned to downstream tasks. Similar to previous work in language [2], vision [29], and\nrobotics [8, 30, 31], we instantiate \u03c0bas a transformer [32] and train it on a generative modeling\nobjective. Specifically, we train \u03c0bto capture the conditional distribution \u03a0(a[t\u2212L:t]|o[t\u2212L:t])\nby optimizing the following loss:\nL(\u03c4;\u03b8) =Et\u223c[1...T][\u2225a[t\u2212L:t]\u2212\u03c0b(o[t\u2212L:t])\u22251]. (2)\nHowever, unlike previous work, our pretraining dataset Tcontains neither real-world demonstra-\ntions nor complete task executions. This is because our data is generated from unstructured 3D\nhand-object interactions, and we disregard the dynamics of the manipulated object during retarget-\ning (Sec. 3.2). Yet, we find that the pre-training paradigm in (2) leads to the emergence of useful\nrepresentations in \u03c0b.\n4Downstream Finetuning. The pretrained policy \u03c0bexhibits primitive manipulation skills, e.g.,\nreaching an object with a reasonable grasp pose, while occasionally grasping successfully. We\nfinetune these skills to a task by optimizing a reward with reinforcement learning or a behavior\ncloning loss on limited demonstrations. We finetune the whole model for the task. Empirically, we\nfind that finetuned policies use the information in \u03c0bto train faster, are more robust to disturbances,\nand generalize better than policies trained from scratch and a set of baselines. In addition, we find\nthat the finetuning process re-utilizes the information in \u03c0beven for tasks not explicitly represented\nin the training videos.\n4 Experimental Setup\nRobot. We use a low-cost 7-DoF xArm robot with a 16-DoF Allegro hand [33] vertically mounted\nat its end effector. The proprioception observation \u03d5kincludes joint position from both robots.\nWhile we don\u2019t make any specific assumption about the robot embodiment, we use a multi-fingered\nhand instead of a parallel joint gripper since demonstration quality increases as the embodiment\ngap between the robot and the human decreases. We empirically found that since the robot base is\nfixed, a 7-DoF arm can track much better human motions than a 6-DoF arm, which often encounters\nsingularities during such trajectories. Visual sensing comes from a single stereo camera (Zed-2)\nmounted on the robot\u2019s right side.\nSimulation Setup. Our simulation environment is developed with the IsaacGym [34] simulator.\nThe robot morphology and action space are identical to the real setup. However, since rendering\ndepth images is prohibitively expensive, we give the agent access to the ground-truth object point-\ncould instead of a depth image (see Section 2). Specific details about the task setup and reward\ndesign can be found in the appendix.\nVideo Datasets. Our pretraining dataset of 3D hand-object trajectories consists of sequences from\ntwo datasets: DexYCB [35] and 100 Days of Hands [36]. We use 250 videos from the DeXYCB\ndataset (right-hand only) annotated with ground truth hand-object trajectories as a source of high-\nquality data. We additionally use approximately 200 videos from the 100 Days of Hands dataset.\nSixty percent of these videos were previously annotated with hand-object interaction trajecto-\nries [11], which we directly use. We annotate the remaining videos with our 3D estimation pipeline\n(Sec. 3.1). Overall, our combined dataset contains approximately 450 videos. We retarget these\nvideos to obtain a pretraining dataset Tof approximately 70,000trajectories.\nRetargeting. We use low-storage BFGS [37] from the NLOpt library [38] for optimization. We\nperform simulation-in-the-loop retargeting in a simple simulated scene with a ground floor on which\nthe robot and a static table are placed 65cm apart. Objects start their trajectories above the table with\na random pose. We run the optimization 700times for each video, randomizing the table location\nand the robot\u2019s initial joint state. We add a trajectory to Tonly if, at any time, their retargeting error\n(See Eq. (1)) is below 3cm and the arm does not collide with the table or the floor. Our code is built\nupon the implementation of Qin et al. [39].\nTransformer. Similar to previous work [40], we represent the policy \u03c0bwith a GPT-2-style causal\ntransformer. The policy takes proprioception and observation input from the past 16 timesteps and\npredicts the next action. Details about the architecture can be found in the appendix.\nPretraining. We train the transformer with the objective in Eq. (2) on T. While we could make\nthe prediction autoregressive and add decoding heads and proxy losses for future proprioception and\nimages (as in [13]), we empirically found these changes to be not very helpful in practice to our tasks.\nTherefore, we predict only future actions for simplicity. We use as optimizer AdamW [41] with\ninitial learning rate of 10\u22124and weight decay of 10\u22122. We trained two distinct base policies\u2014one\nwith depth observations and the other with point cloud observations. The former is used for real-\nworld, and the latter for simulation experiments. However, it\u2019s important to note that both policies\nwere trained on exactly the same trajectories; only the associated sensor observations differed.\n5Finetuning. In simulation , we finetune the transformer with PPO [42] using the default hyperpa-\nrameters from [43]. However, we add a few modifications inspired by [44] for effective fine-tuning:\n(1) we use a small initial exploration noise of 0.1; (2) the value and policy networks share the ob-\nservation tokenizer, but the tokenizer\u2019s weights are not updated by the value function\u2019s gradients;\n(3) we warm up the value function\u2019s parameters for the first 200 gradient steps, keeping the actor\nparameters fixed. Since reinforcement learning requires up to 1 billion steps to convergence, and our\nsimulator does not offer fast multi-gpu rendering, we use pointclouds as visual information, as they\ncan be efficiently simulated. In the real world , we finetune the entire \u03c0bon limited demonstrations\nwith the same objective and hyperparameters used for pretraining. In these experiments, we use\ndepth images as input to our policy since pointcloud estimation in the real world generally requires\nmultiple cameras, while our real-world setup has a single camera.\nInference. At test time, the model operates in closed-loop : it receives the past and current obser-\nvations as input and predicts the next action to execute. The prediction loop runs at 20Hz. The\npredicted action is sent to the xArm and Allegro low-level controllers, which operate at 120Hz and\n300Hz, respectively.\n5 Experimental Results\nWe design an experimental procedure to analyze the advantages brought forward by HOP in terms\nof finetuning efficiency, generalization, and robustness to perturbations. Specifically, we ask the\nfollowing questions: (i) How does HOP compare to vision-only pre-training approaches for robot\nlearning? (ii)How does HOP compare to existing demonstration-guided reinforcement learning\nalgorithms [45, 46]? (iii)How does learning from hand-object interaction trajectories compare to\nlearning hand-pose priors only [47, 48, 9]?. We answer these questions via controlled experiments\nin simulation and the physical world.\n5.1 Comparison to visual pre-training baselines (real-world).\nBaselines. We compare our approach to visual pre-training systems. Such systems are trained on\nlarge image or video datasets but lack a motor component. Specifically, we compare to methods\nusing the following pre-training data:\n\u2022ImageNet We encode the depth image with a VIT-B network [49] pre-trained on ImageNet\nand pass the resulting CLS token embeddings to our transformer. The latter is then trained\nwith real-world data. We consider two variants: using the VIT features zero-shot ( Imagenet\nZS) and finetuning them on the downstream dataset ( ImageNet F ).\n\u2022Internet Videos We use off-the-shelf visual features from R3M [7], VIP [3], and MVP [8].\nThese features were obtained with unsupervised contrastive learning objectives on large\nvideo datasets, e.g. Ego4D [50]. Conversely to ours, these baselines don\u2019t use depth but\nRGB images as input.\nWe additionally compare to Diffusion Policies [23] using a UNet backbone since our tasks exhibit\ntemporally smooth desired action sequences. Similarly to ours, this baseline uses depth as input.\nWith the above baselines, we want to understand how classic methods for behavior cloning work in\nour setting, where a single camera and a limited number of demonstrations are available. Indeed,\nthe previous approaches are generally applied with a large number of demonstrations and multiple\nRGB cameras.\nTasks We evaluate our approach in the real world on three tasks of increasing complexity. In the\nfirst task, Grasp and Drop , the robot needs to unstack a cube and put it in a bowl. The second is\ntheGrasp and Pour task, where the robot needs to pick a bottle and point it towards a bowl. In the\nthird task, Grasp and Lift , the robot must pick up one of 4 different-looking objects, all requiring\ndifferent spatial affordances. One single model is trained to pick up all objects. In this task, we\nevaluate the ability of the approach to adapt with a few demonstrations on very different object\n6shapes. We collect 15 demonstrations for the first two tasks and 50 demonstrations for the third task.\nWe encourage the reader to check our project page for visualizing the tasks. More details about the\nscene setup and evaluation criteria can be found in the appendix.\nResults Table 1 summarizes the result of our study. The findings indicate that for tasks with a single\nobject (Grasp & Drop, Grasp & Pour), all methods perform comparably and achieve, with some\nsporadic exceptions, a close-to-perfect success rate. However, in the hardest task (Grasp and Lift),\nwhere a single policy needs to pick four objects with different affordances, our approach has a mar-\ngin of 30 percentage points to ImageNet Finetuned, the best-performing baseline. We additionally\nfind that the baselines using RGB data are much more successful with a single object than with\nmultiple ones. This is likely because there are not enough demonstrations to learn object-specific\naffordances. Overall, these results empirically validate the value of our sensorimotor pretraining\nstrategy.\nDepth-input RGB-input\nTask Ours Diffusion-Policy Imagenet-ZS Imagenet-F R3M VIP MVP\nGrasp & Drop 0.80 0.90 0.90 0.80 0.0 0.40 0.20\nGrasp & Pour 1.0 0.20 0.80 0.70 1.0 1.0 1.0\nGrasp & Lift 0.65 0.30 0.30 0.35 0.0 0.0 0.0\nTable 1: Real-robot results (success rate % averaged over 20 rollouts). Note that RGB baselines are\nnot directly comparable to our approach since our policy takes depth images as input.\n5.2 Comparison to demonstration-guided reinforcement learning strategies (simulation)\nOur simulation experiments investigate the effectiveness of HOP as a base model for adaptation to\ndownstream tasks using RL. The simulation agent is identical in morphology to the real robot.\nBaselines. We compare our approach to three baselines: (1) training from scratch ( PPO ); (2)\ndemonstration-guided reinforcement learning with a proxy imitation objective [45] ( DAPG ); and\n(3) using adversarial objectives to keep the policy close to the demonstrations [46] ( AMP ). DAPG\nis the closest to our work, as it trains on a weighted sum of behavioral cloning and reinforcement\nlearning losses. However, it assumes access to expert demonstrations in the downstream task. Our\npre-training dataset does not fulfill this assumption. Indeed, humans might not behave optimally\naccording to the reward, or the task might not be well represented in the pre-training dataset. Simi-\nlarly to previous work [51], we found that training from scratch is unsuccessful using joint-position\ncontrol as action space, consistently leading the PPO baseline to fail. Therefore, we use the moving-\naverage action space proposed by Petrenko et al. [43] to improve its performance. All baselines use\nthe same environment settings and training strategy, e.g., domain randomization parameters, as our\napproach.\nTasks and Metrics. We evaluate approaches on three tasks. The first requires picking objects and\nplacing them at a specific location ( Grasp and Lift ). The second is to grasp objects and throw them\nin a basket ( Grasp and Throw ). In the final task, the robot is required to open a cabinet. We evaluate\nperformance using success over 256 environments with different objects and report the mean and\nstandard deviation over three seeds per approach. More details can be found in the appendix.\nHOP enables sample-efficient RL and effective exploration In Figure 3, it is demonstrated that\nour approach outperforms all baselines by a large margin, especially when the pretraining corpus\nis not closely related to the task. This is expected because DAPG strongly biases exploration in\nthe neighborhood of the pre-training trajectories, which may potentially be misaligned with the\ndownstream task. Furthermore, we observed that the adversarial training scheme of AMP is unstable\nand does not scale well with the amount of data. Finally, we find that using HOP leads to a 2-5X\nimprovement in sample efficiency compared to training from scratch. Overall, these experiments\n7Figure 3: Comparison of HOP-initialized actor with baselines. HOP improves sample-efficiency of online\nRL across multiple tasks, particularly when the downstream task and the behaviors in the data are less aligned,\nas in Lift & Throw . Runs are averaged across three randomly chosen seeds.\nFigure 4: Evaluating RL finetuning under out-of-distribution scenarios (Left) To test grasp robustness in\nthe task Grasp & Lift , we apply to the grasped objects, forces in random direction equal to their weights. When\ninitialized with HOP, the resulting policy is more than 3\u00d7more robust compared to training PPO from scratch.\n(Right) We evaluate grasp success on multiple objects from the YCB dataset that were not part of the training\nset. When initialized with HOP, the resulting policy is more than 2\u00d7more robust compared to training PPO\nfrom scratch.\nshow that initializing with HOP enables more informed exploration than the baselines and reduces\nvariance in policy gradients.\nHOP learns robust and general behaviors Policies fine-tuned from HOP can potentially bias ex-\nploration toward human-like behavior, leading to more robustness against forces. This is shown in\nFig. 4. Agents trained with our approach perform better when subject to forces than the ones trained\nfrom scratch. In addition, we show in Fig. 4 that our approach generalizes 3x better than the policy\ntrained from scratch. The training objects are different from the testing ones in their mass, aspect\nratio, and relative size with respect to the hand. The performance generally drops whenever the\ntest object is heavy (power drill), too large (cracker box), or too small (marker and scissors) for the\nallegro hand, which is approximately 1.5X larger than a human hand. Note that in these experiments\nwe train the scratch policy with two billion samples.\nAffordances Training an RL policy from scratch for a dexterous hand often leads to grasping poses\nthat are unlike general human affordances. Online RL exploration near a learned human-object\ninteraction prior biases the optimization landscape to favor human-like affordances. As shown in\nFigure 5, we find that for the Grasp and Lift task, our policy grasps objects with more stable and\nhuman-like affordances than PPO training from scratch.\n8Figure 5: Online exploration around the learnt prior from humans leads to grasps with more human-like and\nstable affordances compared to training PPO from scratch.\nFigure 6: Pre-training only a hand-motion prior leads to decrease in robustness of grasps to force disturbances\n(left). With our approach, the pre-trained policy learns a prior on object affordances which leads to more robust\ngrasps. In addition, pretraining with object poses leads to a more flexible prior and better finetuning to tasks\nless aligned with the pre-training data (right).\n5.3 Comparison to learning a hand-only motion prior (simulation)\nPrior work has shown the benefits of learning a prior on hand motions from videos of human ac-\ntivities [47, 48, 9]. This section aims to understand the benefits of learning a prior on the object\nand the hand jointly . We hypothesize that learning from hand-object interactions gives the base\nmodel useful information beyond eigen-grasps (which are captured by a hand-only motion prior),\nlike, for instance, pre- and post-contact trajectories, intuitive physics of the interaction, and human\npreferences.\nWe evaluate this hypothesis by training a base policy on our pre-training corpus using masked object\nobservations. This encourages the base policy to primarily learn a hand motion prior. As illustrated\nin Figure 6 (left), we observe that such a pre-trained policy exhibits reduced robustness to grasp\ndisturbances. Furthermore, we find that the hand-only prior is insufficient for learning an effective\npolicy in the Grasp and Throw task (Fig. 6, right). Since this task is underrepresented in the pre-\ntraining corpus, the hand motions required are unlikely to be adequately captured by a hand-only\nprior. In contrast, learning a joint hand-object prior provides the model with a more comprehensive\nunderstanding of manipulation, enabling quicker adaptation to this downstream task.\n6 Related Work\nLearning Policies from Human Videos. In-the-wild videos hold the promise of solving the data\nproblem in robotics. One of the pioneering efforts in this direction is by Yang et al. [52], where\nvideo data was used to generate action plans. Several works followed up on this idea, relying on pre-\ndefined action primitives [53, 54, 55, 56, 57, 58]. However, waving the requirement for pre-defined\n9primitives is challenging since in-the-wild videos lack motor information. One way to recover motor\ninformation from videos is training with a trajectory-matching objective [59, 60, 61, 62], possibly\nusing intermediate representations like object segmentation or optical flow [63, 64, 65]. However,\nthis approach requires collecting task- and environment-specific videos where humans and robots\noperate in the same workspace. Therefore, the trajectory-matching formulation largely constrains\nthe number of videos that can be used for training. To overcome these constraints, researchers have\nfocused on either learning exclusively visual representations from videos or extracting 3D human\nposes and mapping them to robot actions. In the following, we cover these works in detail.\nVisual Representation Learning for Robotics. Inspired by successes in computer vision [1] and\nnatural language processing [66], the robot learning community has recently focused on pretraining\nrepresentations on large video datasets like Ego4D [50] and fine-tuning these representations on\ndownstream tasks [4, 7, 3, 5]. However, being the training objective based exclusively on image\nreconstruction, the representations focus primarily on low-level vision features, e.g., shapes or edges.\nThis gives them limited benefits compared to representations trained on standard vision datasets [6].\nOverall, these works focus on visual generalization, e.g., picking up two objects with the same shape\nbut different colors. However, they have not yet demonstrated action generalization, where motor\nskills are adapted to accomplish novel objectives.\nActions from Videos via 3D. One common approach to extracting action information from videos\nis using 3D as an intermediate representation. If the embodiment gap is small, human motions can\nbe mapped to robot actions via inverse kinematics. This is particularly effective when the videos\nare task-specific, i.e., when the robot aims to mimic the human motion [10, 12, 67, 68, 69, 70, 11].\nInstead of learning specific skills, other works focus on learning a re-usable sensorimotor prior from\nvideos. However, this prior only captures human actions [47, 48, 9], disregarding the trajectory of\nthe manipulated object. Conversely, our work aims to use 3D hand-object interactions from in-the-\nwild videos to learn a re-usable prior for object manipulation.\nDexterous Manipulation. Dexterous manipulation has been studied for decades [71, 72, 73, 74,\n75]. In recently years, learning-based approaches make significant progress [76, 77]. They can\nbe generally categorized to learning in simulation and then transferring to the real world (Sim-to-\nReal) [78, 79, 80, 81, 82, 83], and learning in the real-world [39, 84, 85, 86, 87]. Qin et al. [12] uses\nhand-object trajectories but only use it to collect demonstrations. Xu et al. [88] also does functional\ngrasp generation, but the results are limited in simulation. Most of the aforementioned work learn\npolicies from scratch and does not use any internet data as prior. Our approach studies learning\nhand-object interaction prior from human videos and is effective both in simulation and real-world.\n7 Conclusion and Limitations\nThis work presents an approach to learning general yet flexible manipulation priors for robot poli-\ncies from human videos. While our approach demonstrates a way to pre-train on a single object\ninteraction, this can, in practice, be limiting. Indeed, human behavior in a video can potentially be\nconditioned on information encompassing multiple objects in the current and previous scenes. This\nleads to a loss of signal that could be extracted from the raw video. We predict that advances in 3-D\nreconstruction will enable us to use a more complex scene reconstruction and pretraining.\nAcknowledgments\nThis work was supported by the DARPA Machine Common Sense program, the DARPA Transfer\nfrom Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) program, and by the\nONR MURI award N00014-21-1-2801. This work was also funded by ONR MURI N00014-22-1-\n2773. We thank Adhithya Iyer for assistance with teleoperation systems, Phillip Wu for setting-up\nthe real robot, and Raven Huang, Jathushan Rajasegaran and Yutong Bai for helpful discussions.\n10References\n[1] K. He, X. Chen, S. Xie, Y . Li, P. Doll \u00b4ar, and R. Girshick. Masked autoencoders are scalable\nvision learners. In CVPR , 2022.\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog , 2019.\n[3] Y . J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V . Kumar, and A. Zhang. Vip: Towards\nuniversal visual reward and representation via value-implicit pre-training. In ICLR , 2023.\n[4] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot\nlearning with masked visual pre-training. CoRL , 2022.\n[5] A. Majumdar, K. Yadav, S. Arnaud, J. Ma, C. Chen, S. Silwal, A. Jain, V .-P. Berges, T. Wu,\nJ. Vakil, P. Abbeel, J. Malik, D. Batra, Y . Lin, O. Maksymets, A. Rajeswaran, and F. Meier.\nWhere are we in the search for an artificial visual cortex for embodied intelligence? NeurIPS ,\n2023.\n[6] S. Dasari, M. K. Srirama, U. Jain, and A. Gupta. An unbiased look at datasets for visuo-motor\npre-training. In CoRL , 2023.\n[7] S. Nair, A. Rajeswaran, V . Kumar, C. Finn, and A. Gupta. R3m: A universal visual represen-\ntation for robot manipulation. In CoRL , 2022.\n[8] I. Radosavovic, B. Shi, L. Fu, K. Goldberg, T. Darrell, and J. Malik. Robot learning with\nsensorimotor pre-training. In CoRL , 2023.\n[9] K. Shaw, S. Bahl, and D. Pathak. Videodex: Learning dexterity from internet videos. In CoRL ,\n2022.\n[10] X. B. Peng, A. Kanazawa, J. Malik, P. Abbeel, and S. Levine. Sfv: Reinforcement learning of\nphysical skills from videos. Transactions On Graphics , 2018.\n[11] A. Patel, A. Wang, I. Radosavovic, and J. Malik. Learning to imitate object interactions from\ninternet videos. arXiv:2211.13225 , 2022.\n[12] Y . Qin, Y .-H. Wu, S. Liu, H. Jiang, R. Yang, Y . Fu, and X. Wang. Dexmv: Imitation learning\nfor dexterous manipulation from human videos. In ECCV , 2022.\n[13] I. Radosavovic, B. Zhang, B. Shi, J. Rajasegaran, S. Kamat, T. Darrell, K. Sreenath, and J. Ma-\nlik. Humanoid locomotion as next token prediction. arXiv:2402.19469 , 2024.\n[14] A. Kannan, K. Shaw, S. Bahl, P. Mannam, and D. Pathak. Deft: Dexterous fine-tuning for\nreal-world hand policies. CoRL , 2023.\n[15] P. Mandikal and K. Grauman. Dexvip: Learning dexterous grasping with human hand pose\npriors from video. In Conference on Robot Learning , pages 651\u2013661, 2022.\n[16] J. Wu, G. Pavlakos, G. Gkioxari, and J. Malik. Reconstructing hand-held objects in 3d.\narXiv:2404.06507 , 2024.\n[17] H. Choi, N. Chavan-Dafle, J. Yuan, V . Isler, and H. Park. Handnerf: Learning to reconstruct\nhand-object interaction scene from a single rgb image. arXiv:2309.07891 , 2023.\n[18] Y . Ye, A. Gupta, and S. Tulsiani. What\u2019s in your hands? 3d reconstruction of generic objects\nin hands. In CVPR , 2022.\n[19] B. Tekin, F. Bogo, and M. Pollefeys. H+O: Unified egocentric recognition of 3d hand-object\nposes and interactions. In CVPR , 2019.\n11[20] Y . Hasson, G. Varol, D. Tzionas, I. Kalevatykh, M. J. Black, I. Laptev, and C. Schmid. Learning\njoint reconstruction of hands and manipulated objects. In CVPR , 2019.\n[21] Z. Cao, I. Radosavovic, A. Kanazawa, and J. Malik. Reconstructing hand-object interactions\nin the wild. In ICCV , 2021.\n[22] G. Pavlakos, D. Shan, I. Radosavovic, A. Kanazawa, D. Fouhey, and J. Malik. Reconstructing\nhands in 3D with transformers. In CVPR , 2024.\n[23] X. Zhu, J. Ke, Z. Xu, Z. Sun, B. Bai, J. Lv, Q. Liu, Y . Zeng, Q. Ye, C. Lu, M. Tomizuka, and\nL. Shao. Diff-lfd: Contact-aware model-based learning from visual demonstration for robotic\nmanipulation via differentiable physics-based simulation and rendering. In CoRL , 2023.\n[24] A. S. Lakshmipathy, N. Feng, Y . X. Lee, M. Mahler, and N. Pollard. Contact edit: Artist tools\nfor intuitive modeling of hand-object interactions. Transactions on Graphics , 2023.\n[25] A. S. Lakshmipathy, J. K. Hodgins, and N. S. Pollard. Kinematic motion retargeting for\ncontact-rich anthropomorphic manipulations. arXiv:2402.04820 , 2024.\n[26] Y . Kim, H. Park, S. Bang, and S.-H. Lee. Retargeting human-object interaction to virtual\navatars. Transactions on Visualization and Computer Graphics , 2016.\n[27] A. Sivakumar, K. Shaw, and D. Pathak. Robotic telekinesis: Learning a robotic hand imitator\nby watching humans on youtube. In RSS, 2022.\n[28] L. A. Jones and S. J. Lederman. Human hand function . Oxford university press, 2006.\n[29] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever. Generative pretraining\nfrom pixels. In ICML , 2020.\n[30] A. Kumar, A. Singh, F. Ebert, M. Nakamoto, Y . Yang, C. Finn, and S. Levine. Pre-training for\nrobots: Offline rl enables learning new tasks from a handful of trials. In RSS, 2023.\n[31] O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna,\nT. Kreiman, C. Xu, J. Luo, Y . L. Tan, L. Y . Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh,\nC. Finn, and S. Levine. Octo: An open-source generalist robot policy. In RSS, 2024.\n[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In NeurIPS , 2017.\n[33] WonikRobotics. Allegrohand. https://www.wonikrobotics.com/ , 2013.\n[34] V . Makoviychuk, L. Wawrzyniak, Y . Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin,\nA. Allshire, A. Handa, and G. State. Isaac gym: High performance gpu-based physics simula-\ntion for robot learning. In NeurIPS Datasets and Benchmarks , 2021.\n[35] Y .-W. Chao, W. Yang, Y . Xiang, P. Molchanov, A. Handa, J. Tremblay, Y . S. Narang, K. V .\nWyk, U. Iqbal, S. Birchfield, J. Kautz, and D. Fox. Dexycb: A benchmark for capturing hand\ngrasping of objects. CVPR , 2021.\n[36] D. Shan, J. Geng, M. Shu, and D. F. Fouhey. Understanding human hands in contact at internet\nscale. In CVPR , 2020.\n[37] D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization.\nMathematical programming , 1989.\n[38] S. G. Johnson. The NLopt nonlinear-optimization package. https://github.com/\nstevengj/nlopt , 2007.\n[39] Y . Qin, H. Su, and X. Wang. From one hand to multiple hands: Imitation learning for dexterous\nmanipulation from single-camera teleoperation. RA-L , 2022.\n12[40] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and\nI. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. NeurIPS ,\n2021.\n[41] I. Loshchilov and F. Hutter."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary of \u201cClick2Mask: Local Editing with Dynamic Mask Generation\u201d\n\n---\n\n#### 1. Abstract:\nThe paper introduces Click2Mask, a novel approach for local image editing that only requires a single point of reference and a content description to dynamically create and evolve masks during editing. Click2Mask leverages a Blended Latent Diffusion (BLD) process enhanced by a masked CLIP-based semantic loss, addressing the limitations of needing precise masks or detailed location descriptions. This method simplifies user interaction, minimizes effort, and outperforms existing state-of-the-art (SoTA) methods in terms of ease of use and contextual accuracy.\n\n---\n\n#### 2. Introduction:\n**Context:**\n- Recent advancements in generative models elevated image generation and editing, making these techniques accessible to non-experts.\n- Focus of this paper: local editing specifically for adding new content within an image.\n\n**Research Questions:**\n- How to simplify the interaction required for local editing tasks?\n- How to dynamically generate contextually accurate masks?\n\n**Positioning:**\n- Existing methods either require precise masks or detailed natural language descriptions, which can be cumbersome and error-prone.\n- This study proposes a single-click solution that avoids these complications.\n\n---\n\n#### 3. Related Work:\n**Mask-based Approaches:**\n- Requires masks to specify editable regions, like Blended Diffusion and Blended Latent Diffusion. These methods face challenges like tedious mask creation and lack of mask precision.\n\n**Mask-free Approaches:**\n- Employ various methods to describe editing areas without explicit masks, using advanced generative models but still needing expertise in prompt crafting.\n\n**Instruction-based Approaches:**\n- Use textual descriptions for both localization and manipulation but suffer from ambiguities and lack of precision.\n\n**Segmentation-based Approaches:**\n- Utilize pre-existing segments to generate masks, but they are restricted by the need to \"lock\" on existing objects and seldom support adding new objects.\n\n**Advancement Over Prior Work:**\n- Click2Mask provides a single-click solution, dynamically generating masks that evolve based on the content and structure of the original image, guided by text prompts without needing precise masks or detailed descriptions.\n\n---\n\n#### 4. Method:\n**Overview:**\n- Local editing without user-generated masks by dynamically creating and evolving masks around a single-click using a Blended Latent Diffusion (BLD) process enhanced with Alpha-CLIP loss.\n\n**Key Techniques:**\n- **Blended Latent Diffusion (BLD):**\n  - Blends source image latents with prompt-guided latents during denoising steps.\n  - Employs variational auto-encoder (VAE) for latent space transformations.\n  \n- **Dynamic Mask Evolution:**\n  - Starts with a large mask around the clicked point, shrinks dynamically while preserving significant areas based on a semantic alignment loss.\n\n**Algorithm (Click2Mask):**\n1. **Initial Setup:**\n   - Encode input image to latent space.\n   - Initialize a Gaussian potential height-field around the clicked point.\n   \n2. **Blended Latent Diffusion (BLD) Integration:**\n   - At each diffusion step:\n     - Perform denoising conditioned by text prompt to generate foreground latent.\n     - Noise the original latent to generate background latent.\n     - Blend using the current mask.\n   \n3. **Mask Evolution:**\n   - Update mask dynamically using gradients of semantic alignment loss obtained via Alpha-CLIP.\n   - Adjust potential height-field based on these gradients.\n   - Continue updating until mask converges to a contextually significant shape.\n\n4. **Final Blending:**\n   - Perform a final BLD run using the evolved mask to produce the final edited image, applying Gaussian feathering for blending.\n\n**Technical Formulations:**\n- **Mask Blending Equation:**\n  \\( z_t = z_{fg} \\odot m_{latent} + z_{bg} \\odot (1 - m_{latent}) \\)\n- **Mask Update:**\n  Calculate Alpha-CLIP loss and update mask accordingly.\n\n---\n\n#### 5. Experimental Setup:\n**Datasets:**\n- Emu Edit Benchmark filtered for \"addition\" instructions.\n- Randomly sampled evaluation subset of 100 samples from the benchmark.\n\n**Hardware/Software:**\n- Experiments conducted on high-performance GPUs with language support from Stable Diffusion, Alpha-CLIP, and other necessary packages implemented in frameworks like PyTorch.\n\n**Evaluation Metrics:**\n- Human evaluation via user studies.\n- Automatic metrics including CLIP directional similarity, CLIP output similarity, Alpha-CLIP edit similarity, and pixel-level L1 distance.\n\n**Baselines:**\n- Comparison with Emu Edit, MagicBrush, and other state-of-the-art models like InstructPix2Pix.\n\n---\n\n#### 6. Results and Analysis:\n**Empirical Findings:**\n- **Human Evaluation:**\n  - Click2Mask outperformed Emu Edit and MagicBrush in user rankings on majority criteria.\n  - Majority of users favored Click2Mask for realism, lack of artifacts, and accurate execution of instructions.\n\n**Metrics Results:**\n- **CLIP Direct Similarity:** Click2Mask showed superior performance in correlating changes in image and caption.\n- **CLIP Out Similarity:** Higher scores indicating better compliance with desired outputs.\n- **Alpha-CLIP Edit:** Higher similarity in edited regions.\n- **L1 Distance:** Lower pixel distance indicating better preservation of input image details.\n\n**Figures and Tables:**\n- Figures illustrate qualitative performance comparison.\n- Tables summarize human evaluation results and automatic metrics.\n\n**Ablation Studies:**\n- Demonstrated the necessity of large initial mask regions and the dynamic evolution process for capturing desired edits effectively.\n\n---\n\n#### 7. Discussion:\n**Interpretation:**\n- Click2Mask effectively minimizes user effort while providing contextually accurate and realistic edits.\n- The dynamically evolving mask aligns well with the desired content and the semantics of the image.\n\n**Implications:**\n- Easier and more intuitive local image editing for non-experts.\n- Potential for integration into other image editing frameworks.\n\n**Limitations:**\n- Difficulty in fine detailing (e.g., small objects).\n- Challenges in spatially consistent edits when similar objects are near the target area.\n\n---\n\n#### 8. Conclusion:\n**Key Contributions:**\n- Introduction of a single-point guided, dynamic mask generation technique for local image editing.\n- Simplification of user interaction and enhancement of edit precision.\n\n**Future Research Directions:**\n- Refining mask evolution for better handling of small and detailed objects.\n- Exploring additional applications of dynamic masks in broader image manipulation tasks.\n\n---\n\nThis summary captures the essential details for replicating the research and understanding its methodology, empirical findings, strengths, and limitations. It offers a clear path for implementing Click2Mask or building upon this approach in future studies."}, {"role": "assistant", "content": "Click2Mask: Local Editing with Dynamic Mask Generation\nOmer Regev, Omri Avrahami, Dani Lischinski\nThe Hebrew University of Jerusalem\nAbstract\nRecent advancements in generative models have revolution-\nized image generation and editing, making these tasks acces-\nsible to non-experts. This paper focuses on local image edit-\ning, particularly the task of adding new content to a loosely\nspecified area. Existing methods often require a precise mask\nor a detailed description of the location, which can be cum-\nbersome and prone to errors. We propose Click2Mask, a\nnovel approach that simplifies the local editing process by\nrequiring only a single point of reference (in addition to the\ncontent description). A mask is dynamically grown around\nthis point during a Blended Latent Diffusion (BLD) process,\nguided by a masked CLIP-based semantic loss. Click2Mask\nsurpasses the limitations of segmentation-based and fine-\ntuning dependent methods, offering a more user-friendly and\ncontextually accurate solution. Our experiments demonstrate\nthat Click2Mask not only minimizes user effort but also de-\nlivers competitive or superior local image manipulation re-\nsults compared to SoTA methods, according to both human\njudgement and automatic metrics. Key contributions include\nthe simplification of user input, the ability to freely add ob-\njects unconstrained by existing segments, and the integration\npotential of our dynamic mask approach within other editing\nmethods.\n1 Introduction\nRecent advances in generative models have revolutionized\nimage generation and editing capabilities, enabling both\nstreamlined workflows and accessibility for non-experts.\nThe latest approaches utilize natural language to manipulate\nimages either globally \u2013 altering the content or style of the\nentire image \u2013 or locally \u2013 adding, removing, or modifying\nspecific objects within a limited image region.\nIn this work, we focus on local editing, specifically on\nthe task of adding new content in a local area. To accom-\nplish such edits, some existing methods require users to pro-\nvide explicit precise masks (Avrahami, Lischinski, and Fried\n2022; Ramesh et al. 2022; Avrahami, Fried, and Lischinski\n2023; Wang et al. 2023b; Xie et al. 2022), which is tedious\nand may yield unexpected results due to lack of mask pre-\ncision. Other methods describe the desired manipulations in\nnatural language, as an edit instruction (Brooks, Holynski,\nProject page: https://omeregev.github.io/click2maskInput Emu Edit MagicBrush DALL \u00b7E3 Click2Mask\n\u201cAdd a gorilla in the background looking at the bananas\u201d\n\u201cA gorilla looking at the bananas\u201d\n\u201cAdd a pack of rocks to the back of the truck\u201d\n\u201cA pack of rocks\u201d\nFigure 1: Comparisons to SoTA models. A comparison of\nEmu Edit (Sheynin et al. 2023), MagicBrush (Zhang et al.\n2023) and DALL \u00b7E 3 (Betker et al. 2023) with our model\nClick2Mask . In each example, the top prompt was given\nto the other models, while Click2Mask received the simpler\nbottom prompt, in addition to the blue dot (mouse click) on\nthe input. Other models completely change the image, or the\nbackground, fail to edit, or produce unrealistic results.\nand Efros 2023; Sheynin et al. 2023), or by providing a cap-\ntion and the desired change (Bar-Tal et al. 2022; Kawar et al.\n2023; Hertz et al. 2022; Tumanyan et al. 2022). These meth-\nods also require user expertise, and their results may suffer\nfrom ambiguous or imprecise prompts. Moreover, they fail\nto ensure that the changes to the image are confined to a local\narea, or that they occur at all, as demonstrated in Figure 1.\nTo overcome the aforementioned shortcomings, we intro-\nduce Click2Mask, a novel approach that simplifies user in-\nteraction by requiring only a single point of reference rather\nthan a detailed mask or a description of the target area.\nThe provided point gives rise to a mask that dynamically\nevolves through a Blended Latent Diffusion (BLD) process\n(Avrahami, Lischinski, and Fried 2022; Avrahami, Fried,\nand Lischinski 2023), where the evolution is guided by a\nsemantic loss based on Alpha-CLIP (Sun et al. 2023). This\nprocess (Figure 2) enables local edits that are both precise\nand contextually relevant (Figures 1 and 3).\nUnlike segmentation-based methods that depend on pre-\nexisting objects (Couairon et al. 2022; Xie et al. 2023; Wang\net al. 2023a; Zou et al. 2024), Click2Mask does not confinearXiv:2409.08272v1  [cs.CV]  12 Sep 2024\u201cA giraffe\u201d\n\u201cSnowy\nmountains\u201d\nPrompt Input 30% 37% 44% 45% 46% 47% 48% (final Mt) Output\nFigure 2: Mask evolution . A visualization of the mask evolution throughout the diffusion process. Leftmost image is input with\nclicked point, rightmost image is the final Click2Mask output. Intermediate images are decoded latents \u02dczfgat several diffusion\nsteps, where the purple outline depicts the contour of current (upscaled) mask Mt. Percentages indicate the step out of 100\ndiffusion steps, with the last being the final evolved mask.\nthe edit area to the boundaries of an existing segment. Fur-\nthermore, in contrast to editing approaches that require fine-\ntuning the diffusion model (Wang et al. 2023b; Xie et al.\n2022; Kawar et al. 2023; Avrahami et al. 2023), we employ\npre-trained models, and only perform context dependent op-\ntimization on the mask.\nOur experiments demonstrate that Click2Mask not only\nreduces the effort required by users but also achieves com-\npetitive or superior results compared to state-of-the-art\nmethods in local image manipulation.\nIn summary, our contributions are: (i) Reduction of user\neffort by eliminating the need for precise mask outlines,\nor overly descriptive prompts. (ii) Ability to add objects\nin a free-form manner, unconstrained by boundaries of ex-\nisting objects or segments. (iii) Our dynamically evolving\nmask approach is not a stand-alone method, but rather it can\nbe embedded as a mask generation of the fine-tuning step\nwithin other methods that internally employ a mask, such as\nEmu Edit (Sheynin et al. 2023) which currently generates\nmultiple masks (a precise mask using DINO (Caron et al.\n2021) and SAM (Kirillov et al. 2023), an expanded version\nof it, and a bounding box), and filters the best result from\nmultiple images produced using these masks.\n2 Related Work\nIn recent years, much work has been done on image gen-\neration, with diffusion-based models (DMs) (Ho, Jain, and\nAbbeel 2020; Song, Meng, and Ermon 2020; Dhariwal and\nNichol 2021; Rombach et al. 2022; Ramesh et al. 2022; Sa-\nharia et al. 2022) facilitating a host of SoTA text-guided im-\nage editing methods and capabilities.\nMask-based approaches. Text-guided image manipu-\nlation may naturally be limited to a specific region us-\ning a mask. In the context of DMs this was first ex-\nplored in Blended Diffusion (Avrahami, Lischinski, and\nFried 2022), where a user-provided mask is used to blend\nimages throughout a denoising process with a text-guided\nnoisy image. This approach was later incorporated into La-\ntent Diffusion (Rombach et al. 2022) by performing the\nblending in latent space. The resulting Blended Latent Diffu-\nsion (BLD) method (Avrahami, Fried, and Lischinski 2023)serves as the basis for our work and described in more de-\ntail in Section 3. GLIDE (Nichol et al. 2022), Imagen Ed-\nitor (Wang et al. 2023b) and SmartBrush (Xie et al. 2022)\nfine-tuned the DM for image inpainting, by obscured train-\ning images or by conditioning on a mask. However, user-\nprovided masks have a major disadvantage: the success of\nthe edit depends on the exact shape of the mask, which can\nbe tedious and time-consuming for a user to create.\nMask-free approaches. Both Text2Live (Bar-Tal et al.\n2022), which generates a composite layer, and Imagic\n(Kawar et al. 2023), which interpolates target text and op-\ntimized source embeddings, fine-tune the generative model\nfor each image, which is quite costly, contrary to our work.\nSeveral works use attention injection, such as Plug-and-Play\n(Tumanyan et al. 2022) and Prompt-to-Prompt (Hertz et al.\n2022), where the latter requires a time-consuming caption of\nthe input image, unlike our method. Most of these methods\nfocus on altering a certain object (by replacement, removal\nor style change), or applying global changes (style or con-\ntent), in contrast to our focus on adding objects freely.\nInstruction-based approaches. Other methods can add\nobjects in a free manner. InstructPix2Pix (Brooks, Holyn-\nski, and Efros 2023) (subsequently fine-tuned by Mag-\nicBrush (Zhang et al. 2023)) produces (instruction, image)\npairs, used to train an instruction-conditioned DM. Emu Edit\n(Sheynin et al. 2023) is a more recent model trained on a\nwide range of learned task embeddings to enable instruction-\nbased image editing, however, it is not publicly available.\nDALL\u00b7E3 (Betker et al. 2023) is also proprietary, and modi-\nfies the entire image as demonstrated in Figure 1. DALL \u00b7E3\nand DALL \u00b7E 2 (Ramesh et al. 2022) apparently support\nmasked inpainting, but we are unaware of a publicly avail-\nable way to apply it to general real images. MGIE (Fu et al.\n2024) train a DM, utilizing a MLLM to derive expressive\ninstructions. These methods require the user to specify the\ndesired localization in words, which has a few shortcom-\nings. On the user\u2019s side, this requires effort, and it can be\ndifficult or impossible to describe the precise location. From\nthe model\u2019s side, failure to visually ground the text-specified\nlocation may fail to perform the desired edit, and/or make\nunintended changes in other locations instead.Input \u201cA big ship\u201d \u201cA sea monster\u201d \u201cIceberg\u201d\nInput \u201c Snowy\nmountains \u201d\u201cThe Great\nPyramid of Giza \u201d\u201cA town\u201d\nInput \u201cA river\u201d \u201cA hole in\nthe ground \u201d\u201cA sports car\u201d\nInput \u201c A volcano\nerupting \u201d\u201cA fortress\u201d \u201cSkiing slopes\u201d\nInput \u201cA bonfire\u201d \u201cA pile of sand\u201d \u201cflowers\u201d\nInput \u201cA small hut\u201d \u201cIced lake\u201d \u201cAn igloo\u201d\nFigure 3: Examples of Click2Mask outputs. The leftmost\ncolumn is the input image with clicked point. The other\ncolumns are Click2Mask outputs given the prompts below.\nSegmentation-based approaches. Segmentation meth-\nods have been utilized to overcome the need for a precise\nuser-provided mask. DiffEdit (Couairon et al. 2022) and\nEdit Everything (Xie et al. 2023) generate segmentation-\nbased masks by utilizing conditionings on diffusion steps, or\nSAM (Kirillov et al. 2023), but require an input image cap-\ntion, which is painstaking. InstructEdit (Wang et al. 2023a),\nwhich uses Grounding DINO (Liu et al. 2023) and SAM to\ngenerate a mask, does not require one, but requires a descrip-\ntion of the object to alter. This can cause errors due to failure\nof the model to localize. InstDiffEdit (Zou et al. 2024) gen-erates masks based on attention maps during denoising.\nThe segmentation-based methods, however, suffer from a\nfew limitations: (i) Such models need to \u201clock\u201d on an exist-\ning object or segment; consequently, in most cases they alter\nobjects, but do not add new free-form ones, which is our fo-\ncus. (ii) These methods typically require the user to provide\nan input caption or a description of the altered object.\nIn contrast to all the above, our work enables adding ob-\njects to real images (as opposed to merely altering existing\nones), without having to provide a precise mask, to describe\nthe input image, or target image, and without being con-\nstrained to boundaries of existing objects or segments. We\naim to enable edits where the manipulated area is not well-\ndefined in advance, and a free-form alteration is required.\n3 Blended Latent Diffusion\nBlended Latent Diffusion (BLD) (Avrahami, Fried, and\nLischinski 2023) is a method for local text-guided image\nmanipulation, based on Latent Diffusion Models (LDMs)\n(Rombach et al. 2022) and Blended Diffusion (Avrahami,\nLischinski, and Fried 2022). Given a source image x, a guid-\ning text prompt p, and a binary mask m, the model blends the\nsource latents (obtained by DDIM inversion (Song, Meng,\nand Ermon 2020)) with the prompt-guided latents through-\nout the LDM process, to derive a blended final output.\nInitially, inputs are converted to a latent space. A vari-\national auto-encoder (Kingma and Welling 2013) with en-\ncoder E(\u00b7)and decoder D(\u00b7), encodes xto latent space, s.t.\nzinit=E(x). In addition, mis downsampled to mlatent in\norder to meet latent spatial dimensions.\nIn each BLD step t, the following occurs:\n1. The latent resulting from the previous step, zt+1, under-\ngoes denoising conditioned by the prompt p, to yield zfg\n(we refer to the generated content as foreground , orfg).\n2. The original image latent zinitis noised to step t, yielding\nzbg(we refer to the original content as background ,bg).\n3. The next step ztis obtained by blending zfgandzbgusing\nmlatent:\nzt=zfg\u2299mlatent+zbg\u2299(1\u2212mlatent) (1)\nwhere\u2299denotes element-wise multiplication.\nAfter the final step, the output z0is decoded to obtain the\nfinal edited image \u02c6x=D(z0).\nHowever, because information is lost during the V AE\nencoding, the decoded final output \u02c6x, might exhibit some\nartifacts when the unmasked region has important fine-\ndetailed content (such as faces, text, etc.). Avrahami et\nal. (2023) solve this issue by optionally fine-tuning the de-\ncoder weights for each image after the denoising steps, and\nusing these weights to infer the final result. In our experi-\nments, we found that this optional background preservation\nprocess is no longer necessary (possibly due to improve-\nments in the Stable Diffusion V AE), and a final blending\nwith Gaussian feathering suffices (Figure 13 in appendix).\n4 Method\nGiven an image, a text prompt, and a user-indicated location\n(e.g., via a mouse click), our goal is to modify the imageInput Emu Edit MagicBrush InstructPix2Pix Click2Mask Input Emu Edit MagicBrush InstructPix2Pix Click2Mask\n\u201cHave a knife laying between the orange and apple\u201d \u201cAdd a cat behind the glass window looking at the food\u201d\n\u201cA knife\u201d \u201cA cat behind the glass window looking at the food\u201d\n\u201cAdd a fruit stand to the right of the image\u201d \u201cAdd USA for the bag\u201d\n\u201cA fruit stand\u201d \u201cUSA\u201d\n\u201cAdd fringe to the pink lampshade\u201d \u201cAdd a tennis ball on top of the racket\u201d\n\u201cFringe\u201d \u201cA tennis ball\u201d\nFigure 4: Comparisons with SoTA methods. Comparisons of Emu Edit (Sheynin et al. 2023), MagicBrush (Zhang et al. 2023)\nand InstructPix2Pix (Brooks, Holynski, and Efros 2023) with our model Click2Mask . Upper prompts were given to baselines,\nand lower ones to Click2Mask. The inputs contain the clicked point given to Click2Mask. See Figure 8 and appendix for\nadditional comparisons.\naccording to the prompt in an unspecified area roughly sur-\nrounding the provided point. We utilize Blended Latent Dif-\nfusion (BLD) (Avrahami, Fried, and Lischinski 2023) as our\nimage editing backbone, but rather than providing it with\na fixed mask at the outset, we evolve a mask dynamically\nthroughout the diffusion process. We initialize the process\nwith a large mask around the indicated point, and gradually\ncontract the mask towards the center, while guiding the rate\nof contraction along the mask boundary using a semantic\nalignment loss based on Alpha-CLIP (Sun et al. 2023).\nThis iterative process results in a mask whose shape and\nsize are determined by both the text prompt, the content, and\nthe structure of the original input image. Furthermore, the\nshape of the mask adjusts itself to the emerging object, as\nthe mask\u2019s evolution is determined by the gradients obtained\nby the semantic alignment loss (see Section 4.1), which in\nturn depend on the shape of the object being generated (see\nFigure 2 for mask evolution illustration, and Figure 6 for\nexamples of generated masks). Once the mask has settled\ninto its final form, we run BLD once more, using the final\nmask to generate the final result. Our method is outlined in\nAlgorithm 1 and illustrated in Figure 5.\n4.1 Dynamic Mask Evolution\nGiven an image x, a text prompt p, and a user-provided loca-\ntionc, we aim to modify x, so as to align with p, in proxim-\nity to c. We start by encoding the input image zinit=E(x).\nWe also create a 2D potential height-field \u03a6in latent space,\nwhich is initialized to a Gaussian around c.\nWe now perform the BLD process, where at each step twe\nobtain a binary mask Mtby thresholding the potential \u03a6us-\ning a threshold \u03c4. The mask evolves dynamically through the\nBLD process, since the threshold \u03c4and the potential \u03a6areboth updated at each step: the threshold \u03c4increases, while\nthe potential \u03a6is elevated in important areas to ensure they\nremain above the threshold. This prevents the mask from\nshrinking in spatial areas that emerge as important for align-\nment of the generated new content with the guiding prompt\np. As a consequence, the mask evolves into a shape deter-\nmined by the newly generated object.\nCommencing the blending at 25% of the diffusion steps,\nthe initial threshold value \u03c4initis relatively low, such that Mt\nis sufficiently large at the beginning ( \u223c16% of the image).\nThis enables BLD to capture the desired edit, as demon-\nstrated in Figure 7 (this idea was originally introduced in\nBLD to cope with the case of small or thin input masks).\nOn the other hand, we prevent the generation of overly large\nmasks, that would result in large scale changes that might\nnot blend seamlessly with the original content, by increas-\ning\u03c4geometrically, and by initiating the potential elevation\nat a later stage (40% out of total diffusion steps). However,\nwe initiate the potential elevation at an early enough stage\nwhen the blended image is still noisy and can be modified.\nWe stop mask evolution when spatial structure is close to be\ndetermined (at 50% of total diffusion steps).\nThe potential elevation is obtained by generating the es-\ntimated final image \u02dcx0at each step, and calculating the co-\nsine distance between the CLIP (Radford et al. 2021) em-\nbeddings of \u02dcx0and the guidance prompt p.\u02dcx0is obtained\nby blending a predicted final foreground latent \u02dczfg, with the\noriginal latent background zinit:\n\u02dcz0= \u02dczfg\u2299Mt+zinit\u2299(1\u2212Mt) (2)\nThe decoded \u02dcx0=D(\u02dcz0)is passed alongside the current\nmask Mtand the prompt pto Alpha-CLIP to focus on the\narea of Mt. The gradient of the cosine distance with respectAlgorithm 1: Click2Mask\nGiven: models LDM ={noise(z, t),denoise (z, p, t )\u2192\n(zt, z0)},VAE={E(x), D(z)},BLD ={(x, p, m, t )\u2192zt},\nAlpha-CLIP ={\u03b1CLIP(x, m, p )\u2192Sim CLIP}, and hyper pa-\nrameters {\u03c4n...l, lr}with schedulers {n, r, k, l }\nInput: input image x, text prompt p, target coordinates c\nOutput: edited image bxthat matches the prompt pin proximity\nofc, and complies to xoutside edited region\n\u03a6 = Gaussian(c)\nzinit=E(x)\nzn\u223cnoise(zinit, n)\nforalltfromntoldo\nzbg\u223cnoise(zinit, t)\nzfg,\u02dczfg\u223cdenoise (zt, p, t)\nG= 0\nift < r then\n\u02dcz0= \u02dczfg\u2299Mt+zinit\u2299(1\u2212Mt)\nSt\u223c\u03b1CLIP(D(\u02dcz0),upscale (Mt), p)\nG\u223c |gradients (St, Mt)|\nzfg\u223cBLD(x, p, M t, t)\nend if\nift < k andSt> St+1then exit loop\nMt= (\u03a6 + G\u2217lr)> \u03c4t\nzt=zfg\u2299Mt+zbg\u2299(1\u2212Mt)\nend for\nbz\u223cBLD(x, p, M t,0)\nreturn D(bz)\nto the latent mask pixels is then calculated by backpropa-\ngating through the CLIP embeddings and the decoder. The\nlarger the absolute gradient of the cosine distance (i.e. CLIP\nloss) with respect to a pixel in Mt, the more significant this\nlocation is for the alignment of the generated content to the\nprompt p. Adding the absolute gradient values Gto\u03a6, ele-\nvates important areas in the \u03a6height-field (around Mt\u2019s con-\ntour for stable evolution \u2013 Figures 14 and 15 in appendix).\nHalfway through the mask evolution steps, we utilize the\nAlpha-CLIP loss as an optional stopping point for Mt\u2019s evo-\nlution, if the loss did not decrease in subsequent steps.\nAfter each update of Mt, we restart the BLD process, let-\nting it proceed from the beginning to the current step t, us-\ning the mask Mtas a fixed mask. This is to allow pixels\nthat were added (or removed) in Mtto affect the generated\nimage from the beginning (see Figure 16 in appendix).\nWe then apply Equation (1) and blend zfgwithzbgusing\nthe mask Mt, which provides zt\u22121, the input to next step.\nAfter all mask evolution steps have been completed, we\nperform a final BLD run using the final Mtwith several\nseeds to obtain several candidate results, where the best one\nis filtered by Alpha-CLIP. As noted earlier, rather than fine-\ntuning the V AE decoder weights to preserve the original\nbackground details outside the mask, we employ instead a\nsimple Gaussian mask feathering when blending the BLD\noutput and the original input image (in pixel space).5 Results\nGiven that our method is mask-free, we compare ourselves\nto mask-free image editing methods, with the slight differ-\nence being that a clicked point replaces any parts of the\nprompt that describe the edit location. To begin with, we\ncompare to MagicBrush, which is the SoTA method among\nthe open-source models. In addition, we compare to Emu\nEdit, which is the SoTA among closed-source models. Since\nwe are unable to run Emu Edit ourselves, we must rely on the\nEmu Edit Benchmark (Sheynin et al. 2023), which includes\nimages generated by Emu Edit. This benchmark contains\nimages with several categories of editing instructions, such\nas adding objects, removing objects, altering style, etc., and\ncan be filtered by these categories. As our focus is adding\nobjects to images, we filtered the dataset by the \u201caddition\u201d\ninstruction. This resulted in 533 items, from which we ran-\ndomly sampled an evaluation subset of 100 samples.\nWe perform the following fixed routine for each sample:\n(i) Removed the word that instructs addition (e.g., \u201cAdd\u201d,\n\u201cInsert\u201d), (ii) removed the part that describes the edit loca-\ntion, and instead (iii) clicked on the image to direct the edit-\ning location. For instance, the instruction \u201cAdd a black base-\nball cap to the man on the left\u201d becomes \u201cA black baseball\ncap\u201d (non-localized instruction).\nFollowing Emu Edit (Sheynin et al. 2023) and BLD\n(Avrahami, Fried, and Lischinski 2023), each sample run\nproduces multiple results internally (comprising of three\nmask evolutions, each followed by a batch of 8 outputs), and\noutputs the best result, as determined automatically using\nAlpha-CLIP scoring.\nTo evaluate our results, we compared these 100 outputs\ngenerated by Click2Mask, with the outputs generated by\nEmu Edit and by MagicBrush (which ran with the origi-\nnal edit instructions). We conducted the evaluation through\na user study (Section 5.1), as well as through automatic met-\nrics (Section 5.2). In both cases, our method outperformed\nthe SoTA methods.\n5.1 Human Evaluation\nWe conducted a user study, where participants were given\na random batch of survey items out of 200 total items (100\nitems comparing to each model). Each item included an in-\nput image, the original edit instruction, and a pair of edited\nimages: one generated by our model, and the other generated\nby either Emu Edit or MagicBrush. Participants were asked\nto rank which of the edited images performed better accord-\ning to three criteria: executing the instruction, not adding any\nother edits or artifacts, and generating a realistic image. The\nsurvey was completed by 149 participants. Each of the 200\nitems was rated by at least 5 users, where the average rate\nwas 15.67 users in Emu Edit, and 8.06 users in MagicBrush.\nIn order to compare Click2Mask vs. Emu Edit, as well\nas Click2Mask vs. MagicBrush, while taking into account\n\u201cties\u201d (ratings stating equal performance on an item, or items\nwith equal ratings to both methods), we analyzed the results\nusing the following metrics: (A) The percentage of items in\nwhich each method was preferred by the majority, disregard-\ning ties. (B) For each item we counted if the majority votedOutputBlended Latent Diffusion\nInputMask Evolution noise Encoder\"A surfboard\"\nDecoder\n\"A surfboard\"\nDecoder\n downsample \nDenoise\n        Thresholding\n         Addition         Blend with mask  ` update \n  gradients \"A surfboard\"\nFigure 5: Click2Mask: An illustration of our method as described in Algorithm 1. The green block is BLD process, performing\ndiffusion steps while blending noised input latents with text guided latents. The pink block is the mask evolution process, where\nwe utilize Alpha-CLIP to evaluate the gradients with respect to the mask Mtpixels, using them to update Mt, obtaining Mt\u22121.\nInputGenerated\nMaskOutput InputGenerated\nMaskOutput\n\u201cA bag of chips\u201d \u201cA person\u2019s hand holding the phone\u201d\n\u201cA reflection of a ghost\u201d \u201cA ceiling fan\u201d\n\u201cA bunch of grapes\u201d \u201cLeaves on the trees\u201d\nFigure 6: Examples of generated masks. For each triplet,\ngiven an input image with clicked point (left) and a prompt\n(below), a purple overlay shows the generated mask (mid-\ndle). The rightmost image is Click2Mask output.\nfor a tie, and if so marked it as a \u201ctied item\u201d. For the other\n\u201cnon-tied items\u201d, we conducted the same majority vote anal-\nysis described in A. (C) The number of total ratings for each\nmethod. In each parameter our method surpassed the closed-\nsource SoTA method Emu Edit, and the open-sourced SoTA\nMagicBrush, as shown in Table 1. See Figures 1, 4 (and Fig-\nures 17, 18, 19, 20 in appendix) for qualitative comparisons\nto the baselines alongside with InstructPix2Pix, and Figure 8\nwith a detailed comparison. Statistical significance analysis\ncan be found in the appendix.\n5.2 Automatic Metrics\nUtilizing the input captions and output captions (describ-\ning the desired output) provided in Emu Edit benchmark,\na variety of metrics were used to assess each method\u2019s\noutputs on the sampled items: (i) Directional CLIP (Gal\net al. 2022) similarity (CLIP direct) to measure correspon-\ndence of the change between input and output images, with\nthe change between input and output captions. (ii) CLIP sim-\nNo enlarge\n Ours\nInput 27% 29% 31% 34% 40% Output\nFigure 7: Ablation study: No mask enlargement at early\nstages. As explained in Section 4, we begin with a large\nmask (about 16% of the image), to capture the desired edit\ninMt. Top row: Mt(marked by purple contours on decoded\n\u02dczfgs, throughout diffusion steps indicated by percentages be-\nlow) evolves without a large starting point, and the diffu-\nsion identifies and guides the white dog to the prompt \u201cHuge\nbone\u201d , while the small Mtfails capturing the bone. Bottom\nrow: Click2Mask\u2019s enlarged Mtcaptures the guided area\neven though the dog is also initially identified as the bone.\nilarity between output image and output caption, to mea-\nsure the compliance between desired and generated outputs\n(CLIP out). (iii) Mean L1 pixel distance between input and\noutput images, to measure the amount of change in the en-\ntire image (L1). (iv) In addition, we present a new metric,\nEdited Alpha-CLIP ( \u03b1CLIP edit).\nEdited Alpha-CLIP. Besides evaluating the images glob-\nally, it is beneficial to evaluate the edited region . We offer an\nEdited Alpha-CLIP procedure to overcome the lack of input\nor output masks in Emu Edit and MagicBrush: we extract\na mask specifying the edited area in the generated image,\nand calculate the Alpha-CLIP similarity between the masked\ngenerated image and the instruction (removing words de-\nscribing addition and edit locations as mentioned in Sec-\ntion 5). See Appendix A.4 and Figure 12 in appendix for\ndetails and extracted masks demonstrations.\nTable 2 shows that our method surpassed both Emu Edit\nand MagicBrush in all metrics: higher scores in all CLIP-\nbased metrics, indicating stronger similarities, and lower L1\ndistance indicating better compliance with input image.Input Emu Edit MagicBrush InstructPix2Pix Click2Mask\n\u21d4 \u21d4 \u2205\n\u201cAdd a piece of fried chicken to the plate\u201d\n\u201cPiece of fried chicken\u201d\n\u22bc,\u25a0 \u22bc \u22bc\n\u201cAdd Bigfoot in the background along-side one of the cows\u201d\n\u201cBigfoot\u201d\n\u21ab \u21ab \u21ab\n\u201cAdd a butterfly on top of the beans\u201d\n\u201cA butterfly\u201d\n\u2205 \u2205 \u25a0\n\u201cAdd Jupiter to the sky\u201d\n\u201cJupiter\u201d\nFigure 8: Failure cases of baselines . Baselines suffer occa-\nsionally from replacing an existing object instead of adding\none (\u21d4), misplacing the object ( \u21ab), modifying other ob-\njects (\u22bc), altering the image globally ( \u25a0), or failing to pro-\nduce an edit ( \u2205). For additional comparisons to baselines,\nsee Figure 4 and appendix.\n5.3 Ablation Study\nWe conducted several ablation studies to analyze the im-\npact of various components on the overall performance of\nour model. Figure 7 demonstrates the need for a sufficiently\nlarge mask on early diffusion steps. See additional ablation\nstudies in Appendix A.2 accompanied by Figures 13 to 16.\n6 Model Limitations\nDuring the evolution process, our model encounters a diffi-\nculty converging to a small, finely detailed mask shape (e.g.\na dog collar). Additionally, since text guidance in Stable Dif-\nfusion is not spatially driven, BLD sometimes has difficulty\nadding the desired object to the masked area when a similar\nobject is nearby in the unmasked area (e.g., adding a Big-\nfoot next to a person). Since we use BLD as our backbone,\nwe sometimes encounter this problem. However, we have\nconsiderably improved it in comparison to BLD by optimiz-\ning the progressive mask shrinking process, and applying it\nacross all objects, not just thin objects, as part of our mask\nevolution process. Moreover, in comparison to other SOTA\nmethods, they often fail to add the desired object even if a(A) (B) (C)\nMethod % Majority% Tied\nitems% Majority\nfrom non-tied# Total\nvotes\nEmu Edit 42.86%35%47.69% 416\nClick2Mask 57.14% 52.31% 465\nMagicBrush 16.30%27%15.07% 148\nClick2Mask 83.70% 84.93% 362\nTable 1: Human evaluation results. Comparisons of (A): %\nof items each method received majority votes, disregarding\nties. (B): % of items the majority voted as tie (left), and %\nof items \u2013 out of the other non-tied items \u2013 each method re-\nceived majority votes (right). (C): Total votes. Refer to Sec-\ntion 5 for details.\nMethod CLIP direct \u2191 CLIP out\u2191\u03b1CLIP edit\u2191 L1\u2193\nEmu Edit 0.150 0.331 0.186 0.046\nMagicBrush 0.095 0.324 0.166 0.049\nClick2Mask 0.204 0.334 0.195 0.027\nTable 2: Automatic metrics results. Evaluation using au-\ntomatic metrics. CLIP direct measures consistency between\nchanges (from input to output) in images and captions,\nCLIP outmeasures similarity between output image and\noutput caption, \u03b1CLIP editmeasures similarity to the non-\nlocalized instruction in the edited area, and L1 measures the\nalignment with the input image. See Section 5 for details.\nsimilar one is not present, and our method outperforms them\nin both cases. See Figure 9 for examples of these cases.\nInput \u201cA golden necklace\u201d\nInput \u201cA white dog\u201d\nFigure 9: Limitations Top row: the evolving mask has a dif-\nficulty converging to a small fine detailed mask shape as a\ngolden necklace. Bottom row: our Stable Diffusion back-\nbone guides the image to a given prompt globally. When\nthe prompt content already exists in the input image close\nto the generated mask (i.e. the white dog), sometimes the\nBLD process we incorporate does not succeed in directing\nthe guidance into the masked region.\n7 Conclusion\nClick2Mask presents a novel approach for local image gen-\neration, freeing users from having to specify a mask, or de-\nscribing the input or target images, and without being con-strained to existing objects. We look forward to users apply-\ning our method with the source code that is available in the\nproject page (see Footnote in Page 1), either to edit images\nor to embed the method for generating or fine-tuning masks.\nReferences\nAvrahami, O.; Aberman, K.; Fried, O.; Cohen-Or, D.; and\nLischinski, D. 2023. Break-A-Scene: Extracting Multiple\nConcepts from a Single Image. In SIGGRAPH Asia 2023\nConference Papers , SA \u201923. New York, NY , USA: Associa-\ntion for Computing Machinery. ISBN 9798400703157.\nAvrahami, O.; Fried, O.; and Lischinski, D. 2023. Blended\nLatent Diffusion. ACM Trans. Graph. , 42(4).\nAvrahami, O.; Lischinski, D.; and Fried, O. 2022. Blended\nDiffusion for Text-Driven Editing of Natural Images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , 18208\u201318218.\nBar-Tal, O.; Ofri-Amar, D.; Fridman, R.; Kasten, Y .; and\nDekel, T. 2022. Text2live: Text-driven layered image and\nvideo editing. In European Conference on Computer Vision ,\n707\u2013723. Springer.\nBetker, J.; Goh, G.; Jing, L.; Brooks, T.; Wang, J.; Li, L.;\nOuyang, L.; Zhuang, J.; Lee, J.; Guo, Y .; Manassra, W.;\nDhariwal, P.; Chu, C.; Jiao, Y .; and Ramesh, A. 2023. Im-\nproving Image Generation with Better Captions.\nBrooks, T.; Holynski, A.; and Efros, A. A. 2023. Instruct-\nPix2Pix: Learning to Follow Image Editing Instructions. In\nCVPR .\nCaron, M.; Touvron, H.; Misra, I.; J \u00b4egou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging Properties\nin Self-Supervised Vision Transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV) , 9650\u20139660.\nCouairon, G.; Verbeek, J.; Schwenk, H.; and Cord, M. 2022.\nDiffEdit: Diffusion-based semantic image editing with mask\nguidance. arXiv:2210.11427.\nDhariwal, P.; and Nichol, A. 2021. Diffusion models beat\nGANs on image synthesis. Advances in neural information\nprocessing systems , 34: 8780\u20138794.\nFu, T.-J.; Hu, W.; Du, X.; Wang, W. Y .; Yang, Y .; and Gan,\nZ. 2024. Guiding Instruction-based Image Editing via Multi-\nmodal Large Language Models. In The Twelfth International\nConference on Learning Representations .\nGal, R.; Patashnik, O.; Maron, H.; Bermano, A. H.; Chechik,\nG.; and Cohen-Or, D. 2022. StyleGAN-NADA: CLIP-\nguided domain adaptation of image generators. ACM Trans.\nGraph. , 41(4).\nHertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.; Pritch,\nY .; and Cohen-Or, D. 2022. Prompt-to-prompt image editing\nwith cross attention control.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion\nprobabilistic models. Advances in neural information pro-\ncessing systems , 33: 6840\u20136851.Kawar, B.; Zada, S.; Lang, O.; Tov, O.; Chang, H.; Dekel, T.;\nMosseri, I.; and Irani, M. 2023. Imagic: Text-Based Real Im-\nage Editing with Diffusion Models. In Conference on Com-\nputer Vision and Pattern Recognition 2023 .\nKingma, D. P.; and Welling, M. 2013. Auto-Encoding Vari-\national Bayes. arXiv:1312.6114.\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.;\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-\nY .; Doll \u00b4ar, P.; and Girshick, R. 2023. Segment Anything.\narXiv:2304.02643 .\nLiu, S.; Zeng, Z.; Ren, T.; Li, F.; Zhang, H.; Yang, J.; Li,\nC.; Yang, J.; Su, H.; Zhu, J.; and Zhang, L. 2023. Ground-\ning DINO: Marrying DINO with Grounded Pre-Training for\nOpen-Set Object Detection. arXiv:2303.05499.\nNichol, A.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin,\nP.; McGrew, B.; Sutskever, I.; and Chen, M. 2022. GLIDE:\nTowards Photorealistic Image Generation and Editing with\nText-Guided Diffusion Models. arXiv:2112.10741.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\nDesmaison, A.; K \u00a8opf, A.; Yang, E.; DeVito, Z.; Raison, M.;\nTejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;\nand Chintala, S. 2019. PyTorch: An Imperative Style, High-\nPerformance Deep Learning Library. arXiv:1912.01703.\nPearson, K. 1900. X. On the criterion that a given system of\ndeviations from the probable in the case of a correlated sys-\ntem of variables is such that it can be reasonably supposed\nto have arisen from random sampling. The London, Edin-\nburgh, and Dublin Philosophical Magazine and Journal of\nScience , 50(302): 157\u2013175.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision.\narXiv:2103.00020.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen,\nM. 2022. Hierarchical Text-Conditional Image Generation\nwith CLIP Latents. arXiv:2204.06125.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition , 10684\u2013\n10695.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton,\nE. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan,\nB.; Salimans, T.; et al. 2022. Photorealistic text-to-image\ndiffusion models with deep language understanding. Ad-\nvances in Neural Information Processing Systems , 35:\n36479\u201336494.\nSheynin, S.; Polyak, A.; Singer, U.; Kirstain, Y .; Zohar,\nA.; Ashual, O.; Parikh, D.; and Taigman, Y . 2023. Emu\nEdit: Precise Image Editing via Recognition and Generation\nTasks. arXiv:2311.10089.\nSong, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502 .Sun, Z.; Fang, Y .; Wu, T.; Zhang, P.; Zang, Y .; Kong, S.;\nXiong, Y .; Lin, D.; and Wang, J. 2023. Alpha-CLIP: A CLIP\nModel Focusing on Wherever You Want. arXiv:2312.03818.\nTumanyan, N.; Geyer, M.; Bagon, S.; and Dekel, T. 2022.\nPlug-and-Play Diffusion Features for Text-Driven Image-to-\nImage Translation. arXiv:2211.12572.\nWang, Q.; Zhang, B.; Birsak, M.; and Wonka, P. 2023a. In-\nstructEdit: Improving Automatic Masks for Diffusion-based\nImage Editing With User Instructions. arXiv:2305.18047.\nWang, S.; Saharia, C.; Montgomery, C.; Pont-Tuset, J.; Noy,\nS.; Pellegrini, S.; Onoe, Y ."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**1. Abstract:**\nThe paper presents DreamBeast, a novel method based on Score Distillation Sampling (SDS) to generate fantastical 3D animal assets composed of distinct parts. Traditional SDS methods struggle with part-level semantics in text-to-image diffusion models. DreamBeast overcomes these limitations through a part-aware knowledge transfer from the Stable Diffusion 3 model to a 3D Part-Affinity implicit representation, significantly enhancing the quality and reducing the computational overhead of generated 3D creatures with specific user-defined part compositions.\n\n**2. Introduction:**\nThe paper introduces the challenge of generating complex, fantastical 3D animals composed of parts from different species, which current methods struggle to achieve due to limited part-level understanding. The authors highlight that existing SDS methods fail to understand part semantics in textual descriptions and mention that recent models like Stable Diffusion 3 (SD3) have better part-level understanding but are computationally expensive and exhibit issues like multi-face Janus and content drift. DreamBeast addresses these issues by incorporating part-aware knowledge transfer to efficiently generate 3D assets from part-specific textual prompts.\n\n**3. Related Work:**\nThe paper reviews several areas relevant to their research, including:\n- Lifting 2D diffusion models for 3D generation\n- Layout guided 3D generation\n- Diffusion models with cross-attention control\n\nIt highlights the limitations of these methods in generating part-aware 3D models and positions DreamBeast as an advancement by introducing part-level knowledge transfer and efficient part-aware 3D generation.\n\n**4. Method:**\n**Part-Aware Knowledge Extraction:**\n- **Score Distillation Sampling (SDS):**\n  The paper revisits SDS, where a pre-trained 2D diffusion model guides the optimization of 3D structures by applying random noise and extracting gradients.\n\n- **Part-Affinity Map Extraction:**\n  After partially optimizing a Neural Radiance Field (NeRF), DreamBeast uses the cross-attention maps from SD3 during the denoising process to generate part affinity maps, aggregating attention over specific term tokens to depict part correspondences for discrete camera poses.\n\n- **Part-Affinity NeRF:**\n  DreamBeast trains a neural network, particularly an MLP-based NeRF, to learn the part affinity maps and interpolate these for continuous camera poses. The objective minimizes the error between predicted and actual maps across various camera views.\n\n- **Attention Modulation during SDS:**\n  DreamBeast modulates the cross and self-attention mechanisms within the guidance model using the part-affinity maps. This step involves enhancing attention scores for relevant parts and reducing it for unrelated regions.\n\n**Algorithm:**\nProvided in the paper, the detailed algorithm modulates attention scores by including part-specific affinity mappings during diffusion steps.\n\n**5. Experimental Setup:**\n- **Datasets:**\n  Synthetic datasets were generated using GPT-4o-mini prompts describing creatures with combinations of animal parts.\n\n- **Hardware and Software:**\n  Experiments were conducted using NVIDIA A40 GPUs.\n\n- **Evaluation Metrics:**\n  Evaluation used CLIP cosine similarity scores for textual alignment and user studies for perceptual quality.\n\n- **Baselines:**\n  Comparisons were made with DreamFusion, MVDream, Stable Diffusion 2.1, Stable Diffusion XL, DeepFloyd, and GeoDream among others.\n\n**6. Results and Analysis:**\n- **Quantitative Results:**\n  Presented in tables, CLIP scores showed DreamBeast\u2019s assets had higher semantic alignment with textual prompts.\n\n- **Qualitative Results:**\n  Figures compared visual consistency and part-specific adherence across methods, showing DreamBeast\u2019s superior part-aware resolution.\n\n- **User Study:**\n  Results indicated user preference for DreamBeast in both part correspondence and overall 3D quality.\n\n- **Ablation Studies:**\n  Illustrated improvement in part-awareness with increasing camera views for training the Part-Affinity NeRF, stabilizing performance beyond a certain number of viewpoints.\n\n**7. Discussion:**\n- **Interpretation of Results:**\n  DreamBeast accomplishes more detailed and geometrically consistent part-aware 3D generation by utilizing part-specific guidance effectively.\n\n- **Implications:**\n  The findings suggest DreamBeast\u2019s potential in creative 3D content generation, especially for games and simulations.\n\n- **Limitations:**\n  The primary constraint is the initial time required to partially optimize NeRF and generate part-affinity maps.\n\n**8. Conclusion:**\nThe paper concludes by emphasizing the main contributions:\n- Introduction of the part-aware lifting approach for text-to-3D generation.\n- Effective part-affinity knowledge transfer improving computational efficiency.\n- Outperformance in quality and part-awareness against existing methods.\n\n**Future Directions:**\nThe paper suggests exploring more complex part arrangements, real-time applications, and further reducing computational overhead. This comprehensive coverage aims to enable researchers to replicate and extend the work presented in DreamBeast.\n\nBy summarizing and detailing these sections, this rewrite provides enough information for a researcher to understand, replicate, or build upon the work described in the original paper."}, {"role": "assistant", "content": "DreamBeast: Distilling 3D Fantastical Animals with Part-Aware\nKnowledge Transfer\nRunjia Li1Junlin Han1Luke Melas-Kyriazi1Chunyi Sun2Zhaochong An3\nZhongrui Gui1Shuyang Sun1Philip Torr1Tomas Jakab1\n1University of Oxford2Australian National University3University of Copenhagen\ndreambeast3d.github.io\nFigure 1. Generated fantastic 3D beasts composed of diverse animal parts. Our method enables part-level generation, resulting in 3D\ncreatures with unique combinations of heads, limbs, wings, tails, and bodies.\nAbstract\nWe present DreamBeast, a novel method based on score\ndistillation sampling (SDS) for generating fantastical 3D\nanimal assets composed of distinct parts. Existing SDS\nmethods often struggle with this generation task due to\na limited understanding of part-level semantics in text-to-\nimage diffusion models. While recent diffusion models, such\nas Stable Diffusion 3, demonstrate a better part-level under-\nstanding, they are prohibitively slow and exhibit other com-\nmon problems associated with single-view diffusion mod-\nels. DreamBeast overcomes this limitation through a novel\npart-aware knowledge transfer mechanism. For each gener-\nated asset, we efficiently extract part-level knowledge from\nthe Stable Diffusion 3 model into a 3D Part-Affinity im-plicit representation. This enables us to instantly generate\nPart-Affinity maps from arbitrary camera views, which we\nthen use to modulate the guidance of a multi-view diffusion\nmodel during SDS to create 3D assets of fantastical ani-\nmals. DreamBeast significantly enhances the quality of gen-\nerated 3D creatures with user-specified part compositions\nwhile reducing computational overhead, as demonstrated\nby extensive quantitative and qualitative evaluations.\n1. Introduction\nImagine a creature taking flight, its dragon wings catch-\ning the sunlight. Picture its majestic lion\u2019s head surveying\nthe landscape, while a sinuous serpent\u2019s tail trails behind.\n1arXiv:2409.08271v1  [cs.CV]  12 Sep 2024What if \u201dFantastic Beasts and Where to Find Them\u201d was\nnot just a magical story, but we could actually build them in\na digital 3D world? Current methods for generating 3D ob-\njects [1, 3, 15, 18, 38, 48, 53] struggle with generating com-\nplex, artistic, or fantastical shapes and textures, which are\nnot represented in existing datasets. For example, they are\nunable to produce Griffin-like animals composed of parts\nfrom multiple species. More generally, they struggle with\nproducing objects composed of multiple diverse parts.\nOne of the most promising current approaches to open-\nworld 3D asset generation consists of lifting 2D guid-\nance into 3D. Methods such as DreamFusion [32] and\nSJC [44] demonstrate how pre-trained 2D diffusion mod-\nels [12, 34, 35, 37] can guide the generation of 3D ob-\njects through score distillation sampling (SDS). Specifi-\ncally, these methods produce 3D objects from textual de-\nscriptions by utilizing the priors encoded in 2D diffusion\nmodels, which act as approximate log gradients of the den-\nsity of distribution of 2D images conditioned on text.\nThe lifting methods, however, fall short of providing\npart-level controllability for part-specific textual descrip-\ntions. The reason for this is twofold. First, there have not\nbeen any 2D diffusion models capable of sufficiently strong\npart-level understanding. Second, in part due to the first\nreason, there have been no methods proposed in the litera-\nture for part-aware lifting-based (SDS) text-to-3D genera-\ntion from part-specific textual prompts.\nA creature with a body of a \nskunk and wings of a falcon \nand claws of a bear \nA creature with a body of a \njellyfish and a beak of a \nhawk and legs of a kangaroo A creature with a body of a \nfrog and a wingspan of a \nvulture and tail of a squirrel A creature with a body of \na bat and fins of a dolphin MVDream Stable Diffusion 3 \nFigure 2. Comparison of diffusion models on part-level prompt\nunderstanding in 2D generation. Although MVDream can grasp\nthe overall semantic understanding of the described animals, the\ngenerated images often feature deformed animals and fail to accu-\nrately capture specific part-based descriptions, unlike SD3.\nDiffusion models are improving rapidly, and the recent\nrelease of Stable Diffusion 3 (SD3) [12] has led us to recon-\nsider the subject of part-level generation. The starting point\nfor our paper is the observation that SD3 can capture part-\nlevel correspondences significantly better than prior mod-\nels (as shown quantitatively in Table 1 and qualitatively in\nFigure 2). This new capability allows for the generation of\ncomplex part-aware entities through part-specific text de-\nA creature with a dog body and  cat head \nA creature with a human body and  monkey head \nA creature with a head  of a bull and the body of a horse  t=450, l=11  t=450, l=8  t=850, l=11 \nFigure 3. Failing to generate part-aware content even with part\nunderstanding in SD3. Despite its understanding of part corre-\nspondences, as evidenced by the cross-attention maps at certain\ntimesteps tand layers l, SD3 may still fail to generate part-aware\nimages. This is illustrated in above examples where specific ani-\nmal parts are absent, highlighted in red. Our method capitalizes on\nthe observation that only particular timesteps tand layers lexhibit\npart-awareness.\nscriptions. However, such fine-grained understanding capa-\nbilities are not yet available for 3D generation.\nAs SDS is potentially capable of lifting any entities from\n2D to 3D, a straightforward approach might combine SD3\nwith SDS. However, as shown in Figure 3, we observe\nthat SD3 occasionally struggles to generate animals accord-\ning to prompts that specify particular animal body parts,\neven though it understands where those parts should be in\nthe cross attention maps. This issue arises because part-\ncorrespondence understanding is only reliable at certain\ntimesteps and transformer layers, making the SDS process\nless robust to prompts that focus on specific parts. Addi-\ntionally, naively using SD3 within SDS is leading to multi-\nple issues. SD3\u2019s use of the rectified flow match Euler dis-\ncrete scheduler results in deformed outputs with standard\ntimestep sampling used for other diffusion models, as seen\nin Figure 4 and 9. Other issues associated with SDS such\nas multi-face Janus problem and content drift [37] are also\npresent. Furthermore, generating 3D assets with SD3 takes\n7 hours, which can be prohibitive in many applications.\nTo overcome the aforementioned challenges, we intro-\nduce DreamBeast, a novel part-aware knowledge transfer\nmodule designed to efficiently distill part-level understand-\ning from powerful single-view diffusion models, such as\nSD3, and inject it into 3D generation with SDS.\nDreamBeast first performs SDS over several steps to\npartially optimize the NeRF, producing a coarse yet view-\nconsistent layout of the animal\u2019s shape. We then render the\n2A creature with a head  of a hedgehog  and a tail of a scorpion  \nand fins of a whale \nMVDream Dreamfusion \n(SD3) Ours MVDream Dreamfusion \n(SD3) Ours \nA creature with a mane  of a lion and a body  of a cheetah \nFigure 4. MVDream and SD3 have difficulty generating part-\naware 3D animals. While SD3 [12] can understand part corre-\nspondence in images and text, it struggles to generate 3D assets\nusing SDS due to the issues we discussed in our paper. MV-\nDream [37] falls short because it was fine-tuned on Objaverse [9],\nwhich lacks part-level information in the dataset.\nDreamfusion(SD3)\n+MVDreamDreamfusion\n(SD3)Ours MVDream0100200300400500Running Time (minutes)480 min\n420 min\n78 min\n50 minPart-AwareComparison of Running Speed\nFigure 5. Running speed comparison. While Dreamfusion\n(SD3) combined with MVDream and standalone Dreamfusion\n(SD3) take 480 and 420 minutes respectively, our method sig-\nnificantly reduces the runtime to 78 minutes. This reduction is\nachieved without sacrificing part-awareness making our method\nboth faster and more effective in part-aware 3D generation. More\ndetail in Appendix D.\nNeRF from a limited number of camera viewpoints. We use\nthese renderings as the denoising condition for SD3 and per-\nform several denoising steps during which we extract cross-\nattention maps of part-specific tokens from part-aware lay-\ners. We average these cross-attention maps for each of the\ncamera viewpoints and obtain part affinity maps.Next, we train a 3D Part-Affinity representation based on\nNeRF from the extracted part affinity maps, which allows us\nto interpolate part affinity maps from any camera viewpoint\nalmost instantaneously. Subsequently, during SDS, we ren-\nder both the 3D asset and the learned Part-Affinity NeRF of\nDreamBeast from the same camera perspective and modu-\nlate the cross and self-attention mechanisms of the guidance\nmodel using the rendered part affinity map. This modula-\ntion causes regions with high part affinity to have higher\nresponses to part-specific prompts. Consequently, our ap-\nproach (DreamBeast) not only promotes more reliable part-\naware SDS but also significantly reduces the computational\ncost from 7 hours to 78 minutes and cuts GPU memory us-\nage by 24GB, compared to the combination of SD3 with\nSDS as demonstrated in Figure 5. Quantitative and qualita-\ntive evaluations of DreamBeast demonstrate its exceptional\nability to generate part-aware, imaginative 3D animals.\nIn summary, our main contributions are as follows:\n1. We are the first to consider the problem of part-level text\nto 3D generation in an open-world setting.\n2. We propose DreamBeast, a novel knowledge transfer\nmechanism that efficiently transfers part-level knowl-\nedge of a 2D diffusion model into the 3D generation pro-\ncess.\n3. We significantly improve the quality and decrease the\ncomputational cost of creating part-aware 3D animal as-\nsets by integrating DreamBeast within the SDS opti-\nmization process.\n4. We demonstrate through quantitative evaluations and a\nhuman study that our method consistently outperforms\nbaseline methods.\n2. Related Work\nLifting 2D Diffusion Models for 3D Animal Generation.\nDue to the limited generalizability of current 3D genera-\ntive models, efforts have been made to adapt 2D diffusion\npriors or single image/video [24, 49] for 3D assets such\nas animals. The distilling diffusion prior approach primar-\nily employs score distillation sampling (SDS) [32], where\n2D diffusion priors serve as score functions that guide the\noptimization of 3D structures. Similarly, SJC [44] uti-\nlized publicly accessible diffusion models for their method.\nSubsequent studies have aimed at refining 3D representa-\ntions, enhancing loss design, or implementing multi-stage\noptimizations [6, 7, 19, 25, 41, 46, 51, 52]. Some meth-\nods [20, 27, 33, 39] leverage diffusion guidance to optimize\n3D models based on a single image. Another set of meth-\nods uses diffusion guidance to learn the layout for compo-\nsitional generation [5, 11, 31]. Notably, MVDream [37]\nproposed a multi-view consistent diffusion model for SDS\nguidance, significantly improving issues with multi-face\nJanus and content drift.\n3A creature with a \nkangaroo head and \nthe shell of a tortoise .\nView c1\nView cNDiT\nBlock\n\u2026 \u2026 \u2026\n\u2026Stable Diffusion 3\nView c1\nInput TokensT ext TokensAttention\nPart-Affinity Maps MPart-Affinity\nNeRF  f\u03b8\nStep 4:Score Distillation Sampling with Part-Affinity NeRF\nSample\nNew view\nMulti -view \nDiffusion\u2026\nViewc\u2019\nInput TokensT ext\nT okensAttention\nPart-Affinity \nMaps M\u2019\nRendered\nImage xc\u2019\n\u2026\nSDS\nGradient3DAsset NeRF\nStep 2:Extracting Part-Affinity MapsPart-Affinity\nNeRF f\u03b8\nStep3:Optimizing Part-Affinity NeRF\n\u2026View c1\n\u2026\nView cN\n\u2026\u2112pa\u2248\nFeed\nForward\nGradient\nBackward\nPart-Affinity\nNeRF Render\n3DAsset\nNeRF Render\nDiT\nBlockDiT\nBlock\nPartially Optimized\nNeRF\n\u2026\nView cN\nInput Tokens\nStep 1:Obtaining Partially Optimized NeRF\nMulti -view \nDiffusion\nSDS\nGradient\nN steps\nPartially Optimized NeRF Un-Optimized NeRF\nFigure 6. DreamBeast pipeline. DreamBeast is composed of 4 steps: (1)Partially optimize a NeRF under standard SDS. (2) Multiple\nrendered views of the partially optimized NeRF are input into SD3 along with the text prompt to construct Part-Affinity maps based on\ncross-attention of part tokens in SD3. (3) A Part-Affinity NeRF is trained using these extracted maps. (4) Both the trainable 3D asset NeRF\nand frozen Part-Affinity NeRF are rendered from the same camera pose. The rendered Part-Affinity map then modulates cross and self-\nattention in MVDream for SDS, ultimately generating a part-aware 3D animal. The symbol\n\u2744 denotes a frozen model, while\n\ud83d\udd25 indicates\na model that is trainable.\nLayout Guided 3D Generation. Earlier research [29] has\nexplored the application of compositional neural radiance\nfields within an adversarial learning framework to achieve\n3D-aware image generation. A pioneering study [13] uti-\nlized a mesh database to find and combine parts to create\nnew objects. Subsequent research incorporated probabilis-\ntic models for part suggestion [21], semantic attributes [2],\nand fabrication [36]. Some studies [42] employed neural\nradiance fields to represent various 3D elements and render\nthese into a unified 3D model. Recent advancements [31],\nguided by pre-trained diffusion models, have enabled the\ngeneration of compositional 3D scenes using user-provided\n3D bounding boxes and text prompts. Concurrently, other\nworks have used large language models (LLMs) to gener-\nate 3D layouts from text prompts as an alternative to hu-\nman annotations [14, 43, 45], or combined layout learning\nduring the optimization process [5, 11]. While these ap-\nproaches can produce 3D scenes through composition, they\nall rely heavily on scene graphs or descriptions of object-\nto-object relationships for object-to-scene generation. This\ndependency makes it impossible when it comes to com-\nposing part-to-object generational tasks. Unlike object-to-\nscene generation, which requires an understanding of the\nrelationships between distinct objects, part-to-object gen-\neration necessitates a more fine-grained comprehension of\nhow individual parts combine to form a coherent whole.Diffusion with Cross-Attention Control. Since most\ncurrent diffusion models are transformer-based and in-\ncorporate text information through cross-attention layers,\nproviding spatial awareness naturally aligns with cross-\nattention control. Several studies [4, 10, 17, 23, 30] explore\nvarious methods to enhance cross-attention scores between\nregions and their corresponding descriptions in the prompts.\nIn contrast, others [16] propose applying a binary mask to\neliminate attention between regions and non-matching re-\ngion descriptions. To the best of our knowledge, cross-\nattention control is predominantly applied in 2D generation\nmodels and is seldom utilized in 3D SDS settings as in our\npaper.\n3. Method\nTo efficiently transfer part-level knowledge of a 2D diffu-\nsion model into the 3D generation process, we introduce\nDreamBeast as a novel geometry-consistent mechanism de-\nsigned for this purpose. In Section 3.1, we revisit the\nclassic Score Distillation Sampling (SDS) formulation and\ndiscuss the issues that arise when applying SDS directly\nwith SD3. Subsequent sections detail our method: Sec-\ntion 3.2 describes the extraction of 2D part affinity maps\nfrom SD3, Section 3.3 details the construction of our Part-\nAffinity NeRF using the part affinity maps, and Section 3.4\n4Deer Antlers Bear Body \nLemur Limbs \nOstrich Head Fish Tail \nPigeon Head \nFish Fins \nWolf Head \nSnail Shell Figure 7. Learned part affinity map rendered from unseen camera poses. Heatmaps displaying the learned 3D part affinity repre-\nsentation rendered from unseen camera poses for different part-specific descriptions of distinct animals. Warmer colors indicate stronger\naffinities, highlighting our implicit 3D neural representation\u2019s capability to differentiate and localize specific anatomical features.\npresents the integration of the Part-Affinity NeRF within\nSDS to generate part-aware 3D assets.\n3.1. Preliminaries\nBefore delving into our method, we briefly review the con-\ncepts commonly employed in the 3D lifting generation tech-\nniques and further discuss our motivation.\nScore Distillation Sampling. As introduced by [32],\nScore Distillation Sampling (SDS) uses a pre-trained 2D\ndiffusion model with fixed parameters \u03d5to guide the gen-\neration of 3D models with the vast amount of 2D im-\nage prior knowledge. Let \u03b8denote the 3D representation\nsuch as NeRF [28] or Gaussian Splatting [22] and g(\u00b7)be\nthe differentiable rendering function, which renders the 3D\nmodel to an image x=g(\u03b8). During the guidance pro-\ncess with yas the text condition, we first sample a random\ntimestep t\u223c {0, ..., T}and a random noise \u03f5\u223c N (0, I).\nWe then add the noise to the rendered image and we get\nxt=\u221a\u03b1tx+\u221a1\u2212\u03b1t\u03f5. The SDS gradient then is defined\nas follows:\n\u2207\u03b8LSDS(x=g(\u03b8)) =Et,\u03f5\u0014\nw(t)(\u02c6\u03f5(xt;y, t, \u03d5 )\u2212\u03f5)\u2202x\n\u2202\u03b8\u0015\nwhere \u03b1tandw(t)are weighting functions that depend on\nthe timestep t, and \u02c6\u03f5is the predicted noise by the pre-trained\ndiffusion model.\nWhy not Stable Diffusion 3 as the Guidance Directly?\nA straightforward way to leverage part-level knowledge\nfrom SD3 is by performing SDS with SD3\u2019s guidance.\nHowever, we argue that this approach is ineffective for sev-\neral reasons: (1) Even when SD3 manages to generate im-ages with part-level specifications, it often fails because\npart-level understanding is only exhibited at specific trans-\nformer layers and timesteps, as shown in Figure 3. Dur-\ning the denoising forward pass through SD3, information\nabout the animals and their parts can become mixed, lead-\ning to a loss of part-level control. This makes SDS unstable\nand unpredictable at the part level. (2) SD3 cannot pro-\nvide view-consistent guidance, which leads to issues like\nmulti-face Janus problem and content drift problems [37].\n(3) SD3 uses a rectified flow match Euler discrete sched-\nuler, which differs from previous diffusion methods. We\nobserve that timestep sampling strategies from earlier meth-\nods do not yield satisfactory results with SD3. Additionally,\nhyperparameters such as the guidance scale and 3D shape\nloss scales require extensive empirical tuning. (4) The for-\nward process is computationally expensive, requiring 48GB\nof GPU memory and 7 hours of training time on an NVIDIA\nA40 to generate just one 3D asset. In contrast, Dream-\nBeast offers more stable and robust part-level specification\nduring SDS and requires only 78 minutes to complete. This\nis slightly longer than SD2.1 or MVDream, which takes 50\nminutes on an A40 GPU (see Figure 5 for a comparison of\nrunning speeds).\nWhy not Stable Diffusion 3 + MVDream as the Guid-\nance? This approach could potentially solve the multi-\nface Janus problem; however, the remaining issues with\nSD3 still persist. Moreover, this combination requires\n58GB of GPU memory and takes 8 hours to generate a sin-\ngle 3D asset through SDS, rendering it highly inefficient.\n53.2. Part Affinity Map Extraction\nBefore extracting the part affinity map from SD3, we first\nperform SDS for several steps to partially optimize the\nNeRF. We then render this partially optimized NeRF from\nvarious camera angles to obtain view-consistent, coarse\nanimal-shape layouts. These view-consistent layouts serve\nas conditions for SD3, where the rendered animal shape is\nmixed with noise as input for denoising, which helps keep\nthe extracted part affinity maps view-consistent as well.\nWe choose Stable Diffusion 3 (SD3) as our source of\n2D part-level knowledge for two key reasons: (1) Unlike\nperception-driven part-level segmentation frameworks [8,\n40, 47, 50] that rely on well-defined input images, we found\nthat SD3 can operate effectively on noisy images generated\nfrom partially optimized NeRFs.\n(2) Among many open-source models we examined,\nSD3 is the only one that demonstrates part-level understand-\ning in its cross-attention maps.\nTo obtain the part affinity map, we conduct a denois-\ning process for the timesteps between tsandte. Specif-\nically, for a rendered image xfrom a partially optimized\nNeRF under camera pose c, we introduce noise to xusing\na weighting factor \u03b1tsto produce xts. Subsequently, we\nperform denoising in the latent space for each timestep up\ntote. At each timestep t, and for each transformer layer\nlin SD3, we compute an attention map At,l,c\u2208RHW\u00d7n,\nwhere ndenotes the number of tokens in the text prompt\ny\u2208Rn, and HW represents the spatial resolution of the\nfeature maps. From this attention map, we extract a spa-\ntial correspondence map Mt,l,i,c\u2208RHWfor each token yi,\nwhich corresponds to a slice of At,l,cassociated with the\ntoken yi. Let Iprepresent the set of token indices for the\npart-level description in y, for example, \u201dkangaroo head\u201d in\n\u201da creature with a kangaroo head and a tortoise shell\u201d cor-\nresponds to a set of token indices Ip={4,5}. We can then\nderive the part affinity map Mp,c\u2208RHWfor this camera\npose using the following equation:\nMp,c=1\n(ts\u2212te)\u00b7L\u00b7 |Ip|teX\nt=tsLX\nl=0X\ni\u2208IpMt,l,i,c\nwhere Lis the number of transformer layers.\n3.3. Part-Affinity NeRF\nThe part affinity map Mp,capplies only to a specific camera\nposec. This poses a problem, as the camera poses are sam-\npled from a continuous distribution during SDS, implying\nan infinite number of potential camera poses.\nA naive approach would be to generate the part affin-\nity map each time an image is rendered from the 3D as-\nset. However, this approach significantly increases compu-\ntational demands. The forward pass in SD3, combined with\nthe need to obtain cross-attention maps for every layer and\nA creature with a body  of a bear  and the antlers  of a deer \nA creature with a head  of a kangar oo  and the shell  of a tortoise \noptimization Figure 8. Part-Affnity NeRF learning progress visualization.\nWe show the evolution of renderings of the Part-Affinity NeRF\nfor each corresponding part (e.g., body of a bear in the first row)\nthroughout the learning, demonstrating that the Part-affinity NeRF\nquickly converges.\ntimestep, requires substantially more computation. As a re-\nsult, generating a single 3D asset can take up to 58 hours,\nwhich is even longer than using SD3 solely as a guidance\nmechanism for SDS.\nTherefore, as part of DreamBeast, we introduce the Part\naffinity NeRF that learns from part affinity maps of discrete\ncamera poses and is capable of interpolating these maps for\ncontinuous camera poses.\nMathematically, let {{Mp,c|p\u2208 P} | c\u2208 C} denote\nall part affinity maps obtained for all parts Punder a set\nof camera poses C. The optimization goal is to fit the im-\nplicit representation f\u03b8(M|p, c)parameterized by a neural\nnetwork with parameters \u03b8:\nmin\n\u03b8Lpa= min\n\u03b8X\nc\u2208CX\np\u2208P||f\u03b8(M|p, c)\u2212Mp,c||2\nIn our design, f\u03b8is an MLP-based neural radiance field.\nThe continuous nature of MLPs imposes a form of smooth-\nness and continuity in the learned representation, enabling\nDreamBeast to produce part affinity maps from any camera\npose. In conclusion, DreamBeast offers several key ben-\nefits: (1) it provides a 3D consistent representation of part\naffinity maps learned from discrete camera views, allowing\nfor the interpolation of part affinity maps under any cam-\nera pose, which makes it efficient to use with the SDS pro-\ncess; (2) it significantly reduces the computation cost of us-\ning SD3 at every single step to the more economical learn-\ning and rendering cost of DreamBeast, reducing the training\ntime from 58 hours to 78 minutes per 3D asset.\n6Algorithm 1 Layer-wise Attention Modulation with\nDreamBeast\nRequire: Cross attention and self attention of the 2D diffu-\nsion guidance model Scross,Sself, rendered part affin-\nity map {f\u03b8(M|p, c\u2032)|p\u2208 P} , part token indices\n{Ip|p\u2208 P} , enhancement factor \u03b1cross ,\u03b1self\n1:forpinPdo \u25b7Cross Attention\n2: M\u2032\np,c\u2032\u2190f\u03b8(M|p, c\u2032)\n3:Scross[:, Ip]\u2190 S cross[:, Ip] +\u03b1cross logM\u2032\np,c\u2032\n4: Across\u2190Softmax (Scross)\n5:end for\n6:forpinPdo \u25b7Self Attention\n7: M\u2032\np,c\u2190f\u03b8(M|p, c\u2032)\n8: Kp,c\u2032\u2190(M\u2032\np,c\u2032)TM\u2032\np,c\u2032 \u25b7Symmetry\n9:Sself\u2190 S self+\u03b1selflogKp,c\u2032\n10: Aself\u2190Softmax (Sself)\n11:end for\n3.4. SDS with Attention Modulation\nAfter rendering an image x(c\u2032)from the 3D asset under\na specific camera pose c\u2032during SDS, we also generate\nthe rendered affinity maps for each of the parts {f\u03b8(M|\np, c\u2032)|p\u2208 P} using the optimized Part-Affinity NeRF un-\nder the same camera pose. These rendered part affinity\nmaps are utilized to modulate the cross-attention and self-\nattention matrices in the 2D diffusion guidance model for\nSDS. Specifically, we modulate the cross-attention score\nmapsScross\u2208Rhw\u00d7nand the self-attention score maps\nSself\u2208Rhw\u00d7hw(before the softmax operation), where hw\nrepresents the feature spatial resolution and ndenotes the\nnumber of tokens, in the 2D diffusion guidance models at\neach denoising step \u02c6\u03f5(xt;y, t, \u03d5 ). The detailed procedure\nis outlined in Algorithm 1. The cross-attention modula-\ntion ensures that regions corresponding to a specific part\nare guided by their corresponding part-specific token. The\nself-attention modulation increases influence within intra-\npart regions and reduces influence among inter-part regions.\n4. Experiments\n4.1. Implementation Details\nWe consider the cross-attention map between ts= 450 and\nte= 100 at the eleventh layer ( l= 11 ) in SD3, where we\nfound the most significant part-level understanding.\nOur Part-Affinity NeRF is a small MLP with one hid-\nden layer comprising 16 neurons. The output dimension is\nequivalent to the number of parts described in the prompt,\nwith each dimension representing a different part of the fan-\ntastical animal. The part affinity map is rendered similarly\nto NeRF [28], using 128 samples per ray and a rendered\nresolution of 64. We optimize Part-Affinity NeRF for 2000\nsteps.2D Diffusion Model 2 Parts 3 Parts\nMVDream [37] 0.242 0.108\nStable Diffusion 2.1 [34] 0.187 0.022\nStable Diffusion XL 0.297 0.032\nDeepFloyd [35] 0.429 0.097\nStable Diffusion 3 [12] 0.826 0.537\nTable 1. 2D part-aware generation success rate. A user study in-\nvolving five participants found that SD3 has a significantly higher\nsuccess rate than other popular diffusion models in generating\npart-aware images based on part-level prompts (describing 2 or\n3 animal parts in a single prompt). This suggests that SD3 has a\nsuperior ability to understand and generate images at the part level.\nMethodCLIP Score \u2191\nB/32 B/16 L/14\nDreamFusion(SD2.1) [32] 0.271 \u00b13.0e\u221240.274\u00b12.2e\u221240.226\u00b13.6e\u22124\nDreamFusion(SD3) [32] 0.271 \u00b15.8e\u221240.275\u00b15.5e\u221240.229\u00b17.7e\u22124\nMVDream [37] 0.275 \u00b17.8e\u221240.282\u00b14.1e\u221240.230\u00b17.9e\u22124\nGeoDream [26] 0.244 \u00b11.2e\u221240.252\u00b11.5e\u221240.202\u00b11.8e\u22124\nOpenLRM [18] 0.265 \u00b12.1e\u221240.285\u00b12.4e\u221240.223\u00b16.2e\u22124\nVFusion3D [15] 0.268 \u00b11.9e\u221240.281\u00b12.1e\u221240.225\u00b16.2e\u22124\nDreamBeast (Ours) 0.285\u00b12.6e\u221240.289\u00b13.5e\u221240.245\u00b14.6e\u22124\nTable 2. Performance comparison of different methods.\nOur method shows the best CLIP scores among all.\nView NumberCLIP Score \u2191\nB/32 B/16 L/14\n8 0.274 \u00b15.0e\u221240.281\u00b14.6e\u221240.231\u00b15.9e\u22124\n16 0.277 \u00b14.8e\u221240.281\u00b11.9e\u221240.232\u00b15.8e\u22124\n32 0.277 \u00b14.5e\u221240.283\u00b14.3e\u221240.235\u00b15.7e\u22124\n64 0.284 \u00b14.3e\u221240.287\u00b14.0e\u221240.240\u00b15.5e\u22124\n76 0.285 \u00b12.6e\u221240.289\u00b13.5e\u221240.245\u00b14.6e\u22124\nTable 3. Ablation study on the number of views for extracted\npart-affinity maps. Increasing the number of views results in a\nstronger part-affinity NeRF.\nWe choose MVDream [37] as our diffusion guidance\nmodel due to its multiview consistency and computational\nefficiency for SDS. We set \u03b1cross = 0.8and\u03b1self= 0.9\nto modulate the influence of cross and self-attention maps\nwithin MVDream. For the SDS optimization, we adhere\nto the default hyper-parameters specified in MVDream for\nother settings. The training procedure involves several steps\nto create a detailed 3D asset of an animal. Initially, the as-\nset is trained (guided by MVDream) for 1000 steps to ob-\ntain the partially optimized NeRF. Following this, the Part-\nAffinity NeRF is optimized over an additional 2000 steps.\nFinally, the training continues for another 4000 steps, using\npart-level guidance from the rendered part affinity maps, to\nachieve the final part-aware 3D model.\n7A creature with a \nbody of a carp \nand a mane of a \nhorse and claws \nof a hawk \nA creature with a \nbody of a worm \nand the head of a \nfox\nA creature with a \nbody of a lemur \nand the wings of \na bumblebee \nA creature with a \nbody of a dolphin \nand a mane of a \nlion and claws of \nan eagle \nA creature with a \nbody of a penguin \nand a head of a \npanda and tail of a \nlemur \nOurs MVDream DreamFusion (SD3) Figure 9. Qualitative results of generated fantastic animals from different methods. DreamBeastis capable of generating 3D assets\nwith better part correspondence to part-specific prompts compared to MVDream or SD3. The fantastic animals created by MVDream and\nSD3 often either omit certain body parts or blend different animal elements globally, which is not the desired outcome.\n4.2. Evaluation Benchmarks\nWe use GPT-4o-mini to randomly generate 30 prompts un-\nder a template \u201da creature with a [animal 1][part 1], [animal\n2][part 2], and [animal 3][part 3]\u201d. We use CLIP text sim-\nilarity and ranking-based user study to evaluate how well\nthe generated 3D assets match their descriptions. More\nvisualizations and non-animal results are presented in Ap-\npendix C.\n4.3. Main Results\nCLIP Similarity Experiment. We compare Dream-\nBeast with other distillation-based methods [26, 32, 37],\nand popular feedforward 3D generation methods [15, 18],\nresults are shown in Table 2. We observe that Dream-\nBeast consistently has higher similarity scores across CLIP\ntypes, indicating DreamBeast\u2019s better part-correspondence\nin generated 3D assets and the part-specific prompts.\n0 20 40 60 80 100\nRatio of Best Votes (%)3D Asset\nQuality Part   \nCorres-\npondenceQuality TypeMethods\nDreamFusion(SD3)\nMVDream\nOurs\nFigure 10. User study results. Participants were shown multi-\nview images generated from the same prompts and asked to select\nthe best result. Our method receives significantly more votes for\nboth part correspondence and overall 3D asset quality. More detail\nin the Appendix BUser Study Experiment. To assess the part correspon-\ndence quality and overall quality of the generated content,\nwe asked users to compare our results with those from\nDreamFusion (SD3) and MVDream. We showed 24 users\nmulti-view images generated from 20 random prompts and\nasked them to choose the best one. The results, shown\nin Figure 10, clearly demonstrate that DreamBeast outper-\nforms existing methods. This suggests that our method ef-\nfectively understands part compositions and generates high-\nquality 3D results.\nQualitative Results. As illustrated in Figure 9, Dream-\nBeast demonstrates the ability to generate part-aware an-\nimals, with each relevant part closely adhering to its de-\nscription. In contrast, MVDream produces assets with glob-\nally mixed animal features. Similarly, Dreamfusion (SD3)\nstruggles to generate part-aware results, and both its 3D\nshape and texture quality are lacking, as discussed in Sec-\ntion 3.1.\nAblation Study. We examined the impact of the number\nof views on the extracted Part-Affinity maps, as shown in\nTable 3. Increasing the number of views enhances the con-\ntext of part affinity, enabling the optimized Part-Affinity\nNeRF to better interpolate under unseen camera poses. This\nresults in more accurate part-knowledge integration during\nthe SDS process, which improves the CLIP similarity score.\nMoreover, we observed that performance gains diminish be-\nyond 64 views, suggesting that our method can effectively\ncapture complete Part-Affinity information in 3D with a rel-\natively small number of views. We provide additional anal-\n8ysis of our design choices in Appendix A.\n5. Conclusion\nThis work incorporates Part-Affinity knowledge to address\nthe challenges associated with a limited part-level under-\nstanding of SDS-based 3D asset generation methods. Our\nproposed DreamBeast exhibits high precision in generat-\ning 3D assets with detailed part components, outperforming\nexisting techniques in terms of both quality and efficiency.\nThis contributes to the advancement of the field of creative\nand complex 3D content creation, paving the way for the de-\nvelopment of more detailed and imaginative digital worlds.\nAcknowledgments. The authors would like to thank Paul\nEngstler for his insightful feedback on the manuscript.\nReferences\n[1] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun\nHao, Serge Belongie, Noah Snavely, and Bharath Hariharan.\nLearning gradient fields for shape generation. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part III 16 , pages\n364\u2013381. Springer, 2020. 2\n[2] Siddhartha Chaudhuri, Evangelos Kalogerakis, Stephen\nGiguere, and Thomas Funkhouser. Attribit: content creation\nwith semantic attributes. In Proceedings of the 26th annual\nACM symposium on User interface software and technology ,\npages 193\u2013202, 2013. 4\n[3] Kevin Chen, Christopher B Choy, Manolis Savva, An-\ngel X Chang, Thomas Funkhouser, and Silvio Savarese.\nText2shape: Generating shapes from natural language by\nlearning joint embeddings. In Computer Vision\u2013ACCV 2018:\n14th Asian Conference on Computer Vision, Perth, Australia,\nDecember 2\u20136, 2018, Revised Selected Papers, Part III 14 ,\npages 100\u2013116. Springer, 2019. 2\n[4] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free\nlayout control with cross-attention guidance. arXiv preprint\narXiv:2304.03373 , 2023. 4\n[5] Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui\nJia, and Ziwei Liu. Comboverse: Compositional 3d as-\nsets creation using spatially-aware diffusion guidance. arXiv\npreprint arXiv:2403.12409 , 2024. 3, 4\n[6] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu.\nText-to-3d using gaussian splatting, 2024. 3\n[7] Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei\nZhang, Jian Zhang, and Li Yuan. Progressive3d: Progres-\nsively local editing for text-to-3d content creation with com-\nplex semantic prompts, 2023. 3\n[8] Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag\nArnab, Paul Hongsuck Seo, and Seungryong Kim. Cat-seg:\nCost aggregation for open-vocabulary semantic segmenta-\ntion, 2024. 6\n[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, KianaEhsani, Aniruddha Kembhavi, and Ali Farhadi. Obja-\nverse: A universe of annotated 3d objects. arXiv preprint\narXiv:2212.08051 , 2022. 3\n[10] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. 2023. 4\n[11] Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A. Efros,\nand Aleksander Holynski. Disentangled 3d scene generation\nwith layout learning, 2024. 3, 4\n[12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M \u00a8uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim\nDockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yan-\nnik Marek, and Robin Rombach. Scaling rectified flow trans-\nformers for high-resolution image synthesis, 2024. 2, 3, 7,\n1\n[13] Thomas Funkhouser, Michael Kazhdan, Philip Shilane,\nPatrick Min, William Kiefer, Ayellet Tal, Szymon\nRusinkiewicz, and David Dobkin. Modeling by example.\nACM transactions on graphics (TOG) , 23(3):652\u2013663, 2004.\n4\n[14] Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger,\nand Bernhard Sch \u00a8olkopf. Graphdreamer: Compositional\n3d scene synthesis from scene graphs. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 21295\u201321304, 2024. 4\n[15] Junlin Han, Filippos Kokkinos, and Philip Torr. Vfusion3d:\nLearning scalable 3d generative models from video diffusion\nmodels. European Conference on Computer Vision (ECCV) ,\n2024. 2, 7, 8\n[16] Yutong He, Ruslan Salakhutdinov, J. Zico Kolter, and Mona\nLisa. Localized text-to-image generation for free via cross\nattention control. ArXiv , abs/2306.14636, 2023. 4\n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control. arXiv preprint\narXiv:2208.01626 , 2022. 4\n[18] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\nTan. Lrm: Large reconstruction model for single image to\n3d.arXiv preprint arXiv:2311.04400 , 2023. 2, 7, 8\n[19] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xi-\nanbiao Qi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang.\nDreamwaltz: Make a scene with complex 3d animatable\navatars. arXiv:2305.12529 , 2023. 3\n[20] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rup-\nprecht, and Andrea Vedaldi. Farm3D: Learning articu-\nlated 3d animals by distilling 2d diffusion. arXiv preprint\narXiv:2304.10535 , 2023. 3\n[21] Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne\nKoller, and Vladlen Koltun. A probabilistic model for\ncomponent-based shape synthesis. Acm Transactions on\nGraphics (TOG) , 31(4):1\u201311, 2012. 4\n[22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk \u00a8uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics , 42\n(4), 2023. 5\n9[23] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and\nJun-Yan Zhu. Dense text-to-image generation with attention\nmodulation. In ICCV , 2023. 4\n[24] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas\nJakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi,\nand Jiajun Wu. Learning the 3d fauna of the web. arXiv\npreprint arXiv:2401.02400 , 2024. 3\n[25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takik"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "Below is a comprehensive summary of the paper \"AnySkin: Plug-and-play Skin Sensing for Robotic Touch\" by Raunaq Bhirangi and colleagues, formulated to enable replication and further research:\n\n---\n\n### 1. Abstract\n\nThe paper presents **AnySkin**, a versatile, replaceable, and generalizable magnetic tactile sensor for robotic touch. The sensor builds on the simplicity of ReSkin, with enhancements that decouple the sensing electronics from the sensing interface. AnySkin is characterized by three major contributions: an easy-to-assemble sensor with a streamlined fabrication process and a design tool, improved capabilities in slip detection and policy learning, and demonstrated zero-shot generalization of learned manipulation policies to new sensor instances.\n\n### 2. Introduction\n\n**Context and Motivation:**\nTouch sensing is essential for biological movement and control, yet its adoption in robotics lags behind vision and proprioception. This lag is attributed to high costs, inconvenience, and inconsistency associated with tactile sensors.\n\n**Objective:**\nThe paper aims to address these challenges by introducing AnySkin, which is designed to be cheap, convenient, and consistent across different instances.\n\n**Research Questions:**\n1. What are the key technological advancements allowing AnySkin to be easily fabricated and replaced?\n2. How effective is AnySkin at detecting slip and learning manipulation policies?\n3. How well do learned policies transfer to new instances of AnySkin compared to other tactile sensors?\n\n**Literature Review:**\nEarlier works focus primarily on vision and proprioception for training manipulation skills, with limited use of touch sensing due to its drawbacks. The advances in camera technology (low cost, ease of integration, and consistency) are lacking in tactile sensors.\n\n### 3. Related Work\n\n**Capacitative Sensors:**\nHigh sensitivity but prone to breakage and less robust in modeling shear forces.\n\n**Resistive Sensors:**\nDurable but spatially discrete with low resolution.\n\n**Optical Sensors:**\nHigh-resolution but constrained form factors limit integration versatility.\n\n**Magnetic Tactile Sensors:**\ne.g., ReSkin and uSkin, known for robustness, form factor compatibility, and shear force detection. AnySkin builds on the magnetic-based approach due to its ease of fabrication and lower cost.\n\n**Replaceability Concerns:**\nMany tactile sensors suffer from inconsistent signal responses, making it difficult to replace sensors without impacting model performance. AnySkin aims to overcome these replaceability issues by design enhancements.\n\n### 4. Method\n\n**Design and Fabrication:**\n1. **Magnetization:**\n   - Switch from a grid of cube magnets (used in ReSkin) to a pulse magnetizer to generate stronger and more consistent magnetic fields.\n   - Magnetizing skins post-curing to maintain a uniform distribution of magnetic particles.\n2. **Materials and Composition:**\n   - Mixture of Smooth-On DragonSkin 10 Slow elastomer and fine Magnequench MQFP-15-7(25 \u00b5m) magnetic particles.\n3. **Molds:**\n   - Two-part molds with alignment guides for self-adherence and eliminating manual alignment, enhancing durability and consistency.\n4. **Self-Adhering Design:**\n   - Avoids the need for adhesives or fasteners, which degrade over time.\n   \n**Slip Detection and Policy Learning:**\n1. **Data Collection:**\n   - Using a robotic setup (Kinova Jaco arm, Onrobot RG-2 gripper) to collect tactile data across diverse objects.\n2. **Machine Learning Models:**\n   - LSTM for slip detection.\n   - Behavior cloning with the BAKU transformer architecture for task policies.\n\n### 5. Experimental Setup\n\n**Datasets:**\n- Robotic manipulation tasks (e.g., plug insertion, card swiping, USB insertion).\n- Diverse daily objects (40 items) for slip detection experiments.\n\n**Hardware:**\n- Kinova Jaco arm, Onrobot RG-2 gripper, X-Arm 7DOF robot, and Meta Quest 3 for teleoperation.\n\n**Software:**\n- Open-Teach for teleoperation, ResNet-18 for image encoding, MLP for tactile data encoding, and transformers for policy learning.\n\n**Evaluation Metrics:**\n- Success rate of manipulation tasks.\n- Replaceability time and reusability after replacement.\n- Generalization ability of learned policies to new sensor instances.\n\n### 6. Results and Analysis\n\n**Signal Strength and Consistency:**\n- Pulse magnetizer significantly improves magnetic field strength.\n- Finer magnetic particles and physical separation enhance signal consistency.\n- Self-aligning design ensures minimal signal variability.\n\n**Slip Detection:**\n- LSTM model achieves 92% accuracy in detecting slip on unseen objects.\n\n**Replaceability:**\n- AnySkin demonstrates significantly faster replacement time (12 seconds) and maintains reusability, in contrast to ReSkin and DIGIT.\n\n**Policy Generalization:**\n- Success rates show minimal performance drop (average 15.6%) when transferring from the training to a new AnySkin instance.\n- AnySkin outperforms ReSkin and DIGIT in maintaining policy performance post-replacement.\n\n**Detailed Empirical Findings:**\n- **Table I**: Comparison of signal strength and consistency.\n- **Table II**: Replacement time across different sensors.\n- **Table III**: Success rates for policy tasks with original vs. new sensor instances.\n\n### 7. Discussion\n\n**Interpretation of Results:**\n- AnySkin's strengthened design innovations contribute to its replaceability and signal consistency.\n- Significant drop in performance with ReSkin and DIGIT upon replacement underlines AnySkin's advancements.\n- LSTM-based slip detection demonstrates that tactile sensing can be robust and reliable for varied object interactions.\n\n**Implications:**\n- AnySkin facilitates large-scale tactile data collection and model training.\n- Enhances viability of tactile sensing for precise, contact-rich tasks in real-world robotics applications.\n  \n**Limitations and Biases:**\n- Magnetic interference in environmental settings remains a challenge.\n- Future work should explore noise reduction techniques and improved fabrication standardization.\n\n### 8. Conclusion\n\n**Summary:**\nAnySkin represents a significant leap towards integrating tactile sensing in robotic manipulation tasks, boasting advantages in cost, ease of replacement, and signal consistency. \n\n**Future Research:**\n- Investigate calibration schemes to further iron out instance variability.\n- Explore broader applications in different robotic systems and environments.\n- Address remaining environmental interference challenges.\n\nThis comprehensive summary offers the detailed technical and empirical foundation necessary for replicating and extending the research presented in the paper."}, {"role": "assistant", "content": "AnySkin: Plug-and-play Skin Sensing for Robotic Touch\nRaunaq Bhirangi1,2,\u2217, Venkatesh Pattabiraman1, Enes Erciyes1, Yifeng Cao3, Tess Hellebrekers4, Lerrel Pinto1\nhttps://any-skin.github.io\nLEAP Hand Dobb-E stick\n FrankaCompatible with a variety of robots Swappable for learned policies\nAnySkin\nReSkin\nDIGIT\n0 20 40 60 80\nTraining\nNew Skin\nSuccess Rate, in %\nEasy to assemble\nAnySkin\nFingertip\nReSkin circuit\nFig. 1: We present AnySkin, a skin sensor made for robotic touch that is easy to assemble, compatible with different robot end-effectors\nand generalizes to new skin instances. AnySkin senses contact through distortions in magnetic field generated by magnetized iron particles\nin the sensing surface. The flexible surface is physically separated from its electronics, which allows for easy replacability when damaged.\nAbstract \u2014 While tactile sensing is widely accepted as an\nimportant and useful sensing modality, its use pales in compar-\nison to other sensory modalities like vision and proprioception.\nAnySkin addresses the critical challenges that impede the use of\ntactile sensing \u2013 versatility, replaceability, and data reusability.\nBuilding on the simplistic design of ReSkin, and decoupling\nthe sensing electronics from the sensing interface, AnySkin\nsimplifies integration making it as straightforward as putting\non a phone case and connecting a charger. Furthermore,\nAnySkin is the first uncalibrated tactile-sensor with cross-\ninstance generalizability of learned manipulation policies. To\nsummarize, this work makes three key contributions: first, we\nintroduce a streamlined fabrication process and a design tool\nfor creating an adhesive-free, durable and easily replaceable\nmagnetic tactile sensor; second, we characterize slip detection\nand policy learning with the AnySkin sensor; and third, we\ndemonstrate zero-shot generalization of models trained on one\ninstance of AnySkin to new instances, and compare it with\npopular existing tactile solutions like DIGIT and ReSkin.\nI. I NTRODUCTION\nTouch sensing is widely recognized as a crucial modality\nfor biological movement and control [1], [2]. Unlike vision,\nsound, or proprioception, touch provides sensing at the\npoint of contact, allowing agents to perceive and reason\nabout forces and pressure. However, a closer examination\nof robotics literature reveals a different narrative. Prominent\n1New York University, New York, NY , USA\n2Carnegie Mellon University, Pittsburgh, PA, USA\n3Columbia University, New York, NY , USA\n4Meta AI Research, USA\n\u2217Correspondence to raunaqbhirangi@nyu.eduworks and current state-of-the-art in robot learning primarily\nutilize vision sensing in conjunction with proprioception to\ntrain manipulation skills [3], [4], often ignoring touch. If\ntouch is indeed vital from a biological perspective, why does\nit remain a second-class citizen in sensorimotor control?\nTo address this question, let\u2019s examine what made cam-\neras ubiquitous in robotics. Three key factors are at play:\ncost, convenience, and consistency. Cameras are relatively\ninexpensive (under $20), easy to integrate on a wide variety\nof robot platforms (e.g. multi-view, depth, ego-centric), and\nallow for models trained on them (e.g. object detection,\nsegmentation) to easily transfer to images captured with\nnew cameras. In contrast, touch sensors are often costly\ndue to expensive fabrication processes [5] (e.g., pressure-\nbased sensors) or the need for high-end components [6]\n(e.g., Biotac). They are inconvenient to use on different\nrobot platforms, being custom-built for specific robot end-\neffectors and constrained form factors requiring extensive\nadaptation for different shapes [7], [8]. Finally, touch sensors\nare inconsistent. Due to boutique fabrication, sensor profiles\ncan vary significantly even when produced through the same\nprocess [9], [10]. This inconsistency poses a challenge when\ntransferring tactile-based models across different instances of\nthe same sensor. This transfer is particularly critical for touch\nsensors due to their persistent need for replacement. Soft\nsensing interfaces \u2013 imperative to touch sensors to preserve\ngrasp stability \u2013 wear out faster than hard interfaces, leading\nto more frequent replacements.\nIn this work we present AnySkin, a new touch sensor thatarXiv:2409.08276v1  [cs.RO]  12 Sep 2024is cheap, convenient to use and has consistent response across\ndifferent sensor instances. AnySkin builds on ReSkin [9], a\nmagnetic-field based touch sensor, by improving its fabrica-\ntion, separating the sensing mechanism from the interaction\nsurface, and developing a new self-adhering, self-aligning\nattachment mechanism. This allows AnySkin to (a) have\nstronger magnetic fields, which significantly improves its\nsensor response, (b) be easy to fabricate for arbitrary surface\nshapes, which allows easy use on different end-effectors, (c)\nbe easy to replace the sensor without adversely affecting the\ndata collection process or the efficacy of models trained on\nprevious sensors (Fig. 1).\nWe run a suite of experiments to understand the efficacy of\nAnySkin vis-a-viz other prominent touch sensors. Our main\nfindings can be summarized below:\n1) AnySkin can readily be used on a variety of robots\nincluding xArm, Franka, and the four-fingered Leap\nhand (See fabrication details in Section IV).\n2) AnySkin is compatible with ML techniques for slip\ndetection and visuo-tactile policy learning for precise\ntasks such as inserting USBs (See learning details in\nSection V).\n3) AnySkin takes an average of 12 seconds to replace\nand can be reused after replacement (See replacement\nstudy in Section V-C).\n4) Models trained on one AnySkin transfer zero-shot to\na different AnySkin with only a 13% reduction in\nperformance on a plug insertion task compared to the\n43% drop in performance with ReSkin [9] sensors.\nAnySkin is fully open-sourced. Videos of fabrication,\nattachment, and robot policies are best viewed on our project\nwebsite: https://any-skin.github.io/ .\nII. R ELATED WORK\nA. Tactile sensing\nExisting literature on tactile sensing explores a wide range\nof modalities, each with their own set of advantages and\nlimitations. Capacitative sensors [11], [12] sense contact\nthrough changes in capacitance, offering high sensitivity.\nHowever, they struggle to model shear force and are prone\nto breakage due to direct electrical connections between the\ncircuitry and elastomer. Resistive sensors [5], [13], [14] are\nsimple and durable, but tend to provide spatially discrete\nsensing with low spatial resolution. Optical sensors [8],\n[15], [16] capture high-resolution contact information using\ncameras to track the deformation of an elastomer, but often\npose hard, stringent limits on the sensor form factor, due\nto the physical constraints on the camera field of view.\nThis complicates integration for a wide range of applications\nand significantly increases the effort required to sensorize\nsurfaces of different shapes and sizes.\nMagnetic tactile sensors [17], [18] largely overcome these\nlimitations due to three salient advantages: (a) separation of\nthe sensing electronics from the sensing interface to improve\nrobustness (b) compatibility with different form factors, and\n(c) an ability to capture shear forces [9]. Two prominentclasses of magnetic sensors in robotics right now - ReSkin [9]\nand uSkin [18] - use elastomeric sensing interfaces with mag-\nnetic microparticles and macro-sized magnets respectively. In\nthis work, we build on ReSkin sensors due to their lower cost\nand ease of fabrication.\nB. Replaceability for Tactile Sensors\nRecent developments in rapid prototyping and elastomer\ntechnology have spurred a substantial rise in the number\nof robotic tactile sensors. Most tactile solutions rely on\nsoft sensing interfaces to enable stable conformal contact\nwith objects in the environment. Soft interfaces are prone\nto frequent wear and tear from contact-rich interactions, but\ndiscussions on replaceability for tactile sensors remain few\nand far between. There are two main factors to consider\nwhen evaluating replaceability: (a) the physical ease of\nreplacing the sensory interface, and (b) signal consistency\nwhen replacing a worn out instance with a new instance. The\nformer is far more frequently discussed [6], [16], [9], [19]\nand resolved by simply separating the sensing interface \u2013\ngenerally the damage-prone soft elastomer \u2013 from the sensing\nelectronics which last much longer. The latter, however,\nis much less discussed. Prior work in tactile sensing has\nfound significant variation across different instances of the\nsame tactile sensor [9], [10]. Higher susceptibility to wear\ncombined with a lack of generalizability across instances\nseverely restricts the scale of data explored in most existing\nwork on tactile learning [20], [21]. Signal consistency across\ninstances is central to building scalable and generalizable\ntactile models, and making tactile sensing a ubiquitous\npresence in robot learning. In Section V, we quantitatively\ndemonstrate the improved consistency of AnySkin signal\nover ReSkin, and present a direct replaceability comparison\nwith the DIGIT and ReSkin sensors through policy learning\nexperiments.\nIII. A NYSKIN: COMPONENTS\nAnySkin builds on ReSkin [9], a tactile skin composed\nof a soft magnetized skin coupled with magnetometer-based\nsensing circuitry. By detecting distortions in magnetic fields,\nReSkin measures skin deformations caused by normal and\nshear forces [17], [9]. Its adaptability enables integration\nacross various applications, from robotic hands [20] to arm\nsleeves and even dog shoes. AnySkin uses the same 5-\nmagnetometer circuitry as ReSkin, while introducing key de-\nsign and fabrication changes to the skin to improve durability,\nrepeatability, and replaceability.\n\u2022Magnetizing skins post-curing using a pulse magnetizer\n\u2022Introducing physical separation between magnetic elas-\ntomer and magnetometer circuit\n\u2022Utilizing finer magnetic particles to achieve a more\nuniform particle distribution\n\u2022Implementing a self-aligning design for reduced vari-\nability in the positioning of elastomers and circuitry\nWhile some of these additions have been used in isolation in\nprior work [20], [22], there has been little discussion on their\neffect on sensor response. In this section, we elaborate on theOutlet cavity\nInlet cavity\nAlignment guides\nSmooth-On \nDragonSkin 10 Slow\nMagnequench \nMQFP-15-7 (25\u03bcm)+\nPulse Magnetizer\n AnySkin(a) Fabrication of AnySkin\n (b) Top and bottom surfaces of skins\nFig. 2: (a) AnySkin is made by mixing Smooth-On DragonSkin 10 Slow and MQFP-15-7(25 \u00b5m) magnetic particles in a 1:1:2 ratio, and\ncuring it in the two-part molds shown above. Cured skins are magnetized using a pulse magnetizer. (b) Skins made with MQP-15-7(-80\nmesh) and MQFP-15-7(25 \u00b5m) particles. Note the concentration of particles at the surface of the former due to the larger particle size.\nrationale for each of these choices, followed by a quantitative\ncomparison of the sensor response in Section V-A.\nReSkin uses a grid of cube magnets during curing to\nmagnetize the elastomeric skins. While effective, this ap-\nproach has several drawbacks, such as producing skins with\nrelatively weak magnetic fields. As a result, although the\ndesign of ReSkin separates the sensing electronics and the\nsensing interface, adding physical distance between the skin\nand the sensors significantly weakens the signal, making it\ninfeasible in reality. In contact-rich tasks, where the sensing\nskin undergoes considerable strain, the absence of physical\nseparation leads to stress being transmitted directly to the\nelectronics, ultimately compromising their durability.\nAdditionally, applying a magnetic field during elastomer\ncuring increases variability in the signal response. Before\ncuring, magnetic particles are free to move through the liquid\nelastomer under the effect of the magnetic field. As a result,\nthe distribution of particles is influenced by the temporal\nevolution of the applied magnetic field, ie. how you move\nthe magnets into place, which can be difficult to control\nwhen fabricating. To circumvent these disadvantages, we\npropose using a pulse magnetizer to magnetize the skins\nafter curing in line with [20], [22] as shown in Fig. 2a. The\npulse magnetizer can apply a large enough magnetic field\nto magnetize the dipoles in the magnetic elastomer. Curing\noutside the influence of magnetic fields allows for a more\nuniform distribution of magnetic particles through the bulk\nof the sensor.\nHowever, simply changing the magnetization procedure\nresults in other problems. Curing outside the influence of a\nmagnetic field causes the particles to settle to the bottom\nof the sensing skin due to gravity as shown in Fig. 2b. This\nresults in lower durability as the skin begins to shed magnetic\nparticles during contact-rich interactions. To get around this\nproblem, we replace the magnetic particles with much finer\nparticles (details in Section IV-B). The smaller particles\noperate in a sufficiently low Reynolds number regime to\nallow the elastomer to cure before they can settle on one\nsurface of the elastomer.\nFinally, since ReSkin relies on magnetic field distortions to\nmeasure contact characteristics, sensor response is stronglyinfluenced by the relative positioning of the magnetic skin\nand the magnetometer circuitry (see Section V-A). This\nmeans that loss of adhesion, or peeling, over the life of the\nsensing skin can significantly affect the consistency of the\nsignal. Ideally, we would like the skin to stay adhered until\nit needs to be replaced due to wear and tear. Using screws to\nadhere the skin as suggested in [9] results in poor durability\ndue to a stress concentration at the screw-skin interface,\nespecially in tasks involving shear forces. Using an adhesive\nlike Silpoxy [16], [20] can be used to create sticker-like skins\nthat last relatively longer but still tend to peel under repetitive\ncyclic loading. With AnySkin, we eliminate the need for\nadhesives or fasteners by modifying the design of the skins\nto be self-adhering. Additionally, we also eliminate the need\nto manually align skin and circuitry, which can significantly\naffect signal consistency as demonstrated in Section V-A.\nIV. A NYSKIN: FABRICATION\nThe overall fabrication procedure follows the general\noutline of ReSkin: Magnetic particles and elastomer are\nmixed in specified ratios; the resulting mixture is poured\ninto the molds; cured skins are magnetized. The shape of\nthe fingertip-skin assembly is designed to be triangular as\nshown in Figure 1 to improve reachability. In this section,\nwe elaborate on the details of the fabrication procedure\nfor AnySkin, and key changes to the ReSkin fabrication\nprocedure that result in a new, upgraded sensor.\nA. Mold design\nThe shape of the magnetic skin is dictated by the molds\nused for curing. To create self-adhering skins as outlined in\nSection III, we present a two-part mold design as shown in\nFig. 2a. We choose a skin thickness of 2 mm following [9]\nwith a triangular shape for its advantageous form factor. All\nthe experiments presented in this paper use this triangular\nskin. We also open-source a mold design CAD tool that\ngenerates designs for the fingertip as well as 2-part molds\nfrom just a 2D drawing. Unlike tactile sensors that require\nsignificant engineering for changes in form factor [16], [8],\nAnySkin makes it effortless to diversify your tactile sensor.TABLE I: AnySkin\u2019s signal strength is comparable to ReSkin with a significantly lower variability across skin instances, while being\nphysically separated from the magnetometer electronics. Statistics computed over 5 samples of each type (PM: Pulse magnetizer, FP: finer\nparticles, SA: self-aligning)\nExperiment\nReSkin +PM +FP +SA (AnySkin)\nBxy BzBxy Bz Bxy Bz Bxy Bz\nAverage strength, in \u00b5T 1062 302 1818 5212 1602 5784 283 1265\nNormalized std. deviation across instances 0.54 0.87 0.34 0.12 0.21 0.15 0.12 0.10\nNormalized std. deviation across 1mm misalignments 1.38 1.43 0.25 0.11 0.18 0.07 Self-aligning\nB. Elastomer composition\nFor AnySkin, we mix magnetic microparticles and two-\npart polymer (Dragonskin 10 Slow; Smooth-On) in the\nsame 2:1:1 ratio as ReSkin, while using finer Magnequench\nMQFP-15-7(25 \u00b5m). These particles are about 100x smaller\nthan the MQP-15-7(-80 mesh) used with ReSkin, and do not\nsettle before curing, due to their lower Reynolds number.\nThis ensures that magnetic particles are more evenly dis-\ntributed through the volume of the skin, thereby improving\nconsistency of the signal.\nC. Magnetization\nReSkin is magnetized by sandwiching the magnetic elas-\ntomer mix between a grid of magnets while it is curing. This\nresults in higher variance in distribution of magnetic particles\nwithin the core of the skin based on the exact timing of\nsandwiching the skins. Drawing from D\u2019Manus [20], we use\na pulse magnetizer for magnetizing the skins after curing\nis complete. Separating the magnetizing process from the\ncuring process allows the skins to cure undisturbed and\nmaintain a more uniform distribution of magnetic particles.\nFurthermore, the magnetic field applied by the pulse mag-\nnetizer is far stronger than the sandwich of grid magnets.\nThis results in skins with stronger magnetic fields, which in\nturn enables larger separations between magnetic skin and\nmagnetometer circuitry.\nD. Magnetic elastomer fabrication\nThe final fabrication process follows similar steps as\nthe ReSkin fabrication process. The molds are first aligned\nusing the built-in alignment guides and clicked together. We\nuse plastic clamps to hold the parts together. The two-part\nelastomer compound is then mixed and degassed. This is\nfollowed by the addition of magnetic micro particles and\nanother round of mixing and degassing. Once degassing is\ncomplete, the magnetic elastomer mix is poured through the\nmold inlet as shown in Fig. 2 until it emerges at the outlet,\npausing as necessary to allow the mixture to flow through\nand fill the entire mold. The filled mold is then placed in a\nvacuum chamber and a pressure of 29mm of Hg/in is applied,\nagain pausing as necessary to prevent overflow as the liquid\nbubbles. This pressure is held for 10 minutes before releasing\nthe vacuum. The molds are allowed to rest for 16 hours,before prying them open and trimming excess material to\nreveal the fully cured AnySkin.\nV. E XPERIMENTS AND RESULTS\nIn this section, we perform extensive experiments to\ndemonstrate the capabilities of AnySkin as a tactile sensor,\nand within the context of policy learning. These experiments\nare designed to answer the following questions:\n\u2022How do the fabrication changes outlined in Section III\ninfluence signal characteristics?\n\u2022Can AnySkin sensors be used to detect slip?\n\u2022How does AnySkin\u2019s ease of replaceability compare\nwith other sensors like DIGIT and ReSkin?\n\u2022How does replacing AnySkin affect the performance of\nlearned policies, and how does this compare with other\nsensors like ReSkin and DIGIT?\nA. Comparison between ReSkin and AnySkin signal\nTo quantitatively demonstrate the effect of each of the\nfabrication changes listed in Section III towards improving\nthe consistency of AnySkin, we present the following set of\nexperiments analyzing the raw signal from the four different\nskins shown in Table I, tracking the progression from ReSkin\nto AnySkin:\n1) Effect of pulse magnetizer on signal strength: To un-\nderstand the effect of the pulse magnetizer on signal strength,\nwe take five instances of each skin type and measure the\nraw signal corresponding to each instance. We average the\nabsolute values across the three axes of the five magnetome-\nters, and report the results in Table I. We see a significant\nincrease in the raw magnetic field for both sets of pulse\nmagnetized skins. This increase allows us to add a physical\nseparation between sensing skin and the sensory electronics,\nwhich improves replaceability, as well as repeatability of the\nsignal as discussed below.\n2) Comparison of signal consistency across skins: To\ncompare signal consistency across the sensing skins, we\ncompute the standard deviation along each axis of the five\nmagnetometers across the five instance of each skin type.\nTo account for the larger signal strengths of the pulse\nmagnetized skins and allow for a fairer comparison, we\nnormalize the computed standard deviations by the mean\nabsolute values along xyandzaxes for each skin type.\nAggregated statistics for the different skins are presentedTraining objects Data collection setup\nTest objects\nFig. 3: Experimental setup used for slip detection experiments\nin Table I. Note that finer particles reduce the variability in\nsensory signal, which could be attributed to the more uniform\ndistribution of particles resulting from reduced settling under\nthe influence of gravity.\nFig. 4: Effect of circuit-skin alignment on signal consistency\n3) Effect of alignment on signal consistency: Finally, to\nunderstand the importance of the self-aligning design of\nAnySkin, we take a single skin (20mm \u00d720mm) of each\ntype except AnySkin, and collect magnetometer data from\nplacing it at a 1mm offset along each side as shown in Fig. 4.\nNormalized standard deviations are computed across the\naligned and misaligned variants across each skin, and results\nare reported in Table I. Across different skins, variability\nin sensor response from just misalignment is larger than the\ncross-instance variability seen with AnySkin, underlining the\nimportance of the self-aligning design of AnySkin.\nB. Slip Detection\nWe quantify AnySkin\u2019s ability to detect slip of grasped\nobjects through a controlled slip detection experiment. Our\nexperimental setup consists of a Kinova Jaco arm and a\nOnrobot RG-2 gripper with integrated AnySkin. An object,\nthat is held by a human operator, is grasped and liftedup slowly for 1 second. We use a set of 40 daily objects\nwith varying shapes, weights and materials. We collect 6\ntrajectories for each object by changing the grasping force,\nwidth and location. After the data collection is complete, a\nhuman annotator labels the sequence as slip or no-slip from\nthe corresponding videos. Data from 30 objects is used for\ntraining while 10 objects are used for evaluation. Some of\nthe objects used, along with the data collection setup are\nshown in Fig. 3.\nThe data collection frequency for tactile data is 100 Hz.\nWe subsample the signal by 15 along the temporal axis and\ntake the first difference. We use an LSTM [23] to train our\nslip prediction models. Our model is able to detect slip on\nunseen objects with 92% accuracy.\nC. Ease of replaceability\nWe compare the ease of replaceability of AnySkin against\nthe replaceability of other skins like DIGIT and ReSkin. For\nReSkin, we use two different methods of adhering skins to\nthe circuit board \u2013 screws [9] and adhesive stickers [20]. The\nskins are compared along two axes \u2013 replacement time and\nre-usability after replacement \u2013 through a user study with 10\nnon-expert users, and the results are presented in Table II\nTABLE II: Comparison of replaceability of different sensors\nSensor Time to replace, in s Reusable\nReSkin (adhesive) 82\u00b164 No\nReSkin (screws) 236\u00b164 Yes\nDIGIT 58\u00b122 Yes\nAnySkin 12\u00b15 Yes1\n123\n4\n1\n2\n3\n4\nUSB Insertion\nCard Swiping\nPlug InsertionFig. 5: Experimental setup and set of tasks used for evaluating skin replaceability in policy learning\nWe find that users found it significantly faster and easier\nto replace AnySkin than any of the other skins used for com-\nparison. Furthermore, eliminating adhesion allows replaced\nskins to be reused without needing the extra hours to reapply\nadhesion and allow it to cure.\nD. Replaceability in Policy Learning\nThe most important consequence of the signal consistency\nand replaceability of AnySkin outlined so far, is its ability\nto enable policy generalization across different instances of\nthe skin. In this section, we compare the cross-instance\ngeneralizability of policies trained on DIGIT, ReSkin and\nAnySkin on a plug insertion task that requires considerable\nprecision. We follow this up by demonstrating the cross-\ninstance generalizability of AnySkin across three more pre-\ncise manipulation tasks.\n1) Experimental Setup: For our policy learning experi-\nments, we train behavior cloning models for a set of precise\nmanipulation tasks. Our experimental setup is shown in\nFig. 5 and consists of an X-Arm 7DOF robot in a robot cage\nas shown in the image. The setup consists of four different\ncameras \u2013 three fixed to the cage and one egocentric camera\nattached to the robot\u2019s wrist. We attach an AnySkin sensor\nto one of the fingertips and a plain silicone tip to the other.\nA Meta Quest 3 and the accompanying joystick controller\nare used to teleoperate the robot using Open-Teach [24], an\nopen-source teleoperation framework.We demonstrate the replaceability of AnySkin on a set of\nthree contact-rich manipulation tasks shown in Fig. 5:\n\u2022Plug insertion : The robot starts with a plug grasped in\nthe gripper. The task requires the robot to move to the\nfirst socket on the socket strip and insert the plug. The\nlocation of the socket strip is randomized in a 20 cm \u00d7\n20 cm box on the table, and learned policies are tested\non socket locations unseen in the training data.\n\u2022Card swiping : The robot starts with a credit card\ngrasped in the gripper. It must move to the location\nof a credit card machine on the table and swipe the\ncredit card. The location of the credit card machine\nis randomized in a 40 cm \u00d715 cm box and angled\nin the range (\u221230\u25e6to30\u25e6)on the table, and learned\npolicies are tested on card machine locations unseen in\nthe training data.\n\u2022USB insertion : The robot starts with a USB cable\ngrasped in the gripper. It must move to the location\nof the USB port on the table and insert the cable. The\nlocation of the USB port is randomized in a 20 cm \u00d7\n20 cm box on the table, and learned policies are tested\non port locations unseen in the training data.\n2) Model Architecture and Training: Our policies are\ntrained using behavior cloning. For each task presented in\nthis section, we collect a set of 96 demonstration trajectories,\nwith data from the four cameras in addition to unchanged\ninstances of the respective tactile sensor(s) being used. TheBAKU [25] architecture is used as the policy architecture.\nBAKU tokenizes each input using a modality-specific en-\ncoder: image inputs from cameras and DIGITs are encoded\nusing ResNet-18 [26] encoders, while AnySkin and ReSkin\ninputs are encoded using an MLP encoder. An action token\nis appended to the set of encoded tokens before passing\nthe sequence through a transformer encoder, and the output\ncorresponding to the action token is used to predict actions.\nWe use action chunking [27] and predict the next 10 actions\nat every timestep. For every training setting, we train three\nseparate models corresponding to three different seeds, and\npresent aggregated statistics on 10 policy rollouts.\n3) Evaluating cross-instance generalizability: To investi-\ngate the replaceability of AnySkin in the context of policy\nlearning, we evaluate behavior cloning policies trained using\na single instance of AnySkin on a new instance. Note\nthat we use a different training and test skin for each of\nthe presented tasks to avoid over-indexing on specific skin\ninstances. Table III presents a comparison between policy\nperformance with the original and swapped skins for each of\nthe precise, contact-rich tasks outlined above. Additionally,\nwe train and evaluate a policy that only takes camera images\nas input to serve as a control experiment and verify that\nthe policies indeed rely on tactile data. We find that across\ntasks, performance drops by an average of just 15.6% and\nvisuotactile policies with swapped skins continue to do\nsignificantly better than purely visual policies. This result\ndemonstrates the strength and uniqueness of AnySkin as a\ntactile sensor for contact-rich manipulation.\n4) Comparison across sensors: To better contextualize\nthe significance of this result, we present a replaceability\ncomparison with DIGIT [16] and ReSkin [9] sensors. We\ncollect two additional datasets of 96 demonstration trajecto-\nries each for the plug insertion task with these sensors similar\nto AnySkin. Replaceability is evaluated by swapping skins\nas outlined in the previous section. Success rates from 10\nevaluations across three seeds for each setting are reported\nin Table III.\nTABLE III: Success rates (out of 10) for policies when swapping\nout tactile skins. All statistics computed over 3 training seeds\nTask Cameras only Cameras + Skin\nOriginal skin Swapped skin\nCross-instance generalization\nPlug Insertion 1.7\u00b10.6 6 .7\u00b11.5 5 .3\u00b12.5\nCard Swiping 2.0\u00b11.0 7 .0\u00b11.7 6 .3\u00b10.6\nUSB Insertion 1.7\u00b11.2 5 .7\u00b11.5 3 .0\u00b11.0\nComparison across sensors \u2013 Plug Insertion\nAnySkin 1.7\u00b10.6 6.7\u00b11.5 5 .3\u00b12.5\nReSkin 1.7\u00b11.2 6 .0\u00b11.7 1 .7\u00b11.2\nDIGIT 1.7\u00b11.5 2 .3\u00b10.6 1 .3\u00b10.6\nBased on these results, we find that visuotactile policies\ntrained with ReSkin and AnySkin have similar performance\non solving the plug insertion task. However, when the skin\nis replaced, the performance of the ReSkin policy falls\n43% to the same level as the camera-only policy, while the\nperformance of AnySkin policies only drops by 13%. This\n20406080100120Fig. 6: Difference between two DIGIT instances (left); Maximum\nchange in DIGIT signal across 96 demos (right)\nform of transferability is a significant boost to scaling efforts\nlike training large tactile models and real world deployment\nof models trained in the laboratory.\nAn unexpected result from these experiments was the\npoor success rate of policies trained using the DIGIT sensor\nwhich has been shown to be successful in other visuotactile\ntasks, perhaps on less precise ones, in prior work [10], [28].\nConsequently, we don\u2019t see a significant gap between the\npoor performance of the visuotactile policy on the original\nand swapped DIGIT skins. However, the high variability\nacross instances of the DIGIT sensors is previously docu-\nmented [10], and we find that a closer look at the DIGIT\nsignal from our plug insertion dataset indicates that even if\nit were possible to train more performant policies, they are\nunlikely to generalize across instances. Across the 96 demon-\nstration trajectories from the plug insertion, we compute the\nmaximum change in pixel values across channels and across\ntrajectories, and compare it against the pixel-wise differences\nbetween the original and a swapped instance of the DIGIT\nsensor in Fig. 6. Given that the maximum change in sensory\nmeasurement is comparable, and arguably smaller, than the\ndifference in signal between two instances, it is unlikely\nthat policies trained on one sensor will generalize to new\ninstances.\nVI. C ONCLUSION AND LIMITATIONS\nIn this paper, we present AnySkin, a new magnetic tactile\nsensor. AnySkin is versatile, self-adhering and improves on\nthe repeatability of sensor response across different instances\nof the skin. Furthermore, to the best of our knowledge,\nAnySkin is the first sensor to demonstrate zero-shot gen-\neralization of visuotactile policies to new instances of the\ntactile skin. As we work towards developing more capable\nand performant models utilizing tactile data, AnySkin is the\nfirst step towards ensuring that large datasets of tactile data\ncan be collected and effectively used for training useful,\ngeneralizable models. This work opens the door to a host\nof exciting applications of AnySkin. It could be incorporated\ninto a large-scale data collection tool such as UMI [29] or the\nStick [30], [31]. Future work could also investigate simple\ncalibration schemes or conditional learning frameworks to\ncompletely close the gap between training and swapped\nskin instances. Deeper investigations into standardizing the\nfabrication procedure could also help further improve signal\nconsistency.Despite its potential, AnySkin still inherits some of the\ndrawbacks of the ReSkin sensor, primary among them being\ninterference from magnetic and ferromagnetic objects in\nthe environment. Using machine learning approaches for\nnoise reduction in magnetic sensors or improving the skin\ndesign to incorporate a Faraday cage could help resolve these\ndifficulties, and take tactile sensing one step closer to being\na first-class citizen in robotics.\nACKNOWLEDGMENTS\nSpecial thanks for Krishna Bodduluri, Mike Lambeta, and\nteam from Meta AI Research for providing the DIGIT sen-\nsors for comparison. NYU authors are supported by grants\nfrom Honda, Hyundai, NSF award 2339096 and ONR awards\nN00014-21-1-2758 and N00014-22-1-2773. LP is supported\nby the Packard Fellowship.\nREFERENCES\n[1] J. Jenner and J. Stephens, \u201cCutaneous reflex responses and their central\nnervous pathways studied in man,\u201d The Journal of physiology , vol.\n333, no. 1, pp. 405\u2013419, 1982.\n[2] R. S. Johansson, \u201cSensory control of dexterous manipulation in\nhumans,\u201d in Hand and brain . Elsevier, 1996, pp. 381\u2013414.\n[3] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Ir-\npan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, et al. , \u201cOpen\nx-embodiment: Robotic learning datasets and rt-x models,\u201d arXiv\npreprint arXiv:2310.08864 , 2023.\n[4] C. Chi, S. Feng, Y . Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song,\n\u201cDiffusion policy: Visuomotor policy learning via action diffusion,\u201d\narXiv preprint arXiv:2303.04137 , 2023.\n[5] S. Sundaram, P. Kellnhofer, Y . Li, J.-Y . Zhu, A. Torralba, and W. Ma-\ntusik, \u201cLearning the signatures of the human grasp using a scalable\ntactile glove,\u201d Nature , vol. 569, no. 7758, pp. 698\u2013702, 2019.\n[6] N. Wettels, V . J. Santos, R. S. Johansson, and G. E. Loeb, \u201cBiomimetic\ntactile sensor array,\u201d Advanced robotics , vol. 22, no. 8, pp. 829\u2013849,\n2008.\n[7] S. Wang, Y . She, B. Romero, and E. Adelson, \u201cGelsight wedge:\nMeasuring high-resolution 3d contact geometry with a compact robot\nfinger,\u201d in 2021 IEEE International Conference on Robotics and\nAutomation (ICRA) . IEEE, 2021, pp. 6468\u20136475.\n[8] I. H. Taylor, S. Dong, and A. Rodriguez, \u201cGelslim 3.0: High-resolution\nmeasurement of shape, force and slip in a compact tactile-sensing\nfinger,\u201d in 2022 International Conference on Robotics and Automation\n(ICRA) . IEEE, 2022, pp. 10 781\u201310 787.\n[9] R. Bhirangi, T. Hellebrekers, C. Majidi, and A. Gupta, \u201cReskin:\nversatile, replaceable, lasting tactile skins,\u201d in 5th Annual Conference\non Robot Learning , 2021.\n[10] S. Suresh, H. Qi, T. Wu, T. Fan, L. Pineda, M. Lambeta, J. Malik,\nM. Kalakrishnan, R. Calandra, M. Kaess, et al. , \u201cNeural feels with\nneural fields: Visuo-tactile perception for in-hand manipulation,\u201d arXiv\npreprint arXiv:2312.13469 , 2023.\n[11] O. Glauser, D. Panozzo, O. Hilliges, and O. Sorkine-Hornung, \u201cDe-\nformation capture via soft and stretchable sensor arrays,\u201d ACM Trans-\nactions on Graphics (TOG) , vol. 38, no. 2, pp. 1\u201316, 2019.\n[12] T.-Y . Wu, L. Tan, Y . Zhang, T. Seyed, and X.-D. Yang, \u201cCapacitivo:\nContact-based object recognition on interactive fabrics using capacitive\nsensing,\u201d in Proceedings of the 33rd annual acm symposium on user\ninterface software and technology , 2020, pp. 649\u2013661.\n[13] T. Bhattacharjee, A. Jain, S. Vaish, M. D. Killpack, and C. C. Kemp,\n\u201cTactile sensing over articulated joints with stretchable sensors,\u201d in\n2013 World Haptics Conference (WHC) . IEEE, 2013, pp. 103\u2013108.\n[14] S. Stassi, V . Cauda, G. Canavese, and C. F. Pirri, \u201cFlexible tactile sens-\ning based on piezoresistive composites: A review,\u201d Sensors , vol. 14,\nno. 3, pp. 5296\u20135332, 2014.\n[15] W. Yuan, S. Dong, and E. H. Adelson, \u201cGelsight: High-resolution robot\ntactile sensors for estimating geometry and force,\u201d Sensors , vol. 17,\nno. 12, p. 2762, 2017.[16] M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Maloon, V . R. Most,\nD. Stroud, R. Santos, A. Byagowi, G. Kammerer, et al. , \u201cDigit: A\nnovel design for a low-cost compact high-resolution tactile sensor with\napplication to in-hand manipulation,\u201d IEEE Robotics and Automation\nLetters , vol. 5, no. 3, pp. 3838\u20133845, 2020.\n[17] T. Hellebrekers, O. Kroemer, and C. Majidi, \u201cSoft magnetic skin for\ncontinuous deformation sensing,\u201d Advanced Intelligent Systems , vol. 1,\nno. 4, p. 1900025, 2019.\n[18] T. P. Tomo, M. Regoli, A. Schmitz, L. Natale, H. Kristanto, S. Somlor,\nL. Jamone, G. Metta, and S. Sugano, \u201cA new silicone structure for\nuskin\u2014a soft, distributed, digital 3-axis skin sensor and its integration\non the humanoid robot icub,\u201d IEEE Robotics and Automation Letters ,\nvol. 3, no. 3, pp. 2584\u20132591, 2018.\n[19] \u201cGelsight mini,\u201d https://www.gelsight.com/gelsightmini/, 2023.\n[20] R. Bhirangi, A. DeFranco, J. Adkins, C. Majidi, A. Gupta, T. Helle-\nbrekers, and V . Kumar, \u201cAll the feels: A dexterous hand with large-area\ntactile sensing,\u201d IEEE Robotics and Automation Letters , 2023.\n[21] S. Funabashi, G. Yan, A. Geier, A. Schmitz, T. Ogata, and S. Sug-\nano, \u201cMorphology-specific convolutional neural networks for tactile\nobject recognition with a multi-fingered hand,\u201d in 2019 International\nConference on Robotics and Automation (ICRA) . IEEE, 2019, pp.\n57\u201363.\n[22] R. Bhirangi, C. Wang, V . Pattabiraman, C. Majidi, A. Gupta, T. Helle-\nbrekers, and L. Pinto, \u201cHierarchical state space models for continuous\nsequence-to-sequence modeling,\u201d in Forty-first International Confer-\nence on Machine Learning , 2024.\n[23] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\nComputation , vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[24] A. Iyer, Z. Peng, Y . Dai, I. Guzey, S. Haldar, S. Chintala, and\nL. Pinto, \u201cOpen teach: A versatile teleoperation system for robotic\nmanipulation,\u201d arXiv preprint arXiv:2403.07870 , 2024.\n[25] S. Haldar, Z. Peng, and L. Pinto, \u201cBaku: An efficient transformer for\nmulti-task policy learning,\u201d arXiv preprint arXiv:2406.07539 , 2024.\n[26] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2016, pp. 770\u2013778.\n[27] T. Z. Zhao, V . Kumar, S. Levine, and C. Finn, \u201cLearning fine-grained\nbimanual manipulation with low-cost hardware,\u201d arXiv preprint\narXiv:2304.13705 , 2023.\n[28] H. Qi, B. Yi, S. Suresh, M. Lambeta, Y . Ma, R. Calandra, and J. Malik,\n\u201cGeneral in-hand object rotation with vision and touch,\u201d in Conference\non Robot Learning . PMLR, 2023, pp. 2549\u20132564.\n[29] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake,\nand S. Song, \u201cUniversal manipulation interface: In-the-wild robot\nteaching without in-the-wild robots,\u201d arXiv preprint arXiv:2402.10329 ,\n2024.\n[30] N. M. M. Shafiullah, A. Rai, H. Etukuru, Y . Liu, I. Misra, S. Chin-\ntala, and L. Pinto, \u201cOn bringing robots home,\u201d arXiv preprint\narXiv:2311.16098 , 2023.\n[31] H. Etukuru, N. Naka, Z. Hu, S. Lee, J. Mehu, A. Edsinger, C. Paxton,\nS. Chintala, L. Pinto, and N. M. M. Shafiullah, \u201cRobot utility models:\nGeneral policies for zero-shot deployment in new environments,\u201d arXiv\npreprint arXiv:2311.16098 , 2023."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract:\n\nThis paper presents an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories extracted from in-the-wild videos. The methodology involves retargeting human motions to robot actions using a shared 3D space and a physically grounded simulator. The authors develop a generative model that captures flexible manipulation priors and demonstrate that finetuning this base policy with reinforcement learning (RL) and behavior cloning (BC) significantly improves sample efficiency, robustness, and generalizability. Qualitative results are available on their project page.\n\n### 2. Introduction:\n\nThe introduction covers the potential of reusable sensorimotor representations to enhance robotic capabilities, paralleling advances in other areas of AI. It emphasizes the abundance and diversity of human activity videos as a data source for training and the challenges inherent in extracting actionable data from these videos. The paper critiques prior work focusing on specific aspects like visual representation and motion estimation, highlighting their limitations due to misalignment with robotic tasks. The key innovation of this work is lifting both the hand and object into a 3D space and using this for general manipulation pretraining, which is empirically shown to improve downstream task performance.\n\n### 3. Related Work:\n\nThe related work section provides a detailed review of prior approaches to learning from videos, including vision-only pretraining and extracting both sensory and motor information. It critiques methods that require alignment with downstream tasks, compromising generality. The section also reviews the recent use of egocentric videos to learn contact-pose models and discusses the limitations of these methods. The authors position their work as capturing broader hand-object dynamics, leveraging recent advances in 3D vision.\n\n### 4. Method:\n\n**4.1 Hand-Object Trajectory Extraction:**\nThe authors use a pipeline that follows recent advances in 3D vision to extract hand-object trajectories. They refine the predicted poses using time-smoothed hand detections and assume a static camera.\n\n**4.2 Re-targeting Human Motions:**\nThe paper formulates the mapping of 3D human-object pose sequences to robot actions as a non-linear optimization problem, solved using low-storage BFGS. Actions are executed in a high-fidelity simulator, which introduces physical realism. Diversity is added through randomization of scene layouts and environmental conditions.\n\n**4.3 Pretraining with Generative Modeling:**\nThe authors generate a dataset of robot-object interactions and train a base policy represented as a causal transformer on a next-action prediction task. The transformer is trained to capture conditional distributions of actions given sensory observations, forming a task-agnostic manipulation prior.\n\n**4.4 Finetuning for Downstream Tasks:**\nThe base policy is finetuned on specific tasks using reinforcement learning (PPO) or behavior cloning, leveraging the pretrained representations for efficient adaptation.\n\n### 5. Experimental Setup:\n\n**5.1 Datasets:**\nThe pretraining dataset combines sequences from DexYCB and 100 Days of Hands datasets, resulting in approximately 70,000 trajectories.\n\n**5.2 Hardware:**\nExperiments are conducted using a 7-DoF xArm robot with an Allegro hand, employing a stereo camera for visual input and IsaacGym for high-fidelity simulations. \n\n**5.3 Evaluation Metrics and Baselines:**\nThe paper evaluates sample efficiency, robustness, and generalization by comparing the proposed method with visual pretraining baselines and demonstration-guided RL. Tasks include Grasp & Drop, Grasp & Pour, Grasp & Lift, Grasp & Throw, and opening a cabinet.\n\n### 6. Results and Analysis:\n\n#### Real-World Tasks:\n- **Grasp & Drop and Grasp & Pour:** Comparable performance across methods, near perfect success rates.\n- **Grasp & Lift:** The proposed method outperformed others by up to 30 percentage points in complex object manipulation tasks.\n  \n#### Simulation Tasks:\n- **Grasp & Lift, Grasp & Throw, Cabinet Opening:** HOP showed superior sample efficiency, outperforming PPO, DAPG, and AMP.\n- **Robustness and Generalization:** HOP-initialized policies were more robust and general, showing better performance under force disturbances and on new objects.\n\n#### Ablation Studies:\n- Experiments revealed that learning from hand-object interactions, as opposed to hand-only motions, leads to better fine-tuning and more robust grasping behaviors.\n\n### 7. Discussion:\n\nThe paper discusses how HOP's ability to capture pre-/post-contact trajectories and human-object interaction physics leads to more effective and generalizable manipulation skills. The authors note that the diversity of tasks covered in the training corpus is crucial for broad generalization. They identify limitations in the fixed scene assumption and potential biases due to misaligned pretraining videos.\n\n### 8. Conclusion:\n\nThe study confirms that HOP enables robots to learn effective manipulation skills efficiently and robustly from human videos. Future directions include improving 3D reconstruction techniques and incorporating multi-object interactions in videos to further enhance the pretraining process.\n\n### Reproducibility:\n\n- **Data Preparation:** Collect videos, use hand-object pose extraction, and apply 3D reconstruction pipeline.\n- **Retargeting:** Implement non-linear optimization for action mapping and use a high-fidelity simulator for trajectory generation.\n- **Model Training:** Use a causal transformer with a next-action prediction objective, fine-tune with PPO or behavior cloning.\n- **Evaluation:** Conduct empirical studies on provided baselines and metrics, using specified real-world and simulated tasks.\n\nBy following the detailed methodology and experimental setup described, the study can be replicated, and its findings can serve as a foundation for further research in robot learning from video demonstrations."}, {"role": "assistant", "content": "Hand-Object Interaction Pretraining from Videos\nHimanshu Gaurav Singh\u2217Antonio Loquercio\u2217Carmelo Sferrazza Jane Wu\nHaozhi Qi Pieter Abbeel Jitendra Malik\nAbstract: We present an approach to learn general robot manipulation priors\nfrom 3D hand-object interaction trajectories. We build a framework to use in-\nthe-wild videos to generate sensorimotor robot trajectories. We do so by lifting\nboth the human hand and the manipulated object in a shared 3D space and retar-\ngeting human motions to robot actions. Generative modeling on this data gives\nus a task-agnostic base policy. This policy captures a general yet flexible manip-\nulation prior. We empirically demonstrate that finetuning this policy, with both\nreinforcement learning (RL) and behavior cloning (BC), enables sample-efficient\nadaptation to downstream tasks and simultaneously improves robustness and gen-\neralizability compared to prior approaches. Qualitative experiments are available\nat: https://hgaurav2k.github.io/hop/.\nKeywords: Learning from videos, dexterous manipulation.\n1 Introduction\nReusable sensorimotor representations have the potential to give robots access to the versatility\nof their sensorimotor apparatus, thereby enabling them to achieve a wide variety of goals. Simi-\nlar to advancements in other AI domains [1, 2], such representations are likely to be trained with\nunsupervised objectives on large datasets. In this work, we study the feasibility of training such\nrepresentations using human videos in the context of dexterous manipulation.\nUsing videos as a data engine comes with several advantages: (1) they are abundant; (2) they cover a\nwide range of skills that we want robots to master; and (3) they reflect natural or socially acceptable\nbehaviors that we want robots to emulate. However, training sensorimotor representations on videos\nis a challenging endeavor. First, videos only partially capture the nature of an agent\u2019s interaction with\ntheir surroundings. For instance, by looking at a person holding an object, it is almost impossible to\nestimate the force their fingers are exerting. In addition, the larger the embodiment gap between a\nhuman and a robot, the more their actions will differ to achieve the same objectives.\nThe difficulty of learning from videos led previous work to mostly focus on specific aspects of the\nproblem. One line of research focused on training visual representations with off-the-shelf self-\nsupervised vision algorithms on large vision datasets [3, 4, 5, 6, 7]. While simple and effective,\nsuch pretrained representations lack a motor component, making them less effective on downstream\ntasks [8]. Another line of work aims to extract both sensory and motor information from videos by\nestimating human motions in 3D [9, 10, 11, 12, 13]. However, these approaches require alignment\nbetween the training videos and the robot\u2019s downstream tasks, which compromises the generality of\nthe learned representations. Finally, recent works aim to use egocentric videos of human activities to\nlearn an explicit hand-object interaction prior in the form of a contact-pose prediction model [14, 15].\nWhile a contact-pose prior is potentially task-agnostic, useful information in hand-object trajectories\nextends beyond contact-poses, including but not limited to pre/post-contact trajectories, intuitive\nphysics of the interaction and human preferences.\n\u2217denotes equal contribution. All authors are affiliated to UC Berkeley.arXiv:2409.08273v1  [cs.RO]  12 Sep 2024Figure 1: Real world rollouts of the policy finetuned from HOP using less than 50 demonstrations. HOP\nenables sample-efficient downstream adaptaion by learning a general manipulation prior from human videos.\nIn this paper, we present an approach to capture a general manipulation prior from in-the-wild\nvideos. Such a prior is implicitly embedded in the weights of a causal transformer, pretrained with\na conditional distribution matching objective on sensorimotor robot trajectories. These trajectories\nare generated by mapping 3D hand-object interactions to the robot\u2019s embodiment via a physically\ngrounded simulator. The choice of an implicit prior, aligned with the current paradigm in vision and\nlanguage research, has the potential advantage of becoming more and more expressive as the quality\nand diversity of the data increases. The resulting prior can be quickly adapted to any manipulation\ntask either with reinforcement learning or behavioral cloning. After adaptation, the prior takes the\nform of an end-to-end policy mapping the robot\u2019s multi-modal sensory stream to low-level joint\ncommands. This policy can be directly executed on a physical robot (see Fig. 1).\nWe empirically study the advantages brought forward by pretraining with hand-object interactions in\nboth simulation and real-world experiments. The findings of this study indicate that our manipula-\ntion prior considerably speeds up skill acquisition compared to previous methods, even if such skills\nare not represented in the training videos. Additionally, it improves generalization and robustness to\ndisturbances in the downstream policy.\n2 Overview\nThe objective of Hand-Object interaction Pretraing (HOP) is to capture general hand-object inter-\naction priors from videos. In contrast to previous work, we do not assume a strict alignment of\nthe human\u2019s intent in the video and the downstream robot tasks. Our key intuition is that the ba-\nsic skills required for manipulation lie on a manifold whose axes are well covered by unstructured\nhuman-object interactions.\nWe extract sensorimotor information from videos by lifting the human hand and the manipulated\nobject in a shared 3D space. We then bring such 3D representations to a physics simulator, where\nwe map human motion to robot actions. There are several advantages to using a simulator as an\nintermediary between videos and robot sensorimotor trajectories: (i) we can add physics, inevitably\nlost in videos, back to the interactions; (ii) it enables the synthesis of large training datasets without\nputting the physical platform in danger; and (iii) we can add diversity to the data by randomizing\nthe simulation environment, e.g., varying the friction between the robot\u2019s joints, the scene\u2019s layout,\nand the object\u2019s location relative to the robot.\n2Figure 2: 3-D hand-object trajectories from in-the-wild human manipulation videos are re-targeted to a robot\nembodiment within a physics simulator, resulting in physically grounded robot data. General manipulation\npriors are learnt from this using generative modelling of trajectories. Such representation enables sample-\nefficient adaptation for new downstream tasks.\nWe generate a dataset of robot-object interactions D={\u03c41, \u03c42, . . . , \u03c4 N}where \u03c4=\n{(o[0],a[0]),(o[1],a[1]), . . . , (o[T],a[T])}are the observation-action pairs of a single sensorimo-\ntor trajectory. An observation o[k]\u2208Oat time k\u2208[0, . . . , T ]consists of visual scene information\n(a depth image or a pointcloud) and robot\u2019s joint angles \u03d5[k],i.e., its proprioception. The action\na[k]\u2208Aconsists of continuous joint angles, which are converted to joint torques with a low-level\nPD controller. We use Dto train a base policy \u03c0bon the unsupervised objective of next-action\nprediction from a history of sensory observations, i.e.,\u02c6a=\u03c0b(o[t:t\u2212L]), where Lis a fixed\ncontext length. We finetune \u03c0bto generate task-specific policies \u03c0teither optimizing a reward with\nreinforcement learning or a behavioral cloning objective on few task-specific demonstrations. The\nnext section presents each aspect of our method in detail.\n3 Method\n3.1 Lifting Hand-Object Interaction Videos to 3D\nRecovering the underlying 3D structure of hand-object interactions from in-the-wild monocular\nvideos is inherently ambiguous. To alleviate such ambiguity, previous work leveraged the insight\nthat the human hand can be used as an anchor for the 3D location and scale of the manipulated ob-\nject [16, 17, 18, 19, 20, 21]. Our setup to estimate hand-object interaction trajectories from videos\nbuilds upon recent advances in 3D vision. Our approach closely follows MCC-HO [16] with a few\nmodifications to adapt it to our use case.\nGiven a single RGB image and an estimate of the 3D hand geometry from HaMeR [22], MCC-\nHO jointly infers hand-object geometry as point clouds. To fine-tune the quality of the prediction,\nMCC-HO finetunes the object\u2019s pose by fitting it to a CAD model. However, this finetuning assumes\nknowledge of the object the human is interacting with. To increase generality, we wave this assump-\ntion and skip the CAD-based post-processing. This simplification comes at the cost of reduced\nreconstruction quality and temporal smoothness. While we find the first problem not critical for pre-\ntraining, we increase temporal smoothness by anchoring object reconstructions to time-smoothed\nhand detections [22]. In addition, we make the simplifying assumption that the camera from which\nthe video is collected is static. More details of our 3D estimation pipeline are provided in the ap-\npendix. The result of this pipeline is a sequence of 3D hand-object poses.\n3.2 Mapping 3D Human-Object Interactions to Robot-Object Interactions\nWe formulate a non-linear optimization problem to generate a sensorimotor trajectory \u03c4from a se-\nquence of 3D hand-object poses. At each step k, we find the action a[k]by optimizing the following\ncost function:\nmin\na[k]1\n2\u2225xh[k]\u2212f(a[k])\u22252+\u03bb\u2225a[k]\u2212\u03d5[k\u22121]\u22252s.t.a[k]\u2208A, (1)\n3where fis the robot\u2019s forward kinematics, and xh[k]are the 3D coordinates of a set of keypoints\non the human hand. The first term of Eq. 1 represents the difference between the robot\u2019s and the\nhuman keypoints as a function of the robot\u2019s desired joints a[k]. The second term is proportional to\nthe energy required to execute the action a[k], which we minimize to favor smoothness.\nWhile there are approaches that include the object dynamics in the optimization [23, 24, 25, 26],\nthey are challenging to apply to in-the-wild videos due to noise in object pose estimates. In addition,\nin-the-wild videos do not have reliable information about objects\u2019 physical properties, e.g., mass or\nfriction. Therefore we disregard the dynamics of the manipulated object and place it on every step at\nthe location observed in the video. While this can lead to physical implausibility in the object motion\nand possibly lack of force-closure grasps, the data quality does not deteriorate much in practice, as\nwe can still learn useful behaviors. We empirically show that without object-trajectories (Section\n5.3), the quality of the base policy decreases.\nThe optimization is performed independently on each timestep k, and the resulting actions a[k]\nare executed in a high-fidelity simulator to generate o[k]. We refer to this method of mapping hu-\nman motion to robot sensorimotor trajectories \u03c4assimulator-in-the-loop retargeting . The primary\nadvantage of this approach is the optimization in (1) can be conducted using a simplified forward\nkinematics f, reducing the computational burden. Despite this simplification, the actions are exe-\ncuted in a high-fidelity simulator, ensuring realistic behaviors and high-quality observations.\nWe randomize the simulated scene to increase data diversity. Specifically, we add obstacles like\ntables and walls to the scene and vary their positions relative to the robot. This allows us to add ran-\ndom constraints to this optimization problem, which increases the overall diversity in the extracted\njoint trajectories. This is particularly important for robots with kinematic redundancies, since they\nhave multiple joint position trajectories for the same end-effector trajectory. Note that this approach\nto retargeting differs from previous methods, disregarding other objects in the scene and optimizing\nactions via physics-based constraints, e.g., minimum jerk [12, 27, 9] or minimum velocity [13].\nThe quality of the resulting robot trajectories decreases as the difference between the environment\nwhere the video was collected, the simulated scene, and fgrows. However, given the non-convex\noptimization landscape of (1), we can obtain good trajectories by running the optimization multiple\ntimes with various initial positions and scene layouts. High-quality data is then obtained by pruning\nthe trajectories on metrics like collision with obstacles and the tracking error between the hand\u2019s and\nthe robot\u2019s keypoints.\n3.3 Robot Trajectory Pretraining\nThe resulting trajectory dataset Tcontains knowledge that could be valuable to any manipulation\ntasks. For instance, Thas information about object affordance, i.e., where and how to grasp; some\nintuitive (although rudimentary) physics, e.g., an object should be reached upon before being lifted;\nor wrist-hand coordination, i.e., the behavior of orienting and shaping the hand simultaneously while\nmoving the wrist to maximize efficiency [28].\nWe aim to incorporate this knowledge as useful behavioral priors into a policy \u03c0bthat can be\nfinetuned to downstream tasks. Similar to previous work in language [2], vision [29], and\nrobotics [8, 30, 31], we instantiate \u03c0bas a transformer [32] and train it on a generative modeling\nobjective. Specifically, we train \u03c0bto capture the conditional distribution \u03a0(a[t\u2212L:t]|o[t\u2212L:t])\nby optimizing the following loss:\nL(\u03c4;\u03b8) =Et\u223c[1...T][\u2225a[t\u2212L:t]\u2212\u03c0b(o[t\u2212L:t])\u22251]. (2)\nHowever, unlike previous work, our pretraining dataset Tcontains neither real-world demonstra-\ntions nor complete task executions. This is because our data is generated from unstructured 3D\nhand-object interactions, and we disregard the dynamics of the manipulated object during retarget-\ning (Sec. 3.2). Yet, we find that the pre-training paradigm in (2) leads to the emergence of useful\nrepresentations in \u03c0b.\n4Downstream Finetuning. The pretrained policy \u03c0bexhibits primitive manipulation skills, e.g.,\nreaching an object with a reasonable grasp pose, while occasionally grasping successfully. We\nfinetune these skills to a task by optimizing a reward with reinforcement learning or a behavior\ncloning loss on limited demonstrations. We finetune the whole model for the task. Empirically, we\nfind that finetuned policies use the information in \u03c0bto train faster, are more robust to disturbances,\nand generalize better than policies trained from scratch and a set of baselines. In addition, we find\nthat the finetuning process re-utilizes the information in \u03c0beven for tasks not explicitly represented\nin the training videos.\n4 Experimental Setup\nRobot. We use a low-cost 7-DoF xArm robot with a 16-DoF Allegro hand [33] vertically mounted\nat its end effector. The proprioception observation \u03d5kincludes joint position from both robots.\nWhile we don\u2019t make any specific assumption about the robot embodiment, we use a multi-fingered\nhand instead of a parallel joint gripper since demonstration quality increases as the embodiment\ngap between the robot and the human decreases. We empirically found that since the robot base is\nfixed, a 7-DoF arm can track much better human motions than a 6-DoF arm, which often encounters\nsingularities during such trajectories. Visual sensing comes from a single stereo camera (Zed-2)\nmounted on the robot\u2019s right side.\nSimulation Setup. Our simulation environment is developed with the IsaacGym [34] simulator.\nThe robot morphology and action space are identical to the real setup. However, since rendering\ndepth images is prohibitively expensive, we give the agent access to the ground-truth object point-\ncould instead of a depth image (see Section 2). Specific details about the task setup and reward\ndesign can be found in the appendix.\nVideo Datasets. Our pretraining dataset of 3D hand-object trajectories consists of sequences from\ntwo datasets: DexYCB [35] and 100 Days of Hands [36]. We use 250 videos from the DeXYCB\ndataset (right-hand only) annotated with ground truth hand-object trajectories as a source of high-\nquality data. We additionally use approximately 200 videos from the 100 Days of Hands dataset.\nSixty percent of these videos were previously annotated with hand-object interaction trajecto-\nries [11], which we directly use. We annotate the remaining videos with our 3D estimation pipeline\n(Sec. 3.1). Overall, our combined dataset contains approximately 450 videos. We retarget these\nvideos to obtain a pretraining dataset Tof approximately 70,000trajectories.\nRetargeting. We use low-storage BFGS [37] from the NLOpt library [38] for optimization. We\nperform simulation-in-the-loop retargeting in a simple simulated scene with a ground floor on which\nthe robot and a static table are placed 65cm apart. Objects start their trajectories above the table with\na random pose. We run the optimization 700times for each video, randomizing the table location\nand the robot\u2019s initial joint state. We add a trajectory to Tonly if, at any time, their retargeting error\n(See Eq. (1)) is below 3cm and the arm does not collide with the table or the floor. Our code is built\nupon the implementation of Qin et al. [39].\nTransformer. Similar to previous work [40], we represent the policy \u03c0bwith a GPT-2-style causal\ntransformer. The policy takes proprioception and observation input from the past 16 timesteps and\npredicts the next action. Details about the architecture can be found in the appendix.\nPretraining. We train the transformer with the objective in Eq. (2) on T. While we could make\nthe prediction autoregressive and add decoding heads and proxy losses for future proprioception and\nimages (as in [13]), we empirically found these changes to be not very helpful in practice to our tasks.\nTherefore, we predict only future actions for simplicity. We use as optimizer AdamW [41] with\ninitial learning rate of 10\u22124and weight decay of 10\u22122. We trained two distinct base policies\u2014one\nwith depth observations and the other with point cloud observations. The former is used for real-\nworld, and the latter for simulation experiments. However, it\u2019s important to note that both policies\nwere trained on exactly the same trajectories; only the associated sensor observations differed.\n5Finetuning. In simulation , we finetune the transformer with PPO [42] using the default hyperpa-\nrameters from [43]. However, we add a few modifications inspired by [44] for effective fine-tuning:\n(1) we use a small initial exploration noise of 0.1; (2) the value and policy networks share the ob-\nservation tokenizer, but the tokenizer\u2019s weights are not updated by the value function\u2019s gradients;\n(3) we warm up the value function\u2019s parameters for the first 200 gradient steps, keeping the actor\nparameters fixed. Since reinforcement learning requires up to 1 billion steps to convergence, and our\nsimulator does not offer fast multi-gpu rendering, we use pointclouds as visual information, as they\ncan be efficiently simulated. In the real world , we finetune the entire \u03c0bon limited demonstrations\nwith the same objective and hyperparameters used for pretraining. In these experiments, we use\ndepth images as input to our policy since pointcloud estimation in the real world generally requires\nmultiple cameras, while our real-world setup has a single camera.\nInference. At test time, the model operates in closed-loop : it receives the past and current obser-\nvations as input and predicts the next action to execute. The prediction loop runs at 20Hz. The\npredicted action is sent to the xArm and Allegro low-level controllers, which operate at 120Hz and\n300Hz, respectively.\n5 Experimental Results\nWe design an experimental procedure to analyze the advantages brought forward by HOP in terms\nof finetuning efficiency, generalization, and robustness to perturbations. Specifically, we ask the\nfollowing questions: (i) How does HOP compare to vision-only pre-training approaches for robot\nlearning? (ii)How does HOP compare to existing demonstration-guided reinforcement learning\nalgorithms [45, 46]? (iii)How does learning from hand-object interaction trajectories compare to\nlearning hand-pose priors only [47, 48, 9]?. We answer these questions via controlled experiments\nin simulation and the physical world.\n5.1 Comparison to visual pre-training baselines (real-world).\nBaselines. We compare our approach to visual pre-training systems. Such systems are trained on\nlarge image or video datasets but lack a motor component. Specifically, we compare to methods\nusing the following pre-training data:\n\u2022ImageNet We encode the depth image with a VIT-B network [49] pre-trained on ImageNet\nand pass the resulting CLS token embeddings to our transformer. The latter is then trained\nwith real-world data. We consider two variants: using the VIT features zero-shot ( Imagenet\nZS) and finetuning them on the downstream dataset ( ImageNet F ).\n\u2022Internet Videos We use off-the-shelf visual features from R3M [7], VIP [3], and MVP [8].\nThese features were obtained with unsupervised contrastive learning objectives on large\nvideo datasets, e.g. Ego4D [50]. Conversely to ours, these baselines don\u2019t use depth but\nRGB images as input.\nWe additionally compare to Diffusion Policies [23] using a UNet backbone since our tasks exhibit\ntemporally smooth desired action sequences. Similarly to ours, this baseline uses depth as input.\nWith the above baselines, we want to understand how classic methods for behavior cloning work in\nour setting, where a single camera and a limited number of demonstrations are available. Indeed,\nthe previous approaches are generally applied with a large number of demonstrations and multiple\nRGB cameras.\nTasks We evaluate our approach in the real world on three tasks of increasing complexity. In the\nfirst task, Grasp and Drop , the robot needs to unstack a cube and put it in a bowl. The second is\ntheGrasp and Pour task, where the robot needs to pick a bottle and point it towards a bowl. In the\nthird task, Grasp and Lift , the robot must pick up one of 4 different-looking objects, all requiring\ndifferent spatial affordances. One single model is trained to pick up all objects. In this task, we\nevaluate the ability of the approach to adapt with a few demonstrations on very different object\n6shapes. We collect 15 demonstrations for the first two tasks and 50 demonstrations for the third task.\nWe encourage the reader to check our project page for visualizing the tasks. More details about the\nscene setup and evaluation criteria can be found in the appendix.\nResults Table 1 summarizes the result of our study. The findings indicate that for tasks with a single\nobject (Grasp & Drop, Grasp & Pour), all methods perform comparably and achieve, with some\nsporadic exceptions, a close-to-perfect success rate. However, in the hardest task (Grasp and Lift),\nwhere a single policy needs to pick four objects with different affordances, our approach has a mar-\ngin of 30 percentage points to ImageNet Finetuned, the best-performing baseline. We additionally\nfind that the baselines using RGB data are much more successful with a single object than with\nmultiple ones. This is likely because there are not enough demonstrations to learn object-specific\naffordances. Overall, these results empirically validate the value of our sensorimotor pretraining\nstrategy.\nDepth-input RGB-input\nTask Ours Diffusion-Policy Imagenet-ZS Imagenet-F R3M VIP MVP\nGrasp & Drop 0.80 0.90 0.90 0.80 0.0 0.40 0.20\nGrasp & Pour 1.0 0.20 0.80 0.70 1.0 1.0 1.0\nGrasp & Lift 0.65 0.30 0.30 0.35 0.0 0.0 0.0\nTable 1: Real-robot results (success rate % averaged over 20 rollouts). Note that RGB baselines are\nnot directly comparable to our approach since our policy takes depth images as input.\n5.2 Comparison to demonstration-guided reinforcement learning strategies (simulation)\nOur simulation experiments investigate the effectiveness of HOP as a base model for adaptation to\ndownstream tasks using RL. The simulation agent is identical in morphology to the real robot.\nBaselines. We compare our approach to three baselines: (1) training from scratch ( PPO ); (2)\ndemonstration-guided reinforcement learning with a proxy imitation objective [45] ( DAPG ); and\n(3) using adversarial objectives to keep the policy close to the demonstrations [46] ( AMP ). DAPG\nis the closest to our work, as it trains on a weighted sum of behavioral cloning and reinforcement\nlearning losses. However, it assumes access to expert demonstrations in the downstream task. Our\npre-training dataset does not fulfill this assumption. Indeed, humans might not behave optimally\naccording to the reward, or the task might not be well represented in the pre-training dataset. Simi-\nlarly to previous work [51], we found that training from scratch is unsuccessful using joint-position\ncontrol as action space, consistently leading the PPO baseline to fail. Therefore, we use the moving-\naverage action space proposed by Petrenko et al. [43] to improve its performance. All baselines use\nthe same environment settings and training strategy, e.g., domain randomization parameters, as our\napproach.\nTasks and Metrics. We evaluate approaches on three tasks. The first requires picking objects and\nplacing them at a specific location ( Grasp and Lift ). The second is to grasp objects and throw them\nin a basket ( Grasp and Throw ). In the final task, the robot is required to open a cabinet. We evaluate\nperformance using success over 256 environments with different objects and report the mean and\nstandard deviation over three seeds per approach. More details can be found in the appendix.\nHOP enables sample-efficient RL and effective exploration In Figure 3, it is demonstrated that\nour approach outperforms all baselines by a large margin, especially when the pretraining corpus\nis not closely related to the task. This is expected because DAPG strongly biases exploration in\nthe neighborhood of the pre-training trajectories, which may potentially be misaligned with the\ndownstream task. Furthermore, we observed that the adversarial training scheme of AMP is unstable\nand does not scale well with the amount of data. Finally, we find that using HOP leads to a 2-5X\nimprovement in sample efficiency compared to training from scratch. Overall, these experiments\n7Figure 3: Comparison of HOP-initialized actor with baselines. HOP improves sample-efficiency of online\nRL across multiple tasks, particularly when the downstream task and the behaviors in the data are less aligned,\nas in Lift & Throw . Runs are averaged across three randomly chosen seeds.\nFigure 4: Evaluating RL finetuning under out-of-distribution scenarios (Left) To test grasp robustness in\nthe task Grasp & Lift , we apply to the grasped objects, forces in random direction equal to their weights. When\ninitialized with HOP, the resulting policy is more than 3\u00d7more robust compared to training PPO from scratch.\n(Right) We evaluate grasp success on multiple objects from the YCB dataset that were not part of the training\nset. When initialized with HOP, the resulting policy is more than 2\u00d7more robust compared to training PPO\nfrom scratch.\nshow that initializing with HOP enables more informed exploration than the baselines and reduces\nvariance in policy gradients.\nHOP learns robust and general behaviors Policies fine-tuned from HOP can potentially bias ex-\nploration toward human-like behavior, leading to more robustness against forces. This is shown in\nFig. 4. Agents trained with our approach perform better when subject to forces than the ones trained\nfrom scratch. In addition, we show in Fig. 4 that our approach generalizes 3x better than the policy\ntrained from scratch. The training objects are different from the testing ones in their mass, aspect\nratio, and relative size with respect to the hand. The performance generally drops whenever the\ntest object is heavy (power drill), too large (cracker box), or too small (marker and scissors) for the\nallegro hand, which is approximately 1.5X larger than a human hand. Note that in these experiments\nwe train the scratch policy with two billion samples.\nAffordances Training an RL policy from scratch for a dexterous hand often leads to grasping poses\nthat are unlike general human affordances. Online RL exploration near a learned human-object\ninteraction prior biases the optimization landscape to favor human-like affordances. As shown in\nFigure 5, we find that for the Grasp and Lift task, our policy grasps objects with more stable and\nhuman-like affordances than PPO training from scratch.\n8Figure 5: Online exploration around the learnt prior from humans leads to grasps with more human-like and\nstable affordances compared to training PPO from scratch.\nFigure 6: Pre-training only a hand-motion prior leads to decrease in robustness of grasps to force disturbances\n(left). With our approach, the pre-trained policy learns a prior on object affordances which leads to more robust\ngrasps. In addition, pretraining with object poses leads to a more flexible prior and better finetuning to tasks\nless aligned with the pre-training data (right).\n5.3 Comparison to learning a hand-only motion prior (simulation)\nPrior work has shown the benefits of learning a prior on hand motions from videos of human ac-\ntivities [47, 48, 9]. This section aims to understand the benefits of learning a prior on the object\nand the hand jointly . We hypothesize that learning from hand-object interactions gives the base\nmodel useful information beyond eigen-grasps (which are captured by a hand-only motion prior),\nlike, for instance, pre- and post-contact trajectories, intuitive physics of the interaction, and human\npreferences.\nWe evaluate this hypothesis by training a base policy on our pre-training corpus using masked object\nobservations. This encourages the base policy to primarily learn a hand motion prior. As illustrated\nin Figure 6 (left), we observe that such a pre-trained policy exhibits reduced robustness to grasp\ndisturbances. Furthermore, we find that the hand-only prior is insufficient for learning an effective\npolicy in the Grasp and Throw task (Fig. 6, right). Since this task is underrepresented in the pre-\ntraining corpus, the hand motions required are unlikely to be adequately captured by a hand-only\nprior. In contrast, learning a joint hand-object prior provides the model with a more comprehensive\nunderstanding of manipulation, enabling quicker adaptation to this downstream task.\n6 Related Work\nLearning Policies from Human Videos. In-the-wild videos hold the promise of solving the data\nproblem in robotics. One of the pioneering efforts in this direction is by Yang et al. [52], where\nvideo data was used to generate action plans. Several works followed up on this idea, relying on pre-\ndefined action primitives [53, 54, 55, 56, 57, 58]. However, waving the requirement for pre-defined\n9primitives is challenging since in-the-wild videos lack motor information. One way to recover motor\ninformation from videos is training with a trajectory-matching objective [59, 60, 61, 62], possibly\nusing intermediate representations like object segmentation or optical flow [63, 64, 65]. However,\nthis approach requires collecting task- and environment-specific videos where humans and robots\noperate in the same workspace. Therefore, the trajectory-matching formulation largely constrains\nthe number of videos that can be used for training. To overcome these constraints, researchers have\nfocused on either learning exclusively visual representations from videos or extracting 3D human\nposes and mapping them to robot actions. In the following, we cover these works in detail.\nVisual Representation Learning for Robotics. Inspired by successes in computer vision [1] and\nnatural language processing [66], the robot learning community has recently focused on pretraining\nrepresentations on large video datasets like Ego4D [50] and fine-tuning these representations on\ndownstream tasks [4, 7, 3, 5]. However, being the training objective based exclusively on image\nreconstruction, the representations focus primarily on low-level vision features, e.g., shapes or edges.\nThis gives them limited benefits compared to representations trained on standard vision datasets [6].\nOverall, these works focus on visual generalization, e.g., picking up two objects with the same shape\nbut different colors. However, they have not yet demonstrated action generalization, where motor\nskills are adapted to accomplish novel objectives.\nActions from Videos via 3D. One common approach to extracting action information from videos\nis using 3D as an intermediate representation. If the embodiment gap is small, human motions can\nbe mapped to robot actions via inverse kinematics. This is particularly effective when the videos\nare task-specific, i.e., when the robot aims to mimic the human motion [10, 12, 67, 68, 69, 70, 11].\nInstead of learning specific skills, other works focus on learning a re-usable sensorimotor prior from\nvideos. However, this prior only captures human actions [47, 48, 9], disregarding the trajectory of\nthe manipulated object. Conversely, our work aims to use 3D hand-object interactions from in-the-\nwild videos to learn a re-usable prior for object manipulation.\nDexterous Manipulation. Dexterous manipulation has been studied for decades [71, 72, 73, 74,\n75]. In recently years, learning-based approaches make significant progress [76, 77]. They can\nbe generally categorized to learning in simulation and then transferring to the real world (Sim-to-\nReal) [78, 79, 80, 81, 82, 83], and learning in the real-world [39, 84, 85, 86, 87]. Qin et al. [12] uses\nhand-object trajectories but only use it to collect demonstrations. Xu et al. [88] also does functional\ngrasp generation, but the results are limited in simulation. Most of the aforementioned work learn\npolicies from scratch and does not use any internet data as prior. Our approach studies learning\nhand-object interaction prior from human videos and is effective both in simulation and real-world.\n7 Conclusion and Limitations\nThis work presents an approach to learning general yet flexible manipulation priors for robot poli-\ncies from human videos. While our approach demonstrates a way to pre-train on a single object\ninteraction, this can, in practice, be limiting. Indeed, human behavior in a video can potentially be\nconditioned on information encompassing multiple objects in the current and previous scenes. This\nleads to a loss of signal that could be extracted from the raw video. We predict that advances in 3-D\nreconstruction will enable us to use a more complex scene reconstruction and pretraining.\nAcknowledgments\nThis work was supported by the DARPA Machine Common Sense program, the DARPA Transfer\nfrom Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) program, and by the\nONR MURI award N00014-21-1-2801. This work was also funded by ONR MURI N00014-22-1-\n2773. We thank Adhithya Iyer for assistance with teleoperation systems, Phillip Wu for setting-up\nthe real robot, and Raven Huang, Jathushan Rajasegaran and Yutong Bai for helpful discussions.\n10References\n[1] K. He, X. Chen, S. Xie, Y . Li, P. Doll \u00b4ar, and R. Girshick. Masked autoencoders are scalable\nvision learners. In CVPR , 2022.\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog , 2019.\n[3] Y . J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V . Kumar, and A. Zhang. Vip: Towards\nuniversal visual reward and representation via value-implicit pre-training. In ICLR , 2023.\n[4] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot\nlearning with masked visual pre-training. CoRL , 2022.\n[5] A. Majumdar, K. Yadav, S. Arnaud, J. Ma, C. Chen, S. Silwal, A. Jain, V .-P. Berges, T. Wu,\nJ. Vakil, P. Abbeel, J. Malik, D. Batra, Y . Lin, O. Maksymets, A. Rajeswaran, and F. Meier.\nWhere are we in the search for an artificial visual cortex for embodied intelligence? NeurIPS ,\n2023.\n[6] S. Dasari, M. K. Srirama, U. Jain, and A. Gupta. An unbiased look at datasets for visuo-motor\npre-training. In CoRL , 2023.\n[7] S. Nair, A. Rajeswaran, V . Kumar, C. Finn, and A. Gupta. R3m: A universal visual represen-\ntation for robot manipulation. In CoRL , 2022.\n[8] I. Radosavovic, B. Shi, L. Fu, K. Goldberg, T. Darrell, and J. Malik. Robot learning with\nsensorimotor pre-training. In CoRL , 2023.\n[9] K. Shaw, S. Bahl, and D. Pathak. Videodex: Learning dexterity from internet videos. In CoRL ,\n2022.\n[10] X. B. Peng, A. Kanazawa, J. Malik, P. Abbeel, and S. Levine. Sfv: Reinforcement learning of\nphysical skills from videos. Transactions On Graphics , 2018.\n[11] A. Patel, A. Wang, I. Radosavovic, and J. Malik. Learning to imitate object interactions from\ninternet videos. arXiv:2211.13225 , 2022.\n[12] Y . Qin, Y .-H. Wu, S. Liu, H. Jiang, R. Yang, Y . Fu, and X. Wang. Dexmv: Imitation learning\nfor dexterous manipulation from human videos. In ECCV , 2022.\n[13] I. Radosavovic, B. Zhang, B. Shi, J. Rajasegaran, S. Kamat, T. Darrell, K. Sreenath, and J. Ma-\nlik. Humanoid locomotion as next token prediction. arXiv:2402.19469 , 2024.\n[14] A. Kannan, K. Shaw, S. Bahl, P. Mannam, and D. Pathak. Deft: Dexterous fine-tuning for\nreal-world hand policies. CoRL , 2023.\n[15] P. Mandikal and K. Grauman. Dexvip: Learning dexterous grasping with human hand pose\npriors from video. In Conference on Robot Learning , pages 651\u2013661, 2022.\n[16] J. Wu, G. Pavlakos, G. Gkioxari, and J. Malik. Reconstructing hand-held objects in 3d.\narXiv:2404.06507 , 2024.\n[17] H. Choi, N. Chavan-Dafle, J. Yuan, V . Isler, and H. Park. Handnerf: Learning to reconstruct\nhand-object interaction scene from a single rgb image. arXiv:2309.07891 , 2023.\n[18] Y . Ye, A. Gupta, and S. Tulsiani. What\u2019s in your hands? 3d reconstruction of generic objects\nin hands. In CVPR , 2022.\n[19] B. Tekin, F. Bogo, and M. Pollefeys. H+O: Unified egocentric recognition of 3d hand-object\nposes and interactions. In CVPR , 2019.\n11[20] Y . Hasson, G. Varol, D. Tzionas, I. Kalevatykh, M. J. Black, I. Laptev, and C. Schmid. Learning\njoint reconstruction of hands and manipulated objects. In CVPR , 2019.\n[21] Z. Cao, I. Radosavovic, A. Kanazawa, and J. Malik. Reconstructing hand-object interactions\nin the wild. In ICCV , 2021.\n[22] G. Pavlakos, D. Shan, I. Radosavovic, A. Kanazawa, D. Fouhey, and J. Malik. Reconstructing\nhands in 3D with transformers. In CVPR , 2024.\n[23] X. Zhu, J. Ke, Z. Xu, Z. Sun, B. Bai, J. Lv, Q. Liu, Y . Zeng, Q. Ye, C. Lu, M. Tomizuka, and\nL. Shao. Diff-lfd: Contact-aware model-based learning from visual demonstration for robotic\nmanipulation via differentiable physics-based simulation and rendering. In CoRL , 2023.\n[24] A. S. Lakshmipathy, N. Feng, Y . X. Lee, M. Mahler, and N. Pollard. Contact edit: Artist tools\nfor intuitive modeling of hand-object interactions. Transactions on Graphics , 2023.\n[25] A. S. Lakshmipathy, J. K. Hodgins, and N. S. Pollard. Kinematic motion retargeting for\ncontact-rich anthropomorphic manipulations. arXiv:2402.04820 , 2024.\n[26] Y . Kim, H. Park, S. Bang, and S.-H. Lee. Retargeting human-object interaction to virtual\navatars. Transactions on Visualization and Computer Graphics , 2016.\n[27] A. Sivakumar, K. Shaw, and D. Pathak. Robotic telekinesis: Learning a robotic hand imitator\nby watching humans on youtube. In RSS, 2022.\n[28] L. A. Jones and S. J. Lederman. Human hand function . Oxford university press, 2006.\n[29] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever. Generative pretraining\nfrom pixels. In ICML , 2020.\n[30] A. Kumar, A. Singh, F. Ebert, M. Nakamoto, Y . Yang, C. Finn, and S. Levine. Pre-training for\nrobots: Offline rl enables learning new tasks from a handful of trials. In RSS, 2023.\n[31] O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna,\nT. Kreiman, C. Xu, J. Luo, Y . L. Tan, L. Y . Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh,\nC. Finn, and S. Levine. Octo: An open-source generalist robot policy. In RSS, 2024.\n[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In NeurIPS , 2017.\n[33] WonikRobotics. Allegrohand. https://www.wonikrobotics.com/ , 2013.\n[34] V . Makoviychuk, L. Wawrzyniak, Y . Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin,\nA. Allshire, A. Handa, and G. State. Isaac gym: High performance gpu-based physics simula-\ntion for robot learning. In NeurIPS Datasets and Benchmarks , 2021.\n[35] Y .-W. Chao, W. Yang, Y . Xiang, P. Molchanov, A. Handa, J. Tremblay, Y . S. Narang, K. V .\nWyk, U. Iqbal, S. Birchfield, J. Kautz, and D. Fox. Dexycb: A benchmark for capturing hand\ngrasping of objects. CVPR , 2021.\n[36] D. Shan, J. Geng, M. Shu, and D. F. Fouhey. Understanding human hands in contact at internet\nscale. In CVPR , 2020.\n[37] D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization.\nMathematical programming , 1989.\n[38] S. G. Johnson. The NLopt nonlinear-optimization package. https://github.com/\nstevengj/nlopt , 2007.\n[39] Y . Qin, H. Su, and X. Wang. From one hand to multiple hands: Imitation learning for dexterous\nmanipulation from single-camera teleoperation. RA-L , 2022.\n12[40] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and\nI. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. NeurIPS ,\n2021.\n[41] I. Loshchilov and F. Hutter."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "Here is a comprehensive summary of the academic paper \"Click2Mask: Local Editing with Dynamic Mask Generation\" by Omer Regev, Omri Avrahami, and Dani Lischinski. This summary is structured to provide all the necessary details for a researcher to recreate the study from scratch.\n\n---\n\n## 1. Abstract:\nRecent advancements in generative models have made image generation and editing more accessible to non-experts. This paper introduces Click2Mask, a novel approach focused on local image editing to add new content to loosely specified areas using only a single point of reference. The main contribution is a dynamically grown mask around this point, guided by a Blended Latent Diffusion (BLD) process and a masked CLIP-based semantic loss. Click2Mask reduces user effort and produces contextually accurate local manipulations, outperforming state-of-the-art (SoTA) methods based on human judgement and automatic metrics. Notable contributions are the simplification of user input, the freedom to add objects beyond existing segments, and the adaptability of the dynamic mask approach to other editing workflows.\n\n---\n\n## 2. Introduction:\n- **Context**: The paper situates itself within recent strides in generative models, focusing on local image editing involving the addition of new content in a specified region.\n- **Research Problem**: Existing methods often require precise masks or complex descriptions, leading to user difficulties and potential inaccuracies.\n- **Research Contribution**: The paper introduces Click2Mask, which simplifies this process by using just one reference point and a content description to dynamically generate a mask for the Blended Latent Diffusion (BLD) process.\n- **Literature Review**: Critiques existing methods that rely on precise masks or detailed natural language descriptions and their shortcomings in ensuring accurate local edits.\n\n---\n\n## 3. Related Work:\n- **Mask-based Approaches**: Discusses various diffusion-based models and their limitations, especially Blended Diffusion, Latent Diffusion, GLIDE, Imagen Editor, and SmartBrush that depend heavily on user-provided masks.\n- **Mask-free Approaches**: Includes methods like Text2Live, Imagic, Plug-and-Play, and Prompt-to-Prompt which either fine-tune models or use alternative guidance techniques but often fail in precise localization.\n- **Instruction-based Approaches**: Covers methods like InstructPix2Pix, Emu Edit, and DALL\u00b7E3, noting their reliance on user expertise and sometimes ambiguous results.\n- **Segmentation-based Approaches**: Reviews segmentation-based models like DiffEdit and InstructEdit which rely on object detection and captioning, often constraining them to existing segments.\n\n---\n\n## 4. Method:\n### Proposed Approach:\n- **Single Reference Point (Click)**: The user provides a single click on the image as a reference point along with a content description.\n- **Dynamic Mask Generation**: Unlike traditional fixed masks, the mask evolves dynamically during the BLD process.\n- **Blended Latent Diffusion (BLD)**: Utilizes BLD to integrate both prompt-guided and original latent spaces, executing local edits confined by the evolving mask.\n\n### Detailed Process:\n1. **Image Encoding**: Encoding the input image using a variational auto-encoder (VAE).\n2. **Potential Height-Field**: Initialized around the user-provided point for mask generation.\n3. **Diffusion Steps**:\n   - **Denoising-Step**: For each diffusion step, generate text-guided latents.\n   - **Blending**: Combine the generated content (foreground) and the original image (background) using the evolving mask.\n4. **Mask Evolution**:\n   - **Initialization**: Begins with a large mask at early steps, progressively adjusted.\n   - **Guided Adjustment**: CLIP-based cosine distance evaluates mask importance, driving its evolution to fit the generated object\u2019s shape.\n   - **Final Adjustment**: Executes a last BLD pass using the final mask.\n\n### Algorithms:\n- **Mask Evolution Process**: Contains a detailed algorithm for iterating through steps, updating the mask based on semantic alignment losses.\n\n### Models and Techniques:\n- Uses pre-trained models for diffusion and CLIP embeddings.\n- Employs domain-specific heuristics, such as Gaussian feathering, for final blending.\n\n---\n\n## 5. Experimental Setup:\n### Datasets:\n- Utilized a variety of datasets suited for local image editing tasks, details unspecified in provided text.\n### Hardware and Software Specifications:\n- Implemented using PyTorch framework.\n- Experimental runs on high-performance GPUs.\n### Evaluation Metrics:\n- **Human Judgment**: User studies for subjective quality assessment.\n- **Automatic Metrics**: Includes CLIP-based similarity scores and L1 pixel distance.\n### Baselines and Comparison:\n- Compared Click2Mask with other state-of-the-art models like Emu Edit and MagicBrush.\n- Rigorous testing through standard datasets and controlled conditions.\n\n---\n\n## 6. Results and Analysis:\n### Empirical Findings:\n- Demonstrates Click2Mask\u2019s capability to minimize user effort and deliver superior edits.\n- **Quantitative Metrics**: Presents statistical superiority in terms of CLIP-based similarities and adherence to original content.\n- **Qualitative Metrics**: Details user preference percentages from human evaluations, showing Click2Mask's noticeable preference over other methods.\n### Detailed Figures and Tables:\n- Visual examples with incremental mask evolution (e.g., Figures 1, 3, 5, and 6).\n- Tabulated preference voting results and statistical metrics (e.g., Tables 1, 2).\n### Ablation Studies:\n- Assesses variations like mask initialization size and semantic alignment effect on output.\n\n---\n\n## 7. Discussion:\n### Result Interpretation:\n- Highlights how dynamic mask generation aligns semantically with the prompt, ensuring localized and accurate edits.\n### Implications:\n- Potential for broader application in various image-focused fields, integrating into comprehensive image editing software.\n### Limitations:\n- Difficulty in converging small, fine-detailed mask shapes.\n- Challenges with existing object overlap creating unintended edits.\n### Biases:\n- Potential over-reliance on specific configurations, environmental dependencies.\n\n---\n\n## 8. Conclusion:\n### Summary:\n- Reiterates main contributions: user-friendly local editing with a dynamic mask.\n- Discusses Click2Mask\u2019s advantages over state-of-the-art methods in simplifying user input and achieving high-quality edits.\n### Future Research Directions:\n- Focus on improving fine-detailed mask generation.\n- Further integration and optimization within other generative and editing frameworks.\n- Explore application across multi-domain datasets for broader applicability.\n\n---\n\nThis summary captures the comprehensive essence of the paper, providing detailed explanations, methodologies, and findings necessary for replicating or furthering the research."}, {"role": "assistant", "content": "Click2Mask: Local Editing with Dynamic Mask Generation\nOmer Regev, Omri Avrahami, Dani Lischinski\nThe Hebrew University of Jerusalem\nAbstract\nRecent advancements in generative models have revolution-\nized image generation and editing, making these tasks acces-\nsible to non-experts. This paper focuses on local image edit-\ning, particularly the task of adding new content to a loosely\nspecified area. Existing methods often require a precise mask\nor a detailed description of the location, which can be cum-\nbersome and prone to errors. We propose Click2Mask, a\nnovel approach that simplifies the local editing process by\nrequiring only a single point of reference (in addition to the\ncontent description). A mask is dynamically grown around\nthis point during a Blended Latent Diffusion (BLD) process,\nguided by a masked CLIP-based semantic loss. Click2Mask\nsurpasses the limitations of segmentation-based and fine-\ntuning dependent methods, offering a more user-friendly and\ncontextually accurate solution. Our experiments demonstrate\nthat Click2Mask not only minimizes user effort but also de-\nlivers competitive or superior local image manipulation re-\nsults compared to SoTA methods, according to both human\njudgement and automatic metrics. Key contributions include\nthe simplification of user input, the ability to freely add ob-\njects unconstrained by existing segments, and the integration\npotential of our dynamic mask approach within other editing\nmethods.\n1 Introduction\nRecent advances in generative models have revolutionized\nimage generation and editing capabilities, enabling both\nstreamlined workflows and accessibility for non-experts.\nThe latest approaches utilize natural language to manipulate\nimages either globally \u2013 altering the content or style of the\nentire image \u2013 or locally \u2013 adding, removing, or modifying\nspecific objects within a limited image region.\nIn this work, we focus on local editing, specifically on\nthe task of adding new content in a local area. To accom-\nplish such edits, some existing methods require users to pro-\nvide explicit precise masks (Avrahami, Lischinski, and Fried\n2022; Ramesh et al. 2022; Avrahami, Fried, and Lischinski\n2023; Wang et al. 2023b; Xie et al. 2022), which is tedious\nand may yield unexpected results due to lack of mask pre-\ncision. Other methods describe the desired manipulations in\nnatural language, as an edit instruction (Brooks, Holynski,\nProject page: https://omeregev.github.io/click2maskInput Emu Edit MagicBrush DALL \u00b7E3 Click2Mask\n\u201cAdd a gorilla in the background looking at the bananas\u201d\n\u201cA gorilla looking at the bananas\u201d\n\u201cAdd a pack of rocks to the back of the truck\u201d\n\u201cA pack of rocks\u201d\nFigure 1: Comparisons to SoTA models. A comparison of\nEmu Edit (Sheynin et al. 2023), MagicBrush (Zhang et al.\n2023) and DALL \u00b7E 3 (Betker et al. 2023) with our model\nClick2Mask . In each example, the top prompt was given\nto the other models, while Click2Mask received the simpler\nbottom prompt, in addition to the blue dot (mouse click) on\nthe input. Other models completely change the image, or the\nbackground, fail to edit, or produce unrealistic results.\nand Efros 2023; Sheynin et al. 2023), or by providing a cap-\ntion and the desired change (Bar-Tal et al. 2022; Kawar et al.\n2023; Hertz et al. 2022; Tumanyan et al. 2022). These meth-\nods also require user expertise, and their results may suffer\nfrom ambiguous or imprecise prompts. Moreover, they fail\nto ensure that the changes to the image are confined to a local\narea, or that they occur at all, as demonstrated in Figure 1.\nTo overcome the aforementioned shortcomings, we intro-\nduce Click2Mask, a novel approach that simplifies user in-\nteraction by requiring only a single point of reference rather\nthan a detailed mask or a description of the target area.\nThe provided point gives rise to a mask that dynamically\nevolves through a Blended Latent Diffusion (BLD) process\n(Avrahami, Lischinski, and Fried 2022; Avrahami, Fried,\nand Lischinski 2023), where the evolution is guided by a\nsemantic loss based on Alpha-CLIP (Sun et al. 2023). This\nprocess (Figure 2) enables local edits that are both precise\nand contextually relevant (Figures 1 and 3).\nUnlike segmentation-based methods that depend on pre-\nexisting objects (Couairon et al. 2022; Xie et al. 2023; Wang\net al. 2023a; Zou et al. 2024), Click2Mask does not confinearXiv:2409.08272v1  [cs.CV]  12 Sep 2024\u201cA giraffe\u201d\n\u201cSnowy\nmountains\u201d\nPrompt Input 30% 37% 44% 45% 46% 47% 48% (final Mt) Output\nFigure 2: Mask evolution . A visualization of the mask evolution throughout the diffusion process. Leftmost image is input with\nclicked point, rightmost image is the final Click2Mask output. Intermediate images are decoded latents \u02dczfgat several diffusion\nsteps, where the purple outline depicts the contour of current (upscaled) mask Mt. Percentages indicate the step out of 100\ndiffusion steps, with the last being the final evolved mask.\nthe edit area to the boundaries of an existing segment. Fur-\nthermore, in contrast to editing approaches that require fine-\ntuning the diffusion model (Wang et al. 2023b; Xie et al.\n2022; Kawar et al. 2023; Avrahami et al. 2023), we employ\npre-trained models, and only perform context dependent op-\ntimization on the mask.\nOur experiments demonstrate that Click2Mask not only\nreduces the effort required by users but also achieves com-\npetitive or superior results compared to state-of-the-art\nmethods in local image manipulation.\nIn summary, our contributions are: (i) Reduction of user\neffort by eliminating the need for precise mask outlines,\nor overly descriptive prompts. (ii) Ability to add objects\nin a free-form manner, unconstrained by boundaries of ex-\nisting objects or segments. (iii) Our dynamically evolving\nmask approach is not a stand-alone method, but rather it can\nbe embedded as a mask generation of the fine-tuning step\nwithin other methods that internally employ a mask, such as\nEmu Edit (Sheynin et al. 2023) which currently generates\nmultiple masks (a precise mask using DINO (Caron et al.\n2021) and SAM (Kirillov et al. 2023), an expanded version\nof it, and a bounding box), and filters the best result from\nmultiple images produced using these masks.\n2 Related Work\nIn recent years, much work has been done on image gen-\neration, with diffusion-based models (DMs) (Ho, Jain, and\nAbbeel 2020; Song, Meng, and Ermon 2020; Dhariwal and\nNichol 2021; Rombach et al. 2022; Ramesh et al. 2022; Sa-\nharia et al. 2022) facilitating a host of SoTA text-guided im-\nage editing methods and capabilities.\nMask-based approaches. Text-guided image manipu-\nlation may naturally be limited to a specific region us-\ning a mask. In the context of DMs this was first ex-\nplored in Blended Diffusion (Avrahami, Lischinski, and\nFried 2022), where a user-provided mask is used to blend\nimages throughout a denoising process with a text-guided\nnoisy image. This approach was later incorporated into La-\ntent Diffusion (Rombach et al. 2022) by performing the\nblending in latent space. The resulting Blended Latent Diffu-\nsion (BLD) method (Avrahami, Fried, and Lischinski 2023)serves as the basis for our work and described in more de-\ntail in Section 3. GLIDE (Nichol et al. 2022), Imagen Ed-\nitor (Wang et al. 2023b) and SmartBrush (Xie et al. 2022)\nfine-tuned the DM for image inpainting, by obscured train-\ning images or by conditioning on a mask. However, user-\nprovided masks have a major disadvantage: the success of\nthe edit depends on the exact shape of the mask, which can\nbe tedious and time-consuming for a user to create.\nMask-free approaches. Both Text2Live (Bar-Tal et al.\n2022), which generates a composite layer, and Imagic\n(Kawar et al. 2023), which interpolates target text and op-\ntimized source embeddings, fine-tune the generative model\nfor each image, which is quite costly, contrary to our work.\nSeveral works use attention injection, such as Plug-and-Play\n(Tumanyan et al. 2022) and Prompt-to-Prompt (Hertz et al.\n2022), where the latter requires a time-consuming caption of\nthe input image, unlike our method. Most of these methods\nfocus on altering a certain object (by replacement, removal\nor style change), or applying global changes (style or con-\ntent), in contrast to our focus on adding objects freely.\nInstruction-based approaches. Other methods can add\nobjects in a free manner. InstructPix2Pix (Brooks, Holyn-\nski, and Efros 2023) (subsequently fine-tuned by Mag-\nicBrush (Zhang et al. 2023)) produces (instruction, image)\npairs, used to train an instruction-conditioned DM. Emu Edit\n(Sheynin et al. 2023) is a more recent model trained on a\nwide range of learned task embeddings to enable instruction-\nbased image editing, however, it is not publicly available.\nDALL\u00b7E3 (Betker et al. 2023) is also proprietary, and modi-\nfies the entire image as demonstrated in Figure 1. DALL \u00b7E3\nand DALL \u00b7E 2 (Ramesh et al. 2022) apparently support\nmasked inpainting, but we are unaware of a publicly avail-\nable way to apply it to general real images. MGIE (Fu et al.\n2024) train a DM, utilizing a MLLM to derive expressive\ninstructions. These methods require the user to specify the\ndesired localization in words, which has a few shortcom-\nings. On the user\u2019s side, this requires effort, and it can be\ndifficult or impossible to describe the precise location. From\nthe model\u2019s side, failure to visually ground the text-specified\nlocation may fail to perform the desired edit, and/or make\nunintended changes in other locations instead.Input \u201cA big ship\u201d \u201cA sea monster\u201d \u201cIceberg\u201d\nInput \u201c Snowy\nmountains \u201d\u201cThe Great\nPyramid of Giza \u201d\u201cA town\u201d\nInput \u201cA river\u201d \u201cA hole in\nthe ground \u201d\u201cA sports car\u201d\nInput \u201c A volcano\nerupting \u201d\u201cA fortress\u201d \u201cSkiing slopes\u201d\nInput \u201cA bonfire\u201d \u201cA pile of sand\u201d \u201cflowers\u201d\nInput \u201cA small hut\u201d \u201cIced lake\u201d \u201cAn igloo\u201d\nFigure 3: Examples of Click2Mask outputs. The leftmost\ncolumn is the input image with clicked point. The other\ncolumns are Click2Mask outputs given the prompts below.\nSegmentation-based approaches. Segmentation meth-\nods have been utilized to overcome the need for a precise\nuser-provided mask. DiffEdit (Couairon et al. 2022) and\nEdit Everything (Xie et al. 2023) generate segmentation-\nbased masks by utilizing conditionings on diffusion steps, or\nSAM (Kirillov et al. 2023), but require an input image cap-\ntion, which is painstaking. InstructEdit (Wang et al. 2023a),\nwhich uses Grounding DINO (Liu et al. 2023) and SAM to\ngenerate a mask, does not require one, but requires a descrip-\ntion of the object to alter. This can cause errors due to failure\nof the model to localize. InstDiffEdit (Zou et al. 2024) gen-erates masks based on attention maps during denoising.\nThe segmentation-based methods, however, suffer from a\nfew limitations: (i) Such models need to \u201clock\u201d on an exist-\ning object or segment; consequently, in most cases they alter\nobjects, but do not add new free-form ones, which is our fo-\ncus. (ii) These methods typically require the user to provide\nan input caption or a description of the altered object.\nIn contrast to all the above, our work enables adding ob-\njects to real images (as opposed to merely altering existing\nones), without having to provide a precise mask, to describe\nthe input image, or target image, and without being con-\nstrained to boundaries of existing objects or segments. We\naim to enable edits where the manipulated area is not well-\ndefined in advance, and a free-form alteration is required.\n3 Blended Latent Diffusion\nBlended Latent Diffusion (BLD) (Avrahami, Fried, and\nLischinski 2023) is a method for local text-guided image\nmanipulation, based on Latent Diffusion Models (LDMs)\n(Rombach et al. 2022) and Blended Diffusion (Avrahami,\nLischinski, and Fried 2022). Given a source image x, a guid-\ning text prompt p, and a binary mask m, the model blends the\nsource latents (obtained by DDIM inversion (Song, Meng,\nand Ermon 2020)) with the prompt-guided latents through-\nout the LDM process, to derive a blended final output.\nInitially, inputs are converted to a latent space. A vari-\national auto-encoder (Kingma and Welling 2013) with en-\ncoder E(\u00b7)and decoder D(\u00b7), encodes xto latent space, s.t.\nzinit=E(x). In addition, mis downsampled to mlatent in\norder to meet latent spatial dimensions.\nIn each BLD step t, the following occurs:\n1. The latent resulting from the previous step, zt+1, under-\ngoes denoising conditioned by the prompt p, to yield zfg\n(we refer to the generated content as foreground , orfg).\n2. The original image latent zinitis noised to step t, yielding\nzbg(we refer to the original content as background ,bg).\n3. The next step ztis obtained by blending zfgandzbgusing\nmlatent:\nzt=zfg\u2299mlatent+zbg\u2299(1\u2212mlatent) (1)\nwhere\u2299denotes element-wise multiplication.\nAfter the final step, the output z0is decoded to obtain the\nfinal edited image \u02c6x=D(z0).\nHowever, because information is lost during the V AE\nencoding, the decoded final output \u02c6x, might exhibit some\nartifacts when the unmasked region has important fine-\ndetailed content (such as faces, text, etc.). Avrahami et\nal. (2023) solve this issue by optionally fine-tuning the de-\ncoder weights for each image after the denoising steps, and\nusing these weights to infer the final result. In our experi-\nments, we found that this optional background preservation\nprocess is no longer necessary (possibly due to improve-\nments in the Stable Diffusion V AE), and a final blending\nwith Gaussian feathering suffices (Figure 13 in appendix).\n4 Method\nGiven an image, a text prompt, and a user-indicated location\n(e.g., via a mouse click), our goal is to modify the imageInput Emu Edit MagicBrush InstructPix2Pix Click2Mask Input Emu Edit MagicBrush InstructPix2Pix Click2Mask\n\u201cHave a knife laying between the orange and apple\u201d \u201cAdd a cat behind the glass window looking at the food\u201d\n\u201cA knife\u201d \u201cA cat behind the glass window looking at the food\u201d\n\u201cAdd a fruit stand to the right of the image\u201d \u201cAdd USA for the bag\u201d\n\u201cA fruit stand\u201d \u201cUSA\u201d\n\u201cAdd fringe to the pink lampshade\u201d \u201cAdd a tennis ball on top of the racket\u201d\n\u201cFringe\u201d \u201cA tennis ball\u201d\nFigure 4: Comparisons with SoTA methods. Comparisons of Emu Edit (Sheynin et al. 2023), MagicBrush (Zhang et al. 2023)\nand InstructPix2Pix (Brooks, Holynski, and Efros 2023) with our model Click2Mask . Upper prompts were given to baselines,\nand lower ones to Click2Mask. The inputs contain the clicked point given to Click2Mask. See Figure 8 and appendix for\nadditional comparisons.\naccording to the prompt in an unspecified area roughly sur-\nrounding the provided point. We utilize Blended Latent Dif-\nfusion (BLD) (Avrahami, Fried, and Lischinski 2023) as our\nimage editing backbone, but rather than providing it with\na fixed mask at the outset, we evolve a mask dynamically\nthroughout the diffusion process. We initialize the process\nwith a large mask around the indicated point, and gradually\ncontract the mask towards the center, while guiding the rate\nof contraction along the mask boundary using a semantic\nalignment loss based on Alpha-CLIP (Sun et al. 2023).\nThis iterative process results in a mask whose shape and\nsize are determined by both the text prompt, the content, and\nthe structure of the original input image. Furthermore, the\nshape of the mask adjusts itself to the emerging object, as\nthe mask\u2019s evolution is determined by the gradients obtained\nby the semantic alignment loss (see Section 4.1), which in\nturn depend on the shape of the object being generated (see\nFigure 2 for mask evolution illustration, and Figure 6 for\nexamples of generated masks). Once the mask has settled\ninto its final form, we run BLD once more, using the final\nmask to generate the final result. Our method is outlined in\nAlgorithm 1 and illustrated in Figure 5.\n4.1 Dynamic Mask Evolution\nGiven an image x, a text prompt p, and a user-provided loca-\ntionc, we aim to modify x, so as to align with p, in proxim-\nity to c. We start by encoding the input image zinit=E(x).\nWe also create a 2D potential height-field \u03a6in latent space,\nwhich is initialized to a Gaussian around c.\nWe now perform the BLD process, where at each step twe\nobtain a binary mask Mtby thresholding the potential \u03a6us-\ning a threshold \u03c4. The mask evolves dynamically through the\nBLD process, since the threshold \u03c4and the potential \u03a6areboth updated at each step: the threshold \u03c4increases, while\nthe potential \u03a6is elevated in important areas to ensure they\nremain above the threshold. This prevents the mask from\nshrinking in spatial areas that emerge as important for align-\nment of the generated new content with the guiding prompt\np. As a consequence, the mask evolves into a shape deter-\nmined by the newly generated object.\nCommencing the blending at 25% of the diffusion steps,\nthe initial threshold value \u03c4initis relatively low, such that Mt\nis sufficiently large at the beginning ( \u223c16% of the image).\nThis enables BLD to capture the desired edit, as demon-\nstrated in Figure 7 (this idea was originally introduced in\nBLD to cope with the case of small or thin input masks).\nOn the other hand, we prevent the generation of overly large\nmasks, that would result in large scale changes that might\nnot blend seamlessly with the original content, by increas-\ning\u03c4geometrically, and by initiating the potential elevation\nat a later stage (40% out of total diffusion steps). However,\nwe initiate the potential elevation at an early enough stage\nwhen the blended image is still noisy and can be modified.\nWe stop mask evolution when spatial structure is close to be\ndetermined (at 50% of total diffusion steps).\nThe potential elevation is obtained by generating the es-\ntimated final image \u02dcx0at each step, and calculating the co-\nsine distance between the CLIP (Radford et al. 2021) em-\nbeddings of \u02dcx0and the guidance prompt p.\u02dcx0is obtained\nby blending a predicted final foreground latent \u02dczfg, with the\noriginal latent background zinit:\n\u02dcz0= \u02dczfg\u2299Mt+zinit\u2299(1\u2212Mt) (2)\nThe decoded \u02dcx0=D(\u02dcz0)is passed alongside the current\nmask Mtand the prompt pto Alpha-CLIP to focus on the\narea of Mt. The gradient of the cosine distance with respectAlgorithm 1: Click2Mask\nGiven: models LDM ={noise(z, t),denoise (z, p, t )\u2192\n(zt, z0)},VAE={E(x), D(z)},BLD ={(x, p, m, t )\u2192zt},\nAlpha-CLIP ={\u03b1CLIP(x, m, p )\u2192Sim CLIP}, and hyper pa-\nrameters {\u03c4n...l, lr}with schedulers {n, r, k, l }\nInput: input image x, text prompt p, target coordinates c\nOutput: edited image bxthat matches the prompt pin proximity\nofc, and complies to xoutside edited region\n\u03a6 = Gaussian(c)\nzinit=E(x)\nzn\u223cnoise(zinit, n)\nforalltfromntoldo\nzbg\u223cnoise(zinit, t)\nzfg,\u02dczfg\u223cdenoise (zt, p, t)\nG= 0\nift < r then\n\u02dcz0= \u02dczfg\u2299Mt+zinit\u2299(1\u2212Mt)\nSt\u223c\u03b1CLIP(D(\u02dcz0),upscale (Mt), p)\nG\u223c |gradients (St, Mt)|\nzfg\u223cBLD(x, p, M t, t)\nend if\nift < k andSt> St+1then exit loop\nMt= (\u03a6 + G\u2217lr)> \u03c4t\nzt=zfg\u2299Mt+zbg\u2299(1\u2212Mt)\nend for\nbz\u223cBLD(x, p, M t,0)\nreturn D(bz)\nto the latent mask pixels is then calculated by backpropa-\ngating through the CLIP embeddings and the decoder. The\nlarger the absolute gradient of the cosine distance (i.e. CLIP\nloss) with respect to a pixel in Mt, the more significant this\nlocation is for the alignment of the generated content to the\nprompt p. Adding the absolute gradient values Gto\u03a6, ele-\nvates important areas in the \u03a6height-field (around Mt\u2019s con-\ntour for stable evolution \u2013 Figures 14 and 15 in appendix).\nHalfway through the mask evolution steps, we utilize the\nAlpha-CLIP loss as an optional stopping point for Mt\u2019s evo-\nlution, if the loss did not decrease in subsequent steps.\nAfter each update of Mt, we restart the BLD process, let-\nting it proceed from the beginning to the current step t, us-\ning the mask Mtas a fixed mask. This is to allow pixels\nthat were added (or removed) in Mtto affect the generated\nimage from the beginning (see Figure 16 in appendix).\nWe then apply Equation (1) and blend zfgwithzbgusing\nthe mask Mt, which provides zt\u22121, the input to next step.\nAfter all mask evolution steps have been completed, we\nperform a final BLD run using the final Mtwith several\nseeds to obtain several candidate results, where the best one\nis filtered by Alpha-CLIP. As noted earlier, rather than fine-\ntuning the V AE decoder weights to preserve the original\nbackground details outside the mask, we employ instead a\nsimple Gaussian mask feathering when blending the BLD\noutput and the original input image (in pixel space).5 Results\nGiven that our method is mask-free, we compare ourselves\nto mask-free image editing methods, with the slight differ-\nence being that a clicked point replaces any parts of the\nprompt that describe the edit location. To begin with, we\ncompare to MagicBrush, which is the SoTA method among\nthe open-source models. In addition, we compare to Emu\nEdit, which is the SoTA among closed-source models. Since\nwe are unable to run Emu Edit ourselves, we must rely on the\nEmu Edit Benchmark (Sheynin et al. 2023), which includes\nimages generated by Emu Edit. This benchmark contains\nimages with several categories of editing instructions, such\nas adding objects, removing objects, altering style, etc., and\ncan be filtered by these categories. As our focus is adding\nobjects to images, we filtered the dataset by the \u201caddition\u201d\ninstruction. This resulted in 533 items, from which we ran-\ndomly sampled an evaluation subset of 100 samples.\nWe perform the following fixed routine for each sample:\n(i) Removed the word that instructs addition (e.g., \u201cAdd\u201d,\n\u201cInsert\u201d), (ii) removed the part that describes the edit loca-\ntion, and instead (iii) clicked on the image to direct the edit-\ning location. For instance, the instruction \u201cAdd a black base-\nball cap to the man on the left\u201d becomes \u201cA black baseball\ncap\u201d (non-localized instruction).\nFollowing Emu Edit (Sheynin et al. 2023) and BLD\n(Avrahami, Fried, and Lischinski 2023), each sample run\nproduces multiple results internally (comprising of three\nmask evolutions, each followed by a batch of 8 outputs), and\noutputs the best result, as determined automatically using\nAlpha-CLIP scoring.\nTo evaluate our results, we compared these 100 outputs\ngenerated by Click2Mask, with the outputs generated by\nEmu Edit and by MagicBrush (which ran with the origi-\nnal edit instructions). We conducted the evaluation through\na user study (Section 5.1), as well as through automatic met-\nrics (Section 5.2). In both cases, our method outperformed\nthe SoTA methods.\n5.1 Human Evaluation\nWe conducted a user study, where participants were given\na random batch of survey items out of 200 total items (100\nitems comparing to each model). Each item included an in-\nput image, the original edit instruction, and a pair of edited\nimages: one generated by our model, and the other generated\nby either Emu Edit or MagicBrush. Participants were asked\nto rank which of the edited images performed better accord-\ning to three criteria: executing the instruction, not adding any\nother edits or artifacts, and generating a realistic image. The\nsurvey was completed by 149 participants. Each of the 200\nitems was rated by at least 5 users, where the average rate\nwas 15.67 users in Emu Edit, and 8.06 users in MagicBrush.\nIn order to compare Click2Mask vs. Emu Edit, as well\nas Click2Mask vs. MagicBrush, while taking into account\n\u201cties\u201d (ratings stating equal performance on an item, or items\nwith equal ratings to both methods), we analyzed the results\nusing the following metrics: (A) The percentage of items in\nwhich each method was preferred by the majority, disregard-\ning ties. (B) For each item we counted if the majority votedOutputBlended Latent Diffusion\nInputMask Evolution noise Encoder\"A surfboard\"\nDecoder\n\"A surfboard\"\nDecoder\n downsample \nDenoise\n        Thresholding\n         Addition         Blend with mask  ` update \n  gradients \"A surfboard\"\nFigure 5: Click2Mask: An illustration of our method as described in Algorithm 1. The green block is BLD process, performing\ndiffusion steps while blending noised input latents with text guided latents. The pink block is the mask evolution process, where\nwe utilize Alpha-CLIP to evaluate the gradients with respect to the mask Mtpixels, using them to update Mt, obtaining Mt\u22121.\nInputGenerated\nMaskOutput InputGenerated\nMaskOutput\n\u201cA bag of chips\u201d \u201cA person\u2019s hand holding the phone\u201d\n\u201cA reflection of a ghost\u201d \u201cA ceiling fan\u201d\n\u201cA bunch of grapes\u201d \u201cLeaves on the trees\u201d\nFigure 6: Examples of generated masks. For each triplet,\ngiven an input image with clicked point (left) and a prompt\n(below), a purple overlay shows the generated mask (mid-\ndle). The rightmost image is Click2Mask output.\nfor a tie, and if so marked it as a \u201ctied item\u201d. For the other\n\u201cnon-tied items\u201d, we conducted the same majority vote anal-\nysis described in A. (C) The number of total ratings for each\nmethod. In each parameter our method surpassed the closed-\nsource SoTA method Emu Edit, and the open-sourced SoTA\nMagicBrush, as shown in Table 1. See Figures 1, 4 (and Fig-\nures 17, 18, 19, 20 in appendix) for qualitative comparisons\nto the baselines alongside with InstructPix2Pix, and Figure 8\nwith a detailed comparison. Statistical significance analysis\ncan be found in the appendix.\n5.2 Automatic Metrics\nUtilizing the input captions and output captions (describ-\ning the desired output) provided in Emu Edit benchmark,\na variety of metrics were used to assess each method\u2019s\noutputs on the sampled items: (i) Directional CLIP (Gal\net al. 2022) similarity (CLIP direct) to measure correspon-\ndence of the change between input and output images, with\nthe change between input and output captions. (ii) CLIP sim-\nNo enlarge\n Ours\nInput 27% 29% 31% 34% 40% Output\nFigure 7: Ablation study: No mask enlargement at early\nstages. As explained in Section 4, we begin with a large\nmask (about 16% of the image), to capture the desired edit\ninMt. Top row: Mt(marked by purple contours on decoded\n\u02dczfgs, throughout diffusion steps indicated by percentages be-\nlow) evolves without a large starting point, and the diffu-\nsion identifies and guides the white dog to the prompt \u201cHuge\nbone\u201d , while the small Mtfails capturing the bone. Bottom\nrow: Click2Mask\u2019s enlarged Mtcaptures the guided area\neven though the dog is also initially identified as the bone.\nilarity between output image and output caption, to mea-\nsure the compliance between desired and generated outputs\n(CLIP out). (iii) Mean L1 pixel distance between input and\noutput images, to measure the amount of change in the en-\ntire image (L1). (iv) In addition, we present a new metric,\nEdited Alpha-CLIP ( \u03b1CLIP edit).\nEdited Alpha-CLIP. Besides evaluating the images glob-\nally, it is beneficial to evaluate the edited region . We offer an\nEdited Alpha-CLIP procedure to overcome the lack of input\nor output masks in Emu Edit and MagicBrush: we extract\na mask specifying the edited area in the generated image,\nand calculate the Alpha-CLIP similarity between the masked\ngenerated image and the instruction (removing words de-\nscribing addition and edit locations as mentioned in Sec-\ntion 5). See Appendix A.4 and Figure 12 in appendix for\ndetails and extracted masks demonstrations.\nTable 2 shows that our method surpassed both Emu Edit\nand MagicBrush in all metrics: higher scores in all CLIP-\nbased metrics, indicating stronger similarities, and lower L1\ndistance indicating better compliance with input image.Input Emu Edit MagicBrush InstructPix2Pix Click2Mask\n\u21d4 \u21d4 \u2205\n\u201cAdd a piece of fried chicken to the plate\u201d\n\u201cPiece of fried chicken\u201d\n\u22bc,\u25a0 \u22bc \u22bc\n\u201cAdd Bigfoot in the background along-side one of the cows\u201d\n\u201cBigfoot\u201d\n\u21ab \u21ab \u21ab\n\u201cAdd a butterfly on top of the beans\u201d\n\u201cA butterfly\u201d\n\u2205 \u2205 \u25a0\n\u201cAdd Jupiter to the sky\u201d\n\u201cJupiter\u201d\nFigure 8: Failure cases of baselines . Baselines suffer occa-\nsionally from replacing an existing object instead of adding\none (\u21d4), misplacing the object ( \u21ab), modifying other ob-\njects (\u22bc), altering the image globally ( \u25a0), or failing to pro-\nduce an edit ( \u2205). For additional comparisons to baselines,\nsee Figure 4 and appendix.\n5.3 Ablation Study\nWe conducted several ablation studies to analyze the im-\npact of various components on the overall performance of\nour model. Figure 7 demonstrates the need for a sufficiently\nlarge mask on early diffusion steps. See additional ablation\nstudies in Appendix A.2 accompanied by Figures 13 to 16.\n6 Model Limitations\nDuring the evolution process, our model encounters a diffi-\nculty converging to a small, finely detailed mask shape (e.g.\na dog collar). Additionally, since text guidance in Stable Dif-\nfusion is not spatially driven, BLD sometimes has difficulty\nadding the desired object to the masked area when a similar\nobject is nearby in the unmasked area (e.g., adding a Big-\nfoot next to a person). Since we use BLD as our backbone,\nwe sometimes encounter this problem. However, we have\nconsiderably improved it in comparison to BLD by optimiz-\ning the progressive mask shrinking process, and applying it\nacross all objects, not just thin objects, as part of our mask\nevolution process. Moreover, in comparison to other SOTA\nmethods, they often fail to add the desired object even if a(A) (B) (C)\nMethod % Majority% Tied\nitems% Majority\nfrom non-tied# Total\nvotes\nEmu Edit 42.86%35%47.69% 416\nClick2Mask 57.14% 52.31% 465\nMagicBrush 16.30%27%15.07% 148\nClick2Mask 83.70% 84.93% 362\nTable 1: Human evaluation results. Comparisons of (A): %\nof items each method received majority votes, disregarding\nties. (B): % of items the majority voted as tie (left), and %\nof items \u2013 out of the other non-tied items \u2013 each method re-\nceived majority votes (right). (C): Total votes. Refer to Sec-\ntion 5 for details.\nMethod CLIP direct \u2191 CLIP out\u2191\u03b1CLIP edit\u2191 L1\u2193\nEmu Edit 0.150 0.331 0.186 0.046\nMagicBrush 0.095 0.324 0.166 0.049\nClick2Mask 0.204 0.334 0.195 0.027\nTable 2: Automatic metrics results. Evaluation using au-\ntomatic metrics. CLIP direct measures consistency between\nchanges (from input to output) in images and captions,\nCLIP outmeasures similarity between output image and\noutput caption, \u03b1CLIP editmeasures similarity to the non-\nlocalized instruction in the edited area, and L1 measures the\nalignment with the input image. See Section 5 for details.\nsimilar one is not present, and our method outperforms them\nin both cases. See Figure 9 for examples of these cases.\nInput \u201cA golden necklace\u201d\nInput \u201cA white dog\u201d\nFigure 9: Limitations Top row: the evolving mask has a dif-\nficulty converging to a small fine detailed mask shape as a\ngolden necklace. Bottom row: our Stable Diffusion back-\nbone guides the image to a given prompt globally. When\nthe prompt content already exists in the input image close\nto the generated mask (i.e. the white dog), sometimes the\nBLD process we incorporate does not succeed in directing\nthe guidance into the masked region.\n7 Conclusion\nClick2Mask presents a novel approach for local image gen-\neration, freeing users from having to specify a mask, or de-\nscribing the input or target images, and without being con-strained to existing objects. We look forward to users apply-\ning our method with the source code that is available in the\nproject page (see Footnote in Page 1), either to edit images\nor to embed the method for generating or fine-tuning masks.\nReferences\nAvrahami, O.; Aberman, K.; Fried, O.; Cohen-Or, D.; and\nLischinski, D. 2023. Break-A-Scene: Extracting Multiple\nConcepts from a Single Image. In SIGGRAPH Asia 2023\nConference Papers , SA \u201923. New York, NY , USA: Associa-\ntion for Computing Machinery. ISBN 9798400703157.\nAvrahami, O.; Fried, O.; and Lischinski, D. 2023. Blended\nLatent Diffusion. ACM Trans. Graph. , 42(4).\nAvrahami, O.; Lischinski, D.; and Fried, O. 2022. Blended\nDiffusion for Text-Driven Editing of Natural Images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , 18208\u201318218.\nBar-Tal, O.; Ofri-Amar, D.; Fridman, R.; Kasten, Y .; and\nDekel, T. 2022. Text2live: Text-driven layered image and\nvideo editing. In European Conference on Computer Vision ,\n707\u2013723. Springer.\nBetker, J.; Goh, G.; Jing, L.; Brooks, T.; Wang, J.; Li, L.;\nOuyang, L.; Zhuang, J.; Lee, J.; Guo, Y .; Manassra, W.;\nDhariwal, P.; Chu, C.; Jiao, Y .; and Ramesh, A. 2023. Im-\nproving Image Generation with Better Captions.\nBrooks, T.; Holynski, A.; and Efros, A. A. 2023. Instruct-\nPix2Pix: Learning to Follow Image Editing Instructions. In\nCVPR .\nCaron, M.; Touvron, H.; Misra, I.; J \u00b4egou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging Properties\nin Self-Supervised Vision Transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV) , 9650\u20139660.\nCouairon, G.; Verbeek, J.; Schwenk, H.; and Cord, M. 2022.\nDiffEdit: Diffusion-based semantic image editing with mask\nguidance. arXiv:2210.11427.\nDhariwal, P.; and Nichol, A. 2021. Diffusion models beat\nGANs on image synthesis. Advances in neural information\nprocessing systems , 34: 8780\u20138794.\nFu, T.-J.; Hu, W.; Du, X.; Wang, W. Y .; Yang, Y .; and Gan,\nZ. 2024. Guiding Instruction-based Image Editing via Multi-\nmodal Large Language Models. In The Twelfth International\nConference on Learning Representations .\nGal, R.; Patashnik, O.; Maron, H.; Bermano, A. H.; Chechik,\nG.; and Cohen-Or, D. 2022. StyleGAN-NADA: CLIP-\nguided domain adaptation of image generators. ACM Trans.\nGraph. , 41(4).\nHertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.; Pritch,\nY .; and Cohen-Or, D. 2022. Prompt-to-prompt image editing\nwith cross attention control.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion\nprobabilistic models. Advances in neural information pro-\ncessing systems , 33: 6840\u20136851.Kawar, B.; Zada, S.; Lang, O.; Tov, O.; Chang, H.; Dekel, T.;\nMosseri, I.; and Irani, M. 2023. Imagic: Text-Based Real Im-\nage Editing with Diffusion Models. In Conference on Com-\nputer Vision and Pattern Recognition 2023 .\nKingma, D. P.; and Welling, M. 2013. Auto-Encoding Vari-\national Bayes. arXiv:1312.6114.\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.;\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-\nY .; Doll \u00b4ar, P.; and Girshick, R. 2023. Segment Anything.\narXiv:2304.02643 .\nLiu, S.; Zeng, Z.; Ren, T.; Li, F.; Zhang, H.; Yang, J.; Li,\nC.; Yang, J.; Su, H.; Zhu, J.; and Zhang, L. 2023. Ground-\ning DINO: Marrying DINO with Grounded Pre-Training for\nOpen-Set Object Detection. arXiv:2303.05499.\nNichol, A.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin,\nP.; McGrew, B.; Sutskever, I.; and Chen, M. 2022. GLIDE:\nTowards Photorealistic Image Generation and Editing with\nText-Guided Diffusion Models. arXiv:2112.10741.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\nDesmaison, A.; K \u00a8opf, A.; Yang, E.; DeVito, Z.; Raison, M.;\nTejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;\nand Chintala, S. 2019. PyTorch: An Imperative Style, High-\nPerformance Deep Learning Library. arXiv:1912.01703.\nPearson, K. 1900. X. On the criterion that a given system of\ndeviations from the probable in the case of a correlated sys-\ntem of variables is such that it can be reasonably supposed\nto have arisen from random sampling. The London, Edin-\nburgh, and Dublin Philosophical Magazine and Journal of\nScience , 50(302): 157\u2013175.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision.\narXiv:2103.00020.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen,\nM. 2022. Hierarchical Text-Conditional Image Generation\nwith CLIP Latents. arXiv:2204.06125.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition , 10684\u2013\n10695.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton,\nE. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan,\nB.; Salimans, T.; et al. 2022. Photorealistic text-to-image\ndiffusion models with deep language understanding. Ad-\nvances in Neural Information Processing Systems , 35:\n36479\u201336494.\nSheynin, S.; Polyak, A.; Singer, U.; Kirstain, Y .; Zohar,\nA.; Ashual, O.; Parikh, D.; and Taigman, Y . 2023. Emu\nEdit: Precise Image Editing via Recognition and Generation\nTasks. arXiv:2311.10089.\nSong, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502 .Sun, Z.; Fang, Y .; Wu, T.; Zhang, P.; Zang, Y .; Kong, S.;\nXiong, Y .; Lin, D.; and Wang, J. 2023. Alpha-CLIP: A CLIP\nModel Focusing on Wherever You Want. arXiv:2312.03818.\nTumanyan, N.; Geyer, M.; Bagon, S.; and Dekel, T. 2022.\nPlug-and-Play Diffusion Features for Text-Driven Image-to-\nImage Translation. arXiv:2211.12572.\nWang, Q.; Zhang, B.; Birsak, M.; and Wonka, P. 2023a. In-\nstructEdit: Improving Automatic Masks for Diffusion-based\nImage Editing With User Instructions. arXiv:2305.18047.\nWang, S.; Saharia, C.; Montgomery, C.; Pont-Tuset, J.; Noy,\nS.; Pellegrini, S.; Onoe, Y ."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary of \"DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer\"\n\n#### 1. Abstract:\n**Overview:**\n- **Goal:** Introduce DreamBeast, a novel method for generating 3D fantastical animal assets with distinct, user-specified body parts.\n- **Challenge:** Existing Score Distillation Sampling (SDS) methods struggle with part-level semantics and are computationally intensive.\n- **Solution:** Implement part-aware knowledge transfer from the Stable Diffusion 3 (SD3) model into a 3D Part-Affinity representation for efficient and high-quality 3D asset generation.\n- **Result:** DreamBeast significantly improves part-aware 3D asset quality and reduces computational overhead, as demonstrated by quantitative and qualitative evaluations.\n\n#### 2. Introduction:\n**Context and Motivation:**\n- Current methods (e.g., DreamFusion, SJC) that lift 2D guidance into 3D often fail to generate complex, artistic, or compositional shapes due to a lack of part-level understanding in text-to-image diffusion models.\n- Motivated by the recent advancements in diffusion models, especially SD3, which shows better part-level semantic understanding, although it still presents challenges in 3D application due to computational inefficiencies and issues like the multi-face Janus problem.\n\n**Research Questions:**\n- How can the part-level understanding from powerful models like SD3 be effectively harnessed for 3D asset generation?\n- Can we achieve high-quality part-aware 3D generation while minimizing computational resources?\n\n**Literature Review:**\n- Techniques for 3D generation: Lifting 2D diffusion models for 3D (e.g., DreamFusion) and layout guided 3D generation.\n- Part-level semantics in diffusion models: Cross-attention mechanisms and score distillation sampling.\n- Introduction of novel methodologies necessary for overcoming existing limitations in part-aware 3D object generation.\n\n#### 3. Related Work:\n**Review of Relevant Research:**\n- **Lifting 2D Diffusion Models for 3D:** Existing methods like DreamFusion and SJC use 2D priors to guide 3D generation but lack fine-grained part controllability.\n- **Layout Guided 3D Generation:** Previous works use compositional models and adversarial learning for scene generation, but these approaches are not well-suited for detailed part-to-object tasks.\n- **Diffusion with Cross-Attention Control:** Existing cross-attention techniques in 2D are reviewed, but their application to 3D SDS settings remains unexplored.\n\n**Advances Presented:**\n- Introduces part-aware knowledge transfer and an implicit 3D Part-Affinity representation to bridge the gap between 2D semantic understanding and 3D asset generation.\n\n#### 4. Method:\n**Proposed Approach:**\n- **Score Distillation Sampling (SDS):** Employs SDS to guide the optimization of 3D structures using 2D diffusion priors.\n- **Novel Part-Aware Knowledge Transfer:**\n\n  **Steps:**\n  1. **Initial Optimization:** Perform initial SDS steps to partially optimize the NeRF model, producing a coarse layout.\n  2. **Part Affinity Map Extraction:** Generate part affinity maps using cross-attention maps from SD3.\n  3. **Part-Affinity NeRF Creation:** Train a Part-Affinity NeRF using the extracted part affinity maps for continuous viewpoint rendering.\n  4. **Attention Modulation in SDS:** Use the Part-Affinity NeRF to modulate cross and self-attention in the diffusion model, improving part-aware responses during SDS.\n \n**Mathematical Formulations:**\n- **Part Affinity Map Computation:** \n  - \\( M_{p,c} = \\frac{1}{(t_e - t_s) \\cdot L \\cdot |I_p|} \\sum_{t=t_s}^{t_e} \\sum_{l=0}^{L} \\sum_{i \\in I_p} M_{t,l,i,c} \\)\n  \n**Architectural Detail:**\n- **Part-Affinity NeRF:** Implemented as an MLP with one hidden layer, providing a continuous representation of part affinity maps.\n  \n**Algorithm:**\n- **Cross-Attention & Self-Attention Modulation:** Algorithm 1 details the modulation using part affinity maps to enhance the responses to part-specific prompts during SDS.\n\n#### 5. Experimental Setup:\n**Datasets:**\n- **Dataset:** Includes multi-view consistent 3D rendering tasks with randomly generated prompts describing fantastical creatures.\n  \n**Hardware and Software:**\n- **Hardware:** Implementation tested on an NVIDIA A40 GPU.\n- **Software:** Utilizes SD3 for 2D part-aware knowledge extraction, MVDream for diffusion guidance in 3D.\n\n**Evaluation Metrics:**\n- **CLIP Similarity:** Measures how well-generated 3D assets match textual prompts.\n- **User Study Ratings:** Evaluations from human participants to assess part correspondence and overall 3D asset quality.\n\n**Baselines for Comparison:**\n- **Comparison Methods:** Includes DreamFusion, MVDream, and other state-of-the-art techniques.\n\n#### 6. Results and Analysis:\n**Empirical Findings:**\n- **CLIP Scores:** DreamBeast achieves higher CLIP scores across various architectures, indicative of better part-aware generation.\n- **Running Time:** Significantly reduces the computational time to 78 minutes compared to 7 hours with SD3.\n  \n**Figures and Tables:**\n- **Figures 4 & 5:** Illustrate latency improvements and part-awareness consistency across different models.\n- **Table 1 & 2:** Comparison of part-aware generation success rates and CLIP scores, showing DreamBeast's superiority.\n\n**Analysis:**\n- **Ablation Study:** Demonstrates that increasing camera views while training Part-Affinity NeRF enhances part-specific interpolation, improving fidelity.\n  \n**Unexpected Results:**\n- The multi-view and part-affinity integration notably improve the robustness and consistency of part-aware generation.\n\n#### 7. Discussion:\n**Interpretation of Results:**\n- DreamBeast's part-affinity leverage directly translates to better part precision and multi-view consistency in generated 3D assets.\n\n**Implications:**\n- Offers a feasible way to produce high-quality, fantastical 3D characters with user-specified component control, enhancing creative expression in digital content creation.\n\n**Limitations:**\n- While DreamBeast reduces computational overhead, it still requires significant training resources and may depend on the availability of high-quality diffusion models.\n\n#### 8. Conclusion:\n**Key Contributions:**\n- Introduces DreamBeast, an efficient and novel mechanism for part-aware 3D asset generation.\n- Demonstrates substantial improvements in part-specific accuracy and generation efficiency.\n\n**Future Directions:**\n- Explore more efficient architectures for large-scale deployment.\n- Investigate applications in broader domains beyond 3D fantastical creatures (e.g., customized virtual environments).\n\nBy following this comprehensive and detailed summary, a researcher should be able to replicate the study, build upon it, or integrate its findings and methodologies into related work. The steps, algorithms, and empirical validations provided create a roadmap for understanding and implementing DreamBeast in various contexts."}, {"role": "assistant", "content": "DreamBeast: Distilling 3D Fantastical Animals with Part-Aware\nKnowledge Transfer\nRunjia Li1Junlin Han1Luke Melas-Kyriazi1Chunyi Sun2Zhaochong An3\nZhongrui Gui1Shuyang Sun1Philip Torr1Tomas Jakab1\n1University of Oxford2Australian National University3University of Copenhagen\ndreambeast3d.github.io\nFigure 1. Generated fantastic 3D beasts composed of diverse animal parts. Our method enables part-level generation, resulting in 3D\ncreatures with unique combinations of heads, limbs, wings, tails, and bodies.\nAbstract\nWe present DreamBeast, a novel method based on score\ndistillation sampling (SDS) for generating fantastical 3D\nanimal assets composed of distinct parts. Existing SDS\nmethods often struggle with this generation task due to\na limited understanding of part-level semantics in text-to-\nimage diffusion models. While recent diffusion models, such\nas Stable Diffusion 3, demonstrate a better part-level under-\nstanding, they are prohibitively slow and exhibit other com-\nmon problems associated with single-view diffusion mod-\nels. DreamBeast overcomes this limitation through a novel\npart-aware knowledge transfer mechanism. For each gener-\nated asset, we efficiently extract part-level knowledge from\nthe Stable Diffusion 3 model into a 3D Part-Affinity im-plicit representation. This enables us to instantly generate\nPart-Affinity maps from arbitrary camera views, which we\nthen use to modulate the guidance of a multi-view diffusion\nmodel during SDS to create 3D assets of fantastical ani-\nmals. DreamBeast significantly enhances the quality of gen-\nerated 3D creatures with user-specified part compositions\nwhile reducing computational overhead, as demonstrated\nby extensive quantitative and qualitative evaluations.\n1. Introduction\nImagine a creature taking flight, its dragon wings catch-\ning the sunlight. Picture its majestic lion\u2019s head surveying\nthe landscape, while a sinuous serpent\u2019s tail trails behind.\n1arXiv:2409.08271v1  [cs.CV]  12 Sep 2024What if \u201dFantastic Beasts and Where to Find Them\u201d was\nnot just a magical story, but we could actually build them in\na digital 3D world? Current methods for generating 3D ob-\njects [1, 3, 15, 18, 38, 48, 53] struggle with generating com-\nplex, artistic, or fantastical shapes and textures, which are\nnot represented in existing datasets. For example, they are\nunable to produce Griffin-like animals composed of parts\nfrom multiple species. More generally, they struggle with\nproducing objects composed of multiple diverse parts.\nOne of the most promising current approaches to open-\nworld 3D asset generation consists of lifting 2D guid-\nance into 3D. Methods such as DreamFusion [32] and\nSJC [44] demonstrate how pre-trained 2D diffusion mod-\nels [12, 34, 35, 37] can guide the generation of 3D ob-\njects through score distillation sampling (SDS). Specifi-\ncally, these methods produce 3D objects from textual de-\nscriptions by utilizing the priors encoded in 2D diffusion\nmodels, which act as approximate log gradients of the den-\nsity of distribution of 2D images conditioned on text.\nThe lifting methods, however, fall short of providing\npart-level controllability for part-specific textual descrip-\ntions. The reason for this is twofold. First, there have not\nbeen any 2D diffusion models capable of sufficiently strong\npart-level understanding. Second, in part due to the first\nreason, there have been no methods proposed in the litera-\nture for part-aware lifting-based (SDS) text-to-3D genera-\ntion from part-specific textual prompts.\nA creature with a body of a \nskunk and wings of a falcon \nand claws of a bear \nA creature with a body of a \njellyfish and a beak of a \nhawk and legs of a kangaroo A creature with a body of a \nfrog and a wingspan of a \nvulture and tail of a squirrel A creature with a body of \na bat and fins of a dolphin MVDream Stable Diffusion 3 \nFigure 2. Comparison of diffusion models on part-level prompt\nunderstanding in 2D generation. Although MVDream can grasp\nthe overall semantic understanding of the described animals, the\ngenerated images often feature deformed animals and fail to accu-\nrately capture specific part-based descriptions, unlike SD3.\nDiffusion models are improving rapidly, and the recent\nrelease of Stable Diffusion 3 (SD3) [12] has led us to recon-\nsider the subject of part-level generation. The starting point\nfor our paper is the observation that SD3 can capture part-\nlevel correspondences significantly better than prior mod-\nels (as shown quantitatively in Table 1 and qualitatively in\nFigure 2). This new capability allows for the generation of\ncomplex part-aware entities through part-specific text de-\nA creature with a dog body and  cat head \nA creature with a human body and  monkey head \nA creature with a head  of a bull and the body of a horse  t=450, l=11  t=450, l=8  t=850, l=11 \nFigure 3. Failing to generate part-aware content even with part\nunderstanding in SD3. Despite its understanding of part corre-\nspondences, as evidenced by the cross-attention maps at certain\ntimesteps tand layers l, SD3 may still fail to generate part-aware\nimages. This is illustrated in above examples where specific ani-\nmal parts are absent, highlighted in red. Our method capitalizes on\nthe observation that only particular timesteps tand layers lexhibit\npart-awareness.\nscriptions. However, such fine-grained understanding capa-\nbilities are not yet available for 3D generation.\nAs SDS is potentially capable of lifting any entities from\n2D to 3D, a straightforward approach might combine SD3\nwith SDS. However, as shown in Figure 3, we observe\nthat SD3 occasionally struggles to generate animals accord-\ning to prompts that specify particular animal body parts,\neven though it understands where those parts should be in\nthe cross attention maps. This issue arises because part-\ncorrespondence understanding is only reliable at certain\ntimesteps and transformer layers, making the SDS process\nless robust to prompts that focus on specific parts. Addi-\ntionally, naively using SD3 within SDS is leading to multi-\nple issues. SD3\u2019s use of the rectified flow match Euler dis-\ncrete scheduler results in deformed outputs with standard\ntimestep sampling used for other diffusion models, as seen\nin Figure 4 and 9. Other issues associated with SDS such\nas multi-face Janus problem and content drift [37] are also\npresent. Furthermore, generating 3D assets with SD3 takes\n7 hours, which can be prohibitive in many applications.\nTo overcome the aforementioned challenges, we intro-\nduce DreamBeast, a novel part-aware knowledge transfer\nmodule designed to efficiently distill part-level understand-\ning from powerful single-view diffusion models, such as\nSD3, and inject it into 3D generation with SDS.\nDreamBeast first performs SDS over several steps to\npartially optimize the NeRF, producing a coarse yet view-\nconsistent layout of the animal\u2019s shape. We then render the\n2A creature with a head  of a hedgehog  and a tail of a scorpion  \nand fins of a whale \nMVDream Dreamfusion \n(SD3) Ours MVDream Dreamfusion \n(SD3) Ours \nA creature with a mane  of a lion and a body  of a cheetah \nFigure 4. MVDream and SD3 have difficulty generating part-\naware 3D animals. While SD3 [12] can understand part corre-\nspondence in images and text, it struggles to generate 3D assets\nusing SDS due to the issues we discussed in our paper. MV-\nDream [37] falls short because it was fine-tuned on Objaverse [9],\nwhich lacks part-level information in the dataset.\nDreamfusion(SD3)\n+MVDreamDreamfusion\n(SD3)Ours MVDream0100200300400500Running Time (minutes)480 min\n420 min\n78 min\n50 minPart-AwareComparison of Running Speed\nFigure 5. Running speed comparison. While Dreamfusion\n(SD3) combined with MVDream and standalone Dreamfusion\n(SD3) take 480 and 420 minutes respectively, our method sig-\nnificantly reduces the runtime to 78 minutes. This reduction is\nachieved without sacrificing part-awareness making our method\nboth faster and more effective in part-aware 3D generation. More\ndetail in Appendix D.\nNeRF from a limited number of camera viewpoints. We use\nthese renderings as the denoising condition for SD3 and per-\nform several denoising steps during which we extract cross-\nattention maps of part-specific tokens from part-aware lay-\ners. We average these cross-attention maps for each of the\ncamera viewpoints and obtain part affinity maps.Next, we train a 3D Part-Affinity representation based on\nNeRF from the extracted part affinity maps, which allows us\nto interpolate part affinity maps from any camera viewpoint\nalmost instantaneously. Subsequently, during SDS, we ren-\nder both the 3D asset and the learned Part-Affinity NeRF of\nDreamBeast from the same camera perspective and modu-\nlate the cross and self-attention mechanisms of the guidance\nmodel using the rendered part affinity map. This modula-\ntion causes regions with high part affinity to have higher\nresponses to part-specific prompts. Consequently, our ap-\nproach (DreamBeast) not only promotes more reliable part-\naware SDS but also significantly reduces the computational\ncost from 7 hours to 78 minutes and cuts GPU memory us-\nage by 24GB, compared to the combination of SD3 with\nSDS as demonstrated in Figure 5. Quantitative and qualita-\ntive evaluations of DreamBeast demonstrate its exceptional\nability to generate part-aware, imaginative 3D animals.\nIn summary, our main contributions are as follows:\n1. We are the first to consider the problem of part-level text\nto 3D generation in an open-world setting.\n2. We propose DreamBeast, a novel knowledge transfer\nmechanism that efficiently transfers part-level knowl-\nedge of a 2D diffusion model into the 3D generation pro-\ncess.\n3. We significantly improve the quality and decrease the\ncomputational cost of creating part-aware 3D animal as-\nsets by integrating DreamBeast within the SDS opti-\nmization process.\n4. We demonstrate through quantitative evaluations and a\nhuman study that our method consistently outperforms\nbaseline methods.\n2. Related Work\nLifting 2D Diffusion Models for 3D Animal Generation.\nDue to the limited generalizability of current 3D genera-\ntive models, efforts have been made to adapt 2D diffusion\npriors or single image/video [24, 49] for 3D assets such\nas animals. The distilling diffusion prior approach primar-\nily employs score distillation sampling (SDS) [32], where\n2D diffusion priors serve as score functions that guide the\noptimization of 3D structures. Similarly, SJC [44] uti-\nlized publicly accessible diffusion models for their method.\nSubsequent studies have aimed at refining 3D representa-\ntions, enhancing loss design, or implementing multi-stage\noptimizations [6, 7, 19, 25, 41, 46, 51, 52]. Some meth-\nods [20, 27, 33, 39] leverage diffusion guidance to optimize\n3D models based on a single image. Another set of meth-\nods uses diffusion guidance to learn the layout for compo-\nsitional generation [5, 11, 31]. Notably, MVDream [37]\nproposed a multi-view consistent diffusion model for SDS\nguidance, significantly improving issues with multi-face\nJanus and content drift.\n3A creature with a \nkangaroo head and \nthe shell of a tortoise .\nView c1\nView cNDiT\nBlock\n\u2026 \u2026 \u2026\n\u2026Stable Diffusion 3\nView c1\nInput TokensT ext TokensAttention\nPart-Affinity Maps MPart-Affinity\nNeRF  f\u03b8\nStep 4:Score Distillation Sampling with Part-Affinity NeRF\nSample\nNew view\nMulti -view \nDiffusion\u2026\nViewc\u2019\nInput TokensT ext\nT okensAttention\nPart-Affinity \nMaps M\u2019\nRendered\nImage xc\u2019\n\u2026\nSDS\nGradient3DAsset NeRF\nStep 2:Extracting Part-Affinity MapsPart-Affinity\nNeRF f\u03b8\nStep3:Optimizing Part-Affinity NeRF\n\u2026View c1\n\u2026\nView cN\n\u2026\u2112pa\u2248\nFeed\nForward\nGradient\nBackward\nPart-Affinity\nNeRF Render\n3DAsset\nNeRF Render\nDiT\nBlockDiT\nBlock\nPartially Optimized\nNeRF\n\u2026\nView cN\nInput Tokens\nStep 1:Obtaining Partially Optimized NeRF\nMulti -view \nDiffusion\nSDS\nGradient\nN steps\nPartially Optimized NeRF Un-Optimized NeRF\nFigure 6. DreamBeast pipeline. DreamBeast is composed of 4 steps: (1)Partially optimize a NeRF under standard SDS. (2) Multiple\nrendered views of the partially optimized NeRF are input into SD3 along with the text prompt to construct Part-Affinity maps based on\ncross-attention of part tokens in SD3. (3) A Part-Affinity NeRF is trained using these extracted maps. (4) Both the trainable 3D asset NeRF\nand frozen Part-Affinity NeRF are rendered from the same camera pose. The rendered Part-Affinity map then modulates cross and self-\nattention in MVDream for SDS, ultimately generating a part-aware 3D animal. The symbol\n\u2744 denotes a frozen model, while\n\ud83d\udd25 indicates\na model that is trainable.\nLayout Guided 3D Generation. Earlier research [29] has\nexplored the application of compositional neural radiance\nfields within an adversarial learning framework to achieve\n3D-aware image generation. A pioneering study [13] uti-\nlized a mesh database to find and combine parts to create\nnew objects. Subsequent research incorporated probabilis-\ntic models for part suggestion [21], semantic attributes [2],\nand fabrication [36]. Some studies [42] employed neural\nradiance fields to represent various 3D elements and render\nthese into a unified 3D model. Recent advancements [31],\nguided by pre-trained diffusion models, have enabled the\ngeneration of compositional 3D scenes using user-provided\n3D bounding boxes and text prompts. Concurrently, other\nworks have used large language models (LLMs) to gener-\nate 3D layouts from text prompts as an alternative to hu-\nman annotations [14, 43, 45], or combined layout learning\nduring the optimization process [5, 11]. While these ap-\nproaches can produce 3D scenes through composition, they\nall rely heavily on scene graphs or descriptions of object-\nto-object relationships for object-to-scene generation. This\ndependency makes it impossible when it comes to com-\nposing part-to-object generational tasks. Unlike object-to-\nscene generation, which requires an understanding of the\nrelationships between distinct objects, part-to-object gen-\neration necessitates a more fine-grained comprehension of\nhow individual parts combine to form a coherent whole.Diffusion with Cross-Attention Control. Since most\ncurrent diffusion models are transformer-based and in-\ncorporate text information through cross-attention layers,\nproviding spatial awareness naturally aligns with cross-\nattention control. Several studies [4, 10, 17, 23, 30] explore\nvarious methods to enhance cross-attention scores between\nregions and their corresponding descriptions in the prompts.\nIn contrast, others [16] propose applying a binary mask to\neliminate attention between regions and non-matching re-\ngion descriptions. To the best of our knowledge, cross-\nattention control is predominantly applied in 2D generation\nmodels and is seldom utilized in 3D SDS settings as in our\npaper.\n3. Method\nTo efficiently transfer part-level knowledge of a 2D diffu-\nsion model into the 3D generation process, we introduce\nDreamBeast as a novel geometry-consistent mechanism de-\nsigned for this purpose. In Section 3.1, we revisit the\nclassic Score Distillation Sampling (SDS) formulation and\ndiscuss the issues that arise when applying SDS directly\nwith SD3. Subsequent sections detail our method: Sec-\ntion 3.2 describes the extraction of 2D part affinity maps\nfrom SD3, Section 3.3 details the construction of our Part-\nAffinity NeRF using the part affinity maps, and Section 3.4\n4Deer Antlers Bear Body \nLemur Limbs \nOstrich Head Fish Tail \nPigeon Head \nFish Fins \nWolf Head \nSnail Shell Figure 7. Learned part affinity map rendered from unseen camera poses. Heatmaps displaying the learned 3D part affinity repre-\nsentation rendered from unseen camera poses for different part-specific descriptions of distinct animals. Warmer colors indicate stronger\naffinities, highlighting our implicit 3D neural representation\u2019s capability to differentiate and localize specific anatomical features.\npresents the integration of the Part-Affinity NeRF within\nSDS to generate part-aware 3D assets.\n3.1. Preliminaries\nBefore delving into our method, we briefly review the con-\ncepts commonly employed in the 3D lifting generation tech-\nniques and further discuss our motivation.\nScore Distillation Sampling. As introduced by [32],\nScore Distillation Sampling (SDS) uses a pre-trained 2D\ndiffusion model with fixed parameters \u03d5to guide the gen-\neration of 3D models with the vast amount of 2D im-\nage prior knowledge. Let \u03b8denote the 3D representation\nsuch as NeRF [28] or Gaussian Splatting [22] and g(\u00b7)be\nthe differentiable rendering function, which renders the 3D\nmodel to an image x=g(\u03b8). During the guidance pro-\ncess with yas the text condition, we first sample a random\ntimestep t\u223c {0, ..., T}and a random noise \u03f5\u223c N (0, I).\nWe then add the noise to the rendered image and we get\nxt=\u221a\u03b1tx+\u221a1\u2212\u03b1t\u03f5. The SDS gradient then is defined\nas follows:\n\u2207\u03b8LSDS(x=g(\u03b8)) =Et,\u03f5\u0014\nw(t)(\u02c6\u03f5(xt;y, t, \u03d5 )\u2212\u03f5)\u2202x\n\u2202\u03b8\u0015\nwhere \u03b1tandw(t)are weighting functions that depend on\nthe timestep t, and \u02c6\u03f5is the predicted noise by the pre-trained\ndiffusion model.\nWhy not Stable Diffusion 3 as the Guidance Directly?\nA straightforward way to leverage part-level knowledge\nfrom SD3 is by performing SDS with SD3\u2019s guidance.\nHowever, we argue that this approach is ineffective for sev-\neral reasons: (1) Even when SD3 manages to generate im-ages with part-level specifications, it often fails because\npart-level understanding is only exhibited at specific trans-\nformer layers and timesteps, as shown in Figure 3. Dur-\ning the denoising forward pass through SD3, information\nabout the animals and their parts can become mixed, lead-\ning to a loss of part-level control. This makes SDS unstable\nand unpredictable at the part level. (2) SD3 cannot pro-\nvide view-consistent guidance, which leads to issues like\nmulti-face Janus problem and content drift problems [37].\n(3) SD3 uses a rectified flow match Euler discrete sched-\nuler, which differs from previous diffusion methods. We\nobserve that timestep sampling strategies from earlier meth-\nods do not yield satisfactory results with SD3. Additionally,\nhyperparameters such as the guidance scale and 3D shape\nloss scales require extensive empirical tuning. (4) The for-\nward process is computationally expensive, requiring 48GB\nof GPU memory and 7 hours of training time on an NVIDIA\nA40 to generate just one 3D asset. In contrast, Dream-\nBeast offers more stable and robust part-level specification\nduring SDS and requires only 78 minutes to complete. This\nis slightly longer than SD2.1 or MVDream, which takes 50\nminutes on an A40 GPU (see Figure 5 for a comparison of\nrunning speeds).\nWhy not Stable Diffusion 3 + MVDream as the Guid-\nance? This approach could potentially solve the multi-\nface Janus problem; however, the remaining issues with\nSD3 still persist. Moreover, this combination requires\n58GB of GPU memory and takes 8 hours to generate a sin-\ngle 3D asset through SDS, rendering it highly inefficient.\n53.2. Part Affinity Map Extraction\nBefore extracting the part affinity map from SD3, we first\nperform SDS for several steps to partially optimize the\nNeRF. We then render this partially optimized NeRF from\nvarious camera angles to obtain view-consistent, coarse\nanimal-shape layouts. These view-consistent layouts serve\nas conditions for SD3, where the rendered animal shape is\nmixed with noise as input for denoising, which helps keep\nthe extracted part affinity maps view-consistent as well.\nWe choose Stable Diffusion 3 (SD3) as our source of\n2D part-level knowledge for two key reasons: (1) Unlike\nperception-driven part-level segmentation frameworks [8,\n40, 47, 50] that rely on well-defined input images, we found\nthat SD3 can operate effectively on noisy images generated\nfrom partially optimized NeRFs.\n(2) Among many open-source models we examined,\nSD3 is the only one that demonstrates part-level understand-\ning in its cross-attention maps.\nTo obtain the part affinity map, we conduct a denois-\ning process for the timesteps between tsandte. Specif-\nically, for a rendered image xfrom a partially optimized\nNeRF under camera pose c, we introduce noise to xusing\na weighting factor \u03b1tsto produce xts. Subsequently, we\nperform denoising in the latent space for each timestep up\ntote. At each timestep t, and for each transformer layer\nlin SD3, we compute an attention map At,l,c\u2208RHW\u00d7n,\nwhere ndenotes the number of tokens in the text prompt\ny\u2208Rn, and HW represents the spatial resolution of the\nfeature maps. From this attention map, we extract a spa-\ntial correspondence map Mt,l,i,c\u2208RHWfor each token yi,\nwhich corresponds to a slice of At,l,cassociated with the\ntoken yi. Let Iprepresent the set of token indices for the\npart-level description in y, for example, \u201dkangaroo head\u201d in\n\u201da creature with a kangaroo head and a tortoise shell\u201d cor-\nresponds to a set of token indices Ip={4,5}. We can then\nderive the part affinity map Mp,c\u2208RHWfor this camera\npose using the following equation:\nMp,c=1\n(ts\u2212te)\u00b7L\u00b7 |Ip|teX\nt=tsLX\nl=0X\ni\u2208IpMt,l,i,c\nwhere Lis the number of transformer layers.\n3.3. Part-Affinity NeRF\nThe part affinity map Mp,capplies only to a specific camera\nposec. This poses a problem, as the camera poses are sam-\npled from a continuous distribution during SDS, implying\nan infinite number of potential camera poses.\nA naive approach would be to generate the part affin-\nity map each time an image is rendered from the 3D as-\nset. However, this approach significantly increases compu-\ntational demands. The forward pass in SD3, combined with\nthe need to obtain cross-attention maps for every layer and\nA creature with a body  of a bear  and the antlers  of a deer \nA creature with a head  of a kangar oo  and the shell  of a tortoise \noptimization Figure 8. Part-Affnity NeRF learning progress visualization.\nWe show the evolution of renderings of the Part-Affinity NeRF\nfor each corresponding part (e.g., body of a bear in the first row)\nthroughout the learning, demonstrating that the Part-affinity NeRF\nquickly converges.\ntimestep, requires substantially more computation. As a re-\nsult, generating a single 3D asset can take up to 58 hours,\nwhich is even longer than using SD3 solely as a guidance\nmechanism for SDS.\nTherefore, as part of DreamBeast, we introduce the Part\naffinity NeRF that learns from part affinity maps of discrete\ncamera poses and is capable of interpolating these maps for\ncontinuous camera poses.\nMathematically, let {{Mp,c|p\u2208 P} | c\u2208 C} denote\nall part affinity maps obtained for all parts Punder a set\nof camera poses C. The optimization goal is to fit the im-\nplicit representation f\u03b8(M|p, c)parameterized by a neural\nnetwork with parameters \u03b8:\nmin\n\u03b8Lpa= min\n\u03b8X\nc\u2208CX\np\u2208P||f\u03b8(M|p, c)\u2212Mp,c||2\nIn our design, f\u03b8is an MLP-based neural radiance field.\nThe continuous nature of MLPs imposes a form of smooth-\nness and continuity in the learned representation, enabling\nDreamBeast to produce part affinity maps from any camera\npose. In conclusion, DreamBeast offers several key ben-\nefits: (1) it provides a 3D consistent representation of part\naffinity maps learned from discrete camera views, allowing\nfor the interpolation of part affinity maps under any cam-\nera pose, which makes it efficient to use with the SDS pro-\ncess; (2) it significantly reduces the computation cost of us-\ning SD3 at every single step to the more economical learn-\ning and rendering cost of DreamBeast, reducing the training\ntime from 58 hours to 78 minutes per 3D asset.\n6Algorithm 1 Layer-wise Attention Modulation with\nDreamBeast\nRequire: Cross attention and self attention of the 2D diffu-\nsion guidance model Scross,Sself, rendered part affin-\nity map {f\u03b8(M|p, c\u2032)|p\u2208 P} , part token indices\n{Ip|p\u2208 P} , enhancement factor \u03b1cross ,\u03b1self\n1:forpinPdo \u25b7Cross Attention\n2: M\u2032\np,c\u2032\u2190f\u03b8(M|p, c\u2032)\n3:Scross[:, Ip]\u2190 S cross[:, Ip] +\u03b1cross logM\u2032\np,c\u2032\n4: Across\u2190Softmax (Scross)\n5:end for\n6:forpinPdo \u25b7Self Attention\n7: M\u2032\np,c\u2190f\u03b8(M|p, c\u2032)\n8: Kp,c\u2032\u2190(M\u2032\np,c\u2032)TM\u2032\np,c\u2032 \u25b7Symmetry\n9:Sself\u2190 S self+\u03b1selflogKp,c\u2032\n10: Aself\u2190Softmax (Sself)\n11:end for\n3.4. SDS with Attention Modulation\nAfter rendering an image x(c\u2032)from the 3D asset under\na specific camera pose c\u2032during SDS, we also generate\nthe rendered affinity maps for each of the parts {f\u03b8(M|\np, c\u2032)|p\u2208 P} using the optimized Part-Affinity NeRF un-\nder the same camera pose. These rendered part affinity\nmaps are utilized to modulate the cross-attention and self-\nattention matrices in the 2D diffusion guidance model for\nSDS. Specifically, we modulate the cross-attention score\nmapsScross\u2208Rhw\u00d7nand the self-attention score maps\nSself\u2208Rhw\u00d7hw(before the softmax operation), where hw\nrepresents the feature spatial resolution and ndenotes the\nnumber of tokens, in the 2D diffusion guidance models at\neach denoising step \u02c6\u03f5(xt;y, t, \u03d5 ). The detailed procedure\nis outlined in Algorithm 1. The cross-attention modula-\ntion ensures that regions corresponding to a specific part\nare guided by their corresponding part-specific token. The\nself-attention modulation increases influence within intra-\npart regions and reduces influence among inter-part regions.\n4. Experiments\n4.1. Implementation Details\nWe consider the cross-attention map between ts= 450 and\nte= 100 at the eleventh layer ( l= 11 ) in SD3, where we\nfound the most significant part-level understanding.\nOur Part-Affinity NeRF is a small MLP with one hid-\nden layer comprising 16 neurons. The output dimension is\nequivalent to the number of parts described in the prompt,\nwith each dimension representing a different part of the fan-\ntastical animal. The part affinity map is rendered similarly\nto NeRF [28], using 128 samples per ray and a rendered\nresolution of 64. We optimize Part-Affinity NeRF for 2000\nsteps.2D Diffusion Model 2 Parts 3 Parts\nMVDream [37] 0.242 0.108\nStable Diffusion 2.1 [34] 0.187 0.022\nStable Diffusion XL 0.297 0.032\nDeepFloyd [35] 0.429 0.097\nStable Diffusion 3 [12] 0.826 0.537\nTable 1. 2D part-aware generation success rate. A user study in-\nvolving five participants found that SD3 has a significantly higher\nsuccess rate than other popular diffusion models in generating\npart-aware images based on part-level prompts (describing 2 or\n3 animal parts in a single prompt). This suggests that SD3 has a\nsuperior ability to understand and generate images at the part level.\nMethodCLIP Score \u2191\nB/32 B/16 L/14\nDreamFusion(SD2.1) [32] 0.271 \u00b13.0e\u221240.274\u00b12.2e\u221240.226\u00b13.6e\u22124\nDreamFusion(SD3) [32] 0.271 \u00b15.8e\u221240.275\u00b15.5e\u221240.229\u00b17.7e\u22124\nMVDream [37] 0.275 \u00b17.8e\u221240.282\u00b14.1e\u221240.230\u00b17.9e\u22124\nGeoDream [26] 0.244 \u00b11.2e\u221240.252\u00b11.5e\u221240.202\u00b11.8e\u22124\nOpenLRM [18] 0.265 \u00b12.1e\u221240.285\u00b12.4e\u221240.223\u00b16.2e\u22124\nVFusion3D [15] 0.268 \u00b11.9e\u221240.281\u00b12.1e\u221240.225\u00b16.2e\u22124\nDreamBeast (Ours) 0.285\u00b12.6e\u221240.289\u00b13.5e\u221240.245\u00b14.6e\u22124\nTable 2. Performance comparison of different methods.\nOur method shows the best CLIP scores among all.\nView NumberCLIP Score \u2191\nB/32 B/16 L/14\n8 0.274 \u00b15.0e\u221240.281\u00b14.6e\u221240.231\u00b15.9e\u22124\n16 0.277 \u00b14.8e\u221240.281\u00b11.9e\u221240.232\u00b15.8e\u22124\n32 0.277 \u00b14.5e\u221240.283\u00b14.3e\u221240.235\u00b15.7e\u22124\n64 0.284 \u00b14.3e\u221240.287\u00b14.0e\u221240.240\u00b15.5e\u22124\n76 0.285 \u00b12.6e\u221240.289\u00b13.5e\u221240.245\u00b14.6e\u22124\nTable 3. Ablation study on the number of views for extracted\npart-affinity maps. Increasing the number of views results in a\nstronger part-affinity NeRF.\nWe choose MVDream [37] as our diffusion guidance\nmodel due to its multiview consistency and computational\nefficiency for SDS. We set \u03b1cross = 0.8and\u03b1self= 0.9\nto modulate the influence of cross and self-attention maps\nwithin MVDream. For the SDS optimization, we adhere\nto the default hyper-parameters specified in MVDream for\nother settings. The training procedure involves several steps\nto create a detailed 3D asset of an animal. Initially, the as-\nset is trained (guided by MVDream) for 1000 steps to ob-\ntain the partially optimized NeRF. Following this, the Part-\nAffinity NeRF is optimized over an additional 2000 steps.\nFinally, the training continues for another 4000 steps, using\npart-level guidance from the rendered part affinity maps, to\nachieve the final part-aware 3D model.\n7A creature with a \nbody of a carp \nand a mane of a \nhorse and claws \nof a hawk \nA creature with a \nbody of a worm \nand the head of a \nfox\nA creature with a \nbody of a lemur \nand the wings of \na bumblebee \nA creature with a \nbody of a dolphin \nand a mane of a \nlion and claws of \nan eagle \nA creature with a \nbody of a penguin \nand a head of a \npanda and tail of a \nlemur \nOurs MVDream DreamFusion (SD3) Figure 9. Qualitative results of generated fantastic animals from different methods. DreamBeastis capable of generating 3D assets\nwith better part correspondence to part-specific prompts compared to MVDream or SD3. The fantastic animals created by MVDream and\nSD3 often either omit certain body parts or blend different animal elements globally, which is not the desired outcome.\n4.2. Evaluation Benchmarks\nWe use GPT-4o-mini to randomly generate 30 prompts un-\nder a template \u201da creature with a [animal 1][part 1], [animal\n2][part 2], and [animal 3][part 3]\u201d. We use CLIP text sim-\nilarity and ranking-based user study to evaluate how well\nthe generated 3D assets match their descriptions. More\nvisualizations and non-animal results are presented in Ap-\npendix C.\n4.3. Main Results\nCLIP Similarity Experiment. We compare Dream-\nBeast with other distillation-based methods [26, 32, 37],\nand popular feedforward 3D generation methods [15, 18],\nresults are shown in Table 2. We observe that Dream-\nBeast consistently has higher similarity scores across CLIP\ntypes, indicating DreamBeast\u2019s better part-correspondence\nin generated 3D assets and the part-specific prompts.\n0 20 40 60 80 100\nRatio of Best Votes (%)3D Asset\nQuality Part   \nCorres-\npondenceQuality TypeMethods\nDreamFusion(SD3)\nMVDream\nOurs\nFigure 10. User study results. Participants were shown multi-\nview images generated from the same prompts and asked to select\nthe best result. Our method receives significantly more votes for\nboth part correspondence and overall 3D asset quality. More detail\nin the Appendix BUser Study Experiment. To assess the part correspon-\ndence quality and overall quality of the generated content,\nwe asked users to compare our results with those from\nDreamFusion (SD3) and MVDream. We showed 24 users\nmulti-view images generated from 20 random prompts and\nasked them to choose the best one. The results, shown\nin Figure 10, clearly demonstrate that DreamBeast outper-\nforms existing methods. This suggests that our method ef-\nfectively understands part compositions and generates high-\nquality 3D results.\nQualitative Results. As illustrated in Figure 9, Dream-\nBeast demonstrates the ability to generate part-aware an-\nimals, with each relevant part closely adhering to its de-\nscription. In contrast, MVDream produces assets with glob-\nally mixed animal features. Similarly, Dreamfusion (SD3)\nstruggles to generate part-aware results, and both its 3D\nshape and texture quality are lacking, as discussed in Sec-\ntion 3.1.\nAblation Study. We examined the impact of the number\nof views on the extracted Part-Affinity maps, as shown in\nTable 3. Increasing the number of views enhances the con-\ntext of part affinity, enabling the optimized Part-Affinity\nNeRF to better interpolate under unseen camera poses. This\nresults in more accurate part-knowledge integration during\nthe SDS process, which improves the CLIP similarity score.\nMoreover, we observed that performance gains diminish be-\nyond 64 views, suggesting that our method can effectively\ncapture complete Part-Affinity information in 3D with a rel-\natively small number of views. We provide additional anal-\n8ysis of our design choices in Appendix A.\n5. Conclusion\nThis work incorporates Part-Affinity knowledge to address\nthe challenges associated with a limited part-level under-\nstanding of SDS-based 3D asset generation methods. Our\nproposed DreamBeast exhibits high precision in generat-\ning 3D assets with detailed part components, outperforming\nexisting techniques in terms of both quality and efficiency.\nThis contributes to the advancement of the field of creative\nand complex 3D content creation, paving the way for the de-\nvelopment of more detailed and imaginative digital worlds.\nAcknowledgments. The authors would like to thank Paul\nEngstler for his insightful feedback on the manuscript.\nReferences\n[1] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun\nHao, Serge Belongie, Noah Snavely, and Bharath Hariharan.\nLearning gradient fields for shape generation. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part III 16 , pages\n364\u2013381. Springer, 2020. 2\n[2] Siddhartha Chaudhuri, Evangelos Kalogerakis, Stephen\nGiguere, and Thomas Funkhouser. Attribit: content creation\nwith semantic attributes. In Proceedings of the 26th annual\nACM symposium on User interface software and technology ,\npages 193\u2013202, 2013. 4\n[3] Kevin Chen, Christopher B Choy, Manolis Savva, An-\ngel X Chang, Thomas Funkhouser, and Silvio Savarese.\nText2shape: Generating shapes from natural language by\nlearning joint embeddings. In Computer Vision\u2013ACCV 2018:\n14th Asian Conference on Computer Vision, Perth, Australia,\nDecember 2\u20136, 2018, Revised Selected Papers, Part III 14 ,\npages 100\u2013116. Springer, 2019. 2\n[4] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free\nlayout control with cross-attention guidance. arXiv preprint\narXiv:2304.03373 , 2023. 4\n[5] Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui\nJia, and Ziwei Liu. Comboverse: Compositional 3d as-\nsets creation using spatially-aware diffusion guidance. arXiv\npreprint arXiv:2403.12409 , 2024. 3, 4\n[6] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu.\nText-to-3d using gaussian splatting, 2024. 3\n[7] Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei\nZhang, Jian Zhang, and Li Yuan. Progressive3d: Progres-\nsively local editing for text-to-3d content creation with com-\nplex semantic prompts, 2023. 3\n[8] Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag\nArnab, Paul Hongsuck Seo, and Seungryong Kim. Cat-seg:\nCost aggregation for open-vocabulary semantic segmenta-\ntion, 2024. 6\n[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, KianaEhsani, Aniruddha Kembhavi, and Ali Farhadi. Obja-\nverse: A universe of annotated 3d objects. arXiv preprint\narXiv:2212.08051 , 2022. 3\n[10] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. 2023. 4\n[11] Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A. Efros,\nand Aleksander Holynski. Disentangled 3d scene generation\nwith layout learning, 2024. 3, 4\n[12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M \u00a8uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim\nDockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yan-\nnik Marek, and Robin Rombach. Scaling rectified flow trans-\nformers for high-resolution image synthesis, 2024. 2, 3, 7,\n1\n[13] Thomas Funkhouser, Michael Kazhdan, Philip Shilane,\nPatrick Min, William Kiefer, Ayellet Tal, Szymon\nRusinkiewicz, and David Dobkin. Modeling by example.\nACM transactions on graphics (TOG) , 23(3):652\u2013663, 2004.\n4\n[14] Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger,\nand Bernhard Sch \u00a8olkopf. Graphdreamer: Compositional\n3d scene synthesis from scene graphs. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 21295\u201321304, 2024. 4\n[15] Junlin Han, Filippos Kokkinos, and Philip Torr. Vfusion3d:\nLearning scalable 3d generative models from video diffusion\nmodels. European Conference on Computer Vision (ECCV) ,\n2024. 2, 7, 8\n[16] Yutong He, Ruslan Salakhutdinov, J. Zico Kolter, and Mona\nLisa. Localized text-to-image generation for free via cross\nattention control. ArXiv , abs/2306.14636, 2023. 4\n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control. arXiv preprint\narXiv:2208.01626 , 2022. 4\n[18] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\nTan. Lrm: Large reconstruction model for single image to\n3d.arXiv preprint arXiv:2311.04400 , 2023. 2, 7, 8\n[19] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xi-\nanbiao Qi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang.\nDreamwaltz: Make a scene with complex 3d animatable\navatars. arXiv:2305.12529 , 2023. 3\n[20] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rup-\nprecht, and Andrea Vedaldi. Farm3D: Learning articu-\nlated 3d animals by distilling 2d diffusion. arXiv preprint\narXiv:2304.10535 , 2023. 3\n[21] Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne\nKoller, and Vladlen Koltun. A probabilistic model for\ncomponent-based shape synthesis. Acm Transactions on\nGraphics (TOG) , 31(4):1\u201311, 2012. 4\n[22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk \u00a8uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics , 42\n(4), 2023. 5\n9[23] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and\nJun-Yan Zhu. Dense text-to-image generation with attention\nmodulation. In ICCV , 2023. 4\n[24] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas\nJakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi,\nand Jiajun Wu. Learning the 3d fauna of the web. arXiv\npreprint arXiv:2401.02400 , 2024. 3\n[25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takik"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Summary of \"FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally\"\n\n#### 1. Abstract:\nThe paper addresses the challenge of segmenting 3D Gaussian Splatting (3D-GS) using 2D masks. Traditional methods rely on iterative gradient descent for labeling 3D Gaussians, which are slow and yield suboptimal results. The authors propose a globally optimal solver, simplifying the process into a linear programming (LP) problem solvable in closed form. This method, leveraging alpha blending for single-step optimization, integrates background bias to improve noise robustness and completes the segmentation in about 30 seconds, significantly faster than existing methods. Extensive experiments show the approach's efficiency and robustness in various 3D scenes and tasks like object removal and inpainting.\n\n#### 2. Introduction:\n- **Context:** Accurate 3D segmentation is crucial for applications in computer vision, such as object recognition and scene manipulation. Traditional methods face difficulty in effectively labeling 3D Gaussians from 2D masks due to their reliance on slow iterative optimization.\n- **Research Questions:** How to achieve accurate and efficient 3D segmentation from 2D masks? How to overcome the limitations of existing iterative gradient descent approaches?\n- **Literature Review and Positioning:** The paper builds on advancements in 3D Gaussian Splatting, which offers high precision and visual quality for 3D scene rendering. It also references various neural-based 3D scene reconstruction techniques and segmentation methods. The novel contribution lies in formulating the segmentation as a linear programming problem, enabling a single-step optimal solution.\n\n#### 3. Related Work:\n- **3D Gaussian Splatting:** Recent developments have shown its potential for high fidelity 3D scene reconstruction and its integration with dynamic environments and diffusion-based models.\n- **3D Neural Scene Segmentation:** Existing methods like NeRF and NVOS exhibit significant progress in 3D scene reconstruction and segmentation. However, they require complex training and optimization processes.\n- **Key Differentiation:** The proposed method differs by eliminating the need for extensive training and iterative optimization, instead turning the segmentation into a linear programming problem for efficiency and global optimality.\n\n#### 4. Method:\n- **Proposed Approach:** The authors present a method for 3D Gaussian Splatting segmentation that leverages the linear nature of 2D mask rendering from reconstructed 3D scenes. This transforms the problem into an integer linear programming (ILP) optimization.\n- **Mathematical Formulation:**\n  - The rendering process is simplified into a linear function with respect to the Gaussian labels.\n  - Objective function: \\( F = \\sum_v |R(\\{G_i\\}, \\{P_i\\}) - M_v| \\)\n  - Constraints: \\( P_i \\in \\{0, 1\\} \\)\n  - Using lemma and linear combinations, equations and optimization steps are transformed for direct minimal error determination.\n- **Algorithm:**\n  - Rasterization processes pre-compute alpha blending constants (\\(\\alpha_i\\)) and transmittance terms (\\(T_i\\)).\n  - Optimal assignment is computed via LP, avoiding iterative methods.\n  - Noise reduction is achieved by introducing a background bias (\\(\\gamma\\)) during the assignment.\n- **Extended to Scene Segmentation:** The method is adapted for multi-instance scene segmentation by treating the problem as multiple binary segmentations with optimal label assignments.\n\n#### 5. Experimental Setup:\n- **Datasets:** MIP-360, T&T, LLFF, Instruct-NeRF2NerF, and LERF datasets are utilized for both qualitative and quantitative analyses.\n- **Hardware and Software:** Implemented on a single NVIDIA A6000 GPU in CUDA with kernel functions for rapid computation.\n- **Evaluation Metrics:** Mean Intersection over Union (IoU) and mean accuracy are used for performance evaluation.\n- **Baselines:** Comparison methods include recent 3D segmentation approaches using NeRF and 3D-GS techniques like NVOS, ISRF, SGISRF, SA3D, and SAGA.\n\n#### 6. Results and Analysis:\n- **Empirical Findings:**\n  - The method efficiently achieves 3D segmentation in approximately 30 seconds, compared to 18\u201337 minutes for baselines.\n  - Superior performance with a mean IoU and mean accuracy higher than competitive methods (91.8% mIoU and 98.6% mAcc).\n  - Visualization of results shows clean segmentation, object removal, and robust inpainting without significant artifacts.\n- **Figures and Tables:**\n  - Detailed examples of segmentation results and object removal are provided.\n  - Quantitative comparison table shows benchmark results highlighting significant computational efficiency and performance.\n- **Ablation Studies:**\n  - Impact of background bias (\\(\\gamma\\)) on segmentation noise reduction and optimal values for different scenes.\n  - Performance with varying numbers of 2D masks, demonstrating the method's robustness with fewer masks.\n\n#### 7. Discussion:\n- **Interpretation:** The single-step LP optimization significantly reduces computational time while maintaining accuracy. Incorporating a background bias effectively mitigates noise from predicted 2D masks.\n- **Implications:** The method enables real-time and high-accuracy 3D segmentation, which can substantially impact applications requiring 3D scene understanding and manipulation.\n- **Limitations:** Current dependence on SAM's mask quality introduces some segmentation noise, and simplistic alpha quantization might struggle with complex scene geometries.\n\n#### 8. Conclusion:\n- **Key Contributions:** The paper introduces a globally optimal solver for 3D Gaussian Splatting segmentation, transforming the task into a linear programming problem. This innovative approach significantly reduces computation time and increases robustness against noise.\n- **Future Research Directions:** Enhancements in mask association, exploration of different optimization techniques, and potential applications in dynamic 3D environments and real-time processing scenarios.\n\nThis comprehensive summary provides all necessary details to replicate the study or build upon this work, capturing critical insights from problem formulation to implementation specifics and detailed empirical analysis."}, {"role": "assistant", "content": "FlashSplat: 2D to 3D Gaussian Splatting\nSegmentation Solved Optimally\nQiuhong Shen, Xingyi Yang, and Xinchao Wang\u22c6\nNational University of Singapore\n{qiuhong.shen,xyang}@u.nus.edu xinchao@nus.edu.sg\nAbstract. This study addresses the challenge of accurately segmenting\n3D Gaussian Splatting (3D-GS) from 2D masks. Conventional methods\noften rely on iterative gradient descent to assign each Gaussian a unique\nlabel,leadingtolengthyoptimizationandsub-optimalsolutions.Instead,\nwe propose a straightforward yet globally optimal solver for 3D-GS seg-\nmentation. The core insight of our method is that, with a reconstructed\n3D-GS scene , the rendering of the 2D masks is essentially a linear func-\ntion with respect to the labels of each Gaussian. As such, the optimal la-\nbel assignment can be solved via linear programming in closed form. This\nsolution capitalizes on the alpha blending characteristic of the splatting\nprocess for single step optimization. By incorporating the background\nbias in our objective function, our method shows superior robustness\nin 3D segmentation against noises. Remarkably, our optimization com-\npletes within 30 seconds, about 50 \u00d7faster than the best existing meth-\nods. Extensive experiments demonstrate our method\u2019s efficiency and ro-\nbustness in segmenting various scenes, and its superior performance in\ndownstream tasks such as object removal and inpainting. Demos and\ncode will be available at https://github.com/florinshen/FlashSplat.\nKeywords: 3D Segmentation \u00b73D Gaussian Splatting \u00b7Neural-based\nunderstanding\n1 Introduction\nThe pursuit of understanding and interacting with 3D environments represents\na formidable yet pivotal challenge in computer vision. Central to this endeavor is\nthetaskofaccuratelyperceivingandsegmenting3Dstructures[1,5,10,30,32,42],\nataskthatbecomesincreasinglycomplexaswedelveintomoresophisticatedrep-\nresentations of 3D scenes. Recently, 3D Gaussian Splatting (3D-GS) [17] emerges\nas a cutting-edge approach that promises to revolutionize how we render and re-\nconstruct 3D spaces. By employing a multitude of colored 3D Gaussians, this\nmethod achieves a high fidelity representation of 3D scenes, offering a compelling\nblendofprecisionandvisualqualitythatisparticularlysuitedforcomplexobject\nand scene rendering.\n\u22c6Corresponding Author.arXiv:2409.08270v1  [cs.CV]  12 Sep 20242 Shen et al.\nDespite the potential of 3D-GS, a significant obstacle remains: the segmen-\ntationofthese3DGaussianfrommere2Dmasks\u2014aprocesscrucialforarangeof\napplicationsfromobjectrecognitiontoscenemanipulation.Existingapproaches[3,\n54] to this challenge have largely depended on iterative gradient descent methods\nfor labeling 3D Gaussian. However, these methods are marred by their slow con-\nvergencespeedandfrequententrapmentinsuboptimalsolutions,renderingthem\nimpractical for applications requiring real-time performance or high accuracy.\nAddressing this gap, our work introduces a simple but globally optimal solver\ndesigned explicitly for the segmentation of 3D Gaussian Splatting. Our approach\ncapitalizesontheinsightthattheprocessofrenderingsegmented2Dimagesfrom\na reconstructed 3D Gaussian Splatting can be simplified to a linear function con-\ncerning the accumulated contribution of each Gaussian. This realization allows\nus to frame the problem as a linear integer programming that can be solved\nin a closed form, relying solely on the alpha composition term inherent in the\nsplatting process. This breakthrough significantly streamlines the segmentation\ntask, bypassing the need for iterative optimization and directly leading to the\noptimal label assignment.\nMoreover, by integrating a background bias into our objective function, we\nfurther enhance the method\u2019s robustness against 2D masks noise in 3D segmen-\ntation. This refinement not only bolsters the robustness of our solution but also\nits applicability across a wider range of scenes and conditions. Impressively, our\nsolver completes its optimization in approximately 30 seconds\u2014an acceleration\nof 50\u00d7faster than existing methods\u2014without sacrificing accuracy or global op-\ntimality.\nThrough extensive experimentation, we have validated the superiority of our\napproachinefficientlysegmentingavarietyofscenes,demonstratingitsenhanced\nperformance in downstream tasks such as object removal and inpainting. These\nresults underscore the potential of our method to significantly advance the field\nof 3D scene processing and understanding.\nThe primary contributions of our work are summarized as follows:\n\u2013We introduce a globally optimal solver for 3D Gaussian Splatting segmen-\ntation, which significantly enhances the efficiency of lifting 2D segmentation\nresults into 3D space.\n\u2013We simplify the 3D-GS rendering process through linearization, turning the\n2D to 3D segmentation task into a linear integer optimization problem. This\nmethod is effective for both binary and scene segmentation.\n\u2013We introduce a background bias in the optimization, demonstrate superior\nrobustness againt noises in 3D segmentation, showcasing our method\u2019s ro-\nbustness and efficiency across a variety of scene segmentation.\n\u2013Our method achieves a remarkable optimization speed, completing the pro-\ncess within 30 seconds\u2014approximately 50 times faster than existing meth-\nods\u2014while ensuring global optimality for given 2D masks.\n\u2013Extensive experiments validate the superiority of our approach in down-\nstream tasks, including object removal and inpainting, thus highlighting its\npotential to significantly impact 3D data processing and application.FlashSplat 3\n2 Related-work\n2.1 3D Gaussian Splatting\n3D Gaussian Splatting has emerged as an efficient method for inverse rendering,\nfacilitating the reconstruction of 3D scenes from 2D images through learning\nexplicit 3D Gaussian in the space [9,18]. Recent advancements have seen a series\nof studies [28,48,52,53] extending this approach to capture dynamic 3D environ-\nments by learning deformation fields for 3D Gaussians. Concurrently, another\nstream of researches [6,24,35,44,55,58] have integrated 3D Gaussian splatting\nwith diffusion-based models [25,38,39,50,51,57] to generate static [40,56,62] or\ndynamic [36] 3D objects. A notable portion of these works leverages the explicit\nrepresentations provided by 3D Gaussian Splatting for rapid optimization. Our\nmethod primarily concentrates on elevating corresponding 2D masks onto recon-\nstructed 3D Gaussian scenes. We also exploit the explicit representation of 3D\nGaussian Splatting to cast the segmentation of 3D scenes as a Linear Program-\nming optimization problem, thereby enabling instantaneous 3D segmentation.\n2.2 3D Neural scene segmentation\nRecentadvancementsinneural3Dscenerepresentations,includingNeRF,DVGO,\nand several others [1,5,8,10,14,23,30,32,42,49], have demonstrated exceptional\ncapabilities in 3D scene reconstruction. Inspired by these developments, the ex-\nploration of 3D segmentation within such frameworks has emerged as a signifi-\ncantareaofinterest.Semantic-NeRF[61]initiallyshowcasedNeRF\u2019saptitudefor\nsemantic propagation and refinement. NVOS [37] furthered this by enabling user\ninteraction for 3D object selection via a lightweight MLP trained on specialized\n3D features. Concurrently, a series of studies including N3F, DFF, LERF, and\nISRF [12,19,22,46] have integrated 2D vision models to facilitate 3D segmenta-\ntionbyelevating2Dvisualfeatures,therebyprocessingembedded3Dfeaturesfor\nsegmentation. Additionally, various approaches have combined instance and se-\nmantic segmentation methods with radiance fields [2,4,11,12,15,27,33,41,47,59]\nto further explore this domain.\nMost closely related methods to our work are SAGA [3], Gaussian Group-\ning [54], and SAGS [16]. These works concentrate on associating 2D masks, as\ngenerated by the Segment Anything Model (SAM) [20], with 3D Gaussian Splat-\nting for mask lifting. Gaussian Grouping employs video tracker to associate 2D\nmasks, subsequently distilling these as object features for 3D Gaussians and seg-\nmenting via a classifier trained with 2D identity loss and 3D regularization loss.\nSimilarly, SAGA distills 2D masks into 3D features, requiring additional training\nfeatures for each 3D Gaussians with a pack of losses. Both of these methods are\nlimited by their substantial training costs required to optimize Gaussian features\nbefore 3D segmentation. SAGS introduces a training-free approach by project-\ning the centers of 3D Gaussians onto 2D masks. This method adopts a intuitive\nsegmentation criterion, where a projected center landing within a foreground\nmask gets classified as a foreground Gaussian.4 Shen et al.\nOur method distinguishes itself by framing the 2D mask lifting in 3D Gaus-\nsian Splatting as a Linear Programming (LP) optimization problem, solved in\na single step. Focused solely on lifting associated 2D masks into 3D Gaussians\nmasks, our approach offers a standalone, efficient solution for immediate seg-\nmentation without the need for extra training and post-processing.\n3 Methods\nIn this section, we first delve into the rendering process of 3D Gaussian Splatting\n(3DGS), focusing on the tile-based rasterization and alpha blending. We then\ndescribehowthisprocesslendsitselftoformulatingthesegmentationof3DGSas\nan integer linear programming (ILP) optimization, which we demonstrate can be\nresolved in a closed form. Recognizing the typically noisy nature of 2D masks, we\nintroduce a softened optimal assignment to mitigate these noises. Beyond binary\nsegmentation, we extend our method to include scene segmentation, enabling\nthe segmentation of all objects within 3D scenes. Finally, we present a method\nfor rendering 2D masks guided by depth information, which projects the 3D\nsegmentation results onto 2D masks from new viewpoints.\n3.1 Preliminary: Rasterization of 3DGS\n\ud835\udc62!\ud835\udc62\"\ud835\udc62#\nFig.1: GS Rasterization. Here\nwe illustrate projected 2D Gaus-\nsians in 3DGS rasterization, set-\nting each tile as 2\u00d72for il-\nlustration purposes. Gaussians are\nshared between different tiles and\nare likely to be shared among dif-\nferent instances (color blocks).3DGaussianSplatting[18](3DGS)standsout\nas a great method for novel view synthesis.\nUnlike neural radiance fields [31], it recon-\nstructs a 3D scene as explicit 3D Gaussians\nwith real-time speed. Given a set of captured\nviews with paired camera poses, 3D Gaussian\nSplatting reconstruct 3D scenes by learning\n3D Gaussians {Gi}. Each 3D Gaussian Gi\nis parameterized as Gi={mi, qi, si, oi, ci},\nwhere mi\u2208R3is the center position, qi\u2208R4\nis a quaternion representing rotations, si\u2208R3\nis the scale of three dimensions, oi\u2208Ris the\nlearnt opacity, and ci\u2208R48is the three-order\nspherical harmonics to represent view depen-\ndent color. In its rendering process, 3DGS\nadopts a rasterization pipeline to achieve su-\nper efficiency. Specifically, all 3D Gaussians\nare first projected onto the image plane as 2D\nGaussians. Then the whole image is divided into 16\u00d716tiles as illustrated in\nFig. 1, pixels in each tile Bshare identical 3D Gaussians subset {Gi}B\u2208 {Gi}.\nWhenrenderingeachpixels,thetraditionalalphacompositionisappliedtoblend\nproperties xi(color or depth) of these 2D Gaussians into pixel space property\nXby their depth order:FlashSplat 5\nX=X\ni\u2208{Gi}Bxi\u03b1ii\u22121Y\nj=1(1\u2212\u03b1j) =X\ni\u2208{Gi}Bxi\u03b1iTi (1)\nwhere \u03b1iis the alpha value when rendering specific pixels, which is the product\nofoiand the probability of pixel position drops in the projected 2D Gaussians\nparameterized as (m2D\ni, \u03a32D\ni), and Ti=Qi\u22121\nj=1(1\u2212\u03b1j)is the transmittance,\npresenting the fraction that passes through and is not absorbed by the preceding\ni\u22121Gaussians.\n3.2 Binary Segmentation as Integer Linear Programming\nWe start with a reconstructed 3DGS scene, parameterized by {Gi}. It includes L\nrendered views with associated 2D binary masks, denoted as {Mv}. Each mask,\nM\u2208RH\u00d7Whas elements of 0 or 1, where 0 represents the background, and 1\ndenotes the foreground. Our goal is to assign a 3D label, Pi, which can be either\n0 or 1, to each 3D Gaussian Giby projecting the 2D masks into the 3D space.\nTo do this, we optimize Pithrough a differentiable rendering process. We\nassume Pias a property xiin Eq. 1. The aim is to minimize the discrepancy\nbetween the rendered masks and the provided binary masks Mv.\nLuckily, with {Gi}reconstructed, the \u03b1iand the Tibecome constant for\nrendering.Assuch,therenderingfunctionbecomesalinearequationwithrespect\nto the blending properties xi. This gives us a great flexibility to solve for the\noptimal mask assignment using simple linear optimization.\nFormally, our segmentation problem can be formulated as an integer Linear\nProgramming (LP) optimization with mean absolute error:\nMin\n{Pi}F=X\nv|R({Gi},{Pi})\u2212Mv|=X\nv\u2208LX\nMv\njk\u2208Mv\f\f\f\f\fX\niPi\u03b1iTi\u2212Mv\njk\f\f\f\f\f\nsubject to Pi\u2208 {0,1}.\n(2)\nwhere Rdenotes the 3DGS renderer. Given \u03b1i\u22650andTi\u22641for any i, and\nthe given light in alpha composition can only be absorbed, the total absorbed\nlight cannot exceed the initial light intensity, which is normalized to 1. Conse-\nquently, the sum of absorbed light fractions across all samples is bounded by\n0<P\ni\u03b1iTi\u22641. This leads us to introduce:\nLemma 1: In the alpha blending within 3D Gaussian Splatting, for Pi\u2208 {0,1},\n0\u2264X\nPi\u03b1iTi\u2264X\n\u03b1iTi\u22641 (3)6 Shen et al.\nTo solve this LP, we refer Lemma 1 to rewrite the Eq 2 as:\nminF=X\nv,i,j,kPi\u03b1iTiI(Mv\njk,0)\u2212X\nv,i,j,kPi\u03b1iTiI(Mv\njk,1) +C (4)\n=C+X\niPi(X\nv,j,k\u03b1iTiI(Mv\njk,0)\u2212X\nv,j,k\u03b1iTiI(Mv\njk,1)) (5)\n=C+X\niPi(Ai\n0\u2212Ai\n1) (6)\nwhere C=P\nv,j,kMv\njkis a constant value, alongside I(\u00b7,0)andI(\u00b7,1), serve as\nindicator functions signifying background and foreground presence, respectively.\nSolving ILP as Majority Vote. To minimize the function F, as outlined in\nEq. 4, we assign Pi= 0when all the corresponding pixels in masks Mvindicate\nbackground (0). Conversely, if there is a contradiction among the masks, we\nresolve this by as a weighted majority vote. Specifically, we assign the value of\neach Gaussian Gibased on the most frequent label in the masks, as detailed in\nEq 6.\nFormally, if the difference (A0\u2212A1)>0, where A0andA1represent the\ncounts of weighted masks indicating background and foreground respectively,\nwe assign Pi= 0to minimize F. If(A0\u2212A1)<0, then Pi= 1is assigned. In\na more general context, we can formulate the optimal assignment for {Pi}as\nfollows:\nPi= arg max\nnAn, n\u2208 {0,1}\nwhere An=X\nv,j,k\u03b1iTiI(Mv\njk, n)(7)\nIntuitively, this optimal assignment is conceptualized to aggregate the contri-\nbutions of individual 3D Gaussians during its rendering. Gaussians that sig-\nnificantly contribute to the foreground in all given masks are designated for\nforeground as Pi= 1, conversely, those contributing predominantly to the back-\nground are systematically allocated to the background as Pi= 0. Additionaly,\nthanks to the simple linear combination form in the objective of Equation 6, we\ncan concurrently assign optimal labels to each 3D Gaussians.\nRegularized ILP for Noise Reduction. Practically, the given 2D mask set\n{Mv}is typically predicted and associated by trained 2D vision models, which is\nlikely to introduce noise in certain regions (illustrated in column 1 and column 2\nof Fig. 6). This characteristic of the provided 2D masks can lead to noisy 3D seg-\nmentation results. For instance, as depicted in Fig.2 (a), background Gaussians\nmisclassified as foreground due to noise can result in a segmented 3D object with\nsharp, difficult-to-filter edges.\nTo address this, we refine above optimal assignment. This refinement entails\ninitially normalize overall contributions of Gaussians through L1normalization,\nrepresented as \u00afAe=Ae/P\ntAt. We then introduce a background bias, ranging\nbetween \u03b3\u2208[\u22121,1], to recalibrate \u02c6A0=\u00afA0+\u03b3, thereby adjusting the optimal\nassignment as Pi= arg maxn{\u02c6A0,\u00afA1}. Employing of \u03b3 > 0effectively dimin-\nish foreground noise in segmentation outcomes; conversely, \u03b3 < 0results in aFlashSplat 7\n\ud835\udefe=0\ud835\udefe=0.6\ud835\udefe=\u22120.6a)b)c)\nFig.2: The effect of the background bias \u03b3. By adjusting the background bias in\nour optimal assignment, the noisy 3D segmentation caused by 2D masks can be flexibly\nmitigated for various downstream applications.\ncleaner background, as demonstrated in Fig.2 (b) and (c), respectively. This\nsoftened form of optimal assignment provides flexibility to produce accurate 3D\nsegmentation results against 2D masks noises for different downstream tasks.\n3.3 From Binary to Scene Segmentation\nNumerous instances are present across various views in 3D scenes. Segment-\ning multiple objects within these scenes requires executing binary segmentation\nmultiple times according to above formulation. This process involves repeatedly\ngathering {A0, A1}, which inherently slows down the pace of scene segmentation.\nThus, we extend our methodology from binary segmentation to encompass scene\nsegmentation to address this challenge more efficiently.\nThis transition to multi-instance segmentation is motivated by two key con-\nsiderations. Initially, it\u2019s important to recognize that 3D Gaussians do not ex-\nclusively belong to a single object. This is exemplified in Figure 1, where pixel\nu1andu2are influenced by the same 3D Gaussian, despite belonging to dis-\ntinct objects (color block). Furthermore, the introduction of multiple instances\ncomplicates the constraint in Equation 2 to Pi\u2208 {0,1, ..., E \u22121}, with Erep-\nresenting the total number of instances in a scene. Consequently, the set of\nprovided masks Mvbecomes Mv\u2208 {0,1, ..., E \u22121}to accommodate multiple\nsegmented 2D instances. Such constraints prevent achieving a global optimum,\nas the labels across instances are subject to be exchangable.\nTo circumvent these challenges, we reinterpret multi-instance segmentation\nas a combination of binary segmentation, modifying the optimal assignment\nstrategy as outlined in Equation 7. To isolate a specific instance labeled ein a\n3D scene, we redefine all other objects within the instance set as the background8 Shen et al.\nto compute Aothers. This approach is formalized as follows:\nPi= arg max\nnAn, n\u2208 {0, t}\nwhere At=X\nv,j,k\u03b1iTiI(Mv\njk, t),\nA0=Aothers =X\ne\u0338=tX\nv,j,k\u03b1iTiI(Mv\njk, e)(8)\nWith this formulation, we only need to accumulate the set {Ae}once, and then\nperform arg max on this set to get Gaussians subset {Gi}efor each object e.\nConsequently,thisallowsuserstoselectivelyremoveormodifythese3DGaussian\nsubsets by specifying object ID. It is also noteworthy that subsets {Gi}efrom\ndifferent instances may overlap under this formulation when \u03b3 <0, a reflection\nof the inherent non-exclusive nature of 3D Gaussian Splatting (see more details\nin supplementary material Sec. 8.3).\n3.4 Depth-guided Novel View Mask Rendering\nGiven above formulation eschews the need for dense optimization, our approach\nis capable of yielding robust segmentation results using merely approximately\n10% of the masked 2D views. This efficiency also empowers our method with\nthe capability to produce 2D masks \u02c6Mvfor previously unseen views. For ren-\ndering masks in novel views within the context of binary segmentation, we focus\nexclusively on rendering foreground Gaussians identified by Pi= 1. This pro-\ncess involves computing an accumulated alpha value \u03c1jkfor every pixel (j, k).\nFor novel view mask rendering in binary segmentation, we merely render fore-\nground Gaussians with Pi= 1to produce accumulated alpha value \u03c1jkfor each\npixel, then the 2D mask can be obtained by simple quantization on alpha values\n\u03c1jkwith threshold \u03c4, which is a pre-defined hyper parameter. Specifically, 2D\nmasks are derived through straightforward quantization of these alpha values,\nwhere \u02c6Mv\njk=Q(\u03c1jk, \u03c4)and\u03c4represents a predetermined threshold, and Qis\nthe quantization function, when \u03c1jk> \u03c4,Q= 1, conversely Q= 0.\nFor novel view mask rendering in scene segmentation, due to the intersec-\ntions among segmentation results of different objects as indicated in Equation 8,\nthe resultant alpha mask might present ambiguity, as illustrated in 3rd column\nof Fig. 3. Specifically, rendering each object\u2019s associated 3D Gaussian subset\n{Gi}ein the same viewpoint might result in several objects meeting the con-\ndition Q(\u03c1e\njk, \u03c4) = 1. In this case, we introduce depth to determine the final\nsegmentation results. The depth at each pixel location (j, k)is utilized to filter\nthe final 2D mask outcome. The object esatisfying Q(\u03c1e\njk, \u03c4) = 1with minimal\ndepth De\njkrelative to the camera at a given pixel (j, k)is selected as \u02c6Mjk=e.\nFigure 3 illustrates the prediction of 2D masks for both binary and multi-object\nsegmentation in novel views.FlashSplat 9\nrenderedbinarymaskrenderedvieww/odepthguidancewithdepthguidance\nFig.3: Novel view mask rendering . Here we showcase mask rendering on novel\nviews for both binary and scene segmentation. Simple alpha mask quantization can\ngenerate consistent masks in binary segmentation. With depth guidance, scene seg-\nmentation also can generate feasible 2D masks in novel views.\n4 Experiments\n4.1 Data preparation\nDataset. To assess the efficacy of our approach, we collect 3D scenes from\nseveral sources: the MIP-360 [1] dataset, T&T [21] dataset, LLFF [29] dataset,\nInstruct-NeRF2NerF [13], and the LERF [19] dataset, serving as the basis for\nqualitative analysis. For quantitative analysis, we utilize the NVOS [37] dataset.\n2D mask generation and association. In our experimental setup, we utilize\nthe Segment Anything Models (SAM) [20] to extract masks, given that SAM\u2019s\nsegmentationoutputisinherentlysemantic-agnostic.Itbecomesnecessarytofur-\nther associate these 2D masks within our framework. Our approach diverges into\ntwo distinct strategies, tailored respectively for binary and scene segmentation.\nFor binary segmentation, where the objective is to isolate a single foreground\nentity, we initiate by marking point prompts on a single reference view. These\npoint prompts are projected back to the 3D space with reference view camera\npose to find their nearest 3D Gaussians with the smallest positive depth. Subse-\nquently, these point prompts are propagated to other views by projecting their\ncorresponding 3D Gaussians\u2019 center. Leveraging these associated point prompts,\nSAM independently generates a binary mask for each view. For of scene segmen-\ntation, our methodology begins with employing SAM to produce instance masks\nfor individual views. To assign each 2D object with a unique ID in the 3D scene,\nmultiple views are treated akin to a video sequence. Utilizing a zero-shot video\ntracker [7,54], we ensure the consistent association and propagation of objects\nacross viewpoints.\n4.2 Implementation details\nWe implement the optimal 3D segmentation both in Eq. 7 and Eq. 8 within\nCUDA kernel functions. The computation of Aifor each Gaussian employs tile-10 Shen et al.\nbased rasterization with the renderer, excluding the color component ciand\nfocusing solely on Gi={mi, qi, si, oi}. For each pixel in the provided mask set\nM, we perform alpha composition as defined in Equation 1. The product \u03b1iTifor\neach Gaussian is then aggregated into the buffer Ae, corresponding to the pixel\nlabel e=Mv\njk, through atomic operations. Leveraging the rasterization pipeline\nenables the completion of the set Aefor all objects in under 30 seconds. For\nbinary segmentation, the arg max is directly applied to determine the optimal\nassignment. In the case of scene segmentation, arg max is executed iteratively\nalongside dynamic programming to speedup. This assignment ensures that the\ncomputation for both binary and scene segmentation concludes in 1 millisecond,\nthereby facilitating interactive adjustment of the \u03b3value once the set {Ae}is\ncomputed. This adjustment allows users to effectively reduce segmentation noise\nacross a variety of downstream tasks.\n4.3 3D segmentation results\nIn Fig.4, we exhibit the results of both binary and scene 3D segmentation. The\nfirstrowpresentstheFigurinesscenefromtheLERFdataset[19],andthesecond\nrow features the Counter scene from the MIP-360 dataset [1]. On both scenes,\nwe apply our scene segmentation approach, rendering 2 views for 5 segmented\nobjects (circled in the ground-truth images) for each scene, demonstrating our\nmethod\u2019s capability in conducting scene segmentation with instance masks pre-\ndicted by SAM [20]. Additionally, binary segmentation is showcased in rows 3, 4\nand 5, with row 3 illustrating the Horns scene from the LLFF dataset [29], row\n4 displaying the Truck scene from the T&T dataset [21], row 5 displaying the\nKitchen scene from the MIP-360 dataset [1]. Two views of the segmented objects\nare rendered, showing our approach\u2019s capability in segmenting 3D objects.\n4.4 Object Removal\n3D object removal involves entirely eliminating the 3D Gaussians subset of an\nobject from the scene. Results of such removals are illustrated in Figure 4. It\u2019s\nimportant to note that we apply a background bias \u03b3=\u22120.4across all scenes\nto ensure the background remains clear. For small 3D objects in row 1 and row\n2, we simultaneously remove 5 objects from the scene. As the background of\nthese two scenes is likely to be observed in other views, the artifacts in these two\nscenes are minor even imperceptible for certain objects. For more challenging\nscenes depicted in rows 3, 4, and 5, the space vacated by the removed objects\nrevealsanoisierbackgroundorevenblackholes,primarilyduetotheobstruction\nof background by larger foreground objects. This situation is pronounced in the\nHorns scene (row 3), which comprises only facing-forward views.\n4.5 Object Inpainting\nFollowing 3D object removal, object inpainting [34] aims to correct unobserved\nregion artifacts, ensuring view consistency within the 3D scene. Initially, we ren-\nder views post-removal and employ Grounding-DINO [26] to identify artifactFlashSplat 11\nFig.4: Qualitative result of FlashSplat. FlashSplat is capable of performing both\nbinary segmentation and scene segmentation. With our single step optimal assignment,\nall 3D segmentation is completed within 30 seconds. These segmentation results show\nour FlashSplat can robustly segment 3D objects and remove objects in 3D scenes.\nregions in each view, which are tracked across views using a video tracker [7].\nPretrained 2D inpainting models [43] then generate inpainted 2D views. Sub-\nsequently, the 3DGS parameters are refined with these views by introducing\n200Knew Gaussians near the original object locations, maintaining background\nGaussians frozen. Fine-tuning involves L1loss outside object masks to minimize\nbackground impact, and LPIPS loss [60] inside the inpainting masks for scene\nnaturalness and consistency. We display object inpainting results in Fig. 5, ren-\ndering three views per scene. Noises and holes are diminished after the object in-\npainting, demonstrating that our method can effectively separate the foreground\nfrom the background in 3D segmentation.\n4.6 Quantitative comparison\nWe perform a quantitative analysis using the NVOS dataset [37], derived from\nthe LLFF dataset [29]. The NVOS dataset comprises 8 facing-forward 3D scenes\nfrom LLFF, providing a reference and a target view with 2D segmentation mask12 Shen et al.\nMethod mIOU ( %)\u2191mAcc ( %)\u2191\nNVOS [37] 39.4 73.6\nISRF [12] 70.1 92.0\nSGISRF [45] 83.8 96.4\nSA3D [4] 90.3 98.2\nSAGA [3] 90.9 98.3\nFlashSplat (ours) 91.8 98.6\nTable 1: Quantitative comparison on the NVOS [37] dataset.\nannotations for each scene. Our approach begins with sampling point prompts\nfrom the reference view\u2019s annotated mask, which are then propagated to other\nviews to guide the segmentation of SAM [20]. Following this, we apply our bi-\nnary segmentation approach as depicted in Eq. 2 to isolate the foreground 3D\nGaussians. Subsequently, we employ novel view mask rendering with a threshold\n\u03c4= 0.1to render 2D masks for the target view, allowing us to calculate the\nmean Intersection over Union (IoU) and mean accuracy across all 8 scenes. The\ncomparativeresults,presentedinTable1,benchmarkourFlashSplatagainstpre-\nvious NeRF-based 3D segmentation approaches such as NVOS [37], ISRF [12],\nSGIRF [45], SA3D [4], and the 3D-GS method SAGA [3]. Our method demon-\nstrates superior performance in both metrics on this dataset.\n4.7 Computation cost\nWe assess the computational efficiency of FlashSplat in comparison to previous\n3DGS segmentation methods, namely SAGA [3] and Gaussian Grouping [54].\nThis evaluation is conducted on the Figurines scene from the LERF dataset [19]\nusing a single NVIDIA A6000 GPU. Both baseline methods necessitate the uti-\nlization of gradient descent optimization over 30,000iterations to distill 2D\nmasks into object features associated with each 3D Gaussian, thereby incurring\nsignificant extra training time for optimized 3D scene. In contrast, our approach\nmerely requires the calculation of the set {Ai}e, a process that completes in\napproximately 26 seconds, making it roughly 50\u00d7faster than baselines. For seg-\nmenting a single 3D object, these baselines demand network forwarding, whereas\nFlashSplat efficiently employs arg max for optimal assignment determination,\ntaking just 0.4milliseconds. Furthermore, an analysis of GPU memory usage\nreveals that our peak memory consumption is only half of that required by the\nprevious method SAGA [3].\nExtra Time Optimization\nStepsSegmentation\nTimePeak\nMemory\nSAGA [3] 18 min 30000 0.5 s 15G\nGaussian Grouping [54] 37 min 30000 0.3 s 34G\nFlashSplat (ours) 26 s 1 0.4 ms 8G\nTable 2: Computation cost comparisons over the Figurines scene.FlashSplat 13\nRemovalrenderedviewInpaintingrenderedviews\nFig.5: Object inpainting after removal . Here we showcase the object removal re-\nsults from 3D scenes, including the kitchen scene from MIP-360 dataset and bear scene\nfrom Instruct-NeRF2NeRF [15]. After removal, we inpaint the regions with artifacts by\ntuning the optimized 3DGS parameters. Artifacts after object removal are diminished\nby our object inpainting.\n4.8 Ablation study\nThe effect of noise reduction. To further elucidate the previously mentioned\nnoise present in 2D masks, we provide visualizations of the 2D masks generated\nby SAM [20] in the left column of Fig. 6 for two scenes. Additionally, we render\nthe object mask after 3D segmentation on corresponding views, revealing that\nthe broken regions in the provided 2D masks are remedied. This demonstrates\ntherobustnessofourmethodingenerating3Dsegmentationdespitethepresence\nof noise in the 2D masks.\npredictedmasksrenderedmasks\nFig.6:Noisereductionwith3Dsegmentation .Hereweshowtwonoisymasksper\nscene, and the corresponding 2D mask is rendered from the 3D segmentation results.\nThe broken regions in the given 2D masks are remedied from the 3D segmentation.\n3D Segmentation with fewer 2D Masks. Since our approach does not rely\non multi-step gradient descent optimization, it naturally enables the utilization\nof fewer 2D masks for segmentation. To validate this capability, we consider14 Shen et al.\n2D mask subsets comprising 1/4,1/8,1/16, and 1/32of the total number of\nviews in the scene. Subsequently, we visualize the 3D segmentation results by\nrendering the foreground 3D Gaussians onto unseen views. As depicted in Fig. 7,\nour method is capable of producing decent segmentation results even with only\n1/8of the total view masks for these two 360-degree scenes. However, with fewer\nviews, such as 1/16and1/32, many 3D Gaussians remain unseen in the provided\nmasks, leading to potential artifacts in the segmentation outcomes.\n1/4views1/8views1/16views1/32views\nFig.7: 3D Segmentation with fewer masks . Here we render 3D segmentation\nresults on novel views with given fewer 2D masks, showing our method is capable of\nproducing decent segmentation results with only 10%of total view numbers.\n5 Conclusion\nInthiswork,weintroduceanoptimalsolverfor3DGaussianSplattingsegmenta-\ntion from 2D masks, significantly advancing the accuracy and efficiency of lifting\n2D segmentation into 3D space. By breaking the alpha composition in 3D-GS\ninto overall contributions of each Gaussians, this solver only requires single step\noptimization to get the optimal assignment. It not only expedit the optimization\nprocess by approximately 50 times faster compared to previous methods but also\nenhanced robustness against noise with a simple background bias. Further, this\napproach is extended to scene segmentation and capable of rendering masks on\nnovel views. Extensive experiments demonstrate superior performance in scene\nsegmentation tasks, including object removal and inpainting. We hope this work\nwill facilitate 3D scene understanding and manipulating in the future.\nAcknowledgement\nThisprojectissupportedbytheNationalResearchFoundation,Singapore,under\nits Medium Sized Center for Advanced Robotics Technology Innovation.FlashSplat 1\nFlashSplat: 2D to 3D Gaussian Splatting\nSegmentation Solved Optimally\nSupplementary Material\n6 More Implementation details\nThe implementation of FlashSplat unfolds in two main parts. Initially, the focus\nis on deriving the contribution set {Ae}for every Gaussian across objects e.\nThis step involves projecting 3D Gaussians onto each mask Mv, capturing the\nproduct \u03b1iTiwithin the alpha blending formula into the buffer Aewhere a pixel\nMv\nij=e. This procedure compiles the contributions from every object across all\nviewpoints into a matrix A \u2208RE\u00d7|{Gi}|, where Eis total number of objects in\nthe 3D scene, and |{Gi}|represents the total number of 3D Gaussians. Following\nthis, we allocate labels Pito each 3D Gaussian Gibased on this contribution\nmatrix, as delineated in equations Eq. 7 and Eq. 8. For binary segmentation, the\nassignment process simplifies to an arg max operation for optimal assignment.\nScene segmentation is resolved through dynamic programming to manage the\ncomplexity of multiple assignments, with the specific implementation details\nprovided in list 1.1. The segmentation results for scenes are represented within\na matrix S \u2208RE\u00d7|{Gi}|, where each entry Sm,n\u2208 {0,1}specifies whether the\nn-th Gaussian belongs to object m.\n6.1 Mask association details\nOur work primarily concentrates on lifting 2D masks into 3D space, with less\nemphasis placed on mask association within the core sections of main paper.\nIn this context, we provide additional insights into the methodology used to\nassociate 2D masks in the 3D segmentation experiments presented.\nBinary Mask Association. Within binary segmentation scenarios, association\namong 2D view masks is achieved through the propagation of point prompts\nacross different views. Specifically, for a point prompt p2D\ni\u2208R2identified on an\nobject in a single view, this point is back-projected to the 3D space, acquiring\na world coordinate p3D\ni\u2208R3, to locate its corresponding 3D Gaussian Gi. How-\never, due to the prevalence of numerous Gaussians surrounding this 3D point\nprompt, relying solely on distance for Gaussian correspondence can lead to in-\ncorrect outcomes. To mitigate this, we initially identify the Top \u221210closest 3D\nGaussian centers using the L2distance. Subsequently, the specific Gaussian is\ndetermined by selecting the one with the least depth when projected onto the\nreference view. The center positions of these 3D Gaussians are then projected\nonto other views to link point prompts associated with the same object across\ndifferent views. By utilizing SAM [20] to produce masks for each view based on\nthese aligned point prompts, we inherently associate these predicted 2D masks.\nThis method of point prompt propagation is implemented via CUDA kernels,\nenabling the association of point prompts across all views in under one second.2 Shen et al.\nScene Mask Association. Beyond binary segmentation, the segmentation of\nentire 3D scenes without specific point prompts is essential. Therefore, we in-\ntroduce an alternative approach for associating scene masks across all views.\nTo assign each 2D object with a unique ID in the 3D scene, multiple views are\ntreated akin to a video sequence. Utilizing a zero-shot video tracker [7,54], we\nensure the consistent association and propagation of objects across viewpoints.\n7 More qualitative evaluation\nTo validate the effectiveness of our FlashSplat, we conduct qualitative compar-\nisons on the object removal task with prior works in 3D Gaussian Splatting\nsegmentation, specifically Gaussian-Grouping [54]. As outlined in Sec.4.4, 3D\nobject removal involves entirely eliminating the 3D Gaussians subset of selected\nobjects from the scene, which is a fundamental application of 3D segmentation.\nFor fair comparison, we use identical 2D scene mask set {Mv}for both methods.\nOur process begins with conducting 3D segmentation using these scene masks,\nfollowed by the specification of object IDs for removal. We present the multiple\nobject removal results in Fig. 8 and single object removal results in Fig. 9. We\nrender 4 distinct views of the removal results, showing that our FlashSplat can\ncleanly remove these 3D objects with imperceptible artifacts, while the results\nof Gaussian-grouping show severe artifacts near the removed 3D objects. These\ncomparisons underscore our method is not only superior for the efficiency of 3D\nsegmentation, but also excels at 3D scene segmentation quality.\nFig.8: Multiple Object Removal Comparison . Here we show a qualitative com-\nparison by removing multiple objects from the Counter scene in the MIP-360 [1]\ndataset. The first row is the ground truth, the second row shows our FlashSplat, and\nthe third row displays the results from Gaussian-Grouping. A total of 5 objects are\nremoved in this scene, the same as in Fig. 4 row 2.FlashSplat 3\nFig.9: Single object removal comparison . Here we show a qualitative comparison\nby removing a single object from the Bear scene in the Instruct-NeRF2NeRF [15]\ndataset. The first row is the ground truth, the second row is our FlashSplat, and the\nthird row shows the results from Gaussian-Grouping.\n8 More discussions\n8.1 The effect of background bias \u03b3\nTable3presentsourablationstudyonthetruckscenesfromtheT&Tdataset[21].\nWe annotate 5 views 2D mask as target views, and other view masks predicted\nby SAM [20] are used as reference view masks. With the background bias \u03b3rang-\ning from [\u22121,1], we get the 3D segmentation of the truck and then render it to\n2D masks to compute the mean IoU. Among these \u03b3values, a setting of \u03b3= 0.4\nproduces the optimal mean IoU of 94.2%. This is caused by the noise in masks\npredicted by SAM (as visualized in Fig. 6), the assignment with \u03b3= 0is prone to\ntake background Gaussians as foreground, while this softened refinement helps\nto reduce such noises.\n\u03b3 -0.8 -0.4 0 0.4 0.8\nmIoU 82.4 89.6 92.3 94.2 93.8\nTable 3: Effect of the background bias \u03b3on the truck scene.\n8.2 Quantization in novel view mask\nIn Sec. 3.4, we outline projecting masks from 3D segmentation results onto novel\nviews using simple quantization and depth-guidance. Here we take mask ren-\ndering in binary segmentation as an example to claim why this quantization is4 Shen et al.\nnecessary.Withoutthisquantization,novelviewmasksareproducedbyfirstpro-\njecting subsets of 3D Gaussians {Gi}efor instance e, and then the mask value for\neach pixel \u02c6Mv\njkis determined by \u02c6Mv\njk= arg maxe\u03c1e\njk. The absence of quantiza-\ntion leads to masks, displayed in the 2-nd column and 4-th row of Fig. 10, riddled\nwith numerous unintended holes. This phenomenon stems from the nature of 3D\nGaussian Splatting, where each Gaussian is a semi-transparent ellipse with opac-\nityoi. As such, when a pixel (i, j)is rendered, the background 3D Gaussians\nalso affect the alpha blending outlined in Eq. 1, at times more significantly than\nthe foreground Gaussians. Implementing quantization and depth guidance ame-\nliorates these discrepancies, as evident in the first and third columns of Fig. 4.\nHowever, it\u2019s noteworthy that despite the employment of depth guidance, mask\nrendering can still produce vague outcomes in scene mask rendering due to the\nabsence of geometric supervision in 3D Gaussian Splatting reconstruction. The\ngeometry that is learned does not conform precisely to the underlying geometry,\noccasionally impairing the effectiveness of depth guidance.\nFig.10: Quantization in mask rendering .\n8.3 Scene segmentation extension\nIn Sec. 3.3, we extend our optimal assignment for binary segmentation to scene\nsegmentation. This formulation, shown in Eq. 8, is chosen over a straightfor-\nward approach that would simply perform arg maxamong the Einstances. This\nchoiceisdrivenbythenon-exclusivenatureofGaussianSplatting,whereaGaus-\nsian can be shared between objects. For instance, we quantitatively analyze the\nCounter scene in the MIP"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "## Summary of the Paper: \"WINDOWS AGENT ARENA: Evaluating Multi-Modal OS Agents at Scale\"\n\n### 1. Abstract:\nThe paper presents the **WINDOWS AGENT ARENA**, a benchmark suite for evaluating multi-modal agents within the Windows operating system. It addresses challenges in measuring agent performance in realistic environments with over 150 diverse tasks requiring planning, screen understanding, and tool usage. The benchmark can be seamlessly parallelized in Azure, reducing evaluation time significantly. The new multi-modal agent, **Navi**, is introduced, achieving a success rate of 19.5% in the Windows domain compared to 74.5% for an unassisted human. The paper provides insights into future research opportunities in agent development using WINDOWS AGENT ARENA.\n\n### 2. Introduction:\n- **Context**: Computer agents leveraging large language models (LLMs) such as GPT-4V are enhancing productivity and software accessibility. Yet, benchmarking these agents across various environments is challenging.\n- **Research Questions**: How can we measure agent performance in realistic, multi-modal environments? Can tasks be structured to cover a wide range of activities within the Windows OS?\n- **Literature Review**: Reviews benchmarks like OSWorld and AndroidWorld that incorporate real OS environments into testing domains but focus on Linux and mobile respectively, highlighting the need for a Windows-specific benchmark.\n- **Positioning**: WINDOWS AGENT ARENA extends the OSWorld framework to create a diverse set of Windows tasks, significantly improving evaluation scalability by leveraging Azure for parallel task processing.\n\n### 3. Related Work:\n- **Benchmarks**: Several benchmarks like GAIA, Mind2Web, PixelHelp, AiTW, and others are reviewed, emphasizing their limitations in covering comprehensive, cross-domain agent tasks.\n- **Agents**: Discusses advancements in multi-modal LLM agents and the various capabilities tested across these benchmarks.\n- **Advancements**: WINDOWS AGENT ARENA advances the state of the art by focusing on Windows OS tasks, providing a native, scalable environment for evaluation.\n\n### 4. Method:\n- **Proposed Approach**: WINDOWS AGENT ARENA, a benchmark suite within a real Windows OS, using a wide range of applications and web browsers.\n- **Mathematical Formulation**: Tasks are modeled as partially observable Markov decision processes (POMDP) with state space \\( S \\), observation space \\( O \\), action space \\( A \\), transition function \\( T \\), and reward function \\( R \\).\n- **Task Creation**: Over 154 tasks spread across various applications such as LibreOffice, Edge, VSCode, VLC Player, etc.\n- **Architecture**: Utilizes the OSWorld framework to integrate tasks and evaluates the reward based on task completion. The method uses a deterministic script for setting the initial state and post-processing scripts for evaluating task completion.\n- **Novel Techniques**: Combines multiple modalities, such as UI Automation (UIA) trees, OCR, DOM trees, and Set-of-Marks for detailed screen analysis.\n\n### 5. Experimental Setup:\n- **Datasets**: 154 tasks derived from OSWorld, modified for Windows, including office tasks, web browsing, coding, media, system navigation, and utilities.\n- **Hardware/Software**: Evaluations run on local machines using Ubuntu or Windows Subsystem for Linux (WSL), scalable to Azure Machine Learning for parallel evaluations.\n- **Evaluation Metrics**: Success rate (binary and graded) based on task completion and similarity to the target outcome.\n- **Baselines/Comparisons**: Compared human performance (74.5% success) with various models of Navi (best achieving 19.5% success rate).\n\n### 6. Results and Analysis:\n- **Empirical Findings**: \n  - **Best configuration of Navi**: 19.5% success rate, utilizing mixed UIA and proprietary pixel-based element detectors with GPT-4V.\n  - **Detailed Failures**: Common errors from misalignment in visual-language representations and imprecise element selection.\n  - **Baseline Comparisons**: Showed that higher-quality visual prompts significantly improved performance.\n- **Graphs/Tables**: Success rates by task domain, effectiveness of different configurations of visual prompts.\n- **Ablation Studies**: Indicated that high-quality UIA markers increased performance by 57% for model configurations.\n\n### 7. Discussion:\n- **Interpretation**: Significant performance gap exists between human and agent performance due to complexity in task environments and necessity for precise visual-language alignment.\n- **Implications**: Highlights challenges and opportunities in fine-tuning multi-modal agents, especially focusing on specific visual parsing techniques.\n- **Limitations**: Limited by high computation requirements for high-fidelity UIA parsing and visual models alignment, suggesting improvements in efficiency and accuracy for future studies.\n\n### 8. Conclusion:\n- **Key Contributions**: Introduction of a scalable environment for evaluating agents on Windows OS, detailed analysis of agent performance, new state-of-the-art agent Navi for Windows tasks.\n- **Future Research**: Suggested avenues include improving set-of-marks precision, exploring reinforcement learning for task completion, human-in-the-loop approaches, and development of specialized sub-agents for different domains within the Windows OS.\n\nThis comprehensive summary captures the essential details\u2014objective, methodology, technical specifications, empirical findings, and implications\u2014necessary to replicate and build upon this research."}, {"role": "assistant", "content": "WINDOWS AGENT ARENA :\nEVALUATING MULTI -MODAL OS A GENTS AT SCALE\nRogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont,\nSara Abdali, Yinheng Li, Justin Wagle & Kazuhito Koishida\nMicrosoft\nArthur Bucker\u2217& Lawrence Jang\u2217\nCarnegie Mellon UniversityZack Hui\u2217\nColumbia University\nABSTRACT\nLarge language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: ( i) most benchmarks are lim-\nited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and ( ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these challenges,\nwe introduce the W INDOWS AGENT ARENA : a reproducible, general environment\nfocusing exclusively on the Windows operating system (OS) where agents can\noperate freely within a real Windows OS and use the same wide range of appli-\ncations, tools, and web browsers available to human users when solving tasks.\nWe adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Win-\ndows tasks across representative domains that require agent abilities in planning,\nscreen understanding, and tool usage. Our benchmark is scalable and can be\nseamlessly parallelized in Azure for a full benchmark evaluation in as little as 20\nminutes. To demonstrate W INDOWS AGENT ARENA \u2019s capabilities, we also intro-\nduce a new multi-modal agent, Navi . Our agent achieves a success rate of 19.5%\nin the Windows domain, compared to 74.5%performance of an unassisted hu-\nman.Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nofNavi \u2019s performance, and provide insights into the opportunities for future re-\nsearch in agent development and data generation using W INDOWS AGENT ARENA .\nWebpage: https://microsoft.github.io/WindowsAgentArena\nCode:https://github.com/microsoft/WindowsAgentArena\n1 I NTRODUCTION\nThe concept of an agent is not new to the field of artificial intelligence, with its definition and\ntaxonomy being subject to debate for decades (Minsky, 1988; Shoham, 1993; Franklin & Graesser,\n1996; Jennings et al., 1998). In the broadest sense, an agent is \u201c anything that can be viewed as\nperceiving its environment through sensors and acting upon that environment through actuators \u201d\n(Russell & Norvig, 2016). As most of our daily personal and work tasks move to and concentrate\nwithin the digital realm, the development of computer agents that can perceive, plan, and act in\nvirtual environments holds great potential to enhance human productivity and software accessibility.\nHigh-capacity multi-modal models, such as GPT-4V (Achiam et al., 2023), GPT-4o, and Gem-\nini (Team et al., 2023), now serve as the main backbone of agents tested in benchmarks focus-\ning on domains such as web navigation (Deng et al., 2023) mobile screens (Rawles et al., 2024b),\nand software development (Jimenez et al., 2023). The complexity of realistic human workflows,\nhowever, is not fully captured in most testing platforms. We often perform complex cross-domain\n\u2217Work done while interning at Microsoft\n1arXiv:2409.08264v1  [cs.AI]  12 Sep 2024WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\nWINDOWS AGENT ARENAInitial conditions Individual task instructions\nActObserveReason\nAzure cloud parallelization\nReal Windows OS environment\nLLM / VLM \nagentMake the line spacing of first two \nparagraphs into double line spacingMake a shortcut \\ \"ctrl+j\\ \" to move \ncursor focus from terminal to editor in VSCode\nLook up a flight from Mumbai (BOM) to Stockholm (STO) that departs next Monday.\nFigure 1: WINDOWS AGENT ARENA is a benchmark suite that envelops a real Windows OS environment, allowing agents to interact with a\nvariety of applications and web domains. The benchmark is scalable and can be deployed in Azure virtual machines for fast parallel evaluation.\ntasks while switching between programs, application contexts, and graphical or command line in-\nterfaces to achieve our goals. To address the gap between task complexity and simulator realism,\nOSWorld (Xie et al., 2024) and AndroidWorld (Rawles et al., 2024a) stand out as the first bench-\nmarks to embody and envelop a real computer and mobile OS into the testing domain, expanding\nand aligning the observation/action spaces to better match the human computing experience.\nIn this work, we introduce W INDOWS AGENT ARENA \u2014a benchmark suite which builds upon the\nOSWorld framework with the goal of advancing the rigorous development and testing of multi-\nmodal agents within the Windows OS ecosystem. While OSWorld focuses predominantly on Linux\ntasks, our benchmark is developed specifically for Windows\u2014the most common computer OS\namong everyday users and enterprises (with 73% market share1). We provide a diverse set of tasks\nwith initial conditions and automatic execution-based evaluation criteria that reward task completion\nrather than following pre-recorded human trajectory demonstrations.\nWe have designed W INDOWS AGENT ARENA to be fully scalable and deployable in secure Docker\ncontainers running in Azure virtual machines. Agent benchmarks can be notoriously slow for eval-\nuation to complete given the sequential multi-step nature of each task, often taking days to compute\nin series using a local development machine. Unlike physics simulators, it is impractical to speed\nup the simulation clock for an OS since multiple processes run asynchronously. Our solution for\nWINDOWS AGENT ARENA was to instead focus on task parallelization to accelerate evaluation.\nLastly, we develop a new agent for Windows OS, Navi , which can serve as a starting point for\ndevelopers and researchers using W INDOWS AGENT ARENA . We provide several variants of Navi to\nserve as references for future agent iterations. Our contributions can be summarized as follows:\n\u2022 We introduce W INDOWS AGENT ARENA , an agent benchmark with 154 diverse and realistic\nmulti-step tasks focused on the Windows OS distributed across multiple apps and web domains.\nDeployment can be parallelized in Azure using a secure infrastructure where full evaluation takes\nas little as 20minutes (see Appendix A.7), as opposed to multiple hours to over a day with serial\ncomputation. We plan on continuously updating W INDOWS AGENT ARENA with new tasks and\ncapabilities to improve agentic testing and evaluations.\n\u2022 We introduce a new multi-modal agent, Navi , and extensively benchmark its performance (Sec-\ntion 4). The best configuration achieves a 19.5%success rate in W INDOWS AGENT ARENA using\nSet-of-Marks prompting (Yang et al., 2023) combined with the system accessibility tree and\npixel-based element detectors. Additionally, we test Navi on another web-based benchmark\n(Mind2Web) and verify its competitive performance.\n\u2022 We make our code, benchmark, and models available as open-source contributions to the re-\nsearch community2and provide ample discussion on how W INDOWS AGENT ARENA can fa-\ncilitate further agent development, faster experimentation, and data generation at scale in the\nWindows environment.\n1https://gs.statcounter.com/os-market-share/desktop/worldwide\n2https://github.com/microsoft/WindowsAgentArena\n2WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\n2 P RIOR WORK\nPlatform \u2203Env? \u2225Env? # templates # tasks/templ.\n(approx.)Reward Platform\nGAIA \u2717 - 466 1 text-match None\nMind2Web \u2717 - 2,350 1 None Desktop Web\nWebLinx \u2717 - 2,337 1 None Desktop Web\nPixelHelp \u2717 - 187 1 None Android\nMoTIF \u2717 - 4,707 1 None Android (Apps+Web)\nAiTW \u2717 - 30,378 1 None Android (Apps+Web)\nOmniAct \u2717 - 9,802 1 None Desktop (Apps+Web)\nMiniWoB++ \u2713 \u2717 114 variable HTML/JS state Web (synthetic)\nWebArena \u2713 \u2717 241 3.3 text-match Desktop Web\nVisualWebArena \u2713 \u2717 314 2.9 text/img-match Desktop Web\nWorkArena \u2713 \u2717 29 622.4 cloud state Desktop Web\nMMInA \u2713 \u2717 1,050 1 text-match Desktop Web\nAndroidWorld \u2713 \u2717 116 variable device state Android (Apps+Web)\nOSWorld \u2713 \u25cf 369 1 device state Desktop (Apps+Web)\nWINDOWS AGENT ARENA \u2713 \u2713 154 1 device state Desktop (Apps+Web)\nTable 1: Table adapted from Rawles et al. (2024a) comparing different benchmarks for AI agent evaluation. \u201c \u2203Env? \u201d denotes whether an\nenvironment exists for the platform/benchmark and \u201c \u2225Env? \u201d denotes whether the benchmark environment is natively scalable/parallelizable.\nOSWorld can be parallelized within a single machine, but this limits scalability.\nLLMs, Agents, & Multimodality. Advances in LLMs and foundation models have led to signif-\nicant interest in their potential as autonomous agents. Recent improvements in multi-modal (e.g.,\nlanguage and vision) capabilities like image captioning and visual reasoning have spurred applica-\ntions to agent perception and action grounding such as cross-attention between modalities (Alayrac\net al., 2022), trainable visual expert models (Wang et al., 2023), mapping images into frozen LLM\nembedding spaces (Tsimpoukelli et al., 2021), fine-tuning with images/instruction pairs (Liu et al.,\n2023a), and more. Many closed-source LLMs have followed suit to incorporate vision and other\nmodalities such as GPT-4V (OpenAI, 2023) and Gemini (Google-Research, 2023). Various ex-\namples of text-only and vision-language agent models have seen testing on the web, desktop, and\nmobile environments: UFO (Zhang et al., 2024a), CC-Net (Humphreys et al., 2022), AiTW (Rawles\net al., 2024b), CogAgent (Hong et al., 2023), MM-Navigator (Yan et al., 2023), SeeAct (Zheng et al.,\n2024b), WebAgent (Gur et al., 2023), OS-Copilot (Wu et al., 2024), among others.\nBenchmarks. Several benchmarks attempt to test different facets of agentic capabilities (see Ta-\nble 1 for an overview). Some benchmarks rely on a static dataset of human-collected data to evaluate\nagent performance, such as Mind2Web (Deng et al., 2023), WebLinx (L `u et al., 2024), PixelHelp (Li\net al., 2020), MoTiF (Burns et al., 2021), AiTW (Rawles et al., 2024b), and OmniAct (Kapoor et al.,\n2024). The downside of static benchmarks is that they are unable to capture the full range of trajec-\ntories which an agent might take to solve tasks as these can differ from human demonstration.\nInteractive benchmarks, in contrast, can provide reward signals at task completion that allow for\nmore dynamic and open-ended agent evaluation. These evaluations can occur in pre-defined environ-\nments or in open-ended fashion allowing the agent to make use of any tools available. GAIA (Mialon\net al., 2023), for instance, allows the agent to use the web and tools to answer complex questions,\nbut does not specify a closed environment in which the agent can actually operate. On acting in\nspecific environments, notable examples are MiniWoB++ (Shi et al., 2017), WebArena (Zhou et al.,\n2024), VisualWebArena (Koh et al., 2024), WorkArena (Drouin et al., 2024), MMInA (Zhang et al.,\n2024b), AndroidWorld (Rawles et al., 2024a), and OSWorld (Xie et al., 2024). Closest to our work is\nOSWorld, which provides a benchmark for agents to operate within OS environments, with a focus\non Linux. We extend and build upon this work to focus on Windows OS, as discussed in Section 3.2.\nEvaluation scalability. A key challenge which is often not discussed is the time it takes to com-\nplete an evaluation on a benchmark. Several of the works cited here have tasks which require about\n10-20 steps to complete; assuming each step takes 10-30 seconds to process using local models\ncombined with a cloud API call, this translates to an order of magnitude of one or more days to\nevaluate hundreds of tasks sequentially in a single thread. Most works do not address this challenge\ndirectly. While OSWorld allows simulations to be parallelized within a single machine, this does\nnot scale beyond a low, single digits number of agents within a typical development machine due to\nmemory and CPU constraints. We address this challenge by providing native cloud parallelization\nwhich can scale up to the number of benchmark tasks, as discussed in Section 3.3.\n3WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\n3 T HEWINDOWS AGENT ARENA ENVIRONMENT\n3.1 T ASK DEFINITION\nSimilar to Koh et al. (2024) and Xie et al. (2024), we formalize the agent behavior as a partially\nobservable Markov decision process (POMDP) (S,O,A, T,R)with state space S, observation\nspaceO(3.1.1), action space A(3.1.2), transition function T:S\u00d7 A \u2192 S, and reward func-\ntionR:S\u00d7 A \u2192 R. Given current observation ot\u2208 O , an agent generates executable action\nat\u2208 A, resulting in a new state st+1\u2208Sand a new partial observation ot+1\u2208 O . The process\nrepeats until the agent outputs a termination action ( DONE orFAIL ) or when the maximum number\nof steps is reached ( t > t max).\nThe reward function R:S\u00d7 A \u2192 [0,1]returns a non-zero value at the final step if the agent\nstate achieves the task objective, or if the agent accurately predicts failure for an infeasible task;\ndepending on the task, the reward either returns 1 if the outcome is binary (correct vs. incorrect)\nor a non-zero decimal value if task evaluation depends on similarity of agent outcome to the task\u2019s\nobjective. In all other scenarios, it returns 0. We note that the benchmark can potentially be modified\nto allow for intermediate non-zero rewards along the episode to account for partial completion via\nmore complex evaluation functions\u2014we leave this as possible future work.\n3.1.1 O BSERVATION SPACE\nThe observation space Orepresents the OS information available to the agent at each time step. In\nits most general form, this includes the task instruction, the clipboard content, metadata about the\ncurrent session (e.g. title of the current foreground window, titles for all other windows or browser\ntabs currently open), and a representation of the current screen. The screen can be represented in\nvarious formats such as a screenshot (RGB array), DOM (Document Object Model) tree, UI Au-\ntomation tree, or using pixel-based models to augment the raw data and create Set-of-Marks (Yang\net al., 2023) visual lists of interactive elements (e.g. buttons, images, text fields) as described in\nSection 4.1. For each task, we use a deterministic script to set the initial state of the system st=0\n(used to generate the initial observation ot=0) which might involve opening specific applications,\ndownloading files, or navigating to webpages. We provide more details in Appendix A.1.\n3.1.2 A CTION SPACE\nGroup Function\ncomputer.mousemove_id(id)\nmove_abs(x,y)\nsingle_click()\ndouble_click()\nright_click()\nscroll(direction)\ncomputer.\nkeyboardwrite(text)\npress(key)\ncomputer.\nclipboardcopy_text(text)\ncopy_image(image)\npaste()\ncomputer.os open_program(program)\ncomputer.\nwindow_managerswitch_to_\napplication(window)\nTable 2: Computer class actions in W IN-\nDOWS AGENT ARENA . Details in Appendix A.2.We provide two types of action spaces: free-form\npyautogui/python code execution, and function wrap-\npers through the Computer class. The Computer class\nis described in the prompt and allows the agent to use\nsimple and complex OS-related functions with relatively\nhigh precision or, in some cases, to interact directly with\nthe IDs of interactive elements on the screen rather using\nabsolute pixel coordinates (Table 2). We provide more\ndetails on the full action space in Appendix A.2.\n3.1.3 R EWARD EVALUATION BASED ON EXECUTION\nWe follow the guidelines from OSWorld Xie et al. (2024),\nand use post-processing scripts to evaluate task comple-\ntion. Depending on the task, this might involve reading\nthe state of system or app settings, checking the content\nof files, or dynamic functions that use the web to evalu-\nate a result. We provide examples in Table 3 and leave\nadditional details to Appendix A.3.\n3.2 T ASK CURATION FOR WINDOWS OS\nThe initial release of W INDOWS AGENT ARENA consists of 154 diverse tasks spanning applications\nwhich represent the typical user workloads within Windows OS: editing documents and spreadsheets\n4WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\nInitial State Task Instruction Evaluation Script (Simplified)\n\u201cCan you help me clean up my computer\nby getting rid of all the tracking things\nthat Amazon might have saved? I want to\nmake sure my browsing is private and\nthose sites don\u2019t remember me.\u201dcookie_data = get_cookie_data(env)\nrule = {\"type\":\"rule\",\n\"domains\": [\"amazon.com\"]}\nis_cookie_deleted(cookie_data, rule)\n\u201cI have been editing my document and\nsome words that needed to be rewritten\nare highlighted in yellow. As I fix those\nwords, please help me remove all\nhighlight. I want to make sure that there is\nno highlighted word.\u201dvm_file = get_file(env, \"file.doc\")\ngolden_file = get_file(drive,\n\"file.doc\")\ncheck_highlighted_words(vm_file,\ngolden_file)\n\u201cPlease help me modify the setting of VS\nCode to keep my cursor focused on the\ndebug console when debugging in VS\nCode, instead of automatically focusing\nback on the Editor.\u201d.vscode_data = get_vscode_config(env)\nrule = {\"expected\":\n{\"debug.focusEditorOnBreak\": false}\n}\ncheck_json_settings(vscode_data, rule)\nTable 3: Examples of evaluation scripts for task across data manipulation, web navigation, app settings. The task reward Ris calculated based\non the final state of the system after agent execution, allowing for evaluation of complex real-world tasks.\nDomain Activity Examples # Tasks %\nOfficeLibreoffice Writer (Editing,\nwriting) and LibreOffice Calc\n(spreadsheets, plotting, data\nmanipulation)43 28%\nWeb BrowsingOnline shopping & search,\ncustomizing app settings with\nEdge and Chrome30 19%\nWindows SystemFile Explorer (navigate, cus-\ntomize files), Windows Set-\ntings, customization24 16%\nCodingEditing code, customizing\nIDE, installing extensions\nwith VSCode24 16%\nMedia & VideoWatching videos in VLC, lis-\ntening to music, settings21 14%\nWindows UtilitiesMiscellaneous tasks with\nNotepad, Clock, and Paint12 8%\nTotal 154 100%\n5 10 15 20+\n# Steps to Complete0102030# T asksT ask Domain\nWeb Browsing\nCalc\nWriter\nVLC\nVSCode\nUtilities\nSystem\neasy medium hard\nDifficulty0204060# T asks\nFigure 2: Breakdown of available tasks across 11 programs/applications in W INDOWS AGENT ARENA by domain, difficulty, and number of\nsteps required to completion.\n(LibreOffice Calc /Writer), browsing the internet (Microsoft Edge, Google Chrome), Windows sys-\ntem tasks (File Explorer, Settings), coding (Visual Studio Code), watching videos (VLC Player), and\nutility functions (Notepad, Clock, Paint). We provide a breakdown and overview of tasks in Fig. 2.\nEach task is defined by its configuration file: a JSON containing a natural language instruction (e.g.,\n\u201cMake the line spacing of first two paragraphs into double line spacing\u201d ), the necessary steps to\nset up the initial task state (e.g. downloading the required document and opening it before the agent\ntakes over), the information that is retrieved to evaluate the task (e.g. downloading a gold answer\nfile to be compared to the agent\u2019s file at the end of the episode), as well as the style and manner of\nevaluation (e.g. python scripts which implement the file comparison function). We show a detailed\nexample of the task procedure in Appendix A.3.\n5WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\nWe adapted approximately two thirds of tasks in W INDOWS AGENT ARENA from the OSWorld (Xie\net al., 2024) Linux configurations, making the necessary modifications and revisions to make them\nexecutable on Windows. These changes include, but are not limited to, modifying OS-appropriate\nfile paths, converting Linux terminal commands and scripts into ones compatible with Windows\nPowerShell, establishing a reverse proxy to communicate with Edge/Chrome via ports on the host\nmachine, corrections and modifications to task initial conditions and evaluator functions, and revi-\nsions to task instructions. Nonetheless, the basic structure of each task JSON of task configuration\nfollows the same logical structure. The remaining third of tasks were created from scratch to cover\na wider range of applications and activities that are specific to Windows OS. The interested reader\ncan refer to Section 3.2 of Xie et al. (2024) for more details on task creation guidelines.\nHuman performance on W INDOWS AGENT ARENA results in 74.5% success rate with the highest\nsuccess rate coming from Windows Utilities tasks (91.7%) and the lowest on VLC Player tasks\n(42.8%). We offer more details and analysis on success rates, number of steps per task, and perceived\ndifficulty in Appendix A.5.\n3.3 D EPLOYMENT INFRASTRUCTURE\nWe designed the infrastructure behind W INDOWS AGENT ARENA to support flexible local execution\nas well as scalable and secure cloud parallelization via Azure. The core of our system is a Docker\ncontainer that hosts the Windows 11 virtual machine (VM). Within the container, we deploy a client\nprocess for task scheduling and configuration as well as the agent and the evaluation scripts. The\nVM is our main simulation environment: a Python Flask server acts as the bridge between the\ncontainer and the VM by receiving commands from the client processes, executing them within the\nVM, and sending observations and files back to the client. Fig. 3 illustrates the setup.\nDue to licensing restrictions, we cannot provide a pre-built Windows 11 snapshot; however, our\nopen-source instructions explain how users can download a trial image from Microsoft\u2019s servers and\nuse our scripts to automatically prepare it for the benchmark. The VM is deployed using QEMU\nand KVM with a process adapted from the dockur/windows Docker image3.\nWindows 11 \nsnapshot (30GB)\nLogsLocal host machine (native Ubuntu or Windows with WSL) \nCodeMounted volumes\nDocker container\nWindows VM\nAgent\nVM controller\nTask scheduling &\nconfigurationEvaluator\ncommandsobservationsfilesWindows 11 \nsnapshot (30GB)\nLogsAzure Machine Learning resource\nAzure storage \naccount blobs\nWorkers\n(a)Local deployment setup in WSL/Linux\nWindows 11 \nsnapshot (30GB)\nLogsLocal host machine (native Ubuntu or Windows with WSL) \nCodeMounted volumes\nDocker container\nWindows VM\nAgent\nVM controller\nTask scheduling &\nconfigurationEvaluator\ncommandsobservationsfilesWindows 11 \nsnapshot (30GB)\nLogsAzure Machine Learning resource\nAzure storage \naccount blobs\nWorkers\n (b)Cloud deployment setup at scale using Azure\nFigure 3: Comparison between local and cloud deployment of W INDOWS AGENT ARENA . The benchmark is designed for small scale local\ntesting and large scale cloud parallelization.\nLocal deployment details. Local runs can use Ubuntu or Windows Subsystem for Linux (WSL).\nTo ensure a seamless experience, we avoid constant rebuilding of the Docker image by mounting\nexternal volumes with the OS image and code from the user\u2019s host machine onto the container.\nCloud deployment details. We use Azure Machine Learning jobs to parallelize the benchmark\nevaluation using compute instances. The process is similar to the local setup, but the VMs are\ninstantiated and terminated with each experiment submission. We use the Azure Blob Store to\nmanage the Windows 11 snapshot and output logs while the code is pre-configured in the Docker\nimage. Tasks are distributed evenly among the workers, and the results are aggregated at the end of\nthe run. We provide additional information on VM types in Appendix A.7.\n3https://github.com/dockur/windows\n6WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\nThis is in contrast to OSWorld which follows a different approach to parallelization by instantiating\nmultiple VMWare VMs within a single local host. We believe this approach is less scalable since it\nis bounded by the number of VMs that can be instantiated on a single host, which would be in the\norder of low single digits for most consumer-grade machines. Moreover, the overhead of managing\nmultiple agents within the same host can prove impractical if agents further use resource-intensive\nlocal models (e.g., perception models for input parsing/processing, etc.). In contrast, our approach\ncan be scaled to a number of workers equal to the number of tasks in the benchmark, allowing for\nmuch faster evaluation. We provide more details on parallel evaluation run times in Appendix A.7.\nSecurity. By using isolated compute instances with docker containers, we are able to ensure that the\nagent and Windows VM control ports are not openly exposed to the internet so that all communica-\ntion happens locally with minimal risk arising from data or code leakage, etc.\n4 B ASELINE RESULTS AND ANALYSIS\nIn this section, we present the implementation details and results of our agent, Navi , on W IN-\nDOWS AGENT ARENA along with an analysis of its strengths and limitations.\n4.1Navi :AN AGENT FOR WINDOWS NAVIGATION\nWe develop a new multimodal agent, Navi , to operate autonomously within the Windows environ-\nment. We use chain-of-thought prompting (Wei et al., 2022) to instruct the agent to reason about\nthe current state of the computer, its own past actions, and decide on the most appropriate next ac-\ntion (full prompts found in Appendix D). All variations of our agent receive as input the title of the\ncurrent foreground window, titles for all other windows or browser tabs currently open, and a rep-\nresentation of the current screen. We consider several methods to process the screen representation\nfor the agent as input and create Set-of-Marks (SoMs):\n\u2022UIA tree parsing : extracts the visible elements from the Windows UI Automation tree;\n\u2022DOM tree parsing : extracts the visible elements from the DOM tree (browser only);\n\u2022OCR : proprietary4and open models ( Tesseract (Smith, 2007));\n\u2022Icon and image detection : proprietary and open models (Grounding DINO (Liu et al., 2023b));\n\u2022OmniParser : proprietary model trained on multi-element detection in screens (Lu et al., 2024).\nIt detects text, icons, and images, and provides icon captioning in text format.\nFig. 4 shows examples of SoMs generated using our proprietary models. During deployment, we\ntest configurations that combine pixel-based segmentation together with UIA tree information for\nan even more detailed representation of the screen. Our code repository5contains the open-sourced\nversion of the agent, Navi open, which uses only open-sourced models for SoM generation.\nThe full prompt to the agent also contains the task instruction, a description of the action space, a\nhistory of the agent\u2019s previous actions, the computer clipboard content, and a memory block where\nthe agent can store variables or thoughts in text format. We then pass in the annotated image with\nSoMs and an image of the previous state as a screenshot. As outputs, we ask the agent to first answer\na few questions related to screen understanding and long-term task planning before outputting the\npython code that represents the next action be taken. We provide more details in Appendix D.1.\n4https://azure.microsoft.com/en-us/products/ai-services/ai-vision\n5https://WindowsAgentArena.github.io\nFigure 4: Set-of-Marks based on proprietary pixel-based local models. OCR (blue), icon detection (green), and image detection (red).\n7WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\nTable 4: Agent success rates on W INDOWS AGENT ARENA grouped by task categories and sorted by model and input modality types.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Inputs \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nUIA tree OCR Icon det. Image det.Model OfficeWeb\nBrowserWindows\nSystemCodingMedia &\nVideoWindows\nUtilsTotal\nPhi3-V 0% 0% 4.2% 4.3% 0.0% 0.0% 1.3%\nGPT-4o-mini 2.3% 6.7% 12.5% 8.3% 14.6% 0.0% 7.2%\nGPT-4o 0.0% 0.0% 29.2% 0.0% 5.0% 0.0% 5.2%\u2717Pytesseract + DOM Grounding DINO\nGPT-4V-1106 0.0% 10.3% 21.3% 12.5% 9.8% 0.0% 8.6%\nPhi3-V 0% 0% 4.2% 4.3% 5.0% 0.0% 2.0%\nGPT-4o-mini 0.0% 3.3% 8.3% 0.0% 5.0% 0.0% 2.6%\nGPT-4o 0.0% 10.0% 29.2% 8.3% 14.6% 0.0% 9.8%\u2713Pytesseract+ DOM Grounding DINO\nGPT-4V-1106 0.0% 13.3% 25.0% 13.0% 28.9% 8.3% 13.1%\nPhi3-V 0% 6.7% 8.3% 0% 4.8% 0.0% 3.2%\nGPT-4o-mini 0.0% 3.3% 12.5% 4.5% 14.6% 0.0% 5.3%\nGPT-4o 2.3% 17.3% 20.8% 4.5% 9.8% 0.0% 9.3%\u2717OneOCR Proprietary models\nGPT-4V-1106 2.3% 13.7% 16.7% 13.6% 19.3% 8.3% 11.3%\nPhi3-V 0% 6.7% 8.3% 4.5% 5.0% 0.0% 4.0%\nGPT-4o-mini 0.0% 7.3% 20.8% 8.3% 9.8% 0.0% 7.3%\nGPT-4o 0.0% 20.0% 29.2% 9.1% 25.3% 0.0% 13.3%\u2713OneOCR Proprietary models\nGPT-4V-1106 0.0% 26.3% 16.7% 17.4% 19.3% 0.0% 13.1%\nPhi3-V 0.0% 0.0% 8.6% 0.0% 5.0% 0.0% 2.0%\nGPT-4o-mini 0.0% 0.0% 12.5% 0.0% 5.3% 0.0% 2.7%\nGPT-4o 0.0% 6.7% 30.3% 4.3% 15.3% 8.3% 9.4%\u2717Omniparser\nGPT-4V-1106 2.3% 23.6% 20.8% 8.3% 20.0% 0.0% 12.5%\nPhi3-V 0.0% 6.9% 8.3% 0.0% 6.2% 0.0% 3.5%\nGPT-4o-mini 0.0% 14.9% 8.3% 0.0% 0.0% 0.0% 4.2%\nGPT-4o 0.0% 13.7% 29.2% 0.0% 10.3% 0.0% 8.6%\u2713Omniparser\nGPT-4V-1106 0.0% 27.3% 33.3% 27.3% 30.3% 8.3% 19.5%\nHuman performance 75.8% 76.7% 83.3% 68.4% 42.8% 91.7% 74.5%\n4.2 W INDOWS AGENT ARENA BASELINE RESULTS\nWe summarize the main baseline results in Table 4 where we compare variants Navi using dif-\nferent SoM and reasoning models. Fig. 5 displays an example of a successful task in W IN-\nDOWS AGENT ARENA using proprietary models. Additional examples can be found in Appendix B.\nWe highlight some key observations and insights below:\nGeneralist zero-shot VLM agents are still far from human performance : The best of our agents\n(UIA combined with Omniparser and GPT-4V-1106) achieves a success rate of 19.5%, which is low\nin absolute terms when compared to human performance at 74.5%. This discrepancy is not uni-\nform across categories as the agent performs significantly better in tasks with clear text-dominant\ninterfaces (Web Browser, Windows System) and worse in tasks that rely heavily on keyboard short-\ncuts and icons (Office, Windows Utils). However, Navi \u2019s performance is comparable to those from\nsimilar benchmarks like OSWorld and AndroidWorld which also test zero-shot generalist agents.\nPrecise Set-of-Marks are crucial for agent performance : The quality of visual prompting tech-\nniques has a significant impact on the agent\u2019s performance. As shown in Fig. 6a, we commonly\nfind that imprecise SoM bounding box boundaries lead to incorrect element selection. As seen in\nTable 4, adding high-quality UIA markers in addition to pixel-based element detectors boosts per-\nformance by 57% for Omniparser, 52% for open-sourced models, and 15% for proprietary pixel\nmodels when using GPT-4V-1106. Using UIA markers, however, can take from a few seconds up to\nseveral minutes to be queried depending on the complexity of the screen.\nVisual-language mis-alignment is a common cause of failures : We find that a common cause\nof agent failures spanned from the agent\u2019s inability to correctly align its text output with its visual\nunderstanding of the screen. In Fig. 6b we show an example where the agent correctly outputs the\naction textual description ( move to the red color ), but fails to select the visual ID (#59 corresponds to\nthe yellow palette). This behavior is likely caused by the mismatch between the training data distri-\nbution of VLMs and OS tasks, which typically contain hundreds of small-scale markers. We believe\nthat a main reason behind Omniparser\u2019s higher performance lies in its icon captioning capabilities,\nwhich help the agent directly link visual context IDs with textual descriptions.\nPerformance gap between base models : We find a large performance gap between Phi3-V , GPT-\n4o-mini, GPT-4o, and GPT-4V-1106 with the latter achieving better performance in most categories,\nmore than double of GPT-4o\u2019s success rate in the case of UIA+Omniparser. As expected, smaller\nmodels like Phi3-V and GPT-4o-mini perform worse than larger models. We note that Phi3-V tends\nto hallucinate when planning multiple steps ahead or presented with long input contexts.\n8WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\nAzure Linux compute instancesTask: Computer, can you turn the webpage I'm looking at into a PDF file and put it on my main screen, you know, the Desktop? Set desti nation as 'Save as PDF' instead of 'Microsoft Print to PDF' \nStep 1: computer.keyboard.press (\"ctrl+p \") Step 2: # Move the focus to the 'Destination' dropdown\n    computer.keyboard.press (\"tab\")\n    # Attempt to navigate the dropdown with the keyboard \n   computer.keyboard.press (\"down\")\nStep 4: # Move to \"Desktop\" (pinned) to change the location \n    computer.mouse.move_id(id=28)\n             computer.mouse.single_click () \nStep 3: # Move to the \"Save\" button\n    computer.mouse.move_id(id=54)\n    # Click to save the webpage as a PDF          \n        computer.mouse.single_click ()\nStep 5: # Move cursor to the \"Save\" button                \n    computer.mouse.move_id(id =48)\n    # Click to save the webpage as a PDF to the Desktop\n    computer.mouse.single_click ()Result: Task successful, reward 1.0\nFigure 5: Example of a successful episode in W INDOWS AGENT ARENA using a combination of UIA and proprietary models for screen parsing.\nThe agent interacts with mouse and keyboard functions to save a webpage PDF in the desktop.\nStep 3:  computer.mouse.move_id(id=5) # Move to the 'Format' menu option   \n     computer.mouse.single_click() # Open the 'Format' menu \nTask: Convert all uppercase text to lowercase\nStep 7:  computer.mouse.move_id(id=59) # Move to the red color on the palette   \n     computer.mouse.single_click() # Click to select the red color Task: Open Paint and draw a red circle\n(a)Error in SoM bounding box marking leads to imprecise local-\nization, as the OCR grouped distinct elements together.\nStep 3:  computer.mouse.move_id(id=5) # Move to the 'Format' menu option   \n     computer.mouse.single_click() # Open the 'Format' menu \nTask: Convert all uppercase text to lowercase\nStep 7:  computer.mouse.move_id(id=59) # Move to the red color on the palette   \n     computer.mouse.single_click() # Click to select the red color Task: Open Paint and draw a red circle\n(b)Error in visual-language alignment by the model, where text\nexplains the correct action but chosen visual ID is incorrect.\nFigure 6: Common failure cases for the Windows OS agent.\n4.3 A DDITIONAL Navi RESULTS FOR THE MIND2W EB BENCHMARK\nIn Table 5, we evaluate Navi on the Mind2Web benchmark (Deng et al., 2023) in order to assess our\nagent\u2019s performance and robustness in a separate setting. Navi achieves state-of-the-art performance\nwhen compared with the original SeeAct agent Zheng et al. (2024a). We find that a combination of\nimage and DOM SoM prompting provides the best agent performance, similarly to the results from\nWINDOWS AGENT ARENA combining UIA tree with pixel-based markers. We provide more details\non evaluation setup in Appendix C.\n9WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\nModality Model Inputs Averaged Metrics\nElement Accuracy Operation F1 Step Success Rate\nText-Only GPT-4o DOM 26.3% 69.1% 24.6%\nMultimodal GPT-4o SoM (DOM + Pixel) 37.4% 79.2% 35.1%\nSoM (Pixel) 40.6% 84.5% 38.3%\nSoM (DOM) 38.7% 81.1% 36.9%\nImage+SoM (DOM) 42.8% 84.3% 40.7%\nImage+SoM (DOM + Pixel) 47.3% 85.8% 45.2%\nSeeAct GPT-4V Image+SoM(DOM) 44.3% 71.8% 38.3%\nTable 5: Mind2Web evaluation results across various modalities and input combinations.\n5 D ISCUSSION\nBased on the results presented in this paper, we would like to pose different discussion points related\nto the development of generalist OS agents:\nFull autonomy vs. human-in-the-loop. Though our work focuses on autonomous agents, we\nbelieve that considering a human-in-the-loop approach could benefit performance and effectiveness\nin helping the end user. At a cost, agents could choose a question action to actively seek user input\nfor clarification, additional info, or direct human intervention. However, this approach would be\nharder to evaluate and benchmark, likely needing human involvement in the evaluation process.\nGeneralist vs. specialized agents. Most modern agents are based on large models with generalist\nknowledge. However, we believe that a fruitful research direction would be to treat an OS agent as\nanagent of agents which uses specialized fine-tuned sub-systems for specific domains.\nImitation vs. reinforcement learning. A key challenge in agent development is the lack of ground-\ntruth action data for model training/tuning, resembling difficulties found in robotics research. Rein-\nforcement learning could be a potential solution, by leveraging data from imperfect actions, so long\nas the objective functions for rewards are well-defined (e.g. task completion in our benchmark). One\ndirection is to use W INDOWS AGENT ARENA to generate data at scale for agent RL training.\nWhat is the \u201cbest\u201d action space? While we focus on fully autonomous agents with free-form\nactions, we also see potential for agents that can use a stricter predefined action function or skill\nlibrary. This can offer higher execution precision as errors would instead stem mostly from the\nagent\u2019s planning and decision-making in stitching together action primitives.\nSafety and security. In the AI agent design process, it\u2019s crucial to adhere to ethical guidelines\nand principles of responsible AI use. Users should have the capability to understand, direct, diag-\nnose, and if needed, override AI actions. We are committed to the continued development of AI\ntechnologies that respect user privacy, ensure fairness, and contribute positively to society.\n6 C ONCLUSION\nWe introduced W INDOWS AGENT ARENA , an open-source and reproducible environment for evalu-\nating multi-modal agents within a real Windows OS. Our platform provides 154 diverse tasks that re-\nquire abilities including, but not limited to, planning, screen understanding, and tool usage, offering\na realistic framework for the development and testing of autonomous computer agents. Our bench-\nmark is scalable and can be parallelized in Azure, significantly reducing the evaluation time for full\nbenchmark runs. We also introduced a multi-modal agent, Navi , and demonstrated its capabilities\nacross different configurations. W INDOWS AGENT ARENA \u2019s extensive quantitative and qualitative\nanalysis of agent performance provides valuable insights for future research in agent development.\nBy making our code, benchmark, and baseline models available, we aim to facilitate further research\nand accelerate the development of more efficient and capable computer control agents.\n10WINDOWS AGENT ARENA : Evaluating Multi-Modal OS Agents at Scale\nACKNOWLEDGMENTS\nWe thank the Windows Engineering teams led by Eric Carter, Franz Lenhardt, Steve Lim,\nParthasarathy Sundararajan, and Shyamsunder Gopale for contributing with new tasks for the bench-\nmark. We also thank Mohsen Fayyaz, Ebey Abraham, Pashmina Cameron, Justin Cao and Mo-\nhammed Helal for their contributions in early discussions and prototypes of the project.\nAUTHOR CONTRIBUTIONS\nRogerio Bonatti developed the project concept, wrote the initial agent and screen parsing proto-\ntypes, heavily contributed to the different project threads (deployment, tasks, results), and led the\nproject\u2019s technical and organizational development.\nDan Zhao was responsible for the benchmark\u2019s task design, task creation and conversion, task\ncuration and robustness, agent/human task testing and evaluation, and environment testing.\nFrancesco Bonacci designed the original environment enabling agents and their Windows hosts to\nrun on containers, allowing for scalable deployment on Azure ML. He developed the automation to\nready the environment for benchmarking, and co-led the open-source release.\nDillon Dupont developed the original DOM and accessibility tree parsing modules, set up the\nMind2Web benchmark for the agent, and contributed to task evaluation.\nSara Abdali developed proprietary screen parsing modules (OCR, icon detector, object detector),\ncontributed to agent improvements and benchmarking, LLM cloud calls, prompt constructor and\ntask evaluation.\nYinheng Li was the lead for experiments with open source language models for core agent.\nJustin Wagle was a senior advisor to the project.\nKazuhito Koishida was a senior advisor to the project.\nArthur Bucker developed and tested VM-client infrastructure, contributed to UIA tree parsing,\nagent improvements, and logging pipeline.\nLawrence Jang developed and tested VM-client infrastructure, contributed to task creation and\nideation behind the agent history logging pipeline.\nZack Hui Contributed to the development and evaluation of the"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**Summary of \"Learning incomplete factorization preconditioners for GMRES\"**\n\n1. **Abstract**\n   - The paper proposes a data-driven method for generating incomplete LU factorizations for large-scale sparse matrices, which are then used as preconditioners in the GMRES (Generalized Minimal Residual) algorithm. It introduces a graph neural network (GNN)-based approach that predicts an approximate factorization from data, tailored to specific problem distributions. The method aims to reduce sensitivity to hyper-parameters and address numerical instabilities associated with traditional incomplete LU factorization methods.\n\n2. **Introduction**\n   - **Context and Motivation:** The GMRES algorithm efficiently solves large-scale sparse linear systems \\(Ax = b\\), but its performance heavily relies on suitable preconditioners like the incomplete LU factorization. Classical methods are sensitive to hyper-parameters and subject to numerical issues.\n   - **Problem Statement:** Data-driven methods might provide better preconditioner design by leveraging problem-specific data, but existing approaches focus mainly on symmetric, positive definite matrices. This paper extends these methods to general matrices using GMRES.\n   - **Literature Review:** Prior work includes learning preconditioners within the conjugate gradient method and learning approximate inverses, but these often cater to specific matrix properties or require retraining for new problems. Methods like learning weighted inverses or using auto-encoders offer promising directions but are not tailored to GMRES and general matrices.\n   - **Contributions:** The paper introduces a GNN model to predict incomplete LU factors, theoretically and empirically analyzes different loss functions to optimize preconditioners, and provides scalability and performance enhancements over classical methods.\n\n3. **Related Work**\n   - **Incomplete Factorization for CG:** H\u00a8ausner et al. and Li et al. introduced machine learning models to learn incomplete factorizations for the conjugate gradient method, focusing on symmetric, positive definite matrices.\n   - **Data-Driven and Hybrid Methods:** Trifonov et al. combined classical algorithms with data-driven corrections, emphasizing incomplete Cholesky factorization. Chen explored non-linear neural networks to estimate the inverse of matrices directly.\n   - **Sparse Approximate Inverse Preconditioners:** Bankestad et al. and Li et al. suggested learning sparsity patterns or using generative models, but these methods differ from factorized preconditioners.\n   - **Advancements Over Prior Work:** This work expands the applicability of data-driven methods to GMRES by avoiding the assumption of matrix symmetry or positive definiteness.\n\n4. **Method**\n   - **Mathematical Foundations and GMRES Algorithm:** Essential background on the GMRES algorithm, focusing on its iterative updates within the Krylov subspace, and the importance of preconditioners for improving system properties.\n   - **Graph Neural Networks for Preconditioners:** Treating matrices as Coates graphs, the nodes represent rows and columns, and the edges carry non-zero matrix elements. The GNN\u2019s message-passing layer updates node and edge features iteratively to predict the incomplete LU factors \\(L\\) and \\(U\\).\n   - **Network Architecture and Constraints:** The GNN ensures the factors are triangular and introduces mechanisms to handle zero diagonal elements, maintaining invertibility through novel activation functions.\n   - **Loss Functions:** Various loss functions are analyzed and proposed:\n     - \\(L_{max}\\) minimizes the largest singular value, approximated using the Frobenius norm.\n     - \\(L_{min}\\) focuses on the smallest singular value, requiring solutions to ill-conditioned systems.\n     - \\(\\hat{L}_{min}\\) approximates \\(L_{min}\\) using pre-solved training data to avoid real-time computation.\n     - \\(L_{com}\\) combines aspects of minimizing both ends of the spectrum, providing balance in performance.\n\n5. **Experimental Setup**\n   - **Datasets:** Synthetic datasets generated from Poisson equation PDEs, with non-symmetric and non-positive definite properties, perturbed with Gaussian noise for variability.\n   - **Hardware and Software:** Details on computational resources, specific frameworks (Python, PyTorch Geometric), and tools for implementation.\n   - **Evaluation Metrics:** Comprehensive set of metrics: singular values, condition number, Frobenius norm error, computational time, and iteration count for GMRES convergence.\n   - **Baselines and Comparisons:** Baselines include unpreconditioned GMRES, Jacobi preconditioner, and classical ILU(0) for comparison.\n\n6. **Results and Analysis**\n   - **Empirical Findings:** Detailed results for each tested method:\n       - Unpreconditioned GMRES: High iteration count and time.\n       - Jacobi and ILU(0): ILU(0) performs better in reducing iterations but is sensitive to initial matrix conditions.\n       - GNN Models: Effective in decreasing iterations, with specific gains noted for models optimized with \\(L_{min}\\) and \\(\\hat{L}_{min}\\).\n   - **Tables and Graphs:** Tabulated performance of different methods, spectral distributions of preconditioned systems, and corresponding time analyses.\n   - **Ablation Studies:** Exploring the effects of different loss functions on preconditioner performance and GMRES efficiency.\n\n7. **Discussion**\n   - **Interpretation of Results:** The combined approach leveraging data-driven GNN models and classical methods shows promise in improving preconditioner quality and GMRES convergence speed. The paper highlights the balance between training complexity and performance gains.\n   - **Limitations:** The assumption of a similar class distribution of linear systems, computational overhead of training, and sensitivity to hyper-parameters in GNN models.\n   - **Implications for Future Work:** Potential of dynamically learning sparsity patterns, designing loss functions that consider the entire spectrum, and combining data-driven insights with theoretical guarantees from classical methods.\n\n8. **Conclusion**\n   - **Key Contributions:** Proposing a GNN-based method for generating incomplete LU preconditioners tailored for GMRES, theoretical analysis of loss functions on spectral properties, and empirical validation on large-scale sparse matrices.\n   - **Future Directions:** Further exploration into adaptive learning of sparsity patterns, integrating richer spectral information in loss functions, and extending the approach to broader classes of linear equations beyond current formulations.\n\nBy providing this comprehensive summary, any researcher can replicate the study by understanding the detailed methodological innovations, experimental designs, and practical considerations involved in creating learned preconditioners for sparse linear systems."}, {"role": "assistant", "content": "Learning incomplete factorization preconditioners for GMRES\nPaul H\u00a8 ausner\u22171, Aleix Nieto Juscafresa\u22171, and Jens Sj\u00a8 olund1\n1Department of Information Technology, Uppsala University, Sweden\n{paul.hausner, aleix.nieto-juscafresa, jens.sjolund }@it.uu.se\nAbstract\nIn this paper, we develop a data-driven approach\nto generate incomplete LU factorizations of large-\nscale sparse matrices. The learned approximate\nfactorization is utilized as a preconditioner for the\ncorresponding linear equation system in the GM-\nRES method. Incomplete factorization methods\nare one of the most commonly applied algebraic\npreconditioners for sparse linear equation systems\nand are able to speed up the convergence of Krylov\nsubspace methods. However, they are sensitive to\nhyper-parameters and might suffer from numerical\nbreakdown or lead to slow convergence when not\nproperly applied. We replace the typically hand-\nengineered algorithms with a graph neural network-\nbased approach that is trained against data to pre-\ndict an approximate factorization. This allows us to\nlearn preconditioners tailored for a specific problem\ndistribution. We analyze and empirically evaluate\ndifferent loss functions to train the learned precondi-\ntioners and show their effectiveness to decrease the\nnumber of GMRES iterations and improve the spec-\ntral properties on our synthetic dataset. The code\nis available at https://github.com/paulhausner/\nneural-incomplete-factorization .\n1 Introduction\nThe GMRES algorithm is one of the most popular it-\nerative methods to solve large-scale and sparse linear\nequation systems of the form Ax=b. Throughout\nthe paper, we assume the system matrix Ato be real-\nvalued and full rank. Therefore, the unique solution\nA\u22121bexists. However, inverting the matrix directly,\nor equivalently computing a full matrix factoriza-\ntion, scales computationally poorly and suffers from\nnumerical instabilities. Instead, iterative Krylov\nsubspace methods (such as GMRES) which refine\nan approximation of the solution in each step are\nthe most common approach to solving problems of\nthis form. The convergence of these methods highly\ndepends on the conditioning and singular value dis-\ntribution of the system matrix. Therefore, the choice\nof appropriate preconditioner \u2013 designed to improve\nthe system properties \u2013 is critical to obtain a fast\nand accurate solution for the equation system [1].\n\u2217Joint first author.One of the most common choices of precondition-\ners is the incomplete LU factorization (ILU). As the\nname suggests, the matrix Ais factorized but not\nall elements of the factorization are computed to\ndecrease the storage and computational time [2, 3].\nIn this work, we replace the classical computation of\nthe LU factors with a graph neural network (GNN).\nWe train the GNN to predict the corresponding in-\ncomplete factors of the matrix directly by training\nagainst data. The main contributions of our paper\nare the following:\n\u2022We design a GNN model and train it to output\na non-singular incomplete LU factorization.\n\u2022We theoretically analyze existing loss func-\ntions from the literature and showcase their\nconnection to large and small singular values.\n\u2022Based on these insights, we derive a novel loss\nfunction to train learned preconditioners that\ncombines the benefits of previous approaches.\nIn numerical experiments, we showcase the effective-\nness of our model to reduce GMRES iterations in\ncombination with the different loss functions and val-\nidate the theoretical results on a synthetic dataset.\nRelated work Most similar to our work, H\u00a8 ausner\net al. [4] and Li et al. [5] both learn an incomplete\nfactorization for the conjugate gradient method. Tri-\nfonov et al. [6] extend this to combine data-driven\nand classical algorithms by correcting the output of\nthe incomplete Cholesky factorization with a learned\ncomponent. However, all of these methods assume\nthe matrix to be symmetric and positive definite and\nutilize the incomplete Cholesky factorization as the\nunderlying preconditioning technique. In contrast,\nrather focus on the more general incomplete LU\nfactorization, which does not require positive defi-\nniteness or symmetry, within the GMRES algorithm.\nIn previous work, Chen [7] instead proposes di-\nrectly estimating the inverse of the matrix using a\nnon-linear neural network as a preconditioner, requir-\ning the use of the flexible GMRES method, which\nhas a more complex convergence behavior. However,\nthe proposed method requires retraining for each\nnew problem. In contrast, we propose leveraging the\nsimilarity within a class of linear equation systems\nto first train the model offline and then generate\npreconditioners for new problems with a negligible\ncomputational overhead during inference time.\n1arXiv:2409.08262v1  [cs.LG]  12 Sep 2024For the class of sparse approximate inverse pre-\nconditioner, B\u02da ankestad et al. [8] learn the sparsity\npattern of the approximate inverse matrix. Li et\nal. [9] instead propose to learn auto-encoder-based\ngenerative models to generate preconditioners. In\ncontrast, this paper only focuses on learning factor-\nized preconditioners that allow easy inversion.\n2 Background\nWe first briefly introduce the GMRES algorithm and\nfocus in particular on the design choice of precondi-\ntioners. Then, we describe graph neural networks\nwhich we use to parameterize the mapping to learn\ndata-driven preconditioners.\n2.1 GMRES algorithm\nWe focus on the GMRES algorithm [10], a popu-\nlar and widely adopted iterative Krylov subspace\nmethod to solve general linear equation systems of\nthe form Ax=bwhere Ais an\u00d7nnon-singular ma-\ntrix denoted by the set GL( n,R). The core idea of\nKrylov subspace methods is to iteratively refine the\nsolution to the problem starting from an initial guess\nx0. At each iteration, the approximate solution is\nupdated by minimizing the Euclidean norm of the\nresidual within the corresponding Krylov subspace:\nxk= arg min\nx\u2208x0+Kk(A,r0)\u2225b\u2212Ax\u22252, (1)\nwhere the k-th Krylov subspace Kk(A,r0) =\nspan{r0,Ar0,A2r0. . . ,Ak\u22121r0}ofAis generated\nby the initial residual r0=b\u2212Ax0[1]. The new ap-\nproximation xkbuilds on the previous solution xk\u22121\nby efficiently computing it within a progressively\nlarger Krylov subspace by constructing the solu-\ntion in a progressively larger subspace that captures\nmore of the action of the matrix Aon the residual.\nThis expanding subspace offers more directions to\napproximate the solution, improving the condition-\ning and accuracy of the minimization problem over\niterations. The efficiency arises because xkis found\nby solving a smaller, reduced system in the subspace\nrather than the original larger system. The GMRES\nalgorithm is guaranteed to find the exact solution\nto the linear equation system in at most nsteps.\nEach iteration consists of two main steps. In\nthe first step of the algorithm, an orthonormal ba-\nsis for the subspace Kkis computed. This can be\nachieved, for example, using Arnoldi\u2019s algorithm\nwhich is based on the Gram-Schmidt procedure. In\nthe second step, an unconstrained least squares prob-\nlem is solved via the QR factorization of the subspace\nbasis obtained in the first step. Based on this, the so-\nlution of Equation (1) can be constructed efficiently.\nThe detailed algorithm is shown in Appendix A.Preconditioning Arguably, the most important\ndesign choice for any Krylov subspace method is the\nchoice of preconditioner. The goal of preconditioning\nis to replace the original linear equation system\nwith a new preconditioned system that exhibits a\nbetter clustering of singular values, which in turn\nleads to faster convergence of the iterative scheme\n[11]. For a non-singular and easy-to-invert matrix\nP, the linear equation system AP\u22121y=bis solved\ninstead of the original problem. The solution to\nthe original problem is then given by x=P\u22121y.1\nConstructing a preconditioner involves trading off\nthe time required to compute the preconditioner\nitself and the resulting speed up in the iterative\nscheme [2]. Since the original system Ais typically\nsparse, we restrict the preconditioning matrix Pto\nbe subject to sparsity constraints as well.\nAlgebraic preconditioners do not assume any ad-\nditional problem information and compute the pre-\nconditioning matrix solely based on the structure\nand values of the matrix Ain contrast to methods\ntaking into account the underlying problem domain.\nIn the simplest case, the Jacobi preconditioner\napproximates Pusing a diagonal matrix. More ad-\nvanced methods such as incomplete factorizations or\nthe Gauss-Seidel method compute approximate fac-\ntorizations of the original system Awhich implicitly\ngive rise to preconditioners as they allow efficient in-\nversions [2, 12]. Most commonly, the approximation\nutilizes triangular factorizations since efficient inver-\nsion in at most O(n2) operations can be achieved and\nsparsity can be exploited [13]. However, It is also pos-\nsible to approximate the matrix P\u22121directly [14].\n2.2 Graph neural networks\nGraph neural networks (GNNs) are a recently pop-\nularized family of neural network architectures de-\nsigned to process data represented on unstructured\ngrids or graphs [15]. A graph is a tuple G= (V,E)\nconsisting of vertices (or nodes) Vand a bivariate\nrelation of edges E \u2286 V \u00d7 V . With slight abuse of\nnotation, we refer to the edge features of the edge\ne= (i, j)\u2208 Easeij\u2208Rmand node features of node\ni\u2208 Vasni\u2208Rk. The message-passing framework\nupdates the initial edge and node features \u2013 repre-\nsenting the input data \u2013 in each layer las follows:\nel+1\nij=\u03c8\u03b8(el\nij,nl\ni,nl\nj) (2a)\nml+1\ni=M\nj\u2208N(i)el+1\nji (2b)\nnl+1\ni=\u03d5\u03b8(nl\ni, ml+1\ni). (2c)\nHere, the functions \u03c8and\u03d5, which update the node\nand edge features respectively, are parameterized by\n1Here, right preconditioning is used, but other formula-\ntions use left- or split-preconditioning instead often leading\nto similar but not equivalent results.\n2neural networks, and their respective parameters \u03b8\nare learned during the model training. Note that\nall feature vectors within a single layer must be of\nthe same size, although the embedding size can vary\nbetween different layers. Equation (2b) shows the\naggregation of the adjacent edges features for each\nvertex i\u2208 V called the neighborhood N(i). Any\npermutation invariant function \u2295, such as sum and\nmean, can be utilized for this aggregation step [16].\n3 Method\nThroughout this paper, we are interested in\nsolving many linear equation systems that share an\nunderlying structure. Instead of having direct access\nto a probability distribution over problems, we\nobtain a dataset Dwith a finite number of samples\nA1,A2, . . .A|D|. As previously stated, we assume\neachAito be square, non-singular and sparse.\n3.1 Network architecture\nThe goal of this paper is to learn a mapping f\u03b8,\nparameterized by a graph neural network, that\ntakes a non-singular matrix A\u2208GL(n,R) as input\nand outputs the corresponding preconditioner P\nfor the equation system. In order to ensure that the\npreconditioning system is easily invertible \u2013 such\nthat we can apply it within the GMRES algorithm\n\u2013 we learn the LU factorization of the preconditioner\nPinstead, by mapping Ato two real matrices L\nandU. The output matrices are lower and upper\ntriangular, respectively, with non-zero diagonal\nentries to ensure their invertibility as discussed\nlater. For notational simplicity, we do not explicitly\nmention the dependence of the factors on the orig-\ninal matrix Aand the neural network parameters\n\u03b8when the relationship is clear from the context.\nSparsity Further, the two output factors, Land\nU, are subject to sparsity constraints. Similar to\nincomplete LU factorization methods without fill-ins\nILU(0), we enforce the same sparsity patterns to\nthe elements of LandUas the original matrix A.\nThis constraint is directly encoded in the network\narchitecture as we discuss next.\nNetwork parameterization To parameterize the\nfunction f\u03b8that outputs the preconditioner, we ex-\nploit the strong connection of matrices and linear\nalgebra with graph neural networks [17, 18].\nWe treat the system Aas the adjacency matrix\nof a corresponding graph that we use directly within\nthe message-passing framework. This transforma-\ntion is known in the literature as Coates graph rep-\nresentation is depicted in Figure 1 [19]. Similar to\nH\u00a8 ausner et al. [4] and Tang et al. [20], we use node\nfeatures describing the structural properties of the\ncorresponding rows and columns of the matrix. We\uf8ee\n\uf8f02.40 2 .2\n0.53.20\n2.1 1.70\uf8f9\n\uf8fb1 3\n20.52.2\n2.4\n1.7\n3.20\n2.1\nFigure 1. Non-symmetric matrix A(left) and the\ncorresponding sparse Coates graph representation (right).\nAdditionally to the non-zero elements in the matrix, the\ngraph has been modified by adding edges for all missing\ndiagonal elements, corresponding to self-loops.\nuse the non-zero elements of Aas the corresponding\nedge features of the graph.\nCompared to previous approaches which only out-\nput a single lower triangular factor, our model out-\nputs two separate matrices LandU. H\u00a8 ausner et\nal. [4] add positional encodings about the output\nmatrix by explicitly changing the Coates graph rep-\nresentation. This allows them to output the lower\ntriangular factor as the message passing is only ex-\necuted over this set of edges. In contrast, we add\npositional encodings to the input data by adding\nan additional edge feature to each directed edge in\nthe graph. This indicates whether the final output\nembedding of the corresponding edge belongs to the\nupper- or lower-triangular part ( \u00b11 respectively)\nbut does not change the underlying graph structure\nused for message passing.\nInvertibility We require the predicted factors L\nandUof the preconditioner obtained from the for-\nward pass through the model to be invertible in\norder to obtain a valid preconditioner for the GM-\nRES method during inference. Therefore, we need\nto enforce non-zero elements on each diagonal.\nSince the input matrix Ais not guaranteed to\nhave non-zero diagonal elements, we add the corre-\nsponding edges to the graph if necessary before the\nmessage passing. This is visualized for the matrix\nelement A33in Figure 1.\nHowever, it is still possible that the output of the\nGNN results in a singular matrix as the final edge\nrepresentations, obtained from message passing, can\nresult in zero diagonal elements. Through additional\nmeasures, we avoid this in both factors and enforce\nnon-singularity. Inspired by the classical LU factor-\nization, we enforce unit diagonal elements for the U\nfactor. For the diagonal elements in the lower trian-\ngular factor Lwe are using the activation function\n\u03b6(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3x, if|x|> \u03f5,\n\u03f5, if 0\u2264x\u2264\u03f5,\n\u2212\u03f5,if\u2212\u03f5\u2264x <0,(3)\nfor a chosen small value \u03f5= 10\u22124. This guarantees\n3\u22120.3\u22120.2\u22120.1 0 0.1 0.2 0.3\u22120.200.2\nx\u03b6(x)\nActivation\nRelaxation\nFigure 2. Plot of the activation function (3) for \u03f5= 0.1\nand the corresponding relaxation (4).\nthat the output matrix Lis invertible as diagonal\nelements with a small magnitude are shifted away\nfrom zero. However, this function is discontinuous at\nx= 0 making it difficult to optimize using gradient-\nbased methods. Therefore, we use the following\ncontinuous approximation of the activation function\n\u02c6\u03b6(x) =x\u00b7\u0012\n1 + exp\u0012\n\u2212\f\f\f\f4x\n\u03f5\f\f\f\f+ 2\u0013\u0013\n(4)\nduring training. The plots for the activation function\nand the proposed relaxation are shown in Figure 2.\n3.2 Model training\nThe goal of the learned preconditioner P=LUis to\nimprove the spectral properties of the preconditioned\nlinear equation system AP\u22121. However, directly op-\ntimizing the condition number \u03baof the system as\nsuggested by Sappl et al. [21] is computationally\ninfeasible for large-scale problems. For computa-\ntional tractability, we are also not taking the whole\nspectrum of the equation system into account but\nconcentrate on the edges of the spectrum. In other\nwords, we consider only the largest and smallest sin-\ngular values of the preconditioned equation system.\nLemma 1 The largest singular value of the matrix\nP\u22121Ais upper bounded by the Frobenius norm:\n\u03c3max(AP\u22121)\u2264\u03f5\u22121\u2225A\u2212P\u2225F+ 1.\nHere, \u03f5is the hyper-parameter of the preconditioner\nchosen in Equation (3). Further, we can estimate the\nFrobenius norm using Hutchinson\u2019s trace estimator\n[22] which allows us to express the loss to minimize\nthe upper bound on the largest singular value of the\nsystem in the following form:\nLmax(P;A) =\u2225Aw\u2212Pw\u22252\n2, (5)\nwhere wis a standard normal distributed vector.\nThis loss function was first suggested by H\u00a8 ausner\net al. [4] as an approximation of the Froebnius norm.\nLemma 2 The smallest singular value of AP\u22121is\nlower bounded by the following inequalities:\n\u03c3min(AP\u22121)\u2265 \u2225PA\u22121\u2225\u22121\nF\u2265(\u2225PA\u22121\u2212I\u2225F+1)\u22121Based on this lemma, we obtain the following loss\nfunction2by using a similar approximation as in\nthe previous step and optimizing over the inverse:\nLmin(P;A) =\u2225PA\u22121w\u2212w\u22252\n2. (6)\nThe drawback of this loss is that it requires com-\nputing A\u22121wduring training. In other words, we\nneed to solve many potentially ill-conditioned linear\nsystems in order to train the learned preconditioner.\nChen [7] proposes a similar loss function and\napproximates the solutions to the linear equation\nsystem instead using Arnoldi\u2019s method.\nTo maintain computational efficiency we can make\nuse of a supervised dataset instead, consisting of\ntuples ( Ai,xi,bi) such that Aixi=bi. This re-\nquires us to solve each training problem only once\nwhich can be computed beforehand in an offline fash-\nion. We replace the standard distributed samples\nwin Equation (6) with the available training sam-\nples. This avoids solving problem instances during\ntraining but leads to a biased approximation of the\nprevious loss function:\n\u02c6Lmin(P;A) =\u2225PA\u22121b\u2212b\u22252\n2=\u2225Px\u2212b\u22252\n2.(7)\nLi et al. [5] originally suggested this loss function as\nan approximation of the Frobenius norm loss and\nmotivate it by including inductive bias from the\ntraining data. Later, Trifonov et al. [6] obtain the\nsame approximation by reweighting of Equation (5)\nwith the inverse matrix. They conjecture the connec-\ntion of the loss function with small singular values\nand, further, empirically show that minimizing this\nloss increases the smallest singular value of the pre-\nconditioned system.\nBy combining the two previous loss functions\nfrom Equation (6) and the unregularized version\nof Equation (5), we introduce a novel combined loss\nwith the goal to further improve the conditioning by\ntaking into account both ends of the spectrum. The\ncombined loss function and the approximation using\na supervised dataset similar to Equation (7) are\nLcom(P;A) =\u2225Aw\u2212Pw\u22252\n2+\u03b1\u2225PA\u22121w\u22252\n2\n\u2248\u2225Aw\u2212Pw\u22252\n2+\u03b1\u2225Px\u22252\n2,(8)\nwhere \u03b1is a hyper-parameter chosen to balance\nthe two individual loss terms. During the training,\nwe are minimizing the empirical risk over the\ncorresponding unsupervised or supervised dataset\nto learn the parameters:\n\u02c6\u03b8\u2208arg min\n\u03b8|D|X\ni=1L(P\u03b8;Ai,xi,bi). (9)\nThe proofs for the lemmas and the connection of\nthe loss functions to previous methods from the\nliterature can be found in Appendix C.\n2By convention, we always minimize the loss function.\n4Table 1. Average performance of classical and data-driven preconditioners with different loss functions evaluated\non 10 test samples. We show the conditioning of the right-preconditioned system AP\u22121as well as the convergence\nof the GMRES algorithm in terms of the total time required to compute the preconditioner and solve the systems\nand the number of iterations. For the unpreconditioned method, we compute the loss for P=I. Arrows indicate\nif higher or lower is better. The best performance is marked in bold.\nMethod \u03c3min\u2191 \u03c3max\u2193 \u03ba\u2193 \u2225P\u2212A\u2225F\u2193 \u2225PA\u22121\u2212I\u2225F\u2193Time\u2193Iterations \u2193Prec.No preconditioner 0.0014 20.70 31 152.94 255.26 1 577.65 30.85 1 153\nJacobi 0.0003 5.16 31 166.13 205.83 6 319.45 30.12 1 152\nILU(0) 0.0006 30.32 120 740.90 138.31 3 688.45 3.33 413LossLmax: Equation (5) 0.0007 5.00 16 139.46 88.76 3 261.23 3.67 437\nLmin: Equation (6) 1.1375 20 318.88 37 030.72 287.62 50.05 24.41 1 054\n\u02c6Lmin: Equation (7) - - - 287.71 50.30 130.01 2 192\nLcom: Equation (8) 0.0017 52.92 66 691.71 197.82 1 240.60 3.42 418\n4 Experiments & Results\nWe evaluate our method on a synthetic dataset of\nproblems arising from the discretization of the Pois-\nson equation. The problem size in our experiments is\nn= 2 500. We train on 200 and evaluate the results\non 10 problem instances. The details for the dataset\nand implementation can be found in Appendix B.\nComparison of loss functions In Table 1, we\npresent the performance of the learned precondi-\ntioner given different loss functions corresponding\nto the bounds derived in the previous section. We\ncan see that minimizing LmaxandLmindecrease the\nlargest and increase the smallest singular value of\nthe system respectively and the models achieve the\nlowest loss for the respective functions. This shows\nthat during training, we are able to minimize the\nloss functions and by that implicitly optimize the\nbounds for the singular values.\nOptimizing the biased approximation \u02c6Lminstill\nperforms well in terms of decreasing the original loss\nfunction corresponding to the bound of the smallest\nsingular value. The resulting preconditioning matrix\nis highly ill-conditioned for the chosen \u03f5parameter\nleading to numerical instabilities in the inverseion\nofPand inaccurate results for the singular value\ndecomposition due to the overfitting of the model\non the training data.\n0 500 1000 1500 2000 2500\nIndex of ordered singular value m\u22122024log10\u03c3m\nNone\nLmax\nLmin\nLcom\nFigure 3. Distribution of singular values of the precon-\nditioned system for different choices of preconditioner.Optimizing the combined loss Lcomreduces both\nindividual loss functions slightly compared to the\nnon-preconditioned case but leads to worse perfor-\nmance on nearly all metrics besides the acceleration\nof the iterative solver. Additionally, the training of\nthis loss can be numerically unstable as the two loss\nfunctions conflict with each other.\nNote that, the problem at hand is unbalanced\nas the spectrum of the original system is already\nskewed towards small singular values. The per-\nformance in terms of GMRES iterations of the\ndifferent loss functions also depends on the initial\ndistribution of singular values. In Figure 3 we\nshow this distribution for different preconditioners\ncompared to the original distribution.\nComparison with classical preconditioners\nAdditionally, we compare in Table 1 the perfor-\nmance of the learned preconditioner with the\nclassical Jacobi and incomplete LU method without\nfill-ins, as well as, the case where no preconditioner\nis applied. In general, the results show the vital\nimportance of selecting an appropriate precon-\nditioner. For the problem considered, ILU and\nthe learned preconditioner perform nearly on par.\nWe can see that even though the conditioning in\nterms of the largest and smallest singular value\nof the ILU method is by far inferior to the other\npreconditioners, it still manages to substantially\nreduce the number of iterations for convergence.\nThis shows that the spectrum edges, while impor-\ntant, are not the only factor affecting the method\u2019s\nefficiency. It underscores the importance of consider-\ning the entire spectrum of the preconditioned system\nwhen analyzing the method\u2019s convergence.\n5 Discussion & Conclusion\nIn this paper, we learn incomplete factorizations of\nsparse matrices directly from data. When applied as\na preconditioner our approach leads to a significant\nreduction in terms of iterations compared to the\nunpreconditioned GMRES. Combining data-driven\n5methods with existing algorithms has a huge poten-\ntial since it allows obtaining the best results from\nboth worlds: guarantees about the convergence is ob-\ntained from classical analysis of the algorithms while\nthe data-driven components allow us to learn directly\nfrom data to improve upon the existing algorithms.\nLimitations Our approach assumes having access\nto a distribution of linear equation systems that\nshare similarities between the problem instances.\nHowever, in practice, it is not always trivial to obtain\nsuch a distribution of problems. Further, the initial\ncost of training the network needs to be amortized\nover many future problems that need to be solved.\nHowever, given the initial sample problems, new\nproblems can be solved faster in an online fashion\nwhich is important for time-critical applications.\nFuture work The GNN is designed so that the\nlearned preconditioner matches the sparsity pattern\nof the input matrix A. While dynamically learning\nthe sparsity pattern and matrix elements could im-\nprove preconditioner quality, the large search space\nand combinatorial complexity make this challenging.\nOur proposed loss function accounts for the spec-\ntrum edges in the preconditioned system. However,\nbeyond the edges and condition number, the distri-\nbution of singular values also affects the convergence\nof Krylov subspace methods. Designing loss func-\ntions that consider this can further improve the\nperformance of learned preconditioners.\nAcknowledgments\nWe would like to thank Daniel Hern\u00b4 andez Escobar\nand Sebastian Mair for helpful discussions and\nfeedback. This work was supported by the the\nG\u00a8 oran Gustafsson Foundation and the Wallenberg\nAI, Autonomous Systems and Software Program\n(WASP) funded by the Knut and Alice Wallenberg\nFoundation.\nContribution statement\nThe idea to apply learned preconditioner to GM-\nRES was initially proposed by JS and later refined\ntogether with PH. PH suggested the overall network\narchitecture including the novel activation function\nto ensure invertibility and derived the theoretical\nresults that were finalized with help of ANJ. ANJ im-\nplemented and constructed the initial GNN architec-\nture and the implementation of the GMRES method\nand came up with the dataset with inputs and sup-\nport from PH and JS. The final experiments and\nresults were conducted by PH. Writing was primarily\ndone by PH with help of ANJ and inputs from JS.References\n[1]Y. Saad. Iterative Methods for Sparse Linear\nSystems . Second. Society for Industrial and\nApplied Mathematics, 2003. doi:10.1137/1.\n9780898718003 .\n[2]M. Benzi. \u201cPreconditioning techniques for\nlarge linear systems: a survey\u201d. In: Journal of\ncomputational Physics 182.2 (2002), pp. 418\u2013\n477.doi:10.1006/jcph.2002.7176 .\n[3]J. Scott and M. Tuma. Algorithms for sparse\nlinear systems . Springer Nature, 2023. doi:\n10.1007/978-3-031-25820-6 .\n[4]P. H\u00a8 ausner, O. \u00a8Oktem, and J. Sj\u00a8 olund. \u201cNeu-\nral incomplete factorization: learning precon-\nditioners for the conjugate gradient method\u201d.\nIn:arXiv preprint arXiv:2305.16368 (2023).\n[5]Y. Li, P. Y. Chen, T. Du, and W. Matusik.\n\u201cLearning preconditioners for conjugate gradi-\nent PDE solvers\u201d. In: International Conference\non Machine Learning . PMLR. 2023, pp. 19425\u2013\n19439.\n[6]V. Trifonov, A. Rudikov, O. Iliev, I. Oseledets,\nand E. Muravleva. \u201cLearning from Linear Al-\ngebra: A Graph Neural Network Approach to\nPreconditioner Design for Conjugate Gradient\nSolvers\u201d. In: arXiv preprint arXiv:2405.15557\n(2024).\n[7]J. Chen. \u201cGraph Neural Preconditioners for\nIterative Solutions of Sparse Linear Systems\u201d.\nIn:arXiv preprint arXiv:2406.00809 (2024).\n[8]M. B\u02da ankestad, J. Andersson, S. Mair, and J.\nSj\u00a8 olund. \u201cIsing on the Graph: Task-specific\nGraph Subsampling via the Ising Model\u201d. In:\narXiv preprint arXiv:2402.10206 (2024).\n[9]M. Li, H. Wang, and P. K. Jimack. \u201cGenera-\ntive Modeling of Sparse Approximate Inverse\nPreconditioners\u201d. In: Computational Science \u2013\nICCS 2024 . Cham: Springer Nature Switzer-\nland, 2024, pp. 378\u2013392. doi:10.1007/978-\n3-031-63759-9_40 .\n[10] Y. Saad and M. H. Schultz. \u201cGMRES: A Gen-\neralized Minimal Residual Algorithm for Solv-\ning Nonsymmetric Linear Systems\u201d. In: SIAM\nJournal on Scientific and Statistical Comput-\ning7.3 (1986), pp. 856\u2013869. doi:10.1137/\n0907058 .\n[11] G. H. Golub and C. F. Van Loan. Matrix\ncomputations . JHU press, 2013.\n[12] J. W. Pearson and J. Pestana. \u201cPrecondi-\ntioners for Krylov subspace methods: An\noverview\u201d. In: GAMM-Mitteilungen 43.4\n(2020). doi:10.1002/gamm.202000015 .\n6[13] T. A. Davis, S. Rajamanickam, and W. M.\nSid-Lakhdar. \u201cA survey of direct methods\nfor sparse linear systems\u201d. In: Acta Numer-\nica25 (2016), pp. 383\u2013566. doi:10.1017/\nS0962492916000076 .\n[14] M. Benzi and M. Tuma. \u201cA comparative study\nof sparse approximate inverse precondition-\ners\u201d. In: Applied Numerical Mathematics 30.2-\n3 (1999), pp. 305\u2013340. doi:10.1016/S0168-\n9274(98)00118-4 .\n[15] M. M. Bronstein, J. Bruna, T. Cohen, and P.\nVeli\u02c7 ckovi\u00b4 c. \u201cGeometric Deep Learning: Grids,\nGroups, Graphs, Geodesics, and Gauges\u201d. In:\narXiv preprint arXiv:2104.13478 (2021).\n[16] P. W. Battaglia, J. B. Hamrick, V. Bapst,\nA. Sanchez-Gonzalez, V. Zambaldi, M. Mali-\nnowski, A. Tacchetti, D. Raposo, A. Santoro,\nR. Faulkner, et al. \u201cRelational inductive bi-\nases, deep learning, and graph networks\u201d. In:\narXiv preprint arXiv:1806.01261 (2018).\n[17] J. Sj\u00a8 olund and M. B\u02da ankestad. \u201cGraph-\nbased neural acceleration for nonnegative\nmatrix factorization\u201d. In: arXiv preprint\narXiv:2202.00264 (2022).\n[18] N. S. Moore, E. C. Cyr, P. Ohm, C. M. Siefert,\nand R. S. Tuminaro. \u201cGraph neural networks\nand applied linear algebra\u201d. In: arXiv preprint\narXiv:2310.14084 (2023).\n[19] M. Doob. \u201cApplications of graph theory in lin-\near algebra\u201d. In: Mathematics Magazine 57.2\n(1984), pp. 67\u201376. doi:10.2307/2689586 .\n[20] Z. Tang, H. Zhang, and J. Chen. \u201cGraph Neu-\nral Networks for Selection of Preconditioners\nand Krylov Solvers\u201d. In: NeurIPS 2022 Work-\nshop: New Frontiers in Graph Learning . 2022.\n[21] J. Sappl, L. Seiler, M. Harders, and W. Rauch.\n\u201cDeep learning of preconditioners for conjugate\ngradient solvers in urban water related prob-\nlems\u201d. In: arXiv preprint arXiv:1906.06925\n(2019).\n[22] M. F. Hutchinson. \u201cA stochastic estimator of\nthe trace of the influence matrix for Lapla-\ncian smoothing splines\u201d. In: Communica-\ntions in Statistics-Simulation and Computa-\ntion18.3 (1989), pp. 1059\u20131076. doi:10.1080/\n03610919008812866 .\n[23] H. P. Langtangen and A. Logg. \u201cSolving PDEs\nin minutes-the FEniCS tutorial\u201d. In: The FEn-\niCS Book 1 (2016).\n[24] M. Fey and J. E. Lenssen. \u201cFast graph repre-\nsentation learning with PyTorch Geometric\u201d.\nIn:arXiv preprint arXiv:1903.02428 (2019).[25] N. Nytko, A. Taghibakhshi, T. U. Zaman, S.\nMacLachlan, L. N. Olson, and M. West. \u201cOp-\ntimized sparse matrix operations for reverse\nmode automatic differentiation\u201d. In: arXiv\npreprint arXiv:2212.05159 (2022).\n[26] J. Mayer. \u201cILU++: A new software package\nfor solving sparse linear systems with iterative\nmethods\u201d. In: PAMM: Proceedings in Applied\nMathematics and Mechanics . Vol. 7. 1. Wiley\nOnline Library. 2007, pp. 2020123\u20132020124.\ndoi:10.1002/pamm.200700911 .\n[27] C. Hofreither. ilupp \u2013 ILU algorithms for C++\nand Python . 2020. url:https : / / github .\ncom/c-f-h/ilupp .\n7A GMRES algorithm\nThe right-preconditioned GMRES method is shown\nin Algorithm A.1. The non-preconditioned version\ncan be obtained by using the identity matrix as\na preconditioner ( P=I). Operations where the\npreconditioner is applied are highlighted for better\nreadability in this section.\nAlgorithm A.1 Right-preconditioned GM-\nRES [10]\n1:Inputs:\n2:Non-singular matrix A\u2208Rn\u00d7n\n3:Preconditioner P\u2208Rn\u00d7n\n4:Right-hand side b\u2208Rn\n5:Initial guess x0\u2208Rn\n6:Tolerance \u03f5for the residual norm\n7:Maximum number of iterations kmax\n8:Output: Approximate solution xk\n9:k= 0\n10:\u25b7Compute the initial residual\n11:r0= (b\u2212Ax0)\n12:\u03c10=\u2225r0\u22252\n13:\u03b2=\u03c10\n14:while \u03c1k> \u03f5\u03c1 0andk < k maxdo\n15: k\u2190k+ 1\n16: Arnoldi ( A,P,r0, k)\u2192Vk+1,Hk+1,k\n17: Compute the QR factorization of Hk+1,k\n18: \u03c1k=|\u03b2q1,k+1|\n19:\u25b7Solve using the QR factorization of Hk+1,k\n20:yk= argmin\ny\u2208Rn\u2225\u03b2e1\u2212Hk+1,ky\u22252\n21:\u25b7Construct the solution\n22:xk=x0+P\u22121Vkyk\n23:return xk\nArnoldi method In each step of the iterative\nscheme (line 16 in Algorithm A.1), the Arnoldi\nmethod is used to construct an orthonormal basis\nof the Krylov subspace Kkof the current iteration\nshown in Equation (1). This method is crucial as it\nensures that the basis vectors are orthogonal, which\nin turn allows the GMRES algorithm to efficiently\nminimize the residual over the Krylov subspace.\nFor a given number of iterations k, the Arnoldi\nmethod (Algorithm A.2) produces an upper Hessen-\nberg matrix Hk,k+1, which is related to the matrix\nAthrough the orthonormal basis Vk+1as\nAVk=Vk+1Hk,k+1, (10)\nwhere Vkconsists of the first kcolumns of Vk+1,\nforming an orthonormal basis for the Krylov sub-\nspace.\nBased on the factorization of this basis, the resid-\nual and approximate solution can be efficiently ob-\ntained by solving a smaller and upper-triangular\nsystem derived from the QR factorization of Hk,k+1,\nas shown in line 20 of the GMRES algorithm.In lines 21\u201323 of Algorithm A.2, the situation\nwhere the element hk+1,kin the upper Hessenberg\nmatrix Hk+1,kbecomes zero is addressed. If this\ncondition is met, the loop breaks, indicating that\nthe algorithm has prematurely found an invariant\nKrylov subspace. When this occurs, the ( k+ 1)-\nth column of Vk+1does not exist because the last\nrow of Hk+1,kis zero. As a result, Equation (10)\nsimplifies to\nAVk=VkHk.\nThis indicates that the Krylov subspace has reached\nits full dimension, and no further vectors can be\ngenerated. Consequently, the GMRES algorithm\ncan terminate early, as the exact solution lies within\nthe current subspace. This premature convergence\nis referred to as \u201clucky\u201d because it implies that the\nsolution has been found in less than niterations.\nAlso, note that in practice, it is not necessary\nto run the full Arnoldi algorithm in each iteration\nbut the basis vectors found in the previous itera-\ntions can be reused and only the most recent ortho-\nnormalization step needs to be executed.\nAlgorithm A.2 Arnoldi\u2019s Modified Gram-\nSchmidt Implementation\n1:Inputs:\n2:Non-singular matrix A\u2208Rn\u00d7n\n3:Preconditioner P\u2208Rn\u00d7n\n4:Initial residual r0\u2208Rn\n5:Number of iterations k\n6:Outputs:\n7:Orthonormal basis Vk+1={vj}k+1\nj=1,vj\u2208Rn\n8:Hessenberg matrix Hk+1,k\u2208R(k+1)\u00d7k\n9:\u25b7Initialize the Arnoldi basis\n10:\u03b2=\u2225r(0)\u22252\n11:v1=\u03b2\u22121r(0)\n12:fori= 1 to kdo\n13: wi+1=AP\u22121vi\n14: \u25b7Gram-Schmidt ortho-normalization\n15: forj= 1 to ido\n16: hj,i=wT\ni+1vj\n17: wi+1=wi+1\u2212hj,ivj\n18: hi+1,i=\u2225wi+1\u22252\n19: ifhi+1,i\u0338= 0then\n20: vi+1=wi+1/hi+1,i\n21: else\n22: \u25b7Exact solution found (lucky breakdown)\n23: break\n24:return Vk+1,Hk+1,k\nPreconditioning The preconditioning matrix P\n(or its inverse) does not need to be stored explicitly.\nInstead, it is sufficient to have access to an implicit\nrepresentation of the matrix which can be applied to\na vector vto evaluate the product P\u22121vas in line 22\nof Algorithm A.1 and Algorithm A.2 respectively.\n8In the case of preconditioner given by triangu-\nlar factorizations such as ILU and our learned ap-\nproach, the matrix is obtained by solving the system\nLUv =rwhich can be achieved in O(n2) opera-\ntions using the forward-backward substitution and\ncan be further accelerated by exploiting the sparse\nstructure of the matrices. The solution to solving\nthese systems is equivalent to explicitly inverting the\nmatrices and computing the matrix-vector product\nU\u22121L\u22121rwhich forms the preconditioning matrix\nasP\u22121= (LU)\u22121. However, solving the triangular\nsystems is more efficient in practice.\nB Implementation details\nHere, we provide additional details for the implemen-\ntation of the dataset, our learned preconditioner, and\nthe baseline preconditioners used in the experiments.\nB.1 Dataset\nOur goal with the provided dataset is not solving a\nreal-world problem. Rather, we focus on a highly\nill-conditioned synthetic dataset which allows us\nto systematically test the different loss functions\nderived. For the problem scale used, direct methods\nare much superior compared to the shown results.\nPoisson problem The Poisson equation is an\nelliptic partial differential equation (PDE) and one\nof the most fundamental problems in numerical\ncomputational science [23]. The general form of the\nPoisson equation is given by:\n\u2212\u22072u(x) =f(x)x\u2208\u2126,\nu(x) =uD(x)x\u2208\u2202\u2126,(11)\nwhere f(x) is the source function and u(x) is the\nunknown function to be solved for.\nIn our study, we generate the matrix Ausing\npyamg.gallery.poisson , which implicitly assumes\nunit spacing and Dirichlet boundary conditions, typ-\nically set to zero. The right-hand side of the linear\nequation system, b, is derived from the source func-\ntionf(x, y) =sin(\u03c0x)sin(\u03c0y). By discretizing the\nproblem using the finite element method, we obtain\na system of linear equations of the form Ax=b,\nwhere the stiffness matrix Ais sparse and symmetric\npositive definite. Throughout our experiments, we\nuse matrices of size n= 2 500.\nNoisy data However, we are not specifically in-\nterested in spd matrices. Therefore, to obtain a\ndistribution Aover a large number of Poisson equa-\ntion PDE problems that can be efficiently solved\nusing GMRES, we perturb all the nonzero entries\nwith standard Gaussian noise, which yields arbi-\ntrary matrices not necessarily spd. Let us denote\nthe perturbed matrix as A\u2032= (a\u2032\nij).We can represent this mathematically by:\na\u2032\nij=(\naij+Xijifaij\u0338= 0,\n0 if aij= 0,\nwhere Xij\u223c N(0,1) are i.i.d. random variables.\nBy introducing randomness into A, we are ef-\nfectively modeling a scenario where the system\u2019s\nproperties are not perfectly regular, which could\nrepresent, for example, some form of physical or\nnumerical irregularity or perturbation in the grid\nor medium. During testing, we use a deterministic\nright-hand side bas described previously. However,\nfor the supervised training dataset we sample the\nvectors bto be normally distributed in order to\navoid overfitting as this would lead to a significant\ndecrease in model performance.\nProblem instances All problem instances gen"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract:\nThe paper introduces LoRID, a **Low-Rank Iterative Diffusion purification method** that addresses adversarial perturbations in neural networks using an information-theoretic framework. LoRID leverages multiple rounds of **diffusion-denoising** loops and utilizes **Tucker decomposition** to reduce purification errors. It demonstrates superior robustness on datasets like **CIFAR-10/100**, **CelebA-HQ**, and **ImageNet** under both white-box and black-box settings, achieving notable improvements in robustness performance and inference run-time speedup.\n\n### 2. Introduction:\n#### Context of the Research:\nNeural networks face vulnerabilities to adversarial attacks which perturb inputs slightly leading to misclassifications. This work focuses on **diffusion-based purification methods**, state-of-the-art adversarial defenses that eliminate perturbations by iterating over diffusion and denoising processes.\n\n#### Research Questions:\n- How can diffusion-based methods be optimized to minimize purification errors?\n- Can iterating in early stages of diffusion models improve robustness without significant computational overhead?\n\n#### Literature Review:\n- Previous works have explored **adversarial training** and **adversarial purification**.\n- **Diffusion models** have recently shown promise, yet face computational challenges.\n- Current studies lack theoretical analysis of Markov-based diffusion purifications and efficient iterative processes to handle adversarial noise.\n\n#### Contributions:\n- Introduces a **theoretical framework** to characterize purification errors in diffusion models.\n- Proposes **LoRID**, employing multiple rounds of low-rank iterative diffusion and Tucker decomposition to enhance robustness.\n- Demonstrates significant performance gains across various benchmarks and threat models.\n\n### 3. Related Work:\n#### Background:\n- **Denoising Diffusion Probabilistic Models (DDPMs)**: Adds Gaussian noise iteratively and learns to denoise this signal.\n- **Score-based diffusion models**: Utilize the gradient of the data distribution for purification.\n\n#### Advances Over Previous Work:\n- LoRID combines **Markov-based methods** with **Tucker decomposition** to tackle high-noise adversarial inputs effectively.\n- Provides a detailed theoretical exploration of **purification errors** at both distribution and instance levels.\n- Incorporates **multiple iterations at early time-steps** to enhance purification.\n\n### 4. Method:\n#### Theoretical Analysis:\n- **Theorem 1**: Shows the KL-divergence converges for distributions of inputs as diffusion time-step increases.\n- **Theorems 2 & 3**: Quantify purification errors at an instance level, considering MMSE (Minimum Mean Square Error) of the Gaussian channel.\n\n#### Intrinsic Error Reduction:\n- **Theorem 4**: Discusses error reduction via looping early time-steps, highlighting that iterative processing can significantly lower errors.\n- **Theorem 5**: Validates combining **Tucker decomposition** with diffusion-denoising for high-noise regimes.\n\n#### Algorithm\u2014LoRID:\n1. **Tensor Factorization** using Tucker decomposition to project input into a lower-rank tensor.\n2. **Diffusion-Denoising**: Apply multiple loops of early stage diffusion-denoising to reconstruct the purified image from high-dimensional tensors.\n\n### 5. Experimental Setup:\n#### Datasets:\n- **CIFAR-10/100**, **CelebA-HQ**, **ImageNet**\n\n#### Hardware/Software:\n- Describes the use of high-performance computing clusters with detailed specifications of CPUs/GPUs and software libraries (e.g., PyTorch, TensorFlow).\n\n#### Evaluation Metrics:\n- Standard accuracy (pre-perturbation).\n- Robust accuracy (post-perturbation using L\u221eand L2norms).\n- Inference run-time speedup.\n\n#### Baselines:\n- Compared against **state-of-the-art adversarial training methods** and **diffusion-based purifications (DensePure, Guided-DDPM)**.\n\n### 6. Results and Analysis:\n#### Empirical Findings:\n- **CIFAR-10/100 Results**: Significant improvements in robust accuracy (e.g., exceeding SOTA by 23.15% in certain settings).\n- **CelebA-HQ & ImageNet**: Demonstrated high standard and robust accuracy against white-box and black-box attacks.\n\n#### Figures/Tables:\n- Tables highlight accuracy and robustness under different settings.\n- Figures illustrate purification processes and error reductions through looping and Tucker decomposition iterations.\n\n#### Analysis:\n- Ablation studies comparing pure diffusion-denoising vs. combined approach.\n- Observed consistency in improvements across various datasets and attack models.\n\n### 7. Discussion:\n#### Interpretation:\n- **Early Iterations**: Loops at early stages capture essential features before substantial degradation.\n- **Tensor Factorization**: Effectively reduces high-noise perturbations, aiding the denoising process.\n\n#### Implications:\n- LoRID sets a new benchmark in robustness while being computationally efficient.\n- Shows potential adaptation to other high-dimensional input domains.\n\n#### Limitations:\n- High-order Tucker decomposition might not scale well for exceptionally large datasets without optimization.\n- Potential computational burden despite efficiency improvements compared to other methods.\n\n### 8. Conclusion:\n#### Summary of Contributions:\n- Theoretical bounds on purification errors for Markov-based diffusion models.\n- Introduction of an efficient, robust adversarial purification method that integrates tensor factorization and early-stage iterations.\n\n#### Future Research:\n- Optimization of tensor decomposition for larger datasets.\n- Exploration of domain-specific adaptations and real-world adversarial threat models.\n\nThis comprehensive summary is intended to provide sufficient detail for a researcher to replicate and build upon the LoRID method in subsequent studies."}, {"role": "assistant", "content": "LoRID: Low-Rank Iterative Diffusion for Adversarial Purification\nGeigh Zollicoffer1*, Minh Vu1*, Ben Nebgen1, Juan Castorena2, Boian Alexandrov1, Manish\nBhattarai1\n1Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM\n2Computational Sciences, Los Alamos National Laboratory, Los Alamos, NM\ngzollicoffer@lanl.gov, mvu@lanl.gov, bnebgen@lanl.gov, jcastorena@lanl.gov, boian@lanl.gov, ceodspspectrum@lanl.gov\nAbstract\nThis work presents an information-theoretic examination of\ndiffusion-based purification methods, the state-of-the-art ad-\nversarial defenses that utilize diffusion models to remove ma-\nlicious perturbations in adversarial examples. By theoreti-\ncally characterizing the inherent purification errors associated\nwith the Markov-based diffusion purifications, we introduce\nLoRID, a novel Low-Rank Iterative Diffusion purification\nmethod designed to remove adversarial perturbation with low\nintrinsic purification errors. LoRID centers around a multi-\nstage purification process that leverages multiple rounds of\ndiffusion-denoising loops at the early time-steps of the dif-\nfusion models, and the integration of Tucker decomposition,\nan extension of matrix factorization, to remove adversarial\nnoise at high-noise regimes. Consequently, LoRID increases\nthe effective diffusion time-steps and overcomes strong ad-\nversarial attacks, achieving superior robustness performance\nin CIFAR-10/100, CelebA-HQ, and ImageNet datasets under\nboth white-box and black-box settings.\n1 Introduction\nDespite their widespread adoption, neural networks are vul-\nnerable to small malicious input perturbations, leading to un-\npredictable outputs, known as adversarial attacks (Szegedy\net al. 2014; Goodfellow, Shlens, and Szegedy 2015). Various\ndefense methods have been developed to protect these mod-\nels (Qiu et al. 2019), including adversarial training (Madry\net al. 2019; Bai et al. 2021; Zhang et al. 2019) and adversar-\nial purification (Salakhutdinov 2015; Shi, Holtz, and Mishne\n2021; Song et al. 2018; Nie et al. 2022; Wang et al. 2022,\n2023). With the introduction of diffusion models (Ho, Jain,\nand Abbeel 2020; Song et al. 2021) as a powerful class of\ngenerative models, diffusion-based adversarial purifications\nhave overcome training-based methods and achieve state-of-\nthe-art (SOTA) robustness performance (Blau et al. 2022;\nWang et al. 2022; Nie et al. 2022; Xiao et al. 2022). In princi-\nple, the diffusion-based purification first diffuses the adver-\nsarial inputs with Gaussian noises in ttime-steps and utilizes\nthe diffusion\u2019s denoiser to remove the adversarial perturba-\ntions along with the added Gaussian noises. While it is com-\nputationally challenging to attack diffusion-based purifica-\ntion due to vanishing/exploding gradient problems, high\nmemory costs, and substantial randomness (Kang, Song, and\n*These authors contributed equally.Table 1: Performance of SOTA score-based purification ver-\nsus our proposed LoRID, a Markov-based purification, in\nCIFAR-10 (\u03f5= 8/255) and ImageNet ( \u03f5= 4/255) under\nL\u221ewhite-box AutoAttack in WideResNet-28-10.\nPurification Score-based LoRID\nStandard Acc 89.02 / 71.16 84.20 / 73.98\nRobust Acc 46.88 / 44.39 54.14 / 56.54\nInference Run-time Speedup \u00d71/\u00d71 \u00d72.3/\u00d74.6\nLi 2024), recent work has been proposing efficient attacks\nagainst diffusion-based purification (Nie et al. 2022; Kang,\nSong, and Li 2024), which can degrade the model robust-\nness significantly. A naive way to prevent such attacks is\nto increase the diffusion time-step tas it will remarkably\nraise both the time and memory complexity for the attack-\ners (Kang, Song, and Li 2024). However, increasing twould\nnot only introduces additional computational cost of purifi-\ncation (Nie et al. 2022; Lee and Kim 2023), but also in-\nevitably damages the purified samples (see Theorem 2 or\nFig. 3), and significantly degrade the classification accuracy.\nOur work aims to develop a more robust and efficient\ndiffusion-based purification method to counter emerging ad-\nversarial attacks. We first introduce an information-theoretic\nviewpoint on the diffusion-based purification process, in\nwhich the purified signal is considered as the recovered sig-\nnal from a noisy communication channel. Different from the\nprevious purification (Nie et al. 2022) centered on the Score-\nbased diffusion (Song et al. 2021), our work is the first the-\noretical analysis of the inherent error induced by Markov-\nbased purifications (Blau et al. 2022; Wang et al. 2022;\nXiao et al. 2022), which are purifications relying on the De-\nnoising Diffusion Probabilistic Model (DDPM) (Ho, Jain,\nand Abbeel 2020). Our theoretical foundation for DDPM\n(Theorem 1, 2 and 3 ) are essential as they validate the usage\nof DDPM for purification and leverage its substantial advan-\ntage in terms of running time compared to the Score-based\n(as shown in Table 1). Our analysis further points out an\ninteresting finding: the purification error (Corollary 1) can\nbe reduced significantly by conducting multiple iterations at\nthe early time-steps of the DDPM ( Theorem 4 ). Particularly,\nthe application of a single purification with a time-step tis\ntheoretically shown to be less beneficial than the looping of\nLiterations of diffusion-denoising with a time-step of t/LarXiv:2409.08255v1  [cs.LG]  12 Sep 2024Figure 1: The MMSEs induced by Markov-based purifica-\ntion against the iterative factor L(Corollary 1): each point\nis the MMSE of the reconstructed data from a normal-\nized Gaussian through Literative loops of t/L diffusion-\ndenoising calls. Thus, points on a line share the same effec-\ntive denoising step t= (t/L)\u00d7L. The key observation is\nthe purification error generally decreases as Lincreases. The\nright samples compare clean samples, purified samples with\na single large time-step t/L= 600 , and those with the same\neffective denoising step tbut with a larger iterative factor\nL= 10 (Details in Appx. B.1).\n(Fig. 1). Our study additionally suggests the usage of Tucker\ndecomposition (Bergqvist and Larsson 2010), a higher-order\nextension of matrix factorization, to attenuate adversarial\nnoise at the high-noise regime ( Theorem 5 ). We realize the\nadvantages of those findings and propose LoRID, a Low-\nRank Iterative Diffusion purification method designed to\nmitigate the purification errors (Fig. 2). By controlling the\npurification error, LoRID can effectively increase the diffu-\nsion time-step and beat the SOTA robustness benchmark, in\nboth white-box and black-box settings (Table. 1 highlights\nLoRID\u2019s performance in CIFAR-10 (Rabanser, Shchur, and\nG\u00a8unnemann 2017), and Imagenet (Deng et al. 2009)). The\nmain contributions of this work are:\n\u2022 We establish theoretical bounds on the purification errors\nof Markov-based purifications. In particular, Theorem 1\nshow that the the adversarial noise will be removed at a\ndistribution-level as the purification time-steps increases.\nOn the other hand, Theorem 2, and 3 point out the purifi-\ncation at the sample-level.\n\u2022 We show theoretical justifications for looping the early-\nstages of DDPM (Theorem 4), and the usage of Tucker\ndecomposition (Theorem 5) for adversarial purification.\n\u2022 We introduce a Markov-based purification algorithm,\ncalled LoRID (Alg. 1), utilizing early looping and Tucker\ndecomposition and demonstrate rigorously its effective-\nness and high performance in three real-world datasets:\nCIFAR-10/100, CelebA-HQ, and Imagenet.\nOur paper is organized as follows. Sect. 2 provides the\nbackground and related work of this study. Sect. 3 consists\nof our theoretical analysis and the description of our pro-\nposed purification LoRID. Sect. 4 provides our experimental\nresults, and Sect. 5 concludes this paper.2 Background and Related Work\nThis section first briefly reviews the Denoising Diffusion\nProbabilistic Model (Ho, Jain, and Abbeel 2020), which is\nthe backbone of our diffusion purifications. Then, the related\nwork about the usage of diffusion models as adversarial pu-\nrifiers is discussed. Finally, we briefly discuss the Tucker de-\ncomposition, which is a component utilized by our method.\nDenoising Diffusion Probabilistic Models (DDPMs) are\na class of generative models that, during training, iteratively\nadding noise to input signals, then learning to denoise from\nthe resulting noisy signal. Formally, given a data point x0\nsampled from the data distribution q(x0), aforward diffu-\nsion process from clean data x0toxTis a Markov-chain\nthat gradually adds Gaussian noise, denoted by N, to the\ndata according to a variance schedule {\u03b2t\u2208(0,1)}T\nt=1:\n(x1:T|x0) :=QT\nt=1q(xt|xt\u22121), where\nq(xt|xt\u22121) :=N(xt;p\n1\u2212\u03b2txt\u22121, \u03b2tI) (1)\nThe objective of DDPM is to learn the joint distribution\np\u03b8(x0:T), called the reverse process, which is defined as\nanother Markov-chain with learned Gaussian transitions\np\u03b8(x0:T) =p(xT)QT\nt=1p\u03b8(xt\u22121|xt), where\np\u03b8(xt\u22121|xt) :=N(xt\u22121;\u00b5\u03b8(xt, t),\u03a3\u03b8(xt, t)) (2)\nstarting with p(xT) =N(xT;0,I). The mean \u00b5\u03b8(xt, t)\nis a neural network parameterized by \u03b8, and the vari-\nance\u03a3\u03b8(xt, t)can be either time-step dependent con-\nstants (Ho, Jain, and Abbeel 2020) or learned by a neu-\nral network (Nichol and Dhariwal 2021). A notable prop-\nerty of the forward process is that it admits sampling xtat\nan arbitrary time-step tin closed form: using the notation\n\u03b1t:= 1\u2212\u03b2tand\u00af\u03b1t:=Qt\ns=1\u03b1s, we have\nq(xt|x0) =N\u0000\nxt;\u221a\u00af\u03b1tx0,(1\u2212\u00af\u03b1t)I\u0001\nUsing the reparameterize trick, we can define the forward\ndiffusion process to the time-step tasft:\nxt=ft(x0) :=\u221a\u00af\u03b1tx0+\u221a\n1\u2212\u00af\u03b1t\u03f50 (3)\nwhere \u03f50is a standard Gaussian noise.\nFor the reverse process, the recovered signal from the\ntime-step tcan be written as (Ho, Jain, and Abbeel 2020):\n\u02dcx0(t) =1\u221a\u00af\u03b1txt\u2212\u221a1\u2212\u00af\u03b1t\u221a\u00af\u03b1t\u03f5\u03b8(xt, t) (4)\nwhere \u03f5\u03b8is a function approximator predicting \u03f5fromxt,\ni.e., the noise matching term . Given that, we have\n\u02dcx0(t)\u2212x0=\u221a1\u2212\u00af\u03b1t\u221a\u00af\u03b1t(\u03f50\u2212\u03f5\u03b8(xt, t)) (5)\nThus, the approximator \u03f5\u03b8can be trained using MSE loss:\nL(\u03b8) :=Et,x0,\u03f5h\n\u2225\u03f5\u2212\u03f5\u03b8(xt, t)\u22252i\n(6)\nDiffusion models as adversarial purifiers. Diffusion-\nbased purification schemes can be categorized into Markov-\nbased purification (or DDPM-based), and Score-based pu-\nrification, which utilize DDPM (Ho, Jain, and Abbeel 2020)Figure 2: The overall purification process of LoRID: given an input image x, LoRID first transforms the image to a tensor and\nconducts tensor factorization to eliminate some adversarial perturbation. Then, multiple loops of diffusion-denoising, denoted\nbyftandrt, at the early stages of the diffusion models are applied to obtain the final purified image \u02c6x.\nand Score-based diffusion model (Song et al. 2021) to purify\nthe adversarial examples, respectively. In this work, we fo-\ncus on the Markov-based methods, which typically diffuse\nthe adversarial input xato a certain time-step t, then utilize\nthe DDPMs to iterativly solve the reverse process as given\nin (Ho, Jain, and Abbeel 2020):\n\u02c6xt\u22121=1\u221a\u03b1t\u0012\n\u02c6xt\u22121\u2212\u03b1t\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(xt, t)\u0013\n+\u03b2t(1\u2212\u00af\u03b1t\u22121)\n1\u2212\u00af\u03b1t\u03f5\n(7)\nWe denote the process of running (7) iteratively, starting\nfrom \u02c6xt:=xtto finally obtain \u02c6x0byrtand write \u02c6x0=\nrt(xt). The whole process of purification is, therefore, can\nbe referred by the composition rt\u25e6ft.\nRecent Markov-based purifications often apply a modi-\nfied version of (7). The work (Blau et al. 2022) uses a re-\nscaled version of (7) with a larger noise term for purifica-\ntion. The Guided-DDPM purification (Wang et al. 2022)\nintroduces a guided term encouraging the purified image\nto be close to the adversarial image in the reverse pro-\ncess to protect the sample\u2019s semantics. On the other hand,\nDensePure (Xiao et al. 2022) computes multiple reversed\nsamples using (Eq. 7) and determines final predictions by\nmajority voting. We find that the recent findings of Lee and\nKim (2023) are the most closely related to this work: they\nobserve that the gradual noise-scheduling strategy , which\ninvolves looping several times during the early stages of the\nDDPM, can enhance defense mechanisms. However, there\nis no theoretical justification for this strategy. Furthermore,\nas the attackers in their threat models are unaware of this\ndefense, it is unclear whether gradual noise-scheduling truly\noffer better robustness against practical white-box attackers.\nDespite the differences, all methods emphasize a too-large\ntwould damage the global label semantics from the purified\nsample. While the theoretical statement for this is stated in\nthe case of Score-based purifications (Nie et al. 2022), the\ncounterpart for Markov-based is lacking. One of our con-\ntributions is the theoretical statement for the Markov-based\nin Theorem 1 and Theorem 2. Another aspect that distin-\nguishes the technicality our method, LoRID, from previous\nwork is the use of a large number of loops (typically between\n10and40) in the early stages (about 1%\u221210% of the total\ntime-step) of the DDPM, along with the implementation of\nTucker decomposition.\nTucker Decomposition , also known as higher-order sin-\ngular value decomposition (HOSVD) (Bergqvist and Lars-\nson 2010), is a mathematical technique used in multilinearalgebra and data analysis, and can be viewed as an extension\nof the concept of singular value decomposition (SVD) for\nhigher-dimensional data arrays or tensors (Kolda and Bader\n2009). Computing the Tucker decomposition of a tensor can\nencode the essential information and structure of the tensor\ninto a set of core tensor and factor matrices. It\u2019s widely used\nin various fields such as signal processing, image process-\ning, neuroscience, data compression, and in the line of the\nproposed work, feature extraction from high-dimensional\ndata (Kolda and Bader 2009).\nIn our purification context, as the latent tensor Xis ob-\ntained from an original image xvia a tensorization pro-\ncess (Bhattarai et al. 2023), denoted by X:=T(x), the\noverall tensor-factorization denoising process (Fig. 2) can\nbe referred by the following:\n\u02c6x=TF(x+\u03f5) (8)\nwhere T\u22121denotes the recover of the image from the latent\nspace, and TF is defined as TF :=T\u22121\u25e6d\u22121\u25e6d\u25e6T. The\ndetails of this operation is provided in Appx. B.2.\nAs both Tucker and tensorization are linear transforma-\ntions, the denoising-reconstruction error can be bounded as:\n\u2225x\u2212TF(x+\u03f5)\u2225 \u2264\u2225x\u2212TF(x)\u2225+\u2225TF(\u03f5)\u2225.(9)\nHere, the first term, denoted as E TUCKER :=\u2225x\u2212TF(x)\u2225\nrepresents the error introduced by not capturing the full\nvariance in each mode of the data through the Tucker\ndecomposition. The second term \u2225TF(\u03f5)\u2225represents the\nerror caused by the original noise on xremained after\nthe denoising process. E TUCKER can be bounded further by\n((De Lathauwer, De Moor, and Vandewalle 2000) Prop-\nerty 10; (Hackbusch 2012) Theorem 10.2): E TUCKER \u2264\nPN\nn=1PIn\nin=rn+1\u0010\n\u03c3(n)\nin\u00112\n, where {\u03c3(n)\nin}In\nin=1is the singu-\nlar values of the mode- nunfolding of the tensor X.\n3 Method\nThis section provides theoretical results on different aspects\nof Markov-based purification (7) and the details for our pro-\nposed adversarial purification algorithm LoRID.\n\u2022 Subsect. 3.1 provides Theorem 1 about the theoretical re-\nmoval of the adversarial noise as the diffusion time-step\ntin the Markov-based diffusion model increases at the\ndistribution-level. It is the counterpart of Theorem 3.1\nin (Nie et al. 2022) for Score-based purification.\u2022 The purification error between the clean and the purified\nimages at the sample-level are further characterized in\nTheorem 2 and 3 in Subsect. 3.1. While Theorem 3 can\nbe viewed as an adaptation of Theorem 3.2 from Score-\nbased to Markov-based purification, to the best of our\nknowledge, the lower bound on the reconstruction error\nin Theorem 2 has not been previously established for any\ndiffusion-based purification methods.\n\u2022 Subsect. 3.2 demonstrates how we realize our theoret-\nical analysis into practical measurement. Particularly,\nWe analyze the intrinsic purification error arising from\nthe Markov-based purification process (Corollary 1) and\nidentify the advantage of looping the early time-steps\nof the diffusion models for the purification task (Theo-\nrem 4). The result suggests that, with the same effective\ndiffusion-denoising steps, looping can reduce the intrin-\nsic purification error significantly.\n\u2022 Subsect. 3.2 also studies and validates the usage of\nTucker Decomposition combined with Markov-based\npurification at the high-noise regime (Theorem 5).\n\u2022 Based on the theoretical analysis, we design LoRID, the\nLow-Rank Iterative Diffusion method to purify adversar-\nial noise. Its description is provided in Subsect. 3.3.\n3.1 Markov-based Purification\nIntuitively, the diffusion time-step tneed be large enough\nto remove adversarial perturbations; however, the image\u2019s\nsemantics will also be removed as tincreases. That obser-\nvation is captured in the following Theorem 1, which states\nthat the KL-divergence between the distributions of the clean\nimages and the adversarial images converges as tincreases:\nTheorem 1. Letn\nx(i)\nto\nt\u2208{0,...,T}, i\u2208 {1,2}be two dif-\nfusion processes given by the forward equation (1) of a\nDDPM. Denote q(1)\ntandq(2)\ntthe distributions of x(1)\ntand\nx(2)\nt, respectively. Then, for all t\u2208 {0, ..., T \u22121}, we have\nDKL\u0010\nq(1)\nt||q(2)\nt\u0011\n\u2265DKL\u0010\nq(1)\nt+1||q(2)\nt+1\u0011\nSketch of proof (proof in Appx A.1). While Theo-\nrem 1 resembles that stated for the Score-based pu-\nrification (Nie et al. 2022), its proof is greatly differ-\nent since the DDPM\u2019s diffusion is not controlled by an\nStochastic Differential Equation. Instead, we leverage the\nunderlying Markov process governing the forward dif-\nfusion of DDPM (1), and show DKL\u0010\nq(1)\nt+1||q(2)\nt+1\u0011\n+\nDKL\u0000\nq(1)(xt|xt+1)||q(2)(xt|xt+1)\u0001\n=DKL\u0010\nq(1)\nt||q(2)\nt\u0011\nby expanding the KL-divergence between q(1)(xt+1,xt)\nandq(2)(xt+1,xt). Then, due to the non-negativity of the\nKL-divergence, we have the Theorem.\nNote that Theorem 1 captures the purification at the dis-\ntribution level. Similar to the Score-based purification (Nie\net al. 2022), we are also interested in the purification of the\nDDPM at the instance level. In fact, the variational bound\n(Eq. 6) suggests that the reconstruction error \u2225\u02c6x0(t)\u2212x0\u2225\nis directly proportional to the DDPM\u2019s training objective.However, that objective, L(\u03b8), is for all time-steps, while\nthe purification error only depends on the one time-step, at\nwhich, the reverse process is applied to recover \u02c6x0(t). In-\ntuitively, as tincreases, the argument of the approximator\u221a\u00af\u03b1tx0+\u221a1\u2212\u00af\u03b1t\u03f5contains less information about the noise\n\u03f5, thus, results in a higher error. The following two Theo-\nrems formalize that intuition:\nTheorem 2. Let{xt}t\u2208{0,...,T}be a diffusion process de-\nfined by the forward equation (1) where x0is the adversarial\nsample. i.e, x0=xclean +\u03f5a. For any time t, we have\nE[\u2225\u02c6x0(t)\u2212xclean\u2225]\u2265MMSE\u0012\u00af\u03b1t\n1\u2212\u00af\u03b1t\u0013\n\u2212 \u2225\u03f5a\u2225(10)\nwhere the expectation is taken over the distribution of\nxclean andMMSE (SNR)is the minimum mean-square er-\nror achievable by optimal estimation of the input given\nthe output of Gaussian channel with a signal-to-noise ra-\ntio of SNR . The function MMSE (SNR)has the following\nform (Guo, Shamai, and Verdu 2005):\n1\u22121\u221a\n2\u03c0Z\u221e\n\u2212\u221ee\u2212y2/2tanh\u0010\nSNR\u2212\u221a\nSNRy\u0011\ndy.(11)\nTheorem 3. Additionally to the conditions stated in Theo-\nrem 2, if the DDPM is able to recover the original signal x0\nwithin an error \u03b4DDPM (t)in the expectation, i.e., for all t,\nE[\u2225\u02c6x0(t)\u2212x0\u2225]\u2264E[\u2225\u02c6x\u2217\n0(yt)\u2212x0\u2225] +\u03b4DDPM (t)(12)\nwhere \u02c6x\u2217\n0(yt)is the best estimator of x0given yt=\n(\u221a\u00af\u03b1t/\u221a1\u2212\u00af\u03b1t)x0+\u03f50, then, we have the reconstructed\nerrorE[\u2225\u02c6x0(t)\u2212xclean\u2225]is upper-bounded by:\nMMSE\u0012\u00af\u03b1t\n1\u2212\u00af\u03b1t\u0013\n+\u03b4DDPM (t) +\u2225\u03f5a\u2225 (13)\nSketch of proofs (proofs in Appx A.2 and A.3). The proofs\nof both theorems consider the forwarding diffusion of the\nDDPM as a Gaussian channel, and the purification task is\nequivalent to the reconstruction of the channel\u2019s input. Given\na purification time-step t, i.e., the time-step we decide to\nstart the denoising/purification process, the equivalent Gaus-\nsian channel would have an effective signal-to-noise (SNR)\nratio of \u00af\u03b1t/(1\u2212\u00af\u03b1t). Intuitively, the higher the time-step, the\nsmaller the value of \u00af\u03b1t/(1\u2212\u00af\u03b1t), and, even with an optimal\ndenoiser, the more inherent error are introduced to the pu-\nrified sample. In fact, the expression MMSE (\u00af\u03b1t/(1\u2212\u00af\u03b1t))\nappearing in both theorems capture that intrinsic error. Un-\nfortunately, there is currently no closed-form for that ex-\npression. We follow previous work studying noisy Gaussian\nchannel (Guo, Shamai, and Verdu 2005) and provide its in-\ntegral form in expression (11).\nRemark. Regarding \u03b4DDPM (t), it captures how well the\ntrained-DDPM can recover the input given its noisy sig-\nnal at time-step t. The assumption that \u03b4DDPM (t)bounds\nthe reconstruction error (12) is a weaker version of the as-\nsumption made by (Song et al. 2021) in the analysis of the\nScore-based diffusion, which is also utilized to upper-bound\nthe error induced by Score-based purification (Ho, Jain,\nand Abbeel 2020). In fact, both works assume the Score-\nbased diffusion model can perfectly learn the score function\n\u2207xlogp(x0)to establish their theoretical results.Figure 3: Illustration of adversarial purification using\nDDPM. The adversarial samples (left) is purified from the\ntime-step t= 200 (bottom) and t= 500 (top) to recover the\noriginal samples (right). The middles show \u02c6xt(Equation (7))\nobtained by iteratively denoising to the indicated intermedi-\nate time-steps. The top purification with a too large time-step\ninduces unavoidable error (Theorem 2).\nTo conclude this subsection, we illustrate the impact of\nthe time-step ton the purification process based on DDPM\nin Fig. 3. When tis chosen appropriately, all the terms on the\nright-hand-side of (13) of Theorem 3 are controlled, which\nenforces a small difference between the clean input and and\nrecovered signal. This means not only the adversarial noises\nare removed but also the purified images maintain the se-\nmantic of the original data. This is reflected in the purified\nimages at the bottom of Fig. 3) However, when tis too\nlarge as illustrated at the top of Fig. 3, the diffusion-based\npurification induces an intrinsic error reflected in the term\nMMSE (\u00af\u03b1t/(1\u2212\u00af\u03b1t))of Theorem 2. This error makes the\npurified images inevitably different from the original signal.\n3.2 Controlling Purification Error\nThis subsection studies the inherent error introduced by the\npurification process and demonstrates why it instigates a bet-\nter purification scheme based on looping the early stage of\nthe DDPM and the utilization of Tucker decomposition.\nOur analysis starts with the consideration of the trivial\ncase in which there is no adversarial noise. By combining\nthe two Theorems 2 and 3, we have the following corollary:\nCorollary 1. Given the assumptions in Theorem 3, the in-\ntrinsic purification error on a clean purification input x0=\nxclean , i.e.,\u03f5a= 0, is bounded by\nMMSE\u0012\u00af\u03b1t\n1\u2212\u00af\u03b1t\u0013\n\u2264E[\u2225\u02c6x0(t)\u2212xclean\u2225]\n\u2264MMSE\u0012\u00af\u03b1t\n1\u2212\u00af\u03b1t\u0013\n+\u03b4DDPM (t) (14)\nThe corollary reflects the strong connection between the\npurification error and the MMSE term. Especially, when the\nDDPM is well-trained, the gap \u03b4DDPM (t)between the lower\nand upper bounds becomes small, and E[\u2225\u02c6x0(t)\u2212xclean\u2225]\nbecomes more similar to MMSE (\u00af\u03b1t/(1\u2212\u00af\u03b1t)). This obser-\nvation motivates us to investigate purification schemes that\nminimize the MMSE.\nLooping at early time-steps. Several recent work ob-\nserved that repetitive usage of the diffusion-denoising steps\nin parallel (Wang et al. 2022; Nie et al. 2022) or sequential\n(Lee and Kim 2023) can enhance system robustness againstadversarial attacks. However, too many diffusion-denoising\ncalls would not only diminish robustness gain but also de-\ngrade the clean accuracy significantly. Hence, we tackle the\nfollowing question: Given a fixed number of denoiser\u2019s call,\ni.e., total number of diffusion-denoising steps, for the sake of\nadversarial purification, should we diffusion-denoising mul-\ntiple loops of the DDPM at the earlier time-steps or utilize\na few loops with large time-steps?\nWe now provide theoretical justification for the usage of\nmultiple loops in purification. Specifically, we want to com-\npare the purification to the time-step t, i.e., denoted by rt\u25e6ft,\nand the purification of Lloops to the time-step t/L, i.e.,\n(rt/L\u25e6ft/L)L. By denoting the output of ltimes DDPM-\npurification to time-step t,\u02c6xl\n0(t) := ( rt/L\u25e6ft/L)l(x0), we\nformalize the impact of looping purification via the follow-\ning Theorem 4:\nTheorem 4. Let{xt}T\nt=0be a diffusion process defined by\nthe forward (1) where x0is the adversarial sample. i.e, x0=\nxclean +\u03f5a. For any given time t, we have the reconstructed\nerrorE\u0002\n\u2225\u02c6xL\n0(t/L)\u2212xclean\u2225\u0003\nis upper-bouned by:\nL\u00d7\u0012\nMMSE\u0012\u00af\u03b1t/L\n1\u2212\u00af\u03b1t/L\u0013\n+\u03b4DDPM\u0012t\nL\u0013\u0013\n+\u2225\u03f5a\u2225 (15)\nwhere the expectation is taken over the distribution of xclean\n(Proof in Appx. A.4).\nNote that the upper-bound on the recon-\nstruction error of (rt/L\u25e6ft/L)Lis controlled\nby L\u00d7 MMSE\u0000\n\u00af\u03b1t/L/(1\u2212\u00af\u03b1t/L)\u0001\n, instead of\nMMSE (\u00af\u03b1t/(1\u2212\u00af\u03b1t))as in the vanilla purification scheme\nrt\u25e6ft. For an illustration of the impact of looping the\ndiffusion-denoising, we consider the input to compute the\nMMSE as standard Gaussian. The MMSE is then given by\nMMSE (SNR) = 1 /(1 + SNR)(instead of the integral form\n(11)). We further take the values of \u00af\u03b1tin DDPM (Ho, Jain,\nand Abbeel 2020) and plot L\u00d7MMSE\u0000\n\u00af\u03b1t/L/(1\u2212\u00af\u03b1t/L)\u0001\nas a function of Lin Fig. 1. The result shows that purifica-\ntion at a small time-step with a large number of iteration is\ngreatly beneficial for the purification error.\nTucker Decomposition for High-noise Regime. We now\nstudy the utilization of DDPM and Tucker Decomposition to\npurify the adversarial samples, which is characterized by the\noperations rt\u25e6ftand TF =T\u22121\u25e6d\u22121\u25e6d\u25e6T. From the\nprevious analysis, the reconstruction error induced by the\ntwo methods are bounded by:\nMSE rt\u25e6ft(\u03f5a)\u2264MMSE\u0012\u00af\u03b1t\n1\u2212\u00af\u03b1t\u0013\n+\u03b4DDPM(t) +\u2225\u03f5a\u2225(16)\nMSE TF(\u03f5a)\u2264ETUCKER +\u2225TF(\u03f5a)\u2225 (17)\nwhere (16) is from Theorem 3 and (17) is from (9). Here,\nMSE rt\u25e6ft(\u03f5a)and MSE TF(\u03f5a)denote the reconstruction\nerror of the rt\u25e6ftand TF purification schemes (stated in (13)\nand (9), respectively). We now provide the upper-bounds of\nan integration of Tucker Decomposition into DDPM purifi-\ncation in the following Theorem 5.\nTheorem 5. The reconstruction errors introduced of the pu-rification rt\u25e6ft\u25e6TFis bounded by:\nMSE rt\u25e6ft\u25e6TF(\u03f5a)\u2264MMSE\u0012\u00af\u03b1t\n1\u2212\u00af\u03b1t\u0013\n+\u03b4DDPM (t)\n+ETUCKER +\u2225TF(\u03f5a)\u2225 (18)\n(Proof in Appx. A.5)\nIntuitively, comparing to the purification rt\u25e6ft, this pu-\nrification process rt\u25e6ft\u25e6TF have a better upper bound\nwhen the Tucker Decomposition can reduce the adversarial\nnoise before forwarding the signal to the DDPM, i.e., when\nETUCKER +\u2225TF(\u03f5a)\u2225<\u2225\u03f5a\u2225, which suggests the usage of\nTucker Decomposition at a high-adversarial-noise regime.\n3.3 LoRID: Low-Rank Iterative Diffusion for\nAdversarial Purification\nBased on the above analysis, we propose LoRID, Low-\nRank Iterative Diffusion algorithm for adversarial furifica-\ntion. Generally, LoRID consists of two major steps: Tensor\nfactorization, and diffision-denoising. So far, our manuscript\nhas considered four different configurations of LoRID, de-\npending on the usage of looping and on how the TF and\ndiffusions are coupled: Tensor-factorization TF, diffusion-\ndenoising rt\u25e6ft, looping (rt/L\u25e6ft/L)L, and Tensor-\nfactorization with diffusion-denoising TF \u25e6rt\u25e6ft. How-\never, the default configuration that we refer to with LoRID\nwould utilize both Tucker Decomposition (step 1) and mul-\ntiple loops of diffusion-denoising (step 2), which can be de-\nscribed by the expression TF \u25e6(rt/L\u25e6ft/L)L. The pseudo-\ncode of LoRID is described in Appendix. B.5.\n4 Experiments\nThis section is about our experimental setting and robustness\nresults: Subsect. 4.1 highlights the experimental settings and\nSubsect. 4.2 reports our experimental results.\n4.1 Experimental Setting\nDatasets and attacked architectures. We evalu-\nate LoRID on CIFAR-10/100 (Rabanser, Shchur, and\nG\u00a8unnemann 2017), CelebA-HQ (Karras et al. 2018), and\nImageNet (Deng et al. 2009). Comparisons are made against\nSOTA defense methods reported by RobustBench (Croce\net al. 2021) on CIFAR-10 and ImageNet, and against\nDiffPure (Nie et al. 2022), a score-based diffusion purifier,\non CIFAR-10, ImageNet, and CelebA-HQ. We use the\nstandard WideResNet (Zagoruyko and Komodakis 2017)\narchitecture for classification, evaluating defenses using\nstandard accuracy (pre-perturbation) and robust accuracy\n(post-perturbation). When the gradients is not needed\n(black-box setting) in CIFAR-10, all methods are evaluated\n10000 test images. On the other hand, due to the high com-\nputational cost of computing gradients for adaptive attacks\nagainst diffusion-based defenses, we assess the methods\non a fixed subset of 512 randomly sampled test images,\nconsistent with previous studies (Nie et al. 2022; Lee and\nKim 2023). Further experimental details are provided in\nAppx. B with EOT=20.Attacker settings. We consider two common threat mod-\nels: black-box and white-box. In both scenarios, the attacker\nhas full knowledge of the classifier. However, only in the\nwhite-box setting, the attacker also knows about the purifi-\ncation scheme.1For black-box, we adapt (Nie et al. 2022;\nLee and Kim 2023) and evaluate defense methods against\nAutoAttack (Croce and Hein 2020) in CIFAR-10/100 and\nBPDA+EOT (Ferrari et al. 2023) in CelebA-HQ. For white-\nbox, we also follow the literature and consider AutoAttack\nand PGD+EOT (Zimmermann 2019).\nHowever, white-box attacks require gradient backpropa-\ngation through the diffusion-denoising path, causing mem-\nory usage to increase linearly with diffusion step t. This\nmakes exact gradient attacks infeasible on larger datasets\nlike CelebA-HQ and ImageNet (Kang, Song, and Li 2024).\nTherefore, all existing work rely on some approximations\nof the gradients to conduct white-box attacks on those\ndataset (Nie et al. 2022; Lee and Kim 2023).2To the best\nof our knowledge, The strongest approximation to date is\nthesurrogate method (Lee and Kim 2023), which denoises\nnoisy signals using fewer denoising steps (Song, Meng, and\nErmon 2020). This approach reduces the number of denoiser\ncalls while effectively simulating the original process (de-\ntails in Appendix B.4). In summary, we use exact gradients\nfor CIFAR-10 and the surrogate method for CelebA-HQ and\nImageNet in our white-box attacks.\nLoRID settings. LoRID requires the specification of both\nthe time-step tand the looping number L, which are cru-\ncial for its iterative process. These hyperparameters are gen-\nerally selected by evaluating the classifier\u2019s performance on\nthe clean dataset, with tandLchosen to maintain acceptable\nclean accuracy. Further details on this parameter selection\nprocess are provided in Appx. B.6. We report those param-\neters as a tuple (t, L)next to the name of our method. Ad-\nditionally, obtaining an accurate Tucker decomposition for\nlarge datasets can be computationally intensive. Therefore,\nin such cases, LoRID is applied solely with Markov-based\npurification. In our results, the use of Tucker decomposition\nis denoted by TF next to the method\u2019s name, e.g. (TF, t, L).\n4.2 Robustness Results\nWe compare LoRID with the SOTA adversarial train-\ning methods documented by RobustBench (Croce et al.\n2021), as well as leading adversarial purification techniques,\nagainst strong L\u221eandL2attacks.\nCIFAR-10. Tables 2 and 3 show the defense\u2019s perfor-\nmance under L\u221e(\u03f5= 8/255) andL2(\u03f5= 0.5)AutoAt-\ntack on CIFAR-10. Our method achieves significant im-\n1In our white-box setting, the attacker is aware of both tand\nLin our LoRID framework and can fully backpropagate through\nthe DDPM, making this scenario even stronger than the white-box\nassumption used by Lee and Kim (2023).\n2While the adjoint (Nie et al. 2022) against the Score-based\npurification is claimed to be exact, it relies on underlying numer-\nical solvers and they can introduce significant error. We observe\nthat using adjoint-gradients results in significantly weaker attack\nthan using surrogate, which is also observed and reported by Kang,\nSong, and Li (2024); Lee and Kim (2023).Table 2: Standard accuracy and robust accuracy against Au-\ntoAttack L\u221e(\u03f5= 8/255) on CIFAR-10. * indicates the\nusage of extra data. The gray and white boxes indicate the\nblack-box and white-box attacks.\nMethod Standard Acc Robust Acc\nWideResNet-28-10\n(Zhang et al. 2020)* 89.36 59.96\n(Wu, Xia, and Wang 2020)* 88.25 62.11\n(Gowal et al. 2020)* 89.48 62.70\n(Wu, Xia, and Wang 2020) 85.36 59.18\n(Rebuffi et al. 2021) 87.33 61.72\n(Gowal et al. 2021) 87.50 65.24\nLoRID (39,5) 90.41 88.39\n(Wang et al. 2022) 85.66 33.48\n(Nie et al. 2022) 89.02 46.88\nLoRID (20,24) 84.20 59.14\nWideResNet-70-16\n(Gowal et al. 2020)* 91.10 66.02\n(Rebuffi et al. 2021)* 92.23 68.56\n(Gowal et al. 2020) 85.29 59.57\n(Rebuffi et al. 2021) 88.54 64.46\n(Gowal et al. 2021) 88.74 66.60\nLoRID (50,10) 85.30 69.34\nLoRID (60,10) 85.10 70.87\n(Wang et al. 2022) 86.76 37.11\n(Nie et al. 2022) 90.07 45.31\nLoRID (25,20) 84.60 66.40\nLoRID (10,40) 86.90 59.20\nTable 3: Standard accuracy and robust accuracy against Au-\ntoAttack L2(\u03f5= 0.5) on CIFAR-10. * indicates the usage of\nextra data. The gray and white boxes indicate the black-box\nand white-box attacks.\nMethod Standard Acc Robust Acc\nWideResNet-28-10\n(Pang et al. 2022)* 90.83 78.10\n(Rebuffi et al. 2021)* 91.79 78.69\n(Wang et al. 2023)* 95.16 83.68\nLoRID (39,4) 90.34 89.69\n(Wang et al. 2022) 85.66 73.32\n(Nie et al. 2022) 91.03 64.06\nLoRID (15,30) 85.4 77.9\nLoRID (20,24) 84.2 73.6\nprovements in both standard and robust accuracy compared\nto previous SOTA in both black-box and white-box set-\ntings. Particularlly, LoRID improves black-box robust ac-\ncuracy by 23.15% on WideResNet-28-10 and by 4.27%\non WideResNet-70-16. Additionally, our method surpasses\nbaseline robust accuracy in the white-box by 12.26% on\nWideResNet-28-10 and by 21.09% on WideResNet-70-16.\nImageNet. Table 4 shows the robustness performance\nagainst L\u221e(\u03f5= 4/255) AutoAttack on WideResNet-28-\n10. Our method significantly outperforms SOTA baselines\nin both standard and robust accuracies.\nCelebA-HQ. For large datasets like CelebA-HQ, attackers\noften use the BPDA+EOT attack (Tramer et al. 2020; Hill,\nMitchell, and Zhu 2021), which substitutes exact gradients\nwith classifier gradients. We evaluated our approach againstTable 4: Standard accuracy and robust accuracy against\nwhite-box PGD+EOT L\u221e(\u03f5= 4/255) on ImageNet.\nMethod Standard Acc Robust Acc\nWideResNet-28-10\n(Wong, Rice, and Kolter 2020) 53.83 28.04\n(Engstrom et al. 2019) 62.42 33.20\n(Salman et al. 202"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**Title: OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering**  \n**Authors: Jiahao Nick Li, Zhuohao (Jerry) Zhang, Jiaju Ma**\n\n**1. Abstract:**\nOmniQuery is a novel system that answers complex personal memory-related questions by augmenting captured multimodal memories through the integration of scattered contextual details. This augmentation enables the retrieval of relevant memories and employs a large language model (LLM) to generate detailed answers. In evaluations, OmniQuery demonstrated an accuracy of 71.5%, outperforming a conventional Retrieval-Augmented Generation (RAG) system by winning or tying 74.5% of the time.\n\n**2. Introduction:**\nPeople capture memories through photos, screenshots, and videos, forming subsets of their episodic memory. However, these captured memories often lack the contextual details needed to answer complex personal questions. While current AI tools can query this data, they struggle with interpreting interconnected events. OmniQuery addresses this gap. The authors conducted a one-month diary study, collected realistic user queries, developed a taxonomy of necessary contextual information, and introduced OmniQuery to integrate these contexts for answering complex queries.\n\n**3. Related Work:**\nThe related work spans several areas:\n- **Personal Memory Augmentation:** Previous research includes tools for reminders and proactive task support, focusing on data captured via wearable devices. OmniQuery differentiates by exploiting already captured memory data.\n- **Multimodal Question Answering:** Advances in LLMs and multimodal models have enabled questions to be answered across different data types. OmniQuery builds on this by using a taxonomy-based approach rather than solely data-driven methods.\n- **Applications Utilizing Contextual Information:** AI techniques now enable extracting and integrating context from raw multimodal information to enhance memory databases and answer personal questions.\n\n**4. Method:**\nOmniQuery has two main components:\n- **Memory Augmentation Pipeline:**\n  - **Step 1:** Structuring individual memories by processing content (captions, visible text, etc.) using multimodal models and annotating atomic contexts.\n  - **Step 2:** Identifying composite contexts through a sliding window approach. The annotated memories within a window are processed by an LLM to identify and synthesize composite contexts. \n  - **Step 3:** Inferring semantic knowledge from structured memories and identified composite contexts.\n\n- **Question Answering System:**\n  - **Query Augmentation:** The input query is rewritten into a declarative format, decomposed to extract contextual filters, and inferred for related contexts using the taxonomy.\n  - **Retrieval:** The system retrieves relevant memories and knowledge by searching processed content, annotated atomic contexts, and composite contexts.\n  - **Answer Generation:** The retrieved results and the augmented query are sent to an LLM which generates the final answer, including reference memories.\n\n**Mathematics and Algorithms:**\nOmniQuery employs:\n- Text embeddings for vector-based similarity search.\n- Few-shot prompting technique for LLMs to identify composite contexts.\n- Chain-of-thought prompting for the answer generation.\n\n**5. Experimental Setup:**\n- **Datasets:** A researcher's personal data spanning mid-March to mid-August, consisting of 2,590 images and videos.\n- **Hardware and Software:** Implemented custom vector databases and storage for captured memories. Tools like Google Cloud Vision API, OpenAI\u2019s Whisper model, and OpenAI's text-embedding models were utilized.\n- **Evaluation Metrics:** User evaluation metrics revolved around perception of correctness and completeness in blind tests against a RAG-based baseline.\n- **Baseline:** Compared against generic RAG-based systems, known for handling external databases by retrieval augmented generation.\n\n**6. Results and Analysis:**\n- OmniQuery achieved an accuracy of 71.5%, significantly higher than the baseline's 43.9%, showing OmniQuery's effectiveness in comprehensively answering queries.\n- It outperformed the RAG baseline in head-to-head comparisons, winning or tying in 74.5% of the queries based on user-perceived correctness.\n- User evaluations demonstrated that the integration of atomic and composite contexts significantly improves the system\u2019s performance in providing detailed and accurate answers.\n\n**7. Discussion:**\n- The study confirms OmniQuery's ability to handle complex memory-related questions by leveraging multimodal memory augmentation.\n- The effectiveness of contextually enriched retrieval was validated, demonstrating higher user satisfaction.\n- The limitations include the subjective nature of some evaluations and the dependency on high-quality captured memories for reliable context extraction.\n- Future research could explore broader uses, refine emotion analysis, and extend the system to more diverse datasets.\n\n**8. Conclusion:**\nOmniQuery makes significant contributions to personal memory-based question answering by:\n1. Developing a detailed taxonomy of contextual information.\n2. Creating a pipeline for augmenting memories with contextual details.\n3. Implementing a robust query-answering system grounded in RAG principles.\nFuture directions suggest large-scale testing and enhancing the system\u2019s ability to interpret emotional contexts and broaden its application into other domains.\n"}, {"role": "assistant", "content": "OmniQuery: Contextually Augmenting Captured Multimodal\nMemory to Enable Personal Question Answering\nJiahao Nick Li\nUCLA\nLos Angeles, USA\nljhnick@ucla.eduZhuohao (Jerry) Zhang\nUniversity of Washington\nSeattle, USA\nzhuohao@uw.eduJiaju Ma\nStanford University\nPalo Alto, USA\njiajuma@stanford.edu\nCaptur ed Memor y b\n08:18 AM,\u00a0 May 13, 2024\nConv ention center , Honolulu\n\"Sign displa ying\nconf erence WiFi details\nfor CHI 2024\"10:52 PM,\u00a0 May 15, 2024\nHonolulu\n\"A liv ely e vent with a DJ\nperformance at the\nGerman HCI e vent\"09:32 PM,\u00a0 May 16, 2024\nMaui Br ewing Company , Honolulu\n\"A liv ely par ty scene with\npeople dancing\"\n11:04 AM,\u00a0 May 16, 2024\nConv ention center , Honolulu\n\"A conf erence r oom with\nrows of empty chairs and\na few people on stage. \"Example period:\u00a0\n05/11-05/17Identify and inf er\u00a0semantic contexts via\nintegr ating multiple r elated captur ed memories\nContextual A ugmentation c\n\"CHI 2024 t ook place fr om 5/13-5/16  accor ding t o\nmultiple memories mentioning the conf erence. \"\n\"I attended German HCI e vent on May 15th .\"\n\"I attended a par ty at Maui Br ewing\nCompany  on May 16th .\"\"You attended a German HCI\nevent and a par ty at Maui\nBrewing Company  during CHI\"Answer Gener ation d\nReference :\"What social e vents  did I\nattend during CHI 2024 ?\"Personal Question a9/12/24, 3:12 AM teaser.svg\n\ufb01le:///Users/jiahaoli/Library/CloudStorage/Dropbox/02_Career/Projects/OmniQuery/chi_2025/svg/teaser.svg 1/1\nFigure 1: OmniQuery enables answering natural language personal questions (a) on individuals\u2019 captured memories such as\ncaptured photos, saved screenshots and recorded videos (b). This is achieved by augmenting the captured memories through\nidentifying and integrating scattered contextual information from multiple interconnected memories (c). Using the augmented\ncontextual information, OmniQuery retrieves relevant memories, and leverages an LLM to generate a comprehensive answer\nand the reference memories (d).\nAbstract\nPeople often capture memories through photos, screenshots, and\nvideos. While existing AI-based tools enable querying this data us-\ning natural language, they mostly only support retrieving individual\npieces of information like certain objects in photos, and struggle\nwith answering more complex queries that involve interpreting\ninterconnected memories like event sequences. We conducted a one-\nmonth diary study to collect realistic user queries and generated\na taxonomy of necessary contextual information for integrating\nwith captured memories. We then introduce OmniQuery, a novel\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference\u201917, July 2017, Washington, DC, USA\n\u00a92018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXXsystem that is able to answer complex personal memory-related\nquestions that require extracting and inferring contextual infor-\nmation. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple inter-\nconnected memories, retrieves relevant memories, and uses a large\nlanguage model (LLM) to comprehensive answers. In human evalu-\nations, we show the effectiveness of OmniQuery with an accuracy\nof 71.5% and it outperformed a conventional RAG system, winning\nor tying in 74.5% of the time.\nCCS Concepts\n\u2022Human-centered computing \u2192Human computer interac-\ntion (HCI) ;Natural language interfaces ; User studies.\nKeywords\npersonal memory, contextual augmentation, diary study, multi-\nmodal question answering, RAGarXiv:2409.08250v1  [cs.HC]  12 Sep 2024Conference\u201917, July 2017, Washington, DC, USA Li et al.\nACM Reference Format:\nJiahao Nick Li, Zhuohao (Jerry) Zhang, and Jiaju Ma. 2018. OmniQuery:\nContextually Augmenting Captured Multimodal Memory to Enable Personal\nQuestion Answering. In .ACM, New York, NY, USA, 18 pages. https://doi.\norg/XXXXXXX.XXXXXXX\n1 INTRODUCTION\nPeople often record their everyday life by taking photos, screen-\nshots, and videos, whether for saving important information, docu-\nmenting special occasions, or simply capturing a funny moment [ 36].\nThese captured instances, referred to as captured memories , col-\nlectively represent subsets of an individual\u2019s episodic memories [56],\na type of long-term memory that contains both specific past expe-\nriences and associated contextual details. These episodic memories\nare essential for answering memory-related personal questions\nlike, \" What social events did I attend during CHI 2024? \" (Figure 1a),\nwhich can help users reflect on past experiences and make informed\ndecisions in daily tasks.\nHowever, these raw captured memories by themselves are in-\nsufficient to answer personal questions, as they lack contextual\ndetails that are typically implicit and scattered across multiple\npieces of data. For examples, as shown in Figure 1b, memories of\nattending parties during CHI 2024 are not explicitly annotated as\noccurring during the event. Answering such personal questions\nrequires extracting and integrating contextual information not typ-\nically contained within a single captured instance. For example, by\nintegrating multiple memories that mention \u201cCHI 2024\u201d in their\ncontent and extracting their metadata, it is possible to determine\nwhen the users attended the conference and connect related social\nevents memories from that period to CHI 2024 (Figure 1c), enabling\nthe answer of the query (Figure 1d).\nAdvancements in AI have enable question answering (QA) on\nlong documents [ 4,58], knowledge graph [ 28,62], multimodal\ndatabases [ 13,54], egocentric videos [ 27,43]. These methods typi-\ncally rely on data-driven approaches to train powerful models for\nthe target task. However, the private nature of captured memories\nmakes it difficult to curate large datasets, posing challenges for\ntraining models specifically for QA on personal captured mem-\nories. Recent LLM-based work has adopted retrieval augmented\ngeneration (RAG) workflow to handle external databases without\nspecific training [ 34]. However, such methods depend on explicit\nconnections between queries and relevant external data [ 16]. In\ncontrast, captured memories are often unstructured and lack con-\ntextual annotations, making it difficult to establish explicit links\nbetween queries and interconnected memories.\nTo achieve this, we propose OmniQuery, a novel approach de-\nsigned to robustly and comprehensively answer users\u2019 queries on\ntheir captured memories. OmniQuery has two key components:\n(i)a question-agnostic pipeline to augment captured memories\nwith contextual information extracted from other related mem-\nories to produce context-augmented memories, and (ii)a natural\nlanguage question answering system that retrieves these processed\nmemories and generates comprehensive answers with referenced\ncaptured memories as evidence. The processing pipeline in (i)is\ninformed by the taxonomy of contextual information that we gener-\nated from a one-month diary study with 29 participants. Specifically,\nwe collected and analyzed 299 user queries, identifying the types ofcontextual information that would be most useful for augmenting\ncapture memories. We identified three types of personal questions\n(direct content queries, contextual filters, and hybrid queries) and\ngenerated a taxonomy of three major categories (atomic context,\ncomposite context, and semantic knowledge) which then guide\nthe design of memory augmentation pipeline to enhance mem-\nory retrieval comprehensiveness. For (ii), OmniQuery employs a\nretrieval-augmented architecture: given a user input query, it (1)\naugments the query via a rewriting strategy, and (2) retrieves related\nmemories from the augmented data and uses an LLM to generate\nthe final answer, along with the reference memory instances.\nTo evaluate OmniQuery, we conducted a user evaluation with 10\nparticipants against a generic RAG-based baseline. The participants\ntested queries both logged during the diary study and generated dur-\ning the evaluation session on a subset of their own captured mem-\nories. For each tested query, participants rated the user perceived\ncorrectness and completeness of the answers generated by both\nsystems in a blinded manner. The results show that OmniQuery\neffectively answers different types of queries on users\u2019 personal\nmemories, outperforming the baseline with higher accuracy (71.5%,\nexceeding the baseline by 27.6%) and winning or tying 74.5% of\nthe time in direct comparisons, which are based on user-perceived\naccuracy and completeness.\nIn summary, we contribute:\n\u2022A taxonomy of contextual information for augmenting cap-\ntured memories, consisting of three major categories. The\ntaxonomy was derived from authentic queries collected in a\none-month diary study with 29 participants.\n\u2022A pipeline of augmenting captured memories that leverages\ntemporal-based reasoning to extract and infer missing con-\ntextual information from other related captured memories.\n\u2022The design and implementation of an end-to-end system\nthat comprehensively answers user queries.\n\u2022A user evaluation of OmniQuery against a baseline system,\nshowing OmniQuery\u2019s effectiveness with 71.5% accuracy\nand outperforming the baseline (winning or tying 74.5% of\nthe time).\n2 Related Work\nOmniQuery is inspired by and related to prior work in the areas on\npersonal memory augmentation, multimodal question answering\nand applications that utilize contextual information.\n2.1 Personal Memory Augmentation\nA large body of work in human-computer interaction (HCI) has\nbeen focused on augmenting users\u2019 memory. This includes devel-\noping reminder tools for elderlies or people with memory impair-\nments [ 8,31,32,52], providing proactive support in daily tasks [ 11,\n66], or manipulating users\u2019 memory focus in extended reality [ 5].\nThese work typically focus on the \u201ccapturing\u201d stage of the memory\naugmentation, where researchers develop wearable devices that\ncontinuously capture data using designated sensors, which record\nvarious modalities such as videos [ 15,24,44], audios [ 23,57], or\nbio-signals [ 10], to augment the memory database. For example,\nrecent work such as Memoro developed a wearable, audio-based\ndevice that continuously records users\u2019 conversations and enablesOmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering Conference\u201917, July 2017, Washington, DC, USA\nmemory suggestions in real-time, either through explicit queries or\nquery-less contextual cues [ 66]. Differently, OmniQuery focuses on\nthe \u201cpost-capturing\u201d stage, utilizing already-existing memory data\n(e.g., photos and videos users have already captured). It addresses\nchallenges in processing, annotating, and augmenting captured\nmemories with contextual information.\nPrior work in natural language processing (NLP), computer vi-\nsion (CV), and information retrieval (IR) has studies methods of\naugmenting people memory. Perhaps the most related is QA on\negocentric videos, which are also a form of personal data. Represen-\ntative tasks include episodic memory retrieval [ 17,21,22], where\nthe system, given a long egocentric video and a query, localizes\nthe answer within the video. However, these datasets differ from\nthe captured data targeted by OmniQuery. The main challenge in\negocentric videos is filtering through large, often noisy data, us-\ning data-driven approaches to train models for feature extraction.\nIn contrast, captured memories represent a smaller, intentionally\ncollected dataset, where the challenge lies in integrating scattered\ncontextual information across multiple implicitly related memories.\nTherefore, OmniQuery employs a taxonomy-based method to aug-\nment existing data without the need for specific model training,\nimproving QA performance.\n2.2 Multimodal Question Answering\nOver time, natural language QA research has shifted to more com-\nplex settings, including QA across different modalities (e.g., im-\nages [ 2,20], videos [ 41,60,64], tables [ 65] or knowledge graph [ 29,\n63]), QA on large datasets [ 12,34] and tasks that require multi-\nhop reasoning [ 45,61]. Recent advancements in large language\nmodels (LLMs) and multimodal foundation models (e.g., [ 37\u201339])\nhave have enabled improved reasoning and answer generation\nover large, multimodal datasets. This is similar to OmniQuery\u2019s\nuse case as answering personal questions requires handling large\namounts of captured memories and performing complex reason-\ning. Prior work has used retrieval-augmented generation (RAG)\nworkflow [ 34], which retrieve relevant information from external\ndatasets based on a query and then generate output using the re-\ntrieved results. For example, MuRAG leverages RAG to answer\nopen questions via retrieving related information from databases\nof images and text [ 13]. VideoAgent leverages structured memories\nprocessed from long videos to accomplish video understanding\ntasks [ 18]. However, these methods rely on datasets already rich\nin context (e.g., Wikipedia1) and improvements are often achieved\nby designing new retrieval workflows (e.g., Self-RAG [ 3] and tree-\nbased retrieval [ 50]) or query augmentation [ 9]. More recently,\nGraphRAG introduced a data augmentation approach that gener-\nates a knowledge graph from extensive raw data to tackle tasks\nrequiring higher-level understanding, such as query-focused sum-\nmarization [ 16]. While GraphRAG leverages data-driven methods to\nidentify themes and communities within graph nodes, OmniQuery\nextends this concept by adopting a taxonomy-based augmentation\napproach, informed by insights from a diary study, to enhance\nretrieval on personal captured memories.\n1https://www.wikipedia.org/2.3 Applications Utilizing Contextual\nInformation\nContextual information has long been important in HCI research\nfrom early mixed-initiative systems [ 26] to recent agentic work-\nflow [ 30]. Over the past few years, there has been a surge in the\nusage of AI and LLMs in the HCI community, which enables extract-\ning contextual information from processing raw multimodal infor-\nmation. For example, Li et al. studied how visually impaired people\ncook and emphasized the importance of conveying contextual in-\nformation to users through multimodal models [ 35]. Additionally,\nHuman I/O leverages egocentric perceptions of users and detect sit-\nuational impairments through reasoning on the multimodal sensing\ndata [ 40]. GazePointAR develops a context-aware voice assistant\nto disambiguate users\u2019 intent when interacting with real-world in-\nformation [ 33]. OmniActions categorizes digital follow-up actions\non real-world information and provides proactive action prediction\nbased on perceived context [ 36]. Specifically, these system utilized\noff-the-shelf multimodal models to process raw sensory data, and\nleverages the reasoning capabilities of LLMs to infer the semantic\ncontext. OmniQuery builds on this approach by applying these\nAI techniques to extract and integrate semantic context scattered\nacross various unstructured, raw captured memories. This augmen-\ntation enhances users\u2019 memory databases, enabling them to answer\npersonal questions about their memories through natural language\nqueries.\n3 DIARY STUDY\nWhile single captured memory often lacks essential contextual\ninformation, OmniQuery proposes to augment such memories by\nextracting and inferring semantic context from other explicitly\nor implicitly related memories. To understand how to effectively\naugment captured memories, we need to answer the following\nresearch question:\nRQ:What contextual information is essential to integrate with\ncaptured memory instances to ensure accurate retrieval in\nresponse to user queries?\nThis question is important as \u201ccontext\u201d is a broad term, and thus\nthe focus should be on categorizing and identifying the most effec-\ntive contextual information that enables accurate and meaningful\nresponses to the types of queries users generate when reflecting on\npast experiences.\n3.1 Method\nTo answer the research question above, we conducted a diary study,\na methodology that enables participants to log data whenever need\narose [ 53]. Specifically, we adopted the snippet-based technique\nproposed by Brandt et al. [6]. We asked participants to log queries\non their past memories only when they had real intent under a\ngenuine context, rather than brainstorming potential questions\nthey might ask to retrieve specific past memories. This approach\nenabled us to collect authentic and spontaneous queries that users\nhave in real-world scenarios.\nWe collect the data including: (i)the queries participants would\nuse to retrieve or ask about their past memories, (ii)the reasons\nand contexts prompting these queries (e.g., wanting to show a pastConference\u201917, July 2017, Washington, DC, USA Li et al.\nexperience while chatting with a friend) and (iii)(optional) whether\nthey were able to retrieve the corresponding memories from their\nalbum, and if so, how they did it (e.g., by scrolling through the\nphoto album).\n3.2 Participants\nThirty-two participants (i.e., 14 male, 17 female, one binary) were\ninitially recruited through an online RSVP form distributed via the X\nplatform2. Participants come from North America and Asia. Eleven\nparticipants reported using Android devices, while the remainder\nused iOS devices in their daily life. Additionally, 16 participants\nreported actively logging their daily lives, 13 regularly logged im-\nportant events and memorable experiences, 2 logged only essential\ninformation, and one seldom logged their lives. While participants\nwere compensated based on their participation ($50 for full partic-\nipation), they were not required to log a specific number queries\neach day or over the entire study period. This approach was inten-\ntional, as we did not want to pressure them into generating queries\nartificially.\n3.3 Data Summary and analysis\nDuring the diary study, one participant opted out during the first\nweek, and two participants did not log any queries throughout the\nentire study. Of the remaining participants, seven stopped logging\nafter the first week. The rest remained active until the end of the\nstudy. As a result, we collected a total of 299 queries. On average,\neach participant contributed 10.27 queries, with a standard devia-\ntion of 6.09, and the highest number of queries contributed by a\nsingle participant was 25.\nFrom the collected queries, we identified three major types of\nquery: (1) direct content queries (75 queries), (2) context-based\nfilters (28 queries), and (3) hybrid queries (191 queries). The rest five\nqueries fell outside these categories as participants were attempting\nnon memory-related tasks, such as \u201c Mark yesterday pictures as\nfavorites \u201d.\nDirect content queries :These queries aim to get direct answers\nthat can be retrieved by searching for memories via description\n(e.g., \u201c skateboarding in a tie-dye shirt \u201d) or rely on information ex-\nplicitly contained within a single captured instance (e.g., \u201c What is\nmy driver\u2019s license number? \u201d). Typically, this type of query does\nnotrequire additional context not contained in a single captured\nmemory.\nContextual filters :These queries focus on retrieving memories\nbased on specific contexts, such as time, location, or event. For\nexample, a query like \u201c All the photos in Hawaii \u201d might only require\nfiltering based on metadata like location. However, for more com-\nplex queries such as \u201c All the photos from my graduation ceremony \u201d,\nitdoes require a deeper synthesis of multiple interconnected mem-\nories to reconstruct the context surrounding the event.\nHybrid queries :These queries are more complex, combining\nboth direct content queries and contextual filters. For example, a\nparticipant asked \u201c Which meat did I order the last time I came to\nthis Japanese BBQ restaurant? \u201d Answering such a query typically\n2https://x.com/\n9/10/24, 1:20 AM context_count.svg\n\ufb01le:///Users/jiahaoli/Library/CloudStorage/Dropbox/02_Career/Projects/2024_OmniQuery/data_analysis/context_count.svg 1/1Figure 2: Number of appearance of each types of context\n(atomic and composite) in the logged queries. Note that a\nquery may contain multiple types of categories, such as\n\u201cWhat boba tea did I drink lastweek? \u201d\nrequires a multi-hop process: (1) filter all captured memories under\nthe specific context (e.g., dining in this Japanese restaurant); (2)\nanalyze the filtered data to generate the final result.\n3.4 Analysis\nInspired by the psychological memory theory [ 56], our data sum-\nmary indicates that 74.4% of the queries (contextual filters + hybrid\nqueries) require more than just querying the direct content. The\ncomplexity in these queries require integration of contextual infor-\nmation in captured memories for accurate processing and filtering.\nTherefore, we take a step further to build a taxonomy of contextual\ninformation in user queries to inform the design of OmniQuery.\nTo identify this essential contextual information, two researchers\non the team independently analyzed the logged queries, and coded,\nfiltered, and categorized the types of context required to filter cap-\ntured memories and better answer the queries. Their results were\ncompared, and discrepancies in categorizations, hierarchy, naming,\nand granularity were discussed and resolved.\n4 TAXONOMY OF CONTEXTUAL\nINFORMATION\nIn this section, we present the taxonomy built from analyzing user\nqueries. We identified three key types of contextual information\nthat can be integrated with captured memories. These include (1)\natomic context, (2) composite context and (3) semantic knowledge.\n4.1 Atomic Context\nAtomic context refers to contextual information typically obtain-\nable from a single captured memory. This includes data directly\nfrom metadata, sensed from visual and auditory content, or inferred\nfrom the content itself. Table 1 shows the seven types of atomic\ncontexts categorized from the queries. Among them, temporal in-\nformation and geographical info can be directly obtained from the\nmemory media\u2019s metadata. People and visual elements typically\nrequire machine learning models or facial recognition for detection.OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering Conference\u201917, July 2017, Washington, DC, USA\nTable 1: Categorization and examples of atomic and composite context\nCategory Definition Exemplar queries refers to contextual cues\nAtomic context\nTemporal info Specific time period or particular time of the day\u201cWhat boba tea did I drink lastweek? \u201d\n\u201cwhat is my routine inthemorn ing?\u201d\nGeographical info Location data such as city names or venue details \u201c How many churches did I visit inBarcelona? \u201d\nPeople Individuals present in the captured memories \u201cfind the photo of meandmygrandpa last year. \u201d\nVisual elementsOther directly sensible elements, including animals,\nphysical objects, or specific visual features\u201cMy photo with short hair last year. \u201d\n\u201cPhoto of mydog when he was a puppy. \u201d\nEnvironment Inferred environment based on the content \u201c Gym selfies from last year. \u201d\nActivities Actions or activities inferred from the content\u201cHow many cardiosession did I complete last\nmonth? \u201d\nEmotion Subjective emotion or emotional cues \u201c Myhappiestmoment last year \u201d\nComposite context\n- Combination of multiple atomic contexts \u201cWho did I ski with in thelabretreat last year \u201d\nEnvironment, activity, and emotion are more implicit and require\nreasoning based on the content (e.g., a photo of a menu may suggest\nthe person is in a restaurant). The number of appearance of each\ncategory is shown in Figure 2.\n4.2 Composite Context\nComposite context is how people often remember and refer to past\nexperiences with a single phrase, such as \" Who did I ski with in the\nlab retreat last year? \" These contexts can range from significant\nevents like a wedding or a conference trip to smaller incidents like\nhanging out with a friend or a day trip to Seattle. Specifically, com-\nposite context is defined as a combination of multiple atomic\ncontexts . For example, the composite context \u201clab retreat\u201d encom-\npasses atomic contexts including \u201cFeburary, 2024\u201d (temporal), \u201cLake\nTahoe, California\u201d (geographical) and \u201changing out with labmates\u201d\n(activity).\nWhile atomic context is typically available within a single cap-\ntured memory, composite context requires integrating multiple\nmemories to understand the connection between them. Since an\nindividual\u2019s captured memories are linear on the timeline, memo-\nries related to a specific event tend to cluster closely together. We\nleveraged this temporal proximity to identify and extract various\ncomposite contexts from the raw captured memories. For a detailed\ndiscussion of this approach, please refer to Section 5.2.\n4.3 Semantic Knowledge\nIn psychology theories, semantic knowledge refers to the general\nworld knowledge that humans accumulate over time [ 46,56], dis-\ntinct from episodic memories, which are tied to specific experiences\nand events. Similarly, we can generate semantic knowledge from a\nuser\u2019s captured memories, providing broader insights of the user\u2019s\npast experiences. For example, patterns like \u201cJason has a habit of\ngoing to the gym 3-4 times a week\u201d can be inferred from multiple\ncaptured memories. Such patterns are helpful in answering queriesthat not necessarily require specific knowledge such as \u201c How often\ndo I go to the gym in April? \u201d\n5 OMNIQUERY: AUGMENTING CAPTURED\nMEMORIES\nInformed by the generated taxonomy, OmniQuery employs a query-\nagnostic preprocessing pipeline to augment existing captured mem-\nories. The pipeline extracts scattered contextual information from\ninterconnected captured memories, synthesizes it, and augment\neach memory with the enhanced context. Specifically, the aug-\nmentation pipeline involves three steps (as shown in Figure 3):\n(1) structuring individual captured memories via processing their\ncontent and annotating with atomic contexts, (2) identifying and\nsynthesizing composite contexts from multiple captured memories\nusing sliding windows, and (3) inferring semantic knowledge from\nmultiple captured memories and the identified composite contexts.\n5.1 Step 1: Structuring Individual Captured\nMemories\nRaw captured memories are often unstructured and lack contextual\nannotation [ 51]. In this step, OmniQuery structures each captured\nmemory, making it easier to analyze and extract information. Fig-\nure 4 shows an example of structuring an single captured memory,\nwhich involves two key parts: (1) processing and understanding\nthe content of the memory and (2) annotating the memory with\natomic contexts.\nProcessing content .Content of a captured memory includes\nthe overall description of the memory as caption, visible text, and\ntranscribed speech transcribed speech (for videos, not shown in\nFigure 4). Specifically, OmniQuery leverages multimodal models to\nprocess and describe the captured memory as the caption, performs\noptical character recognition (OCR) to recognize visible text, and\nuses audio-to-text models to transcribe speech.Conference\u201917, July 2017, Washington, DC, USA Li et al.\nStep 1:  Structur e\u00a0individual  captur ed memoriesCaptur ed memories\nContent\n(caption, visible text)Atomic ContextsProcess AnnotateStep Sliding window\nComposite Contexts\nStep 2: Identify composite context Step 3: Infer semantic knowledgeSemantic KnowledgeLLM LLMTimeline\n9/12/24, 2:37 AM augmentation_v4.svg\n\ufb01le:///Users/jiahaoli/Library/CloudStorage/Dropbox/02_Career/Projects/OmniQuery/chi_2025/svg/augmentation_v4.svg 1/1\nFigure 3: Augmenting captured memories involves three steps: (1) structuring memories by processing content and annotating\nwith atomic contexts; (2) identifying composite context through sliding windows; (3) inferring semantic knowledge from the\nstructured memories and identified contexts.\nAnnotating atomic contexts .With the content processed, Om-\nniQuery annotates each captured memory with each type of atomic\ncontext. As shown in Figure 4b, OmniQuery extracts the temporal\nand geographical information from the metadata and uses multi-\nmodal models to detect people and other visual elements. Then\nOmniQuery synthesizes the processed information and infers the\nenvironment and activities. For example, based on a photo of a sign\ndisplaying conference Wi-Fi details, OmniQuery infers that the user\nis likely attending a conference (activity) and is at the conference\nvenue (environment). Note that due to the subjective matter of\nemotions, which often requires user input, emotion inference is\nexcluded in current implementation.\n5.1.1 Indexing. After each captured memory is structured, it is\nindexed and stored in a database. Additionally, the annotations in\ntextual format are encoded into text embeddings to enable vector-\nbased search during the retrieval process. In the database, each\ndata entry represents a data entry corresponding to a captured\nmemory, containing both the original media (e.g., photo, video) and\nits structured annotations in text and text embeddings.\n5.2 Step 2: Identifying Composite Context\nAs captured memories are recorded in a linear manner along a\npersonal timeline, those interconnected through semantic contexts\noften cluster closely together. For example, memories related to\nCHI 2024 are likely to occur during the event itself. Taking advan-\ntage of this temporal proximity , OmniQuery adopts a sliding\nwindow approach to analyze potentially interconnected captured\nin segments, identifying composite contexts.\nAs shown in Figure 5a, a static window size of seven days is\nused in current implementation. The inference is performed via\nan LLM, in which the input is the structured annotations of these\nmemories and the output is the identified composite contexts along\nwith their start and end dates and the associated captured memories(Figure 5b). To account for cases where composite contexts are split\nin half, we use a step size (4 days in the current setup) smaller\nthan the window size, allowing for overlap and comprehensive\nprocessing. For longer composite contexts (e.g., lasting more than\ntwo weeks), each segment of the context is identified separately\nwithin the sliding windows and then merged into a single composite\ncontext. Additionally, any duplicated composite contexts caused\nby the overlap between sliding windows are also merged to avoid\nredundancy (Figure 5c).\nSpecifically, as opposed to including detailed predefined cate-\ngories (as with atomic contexts) in the prompt for LLMs, we adopt a\nfew-shot prompting technique [ 7], providing examples of compos-\nite contexts summarized from the collected questions in the prompt.\nFor detailed prompt, please refer to Appendix A.2.\nExplicitly mentioned contexts. However, some composite contexts\nareexplicitly mentioned in the captured memories. For example, a\nscreenshot of a flyer may reference the upcoming \"CHI 2024\" event\nhappening next month, or a transcribed conversation might discuss\na \"Hawaii trip\" that took place the previous year. We leverage LLMs\nto differentiate between atomic contexts and composite contexts.\nFor example, \u201ca workout session\u201d is identified as an activity (atomic\ncontext), whereas \u201cCHI 2024\u201d is recognized as a composite context\nwhich likely involves multiple interconnected atomic contexts. Such\nidentified composite contexts are either merged with an existing\ncomposite context (e.g., if \"CHI 2024\" has already been identified)\nor added as a new composite context if it is unique.\n5.3 Step 3: Inferring Semantic Knowledge\nDifferent from composite contexts, semantic knowledge focuses on\nhigh-level general knowledge rather than specific memory details.\nFor example, if a chat message screenshot mentions celebrating Ja-\nson\u2019s birthday, the inferred semantic knowledge might be \"Jason\u2019s\nbirthday is on [SPECIFIC DATE].\" Similarly, analyzing multipleOmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering Conference\u201917, July 2017, Washington, DC, USA\n303030\n08:18 AM,\u00a0 May 13, 2024\nMorning, Monday\nConvention center, Honolulu\n\"Sign displaying conference\nWiFi details for CHI 2024\"\n\"CHI 2024\u00a0 Sur\u0000ng the World\nCONFERENCE WIFI\nNetwork Name ...\"Multimodal\nModel\nOCR\nMetadata\nMultimodal\nModelNone\nWi\u0000 sign, table\nLLM\nConference venue\nAttending a conference- Environment\n- Activities- Visual elements- People- Geographical info- Temporal info- Visible text- Caption\nAtomic ContextsContent\nInferRaw captured memory aStructured captured memoryb\nStructure individual\ncaptured memoriesStep 19/12/24, 2:33 AM augmentation_v3.svg\n\ufb01le:///Users/jiahaoli/Library/CloudStorage/Dropbox/02_Career/Projects/OmniQuery/chi_2025/svg/augmentation_v3.svg 1/1\nFigure 4: An example of structuring an individual captured memory (a photo of the Wifi details of CHI 2024 conference).\n10:52 PM,\u00a0 May 15, 2024\n 09:32 PM,\u00a0 May 16, 2024\n 11:04 AM,\u00a0 May 16, 2024\n 08:18 AM,\u00a0 May 13, 2024\nWindow 1\n16:15 PM,\u00a0 May 15, 2024\n1 2 3 4 5\nAttending CHI 2024 in Honolulu,\u00a0\nDate: 5/13-5/15German HCI par ty,\u00a0\nDate: 5/15German HCI par ty,\u00a0\nDate: 5/15Academic conf erence,\u00a0\nDate: 5/16Party in a bar ,\nDate: 5/16\nI attended CHI 2024 in Honolulu in Ma y.\nI listened t o talks during the conf erence.\nI was at the German HCI par ty.I attended a conf erence in honolulu.\nI was at the German HCI par ty and a\nparty in a bar .\nMerge and update\nI attended CHI 2024 in Honolulu in Ma y.I was at the German HCI par ty and a par ty at Maui\nBrewing Company during CHIParty in a bar ,\u00a0Date: 5/16\nMemor y ID: 5German HCI P arty,\u00a0Date: 5/15\nMemor y ID: 3CHI 2024 trip in Honolulu, \u00a0Date: 5/13- 5/16\nMemor y ID: 1,2,3,4,5Composite context and\nsemantic knowledge\nidenti\u0000ed in window 1Composite context and\nsemantic knowledge\nidenti\u0000ed in window 2a\nb\ncComposite\ncontext\nSemantic\nknowledge\n\u00a0 \u00a0 \u00a0 : updated when mer gingWindow 2Example r esult of Step 2  and\u00a0 Step 39/10/24, 9:36 PM sliding_window.svg\n\ufb01le:///Users/jiahaoli/Library/CloudStorage/Dropbox/02_Career/Projects/OmniQuery/chi_2025/svg/sliding_window.svg 1/1\nFigure 5: An example of using sliding windows to identify composite contexts and infer semantic knowledge: (a) two consecutive\nsliding windows; (b) composite contexts and semantic knowledge generated in each window; (c) merging results from the two\nwindows.\ngrocery shopping receipts that consistently include lactose-free\nmilk could lead to the inference that the user is possibly lactose\nintolerant. Semantic knowledge is inferred in each sliding window,\nwhile also taking into account the identified composite contexts to\ngain higher-level understanding of the user\u2019s past and generalized\ninformation (Figure 5b bottom). The output is a list of inferred\ndeclarative semantic knowledge independent from specific memo-\nries. The instructions provided to the model are specifically tailored\nto guide the inference process toward overarching patterns andtrends rather than specific event details. The detailed prompt for\nidentifying semantic knowledge can be also found in Appendix A.2.\nEach inferred entry of semantic knowledge is either merged with\nexisting entries or added to the knowledge list if new.\n5.4 Implementation Details\nTo deduplicate images as people tend to capture similar content\nmultiple times, we use CLIP [ 49] to encode images to embeddingsConference\u201917, July 2017, Washington, DC, USA Li et al.\nInput Quer yAugmented\nQuer yStructur ed\nCaptur ed\nMemories\nComposite\nContexts\nSemantic\nKnowledgeRetrie ved\nMemories\nRetrie ved\nKnowledgeAnswer &\nExplanation\nLLM\nReference\nMemoriesQuer y Augmentation a Retrie val b Answer Gener ation c\n\"What social e vents did I\nattend during CHI 2024? \"Example Quer y d\nRewrite\nDecompose\nInfer\"The social e vents I\nattended during CHI 2024\"\nActivities: \"Social e vents\"\nComposite: \"CHI 2024\"\nPeople: \" crowd\", \" many\npeople \"\nActivities: \" dancing\",\n\"chatting with people \", ...Composite\nContextsSemantic\nKnnowledge\nStructur ed\nCaptur ed\nMemoriesAugmented Quer y f Retrie val tar get g Augment e9/12/24, 2:24 AM qa_system_v3.svg\n\ufb01le:///Users/jiahaoli/Library/CloudStorage/Dropbox/02_Career/Projects/OmniQuery/chi_2025/svg/qa_system_v3.svg 1/1\nFigure 6: The question-answering system consists of: (a) query augmentation by decomposing and inferring contextual\ninformation; (b) retrieving memories and semantic knowledge; (c) generating answers with an LLM using referenced memories.\nGiven a query: (d) OmniQuery rewrites and decomposes it; (e) infers contextual information; (f) retrieves relevant memories;\nand (g) retrieves knowledge from memory and knowledge storage.\nand calculate the similarities between images and merge those ex-\nceed the similarity of 0.85. We use the Google Cloud Vision API3\nfor OCR to detect text in images and OpenAI\u2019s Whisper model4\nfor audio-to-text conversion. Note that Whipser is known for hal-\nlucination when there is no speech in the audio, thus we applied\nfurther data cleaning to validate the transcribed result using Ope-\nnAI\u2019s GPT-4o-mini . For other visual processing, We use GPT-4o\nhandles multimodal sensing, including identifying people and vi-\nsual elements in images and generating scene descriptions. For\nvideo processing, as a proof-of-concept, we consider only the first\n10 seconds of each video, sampling 10 frames to be analyzed by\nGPT-4o for content understanding. Text is encoded into embeddings\nusing OpenAI\u2019s text-embedding-3-small model. Currently, we\nutilize a custom vector database and matrix-based similarity search\n3https://cloud.google.com/vision/docs/ocr\n4https://github.com/openai/whisperimplemented with NumPy in Python. However, for real-world ap-\nplications, more advanced vector databases (e.g., Pinecone5) would\nbe necessary to handle larger volumes of personal data.\n6 OMNIQUERY: QUESTION-ANSWERING\nSYSTEM\nWith Captured memories augmented with contextual information,\nOmniQuery adopts a RAG architecture for the question answering\nsystem. RAG-based systems are effective in handling large datasets\nand mitigating hallucination issues by retrieving relevant content\nand grounding the generated results in this retrieved information.\nThis approach ensures that the output is both relevant and accurate,\nleveraging specific data rather than relying solely on the model\u2019s\ninternal knowledge. This approach is chosen because, on average,\npersonal captured memories often exceed 30,000 photos and videos\n(as reported by participants in our diary study), which exceeds the\nlimit of most foundation models nowadays.\n5https://www.pinecone.io/OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering Conference\u201917, July 2017, Washington, DC, USA\nAs shown in Figure 6, given an input query, OmniQuery first\naugments the query by disambiguating and decomposing it into\nspecific contextual elements (Figure 6a). Then, it retrieves the rele-\nvant captured memories from the structured captured memories\nand the composite contexts, along with related knowledge from the\nlist of semantic knowledge (Figure 6b). The retrieved memories and\nknowledge, along with the augmented query, are then sent to an\nLLM to generate a comprehensive answer (Figure 6c). The detailed\nimplementation of each step is discussed below.\n6.1 Query Augmentation\nAs mentioned in Section 3.3, most user queries tend to be hybrid\nin nature or require contextual information. This means that di-\nrectly searching based solely on the content of captured memories\noften results in an incomplete or insufficient retrieval of relevant\nmemories. To enhance the retrieval process, OmniQuery adopts\nthe query refinement concept from NLP [ 9] to augment the queries.\nThe query augmentation process involves\n(1)Rewriting the query to declarative format to improve\nsearch accuracy of vector-based similarity matching;\n(2)Decomposing the query to extract necessary contextual fil-\nters, such as time, location, or events, which are grounded in\nthe taxonomy. Note that only explicitly mentioned temporal\ncontexts like \u201c... last week\u201d will be recognized temporal fil-\nters. Phrase like \u201c... during CHI 2024\u201d are part of a composite\ncontext, thus not counted as a temporal filter;\n(3)Inferring potential related contexts that may not be ex-\nplicitly mentioned in the query but can enhance the filtering\nprocess also grounded in the taxonomy.\nFor example, as shown in Figure 6d-g, the query \u201c What social\nevents did I attend during CHI 2024? \u201d is rewritten into a declarative\nformat of \u201c The social events I attended during CHI 2024 \u201d. Since \u201cCHI\n2024\u201d is explicitly mentioned and identified as a composite context,\nit is extracted and labeled with the appropriate composite context\ntag. \u201cSocial events\u201d is also extracted and identified as an atomic con-\ntext (activities). Additionally, because \"social events\" might include\nvarious activities like parties, dancing, or casual conversations, and\nlikely involve multiple people present, OmniQuery infers the rele-\nvant atomic contexts (people and activities) and annotates them in\nthe corresponding context category.\n6.2 Retrieving Relevant Augmented Memories\nThe decomposed augmented query is used to comprehensively\nretrieve from the augmented captured memories. Specifically, the\naugmented captured memories consist of the structured captured\nmemories (with processed content and annotated atomic contexts),\nthe list of identified composite contexts and the list of semantic\nknowledge. OmniQuery uses the decomposed components from\nthe augmented query to perform a multi-source retrieval, pulling\nrelated memories from each of these sources. The results are then\nconsolidated into a comprehensive list of relevant memories, which\nare used to generate an accurate and detailed answer for the user\u2019s\nquery.\nDeclarative query \u2192Semantic knowledge & Processed content.\nThe declarative query is first encoded into text embeddings and usedto search both the semantic memories and processed content (cap-\ntion and visible text) of the captured memories This initial search\nstep focuses on finding knowledge and memories directly related to\nthe input query, without incorporating additional contextual filters.\nDecomposed atomic contexts \u2192Annotated atomic contexts.\nEach element of the decomposed atomic contexts (both extracted or\ninferred) is encoded into text embeddings and searched through the\ncorresponding categories in the structured captured memory data-\nbase. For example, if the query involves activities like \"party\" and\n\"dancing,\" OmniQuery searches for captured memories annotated\nwith similar activities. Any memories that have been annotated\nwith related or similar activities will be retrieved, ensuring that\nrelevant memories are included in the results. Additionally, tem-\nporal contexts apply a strict filter, excluding memories outside the\nspecified time frame (e.g., \u201clast month\u201d) from the retrieval process.\nDecomposed composite contexts \u2192Identified composite con-\ntexts. Any composite context decomposed in the augmenting pro-\ncess is also encoded into text embeddings and searched through the\nlist of identified composite contexts. All captured memories linked\nto the semantically similar composite contexts are retrieved. This\nensures all memories related to the composite contexts are included.\nAdditionally, OmniQuery leverages an LLM to assess whether a\ncomposite context includes temporal constraints. For example, \u201c ...\nduring CHI 2024 \u201d implies a strict temporal filter, while photos related\nto CHI 2024 \u201d does not.\n6.3 Answer Generation\nThe retrieved results is then sent to an LLM to generate the final\nanswer. Specifically, the input for the LLM consists of: (1) the aug-\nmented query, (2) the retrieved semantic knowledge from the list,\n(3) all the retrieved captured memory entries from the annotated\ndatabase, including both the memory content and its associated\ncontextual annotations.\nThe model analyzes and reasons which captured memories serve\nas references for the generated answer. These reference memo-\nries are also included in the final answer presented to the user.\nTo enhance the reasoning process, OmniQuery leverages chain-of-\nthought prompting [ 59], ensuring the generation is more accurate\nand contextually rich. For specific prompts, please refer to Appen-\ndix A.3\n7 RESULTS\nTo demonstrate OmniQuery\u2019s capabilities in answering users\u2019 per-\nsonal memory-related questions, we showcase four challenging\nexamples of answering hybrid queries as shown in Figure 7.\n7.1 Experiment Setting\nOmniQuery was tested on the personal data of one of the re-\nsearchers from the team, consisting of 2,590 images and videos\ndownloaded from the researcher\u2019s smartphone. The data spans\nfrom mid-March to mid-August "}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "This summary provides a comprehensive and detailed overview of the machine learning paper, \u201cStyle Based Clustering of Visual Artworks\u201d by Abhishek Dangeti, Pavan Gajula, Vivek Srivastava, and Vikram Jamwal from TCS Research. The summary has been structured to enable a researcher to recreate the study or build upon this work, including all relevant details covering the abstract, introduction, related work, methodology, experimental setup, results and analysis, discussion, and conclusion.\n\n### 1. Abstract\nThe paper addresses the largely unaddressed problem of clustering artworks based on style, which has potential applications in art recommendations, style-based search, and analyzing the evolution of artistic styles. Current methods rely on generic image features and do not specifically target artistic styles. The authors explore neural feature representations and architectures suitable for style-based clustering. They develop several methods and assess their effectiveness through extensive qualitative and quantitative analysis on multiple datasets, providing novel insights into suitable architectures, feature representations, and evaluation methods for style-based clustering.\n\n### 2. Introduction\nThe digitization of visual artworks through platforms like WikiArt and the Munch Museum\u2019s digital archive has enabled deeper study of these artworks through modern AI methods. Artistic style is critical to an artist\u2019s identity, expression, and cultural grounding. Traditionally, artworks have been classified into various art movements and styles. However, a higher diversity of styles exists at a more granular level beyond these categories. Individual artists often explore multiple styles in their work, which evolve over time.\n\nWhile some existing methods focus on style-based classification, few address the artistic style-based clustering of artworks, particularly due to the lack of fine-granular style-specific labeled data. The paper proposes using unsupervised clustering to identify artworks with similar style characteristics, aiding in a deeper understanding of stylistic expression and evolution.\n\n### 3. Related Work\n#### 3.1 Artwork Classification\nPast methods have used handcrafted features like color and brushstroke, while recent approaches use neural networks for feature extraction and classification. Most models employ supervised learning, focusing on predicting the artist or art movement rather than identifying new or unknown styles.\n\n#### 3.2 Artwork Retrieval\nArtwork retrieval methods often leverage content similarity, such as querying for monochromatic paintings or retrieving similar images based on textual queries. These approaches, however, do not specifically address style-based clustering.\n\n#### 3.3 Artwork Generation\nTechniques like diffusion models and GANs have enabled the generation of high-quality images in various styles, emphasizing the importance of cataloging style through clustering.\n\n#### 3.4 Artwork Clustering\nFew studies have explored clustering artworks using features extracted from neural networks, focusing more on content and less on style. This paper aims to fill this gap.\n\n#### 3.5 Artwork Datasets\nDatasets like WikiArt and BAM-FG have been used for style-based classification but are not publicly available or lack validated style categories. The paper addresses the need for a curated dataset for style-based clustering evaluation.\n\n### 4. Methodology\nThe paper develops a style-based clustering method utilizing deep embedded clustering (DEC) and evaluates different feature representations for style information.\n\n#### 4.1 Style Feature Exploration\nThree types of style-based features are explored:\n1. **Neural Network-Based Style Features:** Extract features traditionally used in style classification and style transfer.\n   - **DenseNet Features (Fdense):** Use the last layer of DenseNet to extract style-related features.\n   - **Convolutional Neural Network-Based Style Features (Fgram and Fg\u00b7c):** Use features from specific layers of CNN for style classification.\n\n2. **StyleGAN-Based Style Features (Fstylegan):** Extract style features from intermediate layers of StyleGAN used for image transformation, categorized into coarse, middle, and fine-grained styles.\n\n3. **Textual Style Features (Ftext):** \n   - **Artwork Caption Representation (Fcaption):** Use large vision-language models to generate descriptive captions containing style information.\n   - **Artwork Style Concept Annotation Representation (Fannot):** Annotate artworks with style concepts based on fundamental principles of art.\n\n#### 4.2 Deep Embedded Clustering Model\nThe DEC model projects input features into a latent space of an autoencoder, simultaneously learning the latent representations and cluster assignments. The traditional K-Means model is also used for comparison.\n\n#### 4.3 Dimensionality Reduction\nMethods like global average pooling (GAP) are used to reduce feature dimensions and make them suitable for clustering tasks.\n\n### 5. Experimental Setup\n#### 5.1 Datasets\nFour core stylistically diverse datasets are used:\n1. **Edvard Munch Archive (EMA):** Contains 7410 artworks.\n2. **WikiArt Dataset (WAD):** Contains 78,978 artworks.\n3. **Brueghel Dataset (BD):** Contains 1587 artworks.\n4. **Clip Art Illustrations Dataset (CAID):** Contains 4591 illustrations.\n\nAdditionally, a curated dataset is created for evaluation using style-transfer models on COCO dataset images to form ground truth style clusters.\n\n#### 5.2 Evaluation Metrics\nCore datasets are evaluated using:\n- **Silhouette Coefficient (SC):** Measures similarity within and between clusters.\n- **Calinski-Harabasz Index (CHI):** Measures the ratio of intra-cluster dispersion to inter-cluster dispersion.\n\nThe curated dataset further uses:\n- **Adjusted Rand Index (ARI):** Measures similarity between predicted and ground truth clustering.\n- **V-Measure (V-Mes):** Evaluates clustering quality based on homogeneity and completeness.\n\n#### 5.3 Baselines and Comparison\nThe DEC model is compared with traditional clustering methods like K-Means using the above metrics.\n\n### 6. Results and Analysis\n#### 6.1 Quantitative Analysis\nKey insights from the experiments include:\n- **Fgram features** formed the best stylistic clusters.\n- **Fg\u00b7c and Fannot** features also performed well in style-based clustering.\n- **DEC model** significantly improved cluster separation and density compared to K-Means.\n- **Fcoarse, Fmiddle, and Ffine** features from StyleGAN did not perform well.\n\n#### 6.2 Qualitative Analysis\nClusters formed using the different features were visually evaluated based on similarity in color, texture, shape, etc. Uneven style distribution in core datasets led to hierarchical and unbalanced clusters.\n\n#### 6.3 Clustering Model Comparison\nThe DEC model formed more distinct and well-separated clusters compared to K-Means, verifying the effectiveness of deep clustering approaches for this task.\n\n### 7. Discussion\nThe findings reveal that:\n- Gram matrices effectively capture artistic styles.\n- Textual features from LVLMs can assist in style elicitation.\n- Uneven style distribution affects cluster size and hierarchy.\n- DEC forms better clusters than traditional methods.\n\nThe study opens up new research directions for fine-granular style discovery and artistic cataloging.\n\n### 8. Conclusion\nThis paper contributes to the underexplored field of artistic style-based clustering. It presents various neural feature representations and validates them through extensive experiments, showing that deep embedded clustering models are effective for this task. Future research could explore branching out to discover finer style concepts and utilize LVLMs for specific style elicitation in artworks.\n\nBy providing these detailed methodologies, findings, and analyses, this summary equips researchers with comprehensive information to replicate the study or extend it further."}, {"role": "assistant", "content": "Style Based Clustering of Visual Artworks\nAbhishek Dangeti, Pavan Gajula, Vivek Srivastava, Vikram Jamwal\nTCS Research\nINDIA\n{abhishek.dangeti, pavanbhargav.gajula, srivastava.vivek2, vikram.jamwal }@tcs.com\nAbstract\nClustering artworks based on style has many potential\nreal-world applications like art recommendations, style-\nbased search and retrieval, and the study of artistic style\nevolution in an artwork corpus. However, clustering art-\nworks based on style is largely an unaddressed problem.\nA few present methods for clustering artworks principally\nrely on generic image feature representations derived from\ndeep neural networks and do not specifically deal with the\nartistic style. In this paper, we introduce and deliberate\nover the notion of style-based clustering of visual artworks.\nOur main objective is to explore neural feature represen-\ntations and architectures that can be used for style-based\nclustering and observe their impact and effectiveness. We\ndevelop different methods and assess their relative efficacy\nfor style-based clustering through qualitative and quanti-\ntative analysis by applying them to four artwork corpora\nand four curated synthetically styled datasets. Our analysis\nprovides some key novel insights on architectures, feature\nrepresentations, and evaluation methods suitable for style-\nbased clustering.\n1. Introduction\nThe digitization of visual artworks through platforms such\nas WikiArt [57] and the Munch Museum\u2019s digital archive\n[41] not only renders them accessible to a global audience\nbut also facilitates a deeper study of these artworks through\nmodern AI methods. Style of an artwork or an artist is an\nessential aspect of art as it has a bearing on the artist\u2019s iden-\ntity, expression (emotional resonance, communication), cul-\ntural grounding, and aesthetics.\nThe artworks produced by various artists have been his-\ntorically categorized into (or labeled as) various visual art\nmovements and styles such as Renaissance, Classicism, Cu-\nbism, Expressionism, Abstract, Baroque, Modern, Futur-\nism, etc. based on the medium and philosophy of expres-\nsion. However, after a careful study, one can observe a\nhigher diversity of styles, sub-styles, or categories that can\nA B\nD CFigure 1. A snapshot of Style-based manual clustering of Edvard\nMunch artworks. Each group represents a unique, but unlabeled,\nfine-granular artistic style.\nbe more fine-granular than those covered by these art move-\nments. Moreover, individual artists do not draw in a singu-\nlar style and their art evolves depending upon their expo-\nsure, circumstances, and changing tastes. For example, in\nFigure 1, we observe examples of different artistic styles of\nEdvard Munch in his collection of sketches and watercolor\npaintings [41].\nPreviously, several attempts have been made to ana-\nlyze and understand artistic styles through methods such as\nstyle-based classification of artworks. A majority of the cur-\nrent artwork classification methods [1, 27, 42] classify art-\nworks based on art movements as described above. Some\nmethods such as [48] have attempted to create more fine-\ngranular artistic datasets through crowd-sourcing methods.\nHowever, due to the lack of publically available labeled\nfine-granular style-specific data, and the subjective nature\nof style, the artistic style-based clustering of artworks is a\nlargely unexplored task.\nAlternatively, by using pattern recognition methods like\nunsupervised clustering, we can hope to identify artworks\n1arXiv:2409.08245v1  [cs.CV]  12 Sep 2024with similar style characteristics. This approach can aid in\ngaining a deeper understanding of an artist\u2019s various stylis-\ntic expressions and also allow for a more accessible anal-\nysis of artistic style evolution. Only a handful of studies,\nfor example, [5, 6] have delved into unsupervised cluster-\ning of artworks. However, these approaches are generic and\ndo not disentangle the content and style information to be\nuseful specifically for style-level clustering of the artworks.\nIn this paper, we comprehensively study the problem of\ndeveloping suitable methods for style-based clustering of\nartworks. We rely on two components: (1) Deep neural\nfeatures that could help us get a good handle on style and\n(2) Deep neural architectures that can effectively help us\ndo unsupervised clustering of artworks based on their deep\nfeatures.\nWe explore three different types of style-based feature\nrepresentations for capturing style-based information: (i)\nNeural Network-based Style Features, (ii) StyleGAN based\nStyle Features, and (iii)Textual Style Features. For cluster-\ning we deploy an architecture similar to the Deep Embed-\nded Clustering model (DEC) [60] wherein we project the\ninput features into the latent space of a deep autoencoder,\nand then simultaneously learn the latent representations of\neach artwork\u2019s features and the cluster assignments (refer to\nSection Methodology ).\nWe do extensive experiments on two types of dataset\ngroups: (i) Core Datasets consisting of four diverse\ndatasets, viz., Wikiart, Edvard Munch Archive, Brueghel\ndataset, and Clip art illustrations dataset (ii) Synthetically\nCurated Dataset , wherein we derive a dataset of 50 images\nfrom Coco-dataset and use 10 specialized images from each\nof the above datasets and style-transform the Coco-dataset\n(refer to Section Datasets and Evaluation Criteria ).Our\nprimary objective is to understand the effectiveness of our\ndevised methods with different neural representations for\nstyle-based clustering using the given neural architectures.\nWe present the insights and the results in Section Results\nand Discussion .\nOur paper contributes in the following manner:\n1. We present and provide solutions for the under explored\nproblem of artistic style-based clustering of artworks.\n2. We explore different style-based features for artwork\nstyle classification, and style transfer and use them as\nneural feature representations for artwork clustering.\n3. We use textual style feature representations obtained\nthrough captioning/annotating large vision language\nmodels for artwork clustering.\n4. We present a self-curated dataset with pre-style-clusters\nwhere we utilize the Styleshot [17] style-transfer model\nto create style-based ground truth clusters based on the\nbase style images.\n5. We present a novel evaluation method to gauge the style-\nbased clustering of artworks through ground truth data.2. Related Work\nWhile ours is the first work to comprehensively study style-\nbased clustering in artworks, in this section we discuss the\nrelated approaches in deep learning as applied to the Style\nin artworks.\n2.1. Artwork Classification\nSeveral methods have explored artworks-classification\nbased on handcrafted features such as color [62] and brush-\nstroke [33]. Recently, neural network-based architectures\nsuch as Convolution Neural Networks (CNNs) have been\nused to extract features from the artworks which are further\nused in artwork classification tasks [8]. Majority of artwork\nclassification models employ supervised learning and are\nlimited to predicting the artist [23, 28] or the popular art\nmovement in the history [15]. These methods are not di-\nrectly useful for identifying new or unknown artistic styles.\n2.2. Artwork retrieval\nMost of the Artwork retrieval methods leverage content\nsimilarity to retrieve the artwork from a collection. For in-\nstance, in [4], monochromatic painting images are retrieved\nusing a query consisting of a combination of classes or key-\nwords, whereas [7] fine-tunes a pre-trained CNN to retrieve\npaintings with similar artistic motifs given a textual user\nquery. In contrast to retrieving artwork from textual queries,\nretrieving paintings from the given image(s) is also explored\n[12, 16, 50, 58]. However, none of these approaches directly\napproach the problem of style-based clustering of artworks\nas discussed in this paper.\n2.3. Artwork generation\nRecently, techniques such as diffusion models and gener-\native adversarial networks have facilitated models to gen-\nerate high-quality images or transform given images in\nthe style of artwork or an artist [22, 29, 52, 61]. Such\ntechniques enable the artists to diversify their style palette\nwhich further underlines the importance of cataloging style\nthrough style-based clustering of artworks.\n2.4. Artwork clustering\nOver the years, several novel clustering algorithms have\nbeen proposed in computer vision spanning across super-\nvised, semi-supervised, and unsupervised [9, 21, 43, 56]\napproaches. Recently, [6, 7] have explored the problem\nof clustering artworks leveraging the artwork features ex-\ntracted with deep convolutional neural networks such as\nDenseNet [24]. However, the approaches are generic in the\nnature of clustering and do not specifically deal with style.\n2.5. Artwork Datasets\nWorks such as [53] use the WikiArt collection [57] to cre-\nate a style-based dataset comprising of artworks with art\n2Artwork \nFeature \nExtractorFArtwork\nFeatures\nEncoder DecoderCluster\nInitializationClustering \nLayer\nAutoencoderDeep Embedded Clustering Model\nArtwork \nFeature \nExtractorFArtwork\nFeatures\nK-Means \nClustering\nK-Means Clustering Model\nDimensionality \nReduction\nDimensionality \nReduction(Optional)(Optional)\n(A)\n(B)Figure 2. The architecture for our approach with the (A) K-Means clustering model [38] and the (B) Deep embedded clustering model\n(DEC) [60]. For the K-Means clustering model, the dimensionally reduced artwork features are directly used for clustering, whereas for\nthe DEC model, the features are first projected into the latent space before being used for clustering. Refer to Appendix D for details of\nour dimensionality reduction methods.\nmovement class labels. The BAM! dataset [58] includes\nartworks and images with different style labels which in-\nclude style categories such as emotion, medium, art move-\nment, etc. [48] introduces a new style-based artworks\ndataset called the BAM-FG (BAM-fine-grained) dataset\nwith proper style labels through crowd-sourcing. These\ndatasets BAM-FG dataset however are not publically avail-\nable at the present. [17], introduce a style evaluation dataset\ntitled the StyleBench which consists of a mixture of differ-\nent types of images ranging from artworks, photographs,\netc. with labels based on different style categories such as\nmedium, art movement, etc. However, these categories are\nnot validated. For our style-specific evaluation, we synthet-\nically curate our own dataset.\n3. Methodology\nFor our base clustering model, we utilize the Deep Embed-\nded Clustering model (DEC) [60] wherein we project the in-\nput features into the latent space of a deep autoencoder, and\nthen simultaneously learn the latent representations of each\nartwork\u2019s features and the cluster assignments. We chose\nthe DEC model for our base clustering model as the tra-\nditional clustering methods struggle with clustering highly\ncomplex artwork features. DEC can assist in finding the\nnon-linear relationships between the fine-grained style as-\npects of these features by learning better latent represen-\ntations and cluster assignments of these features (refer to\nAppendix C for a brief background on the working of DEC\nmodel). For comparison of our chosen architecture with the\ntraditional methods, we also perform experiments with the\ntraditional K-Means clustering model for select feature rep-\nresentations. Please refer to Figure 2 for an architecturaloverview of both the clustering models.\n3.1. Style Feature Exploration\nThe foremost essential component in style-based clustering\nis the choice of features in an artwork that can help us lo-\ncate its style. In this section, we detail the disparate forms\nof style features that we utilize to achieve style-based clus-\ntering of visual artworks. We explore three different types\nof style-based features:\n1.Neural Network Based Style Features wherein we ex-\nplore neural-network-based style features that have been\ntraditionally used in style transfer and style classifica-\ntion.\n2.StyleGAN Based Style Features wherein we explore\nintermediate style features extracted from Stylegan that\nhave been used for style-based image transformations.\n3.Textual Style Features wherein we propose using de-\nscriptive language-based style features extracted from\nartworks using Vision Language Models (VLMs).\n3.1.1 Neural Network based Style Features ( FNN)\nNeural networks (NN) have been used in previous works\nto extract style-based features for various style-based tasks\nsuch as style classification [10, 11], style-transfer [19],\nstyle-based retrieval [40], etc. We study two different types\nofFNNfeatures for style-based clustering:\nDenseNet Features ( Fdense ): For the application of\nclustering of visual artworks, the approach in [6] utilizes\nthe last layer of the DenseNet [25] to extract features from\nthe artworks and incorporate them into the DEC model. The\nlast layer would contain rich information about artwork as\n3Fdensefinal Layer\nDenseNet\nconv5_1  LayerGram \nMatrix \nCalculator\nCosine \nSimilarity\nCalculatorFgram\nDot \nProduct Fg.cVGG -19(a)\nSubject: representational\nLine: blurred, controlled, meandering, \nthick , thin\nTexture: smooth, gestural, rough\nColor: monochromatic\nCaption \nGeneratorA drawing of two men sitting at a table\nStyle \nConcept \nAnnotatorCaption\nAnnotation\nInstruction\nStyle \nTaxonomyFcaption\nFannotEncoding\nEncoding\nEncoder zLatent\nMapping \nNetworkwIntermediate\nLatent\nA\nAAffine \nTransformers\u2026\nFstyleganSynthesis \nNetworkStyleG AN\n(b)Visual Elements Concepts\nSubject Representational, Non-representational\nLineBlurred, Broken, Controlled, Curved, Diagonal,\nHorizontal, Vertical, Meandering, Thick, Thin,\nActive, Energetic, Straight\nTexture Bumpy, Flat, Smooth, Gestural, Rough\nColorCalm, Cool, Chromatic, Monochromatic, Muted,\nWarm, Transparent\nShapeAmbiguous, Geometric, Amorphous, Biomorphic,\nClosed, Open, Distorted, Heavy, Linear, Organic,\nAbstract, Decorative, Kinetic, Light\nLight and SpaceBright, Dark, Medium, Atmospheric, Planar,\nPerspective\nGeneral Principles\nof ArtOverlapping, Balance, Contrast, Harmony, Pattern,\nRepetition, Rhythm, Unity, Variety, Symmetry,\nProportion, Parallel\n(c)\nFigure 3. Architecture for extracting different artwork features. The first column showcases the process of extracting (a) FNNand (b)\nFstylegan features. The second column showcases the process of extracting (c) Ftextfeatures and the style taxonomy used for Fannot .\neach layer of the DenseNet is connected to all its previ-\nous layers. This work focuses mainly on content and art-\nmovement based clustering. In our exploration, we utilize\nFdense features to check whether these features can be uti-\nlized for style-based clustering for visual artworks. Similar\nto [6], after obtaining the features for each artwork in the\ndimensions 1024 \u00d77\u00d77, we apply global average pooling\n(GAP) [35] to obtain 1024-feature vector for each artwork.\nConvolutional Neural Network based Style Features\n(Fgram andFg\u00b7c): Works such as [10, 11] utilize the style\nfeatures introduced by [19] for style classification of art-\nworks. In their work, they explore different combinations\nof features extracted from different layers of the CNN as\nwell as different mathematical correlations and combina-\ntions of these features for style classification. They observe\nthat the gram matrices of features obtained from conv51\n(Fgram ) yield one of the best results in the task of clas-\nsification. They also observe that the dot product of Fgram\nand cosine similarity of the features extracted from conv51\n(Fg\u00b7c) also produces good results. In our work, we utilize\nFgram andFg\u00b7cand explore their impact on unsupervised\nstyle-based clustering. We reduce the dimensions of Fgram\nandFg\u00b7cfrom 512 \u00d7512 to 512 for each artwork by utilizing\nGAP. The architecture for extracting FNNfeatures can be\nseen in Figure 3 (a).\n3.1.2 StyleGAN based Style Features ( Fstylegan )\nDifferent features are used in StyleGAN[31] to train a gen-\nerative adversarial network (GAN) [20] to capture the dif-\nferent types of style information present in an image. Intheir work, they modify the GAN architecture to disentan-\ngle style from content by starting from a constant vector\nrather than the encoded latent vector and incorporating style\ninputs at different convolutional layers in the synthesis net-\nwork thereby providing fine-grained control over styles dur-\ning image generation. We extract those style features and\nuse them as Fstylegan features in our approach. In Style-\nGAN, they primarily focus on generating facial images and\nhave divided the 7 blocks present in the synthesis network\ninto three levels based on the style aspects captured by each\nlayer for face images as follow:\n1.Coarse-grained style ( Fcoarse ): The style inputs to the\nfirst two blocks of the synthesis network (4 \u00d74, 8\u00d78)\ncapture the coarse level information present in an image\nsuch as pose, general hairstyle, face shape, eyeglasses,\netc.\n2.Middle-grained style ( Fmiddle ): The style inputs to the\nthird and fourth blocks of the synthesis network (16 \u00d716,\n32\u00d732) capture the middle-level information present in\nan image such as facial features, eyes open or closed, etc.\n3.Fine-grained style ( Ffine): The style inputs to the\nfifth, sixth, and seventh blocks of the synthesis network\n(64\u00d764, 128 \u00d7128, 256 \u00d7256) capture the fine-grained\nlevel style information such as color scheme and mi-\ncrostructure.\nIn their experiment, the authors have experimented\npurely on photo-realistic images. In our work, we experi-\nment with Stylegan for artwork. We extract the style fea-\ntures from the transformation layers (A in Figure 3 (b))\npresent in each block of the synthesis network. We cate-\n4gorize the extracted features based on the different styles\ndescribed above and concatenate them column-wise. The\ndimensions of the features for each artwork are 1536, 2048,\nand 1344 for Fcoarse ,Fmiddle , and Ffine respectively.\nWe experiment with each category of these features to\nachieve style-based clustering. The architecture for extract-\ningFstylegan features can be seen in Figure 3 (b).\n3.1.3 Textual Style Features ( Ftext)\nIn this subsection, we discuss in detail our two proposed\napproaches for style based clustering that explicitly factor\nin the artistic style information from the artworks through\ntext. The architecture for extracting Ftextfeatures can be\nseen in Figure 3 (c).\nArtwork caption representation ( Fcaption )In this ap-\nproach, we generate the artwork representation using the\ntext description of the artwork. The text description of the\nartwork encloses the content as well as the coarse-grained\nstyle information.\nThe coarse-grained style information in the text descrip-\ntion is usually depicted with keywords such as painting,\ndrawing, portrait, etc. Formally, we obtain Fcaption for an\nartwork aias:\nFcaption =T(C(ai)) (1)\nwhere Cis the caption generator, and Tis the text en-\ncoder to encode the caption for the artwork ai.\nIn this work, we use the Bootstrapping Language-Image\nPre-training for Unified Vision (BLIP) [34] model as the\ncaption generator ( C) for artworks. Specifically, we use the\nBLIP Image Captioning Large model which uses the BERT\n[13] model as the text encoder and the ViT-L [14] model as\nthe vision backbone. It has been trained on 129 Million im-\nages from the COCO, VG, CC, SBU, and LAION datasets.\nThe BLIP model takes in the input artwork and gives us an\noutput text that describes the artwork in a sentence.\nNext, we use Sentence-transformer [45] as the text\nencoder ( T) for artwork captions. We obtain a 384-\ndimensional artwork representation Fcaption for each art-\nwork. We then use these artwork representations Fcaption\nas input artwork features in the DEC model. Note that we\nuse the BLIP captioning model and sentence transformer as\na proxy for CandT, and it may be replaced with any other\nsuitable image captioning and text encoder models.\nArtwork style concept annotation representation\n(Fannot )In this approach, we annotate artworks with style\nconcepts based on the fundamental principles of art as de-\nscribed in [32], wherein a set of 59 different concepts across\nseven visual elements have been proposed (see Figure 3 (c)).\nFormally, we obtain the artwork representation for an art-\nwork aiwith style concept annotation as:Fannot =T(S(ai, taxonomy, instruction ) (2)\nwhere Sis the style concept annotator and Tis the\ntext encoder to encode the style concepts of an artwork.\nThe style-concept-annotator considers the taxonomy given\nin Figure 3 (c) and the instruction and associates the style\nconcepts (for each visual element) from the taxonomy to a\ngiven artwork ai.\nIn our work, we leverage the Large Language and Vi-\nsion Assistant (LLaV A) 1.5 model [37] as a style concept\nannotator ( S). It is a large multi-modal model that com-\nbines the CLIP ViT-L [44] vision encoder and the Vicuna\n1.5 13B text encoder fine-tuned from the LLaMA model\n[55]. LLaVa can be used for general-purpose visual and\nlanguage understanding tasks where, given an image and\na query, the model can answer the given query based on\nthe image. We use sentence-transformer [45] as the text\nencoder ( T) to obtain the 384-dimensional representation\nFannot for each artwork.\nThe style information available with this method is fine-\ngrained across various artistic style dimensions. Note that\nwe use LLaV A and sentence-transformer as a proxy, and it\nmay be replaced with other suitable style concept annotators\nand text encoders, respectively.\n4. Datasets and Evaluation Criteria\nIn this section, we present the details of the input datasets\nand evaluation criteria deployed for our experiments on\nstyle-based clustering of visual artworks.\n4.1. Core Artwork Datasets\nIn our experiments, we leverage the following four stylisti-\ncally diverse artwork datasets:\n1.Edvard Munch Archive (EMA) : We experiment with\nthe artwork collection dedicated to the artist Edvard\nMunch. We specifically consider sketch and watercolor\npaintings, comprising 7410 artworks created by Edvard\nMunch [51].\n2.WikiArt Dataset (WAD) : WikiArt is the largest col-\nlection of digitized artworks encompassing artists from\nseveral art movements. Similar to [6], in this work, we\nuse the WikiArt dataset created by [53], which contains\n78,978 artworks.\n3.Brueghel Dataset (BD) : The Brueghel dataset [49] con-\nsists of 1587 artworks created by Jan Brueghel The El-\nder. This dataset consists of artworks in different media\nlike oil, ink, and watercolor, along with various painting\nsurface materials such as paper, panel, and copper.\n4.Clip Art Illustrations Dataset (CAID) : Clip art images\nconsist of various styles such as sketches, woodcuts, car-\ntoons, and gradient-shading. We adopt the clip art illus-\n5trations dataset used in [18] consisting of 4591 clip art il-\nlustrations. 1000 of the illustrations have been collected\nfrom the Art Explosions dataset [2], and 3591 of those\nillustrations are from the clip art included in Microsoft\nOffice.\nWe also compare the DEC model with the traditional\nclustering methods such as K-Means for Fdense andFgram\nas they have been utilized for clustering artworks in pre-\nvious works [6, 16]. These features have not been evalu-\nated on their stylistic cluster-ability in the previous works.\nFor each dataset, we experiment with the optimal number\nof clusters ( k), which is calculated using the elbow method\n[54] for all the features and set k=8, 24, 8, and 10 for\nEMA, WAD, BD, and CAID datasets respectively. The\nDEC model is trained using Adam optimizer. The number\nof iterations is set to 8000, and the convergence threshold is\nset to 0.0001.\n4.2. Evaluation Metrics - Core Datasets\nSimilar to [6], we quantitatively evaluate the performance of\nour method using the two metrics: Silhouette Coefficient\n(SC) [47] and Calinski Harabasz Index (CHI) [3]. SC\nis the measure of how similar a data point is to other data\npoints in its own cluster and how similar the same data point\nis to the data points in a separate cluster. This metric is\ncalculated on the data point level. SC ranges from -1 to +1,\nwhere a high value indicates that the data points are well-\nmatched to their own clusters and poorly matched to other\nclusters. A lower value would indicate that the data points\nare wrongly assigned to clusters. CHI is the ratio of the\nsum of intra-cluster dispersion and inter-cluster dispersion\nfor all clusters. This metric is calculated at the cluster level.\nA higher value of CHI indicates that the clusters are more\nspread out and dense.\n4.3. Synthetically Curated Dataset\nThere is a lack of dedicated evaluation metrics and ground\ntruth datasets for style-based clustering. To this extent,\nwe develop a synthetically curated dataset for style-based\nclustering of artworks. We select 10 stylistically distinct\nstyle reference images from each of our four core datasets\nand 50 distinct content images from the MS COCO dataset\n[36]. For each dataset, we use the 10 style reference images\nand style-transform 50 content images through an image-\nto-image style-transfer model Styleshot [17] to obtain 500\nstylized images (refer to tables 8, 9, 10 and 11 in Appendix\nfor a sample of style transformations). For ground truth, we\ntreat the images stylized from the same style reference im-\nage as part of the same style cluster. Thus, we obtain 10\ndifferent style clusters, each having 50 images of the same\nstyle.4.4. Evaluation Metrics - Curated Dataset\nFor additional experiments, we consider this curated dataset\nas the ground truth. We also consider two more evalua-\ntion metrics, viz., Adjusted Rand Index (ARI) [26] and\nV-Measure (V-Mes) [46] that take into consideration the\nground truth clusters. ARI is a measure of similarity be-\ntween two data clusterings. It takes all pairs of samples\nfrom both ground truth and predicted clusterings and con-\nsiders all pairs of agreements and disagreements in their as-\nsignments to clusters. It then adjusts the index to account\nfor chance by taking into account the expected similarity be-\ntween the two clusterings. The ARI score ranges from -1 to\n1. Values ranging between -1 to 0 indicate disagreement be-\ntween the two data clusterings whereas values ranging from\n0 to 1 indicate agreement between the two data clusterings.\nV-Mes is used to evaluate the quality of clustering results,\ntaking into account both homogeneity (each cluster contains\nonly members of a single class) and completeness (all mem-\nbers of a given class are assigned to the same cluster). The\nV-measure ranges from 0 to 1. A low value would indi-\ncate each cluster contains data points from different classes,\nand the data points belonging to the same class are assigned\nto multiple different clusters. A high value would indicate\nthat each cluster contains data points belonging to the same\nclass, and data points of a particular class are assigned to\nthe same cluster. For the synthetically curated dataset, we\nutilize these two metrics to measure how close the predicted\nclustering is to the synthesized ground truth clustering; we\nalso evaluate how dense and sparse the clusters are with SC\nand CHI.\n5. Results and Discussion\nIn this section, we discuss the key results of the different ex-\nperiments for style-based clustering of artworks. We study\nthe efficacy of the eight different features with two differ-\nent clustering algorithms (K-Means and DEC) across two\ndimensions: the cluster formation ability and the stylistic\nclustering ability. We utilize the SC and CHI metrics for\nassessing the cluster formation ability and use the ARI and\nV-Mes for gauging the stylistic clustering ability. We fur-\nther observe the UMAPs to gain deeper insights into the\nfeature spaces and the clusters formed by the clustering al-\ngorithms with different features across all datasets which\ncan be found in Appendix F.5. We qualitatively evaluate the\nclusters based on the visual similarity between the artworks\nby taking into account certain style attributes like the color,\ntexture, shape, etc.\nWe present the quantitative performance evaluation and\nthe qualitative results for experiments performed on the four\ncore datasets in Table 1 and Figure 4 respectively. We\nalso present the quantitative performance evaluation and the\nqualitative results for experiments performed on the syn-\n6Clustering\nTechniqueFeaturesEdvard Munch Archive WikiArt Dataset Brueghel Dataset Clip Illustrations Dataset\nSC CHI SC CHI SC CHI SC CHI\nK-Means FNNFdense 0.03 275.64 0.014 825.41 0.053 73.31 0.015 94.38\nFgram 0.063 569.26 0.046 3500.99 0.075 142.4 0.01 212.81\nDeep\nEmbedded\nClusteringFNNFdense 0.917 130430.97 0.418 135640.22 0.219 680 0.081 740.63\nFgram 0.223 545.42 0.118 8059.72 0.155 478.76 0.232 710.528\nFg\u00b7c 0.364 764.08 0.519 789121.99 0.204 405.41 0.263 808.99\nFstyleganFcoarse 0.485 6683.83 -0.024 7156.08 -0.064 326.97 0.007 666.83\nFmiddle 0.207 2397.75 -0.07 5755.32 -0.128 406.79 -0.046 955.41\nFfine 0.499 3394.73 0.044 9933.89 0.147 2540.5 -0.106 2228.97\nFtextFcaption 0.974 28265.93 0.763 972773.94 0.956 32260.69 0.97 350229.38\nFannot 0.989 3283334.32 0.757 485694.16 0.962 1199640.78 0.98 545357.73\nTable 1. Quantitative results for all features for the core datasets.\nEdvard Munch Archive WikiArt Dataset Brueghel Dataset Clip Illustrations Dataset Clustering\nTechniqueFeaturesSC CHI ARI V-Mes SC CHI ARI V-Mes SC CHI ARI V-Mes SC CHI ARI V-Mes\nFdense 0.101 18.49 0.601 0.615 0.098 19.44 0.511 0.719 0.09 19.12 0.689 0.723 0.07 16.73 0.433 0.457K-Means FNNFgram 0.282 105.2 1.00 1.00 0.221 136.26 0.995 0.995 0.282 113.88 0.991 0.991 0.193 67.8 0.65 0.74\nFdense 0.547 1425.69 0.498 0.724 0.391 479.6 0.492 0.698 0.351 584.72 0.463 0.692 0.31 455.41 0.461 0.63\nFgram 0.276 828.18 0.554 0.762 0.276 1458.85 0.58 0.792 0.237 447.59 0.634 0.817 0.18 657.55 0.371 0.589 FNN\nFg\u00b7c 0.554 3574.71 0.191 0.362 0.532 5254.83 0.185 0.419 0.55 3655.1 0.191 0.394 0.555 2966.18 0.107 0.263\nFcoarse -0.179 110.85 0.021 0.088 -0.226 31.16 0.006 0.052 -0.191 92.27 0.025 0.104 -0.125 172.7 0.013 0.072\nFmiddle 0.136 820.54 0.013 0.081 0.194 993.13 0.007 0.055 -0.877 4516.36 0.013 0.077 -0.39 4601.97 0.002 0.048 Fstylegan\nFfine 0.092 665.05 0.015 0.081 0.041 554.35 0.007 0.052 0.087 757.6 0.017 0.083 -0.07 315.51 0.003 0.056\nFcaption 0.982 81219.88 0.051 0.148 0.987 31636.11 0.04 0.118 0.987 32482.51 0.079 0.164 0.991 93104.86 0.023 0.07Deep\nClustering\nModel\nFtextFannot 0.592 3389.48 0.027 0.085 0.948 52470.92 0.035 0.098 0.767 5944.12 0.019 0.079 0.433 552.33 0.014 0.068\nTable 2. Quantitative results for all features for the curated datasets.\nthetically curated dataset in Table 2 and Figure 5 respec-\ntively. For qualitative results, we choose to present the\ntwo best style-clustering features for both the core datasets\n(Fgram (DEC) and Fg\u00b7c(DEC)) as well as the synthetically\ncurated datasets ( Fgram (K-Means) and Fgram (DEC)). The\nqualitative results for the other features can be found in Ap-\npendix F.6.\n1. Gram-based features form better stylistic clusters\nand the order of top style capturing is Fgram > F g.c>\nFannot :\nThe qualitative evaluation of the clusterings, for both\ncore and curated data, reveals that Fgram features give high-\nquality stylistic clusters for both the clustering algorithms\nwith very few outliers. This may be attributed to the fact\nthat the gram matrices capture the styles of images in the\nform of textures. This is followed by Fg\u00b7candFannot\nfeatures. Quantitative evaluation with the curated ground\ntruth dataset on ARI and V-Mes metrics further supports\nthis observation. For Fdense features, even though the met-\nric scores are higher than the scores for the remaining fea-\ntures(except Fgram ), we found the qualitative results for\nFdense to be poor.\nFor the applications of clustering, we observe that\nFstylegan doesn\u2019t capture the style present in the artworks\nwell as indicated by the low quantitative metrics scores for\nboth the core as well as the curated dataset experiments.\nThis is further reinforced by the qualitative results where\nFstylegan doesn\u2019t form suitable stylistic clusters.\n2. Uneven style distribution of styles in the datasets\ncan lead to uneven-sized and hierarchical clusters : On the\ninitial inspection of the core datasets and their correspond-\ning metadata, we noticed the presence of multiple stylesand the non-uniform distribution across these styles. Also,\nsome styles share a higher degree of similarity while others\nare completely distinct. For instance, the Wikiart dataset\nhas artworks spanning 129 artists and 27 style classes with\n\u223c30% of artworks belonging to just four artists. Similarly,\nthe majority of samples from Edvard Munch archive are\npencil sketches (88% of the total samples). Based on this\nstyle imbalance, we hypothesize that uneven style distribu-\ntion and the presence of similar styles in the datasets can\nlead to uneven-sized and hierarchical clusters. The follow-\ning experimental evidence supports this hypothesis:\n(i) Majority of the artworks are assigned to just a few\nclusters suggesting the presence of uneven-sized or hierar-\nchical clusters : For Fgram andFg\u00b7c, we observe that the\nmajority of the artworks are assigned to just a few clusters\n(1-3) for the core datasets leading to un-even sized clus-\nters (refer to tables 14 and 15 in Appendix for the distri-\nbution of number of artworks per cluster). When we look\nat these specific clusters for all the datasets, we observe that\nthe majority of artworks present in these clusters are of sim-\nilar style on the first inspection. However, when we per-\nform sub-clustering on a single cluster for these datasets,\nwe observe that the artworks can be further divided based\non fine-granular style. For example, we experiment with\nsub-clustering a single cluster with the highest number of\nsamples for the Brueghel dataset. We observe that after sub-\nclustering, a seemingly singular style cluster, can be further\ndivided into distinct stylistic clusters. This indicates the\npresence of hierarchical clusters and that the high correla-\ntion within the Fgram andFg\u00b7cfeatures caused the artworks\nto be assigned to just a few clusters. More details of this\nexperiment can be found in Appendix E.\n7(a) Edvard Munch Archive\n(b) WikiArt Dataset\n(c) Brueghel Dataset\n(d) Clip Art Illustrations Dataset\nFigure 4. Random samples from the clustered artworks for four\ndifferent core datasets and for the two best style-based clustering\nfeatures Fgram andFg\u00b7c.\n(ii) Increasing CHI score and a stable SC score with an\nincrease in the number of clusters suggests the presence\nof hierarchical clusters : Since WikiArt contains artworks\nwith a large number of styles, we further experiment on the\nWikiArt dataset (refer Appendix F.2) to understand the ef-\nfect of higher values of k(number of clusters) on the cluster\nformation. As the value of kincreases from 10 to 1000, the\nSC metric remains stable while the CHI metric increases.\nSC measures the cluster separation and CHI measures the\nratio of inter-cluster variance to intra-cluster variance. In-\ncreasing values for CHI would mean that either inter-cluster\nvariance is increasing or intra-cluster variance is decreasing.\nSince SC remains stable, it highlights that the cluster sepa-\nration remains constant and that the intra-cluster variance is\n(a) Curated Edvard Munch Archive\n(b) Curated WikiArt Dataset\n(c) Curated Brueghel Dataset\n(d) Curated Clip Art Illustrations Dataset\nFigure 5. Random samples from the clustered artworks for four\ndifferent synthetically curated datasets and for the best style-based\nclustering features Fgram for K-Means and DEC.\ndecreasing. The clusters are becoming more compact while\nthe cluster separation remains constant. As the number of\nclusters increases, the large clusters get broken up into mul-\ntiple small clusters thereby reducing intra-cluster variance.\nThis suggests that the feature space likely has dense regions\nforming hierarchical clusters. UMAPs also show the pres-\nence of tightly packed features suggesting the presence of\ndense regions in the feature space (refer to tables 12 and\n13 in Appendix). A similar hypothesis holds for the syn-\nthetically curated dataset as well. Since there are a unique\nand fixed number of styles and a fixed number of samples\nper style, we get a uniform style distribution with balanced\nstyle clusters. The same thing is observed in the respective\nUMAPs where there are distinct and separated clusters. The\n8SC and CHI metrics, with comparatively better scores than\ncore data metrics, support this observation.\n3. DEC can form distinct and well-separated stylistic\nclusters : Since the feature spaces are forming less sparse\nclusters, we test a deep clustering technique called DEC.\nDEC learns a new latent space with the help of an autoen-\ncoder and labels the data points using a clustering layer. We\nobserve that DEC, with its newly learned latent space, can\nform distinct and well-separated clusters with the SC and\nCHI metric scores showing substantial improvement over\nthe initial K-Means metric scores in most cases. UMAPs for\nDEC show comparatively better-separated clusters, further\nsupporting this observation. Style-wise, DEC can match\nthe performance of K-Means baseline qualitatively, even\nthough the ARI and V-Mes metrics are slightly low. This\ndemonstrates that DEC is on par with K-Means in the stylis-\ntic clustering ability and outperforms K-Means in cluster\nformation ability.\n4. Certain styles are dependent on content : Certain\nstyles are dependent on the content present in the image\n[30]. For Fdense features, we observe that the features fo-\ncus on clustering the artworks based on the objects present\nin them. The same is true for Fcaption features. The main\ndifference between the two features is that for Fdense , the\nclusters tend to focus on content and not consider the style\npresent in the artworks. On the contrary, for Fcaption , we\nobserve that the clusters include artworks that are similar in\ncontent as well as style. The style might be due to some\nstyle information, even though coarse-grained, captured by\nthe captions.\n5. Large Vision Languages Models (LVLM) can be\nutilized for specific style elicitation : For Fannot , we ob-\nserve that the formed clusters comprise artworks with sim-\nilar styles. This indicates that based on the style taxonomy\nprovided, the LVLM can predict the style concepts present\nin artworks to a good extent. This indicates that given a style\ntaxonomy, the LVLM can potentially elicit the specific type\nof style information inherent in the artworks. The LVLM\nmay identify style categories such as art movements, emo-\ntions, art medium, technique, etc. Exploring specific style\ncategories through LVLMs is interesting for future research.\n6. Conclusion\nIn this paper, we propose and explore the artistic style-based\nclustering of artworks. We also propose to use three dis-\nparate novel artwork representations derived from the deep\nneural features in classification networks, generative deep\nmodels, and textual annotations that explicitly consider the\nstyle information from the artwork. We find that these rep-\nresentations enhance the capability of the deep-embedded\nclustering method for targeting artistic style and outperform\nthe current methods on the state-of-the-art metrics for art-\nwork clustering based on generic features. Our exploration\nalso opens up several compelling application and researchquestions pertaining to artwork clustering based on artistic\nstyle. For example: (i) the approach opens up discovering\nfiner-grained style concepts within a corpus of work in an\nunsupervised manner and helps the artists and museums\ndigitally catalog and monetize their styles, and (ii) it also\nhelps explore and create the art style evolution-based\nnarratives and curative practices for an artwork collection.\nReferences\n[1] Siddharth Agarwal, Harish Karnick, Nirmal Pant, and\nUrvesh Patel. Genre and style based painting classification.\nIn2015 IEEE Winter Conference on Applications of Com-\nputer Vision , pages 588\u2013594. IEEE, 2015. 1\n[2] Art-Explosion. Art explosion collection. http://www.\nnovadevelopment.com/ , 2024. 6, 25\n[3] Tadeusz Cali \u00b4nski and Harabasz JA. A dendrite method for\ncluster analysis. Communications in Statistics - Theory and\nMethods , 3:1\u201327, 1974. 6\n[4] Gustavo Carneiro, Nuno Pinho Da Silva, Alessio Del Bue,\nand Jo \u02dcao Paulo Costeira. Artistic image classification: An\nanalysis on the printart database. In Computer Vision\u2013ECCV\n2012: 12th European Conference on Computer Vision, Flo-\nrence, Italy, October 7-13, 2012, Proceedings, Part IV 12 ,\npages 143\u2013157. Springer, 2012. 2\n[5] Giovanna Castellano and Gennaro Vessio. Deep convolu-\ntional embedding for digitized painting clustering, 2020. 2\n[6] Giovanna Castellano and Gennaro Vessio. A deep learning\napproach to clustering visual arts. International Journal of\nComputer Vision , 130(11):2590\u20132605, 2022. 2, 3, 4, 5, 6\n[7] Giovanna Castellano, Eufemia Lella, and Gennaro Vessio.\nVisual link retrieval and knowledge discovery in painting\ndatasets. Multimedia Tools and Applications , 80:6599\u20136616,\n2021. 2\n[8] Eva Cetinic, Tomislav Lipic, and Sonja Grgic. Fine-tuning\nconvolutional neural networks for fine art classification. Ex-\npert Systems with Applications ,"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Summary of \"IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation\"\n\n#### 1. Abstract:\nThe paper introduces the Instance Feature Adapter (IFAdapter) to enhance text-to-image diffusion models' ability to generate images with precise positional accuracy and detailed instance features. IFAdapter addresses shortcomings in the Layout-to-Image (L2I) task, improving both feature depiction and spatial control by incorporating appearance tokens and an Instance Semantic Map (ISM). The model's plug-and-play design enables it to work seamlessly with various community models. Experimental results on the newly developed COCO-IFG benchmark show that IFAdapter outperforms existing models in both instance positioning and feature accuracy.\n\n#### 2. Introduction:\n- **Research Context**:\n  - Text-to-image (T2I) diffusion models excel in generating individual instances but struggle with positioning and feature generation of multiple instances.\n  - Layout-to-Image (L2I) methods use bounding boxes to control instance positioning but often fail to depict detailed features.\n- **Research Questions**:\n  - How to enhance diffusion models' ability to generate detailed instance features while maintaining positional accuracy?\n  - How to design an effective plug-and-play module to improve existing models without retraining?\n- **Literature Review**:\n  - Advances have been made in the L2I task using GANs and diffusion models, but current models like GLIGEN, Instance Diffusion, and others lack the capability to generate fine-grained instance features.\n  - Existing models use category labels and single tokens, which inadequately capture the necessary detail.\n\n#### 3. Related Work:\n- **Layout-to-Image Generation**:\n  - Early methods focused on GANs solving specific issues like object relations and handling bounding boxes.\n  - Recent methods integrate diffusion techniques to improve quality, diversity, and controllability. Key models include LayoutDiffusion, LayoutDiffuse, MIGC, GLIGEN, and Instance Diffusion.\n- **Controllable Diffusion Models**:\n  - Diffusion models use various control conditions, like textual, segmentation, and depth-guided methods, for image generation.\n  - Recent methods include ControlNet, Composer, and Adapter-based techniques specifically for improved spatial control.\n\n#### 4. Method:\n- **Proposed Approach**:\n  - **Instance Feature Generation Task**:\n    - Aims to improve both position and feature generation accuracy in T2I models.\n    - Uses detailed instance-level descriptions and location bounding boxes for training.\n  - **Instance Feature Adapter (IFAdapter)**:\n    1. **Appearance Tokens**:\n      - Utilizes learnable queries interacting with text embeddings to extract detailed features, forming appearance tokens.\n      - Tokens ensure high-frequency details complement the End of Text (EoT) token, improving feature generation.\n    2. **Instance Semantic Map (ISM)**:\n      - Converts appearance tokens and spatial locations into a semantic map ensuring robust spatial guidance during generation.\n      - **Per-instance Feature Generation**:\n        - Generates semantic maps for each instance individually, avoiding feature leakage.\n      - **Gated Semantic Fusion**:\n        - Aggregates maps using a weighted mechanism based on instance importance and size, ensuring the correct features dominate overlapping regions.\n    - **Mathematical Formulations**:\n      - Describes loss function, cross-attention mechanisms, and map construction equations in detail.\n\n#### 5. Experimental Setup:\n- **Datasets**:\n  - COCO2014 and a subset of LAION 5B are used. Instances are annotated with SOTA Vision-Language Models (VLMs).\n- **Training Details**:\n  - Implements IFAdapter on the Stable Diffusion XL (SDXL) model.\n  - Trained with AdamW optimizer for 100,000 steps. Data augmentation includes dropping local and global descriptions.\n- **Evaluation Metrics**:\n  - **Instance Feature Success (IFS) rate**: Measures spatial accuracy and feature-detail consistency using VLMs and instance detection.\n  - **Grounding-DINO AP**: Evaluates positional accuracy.\n  - **FID and Global CLIP Score**: Assess image quality and image-caption consistency.\n- **Baselines**:\n  - Includes InstanceDiffusion, MIGC, GLIGEN, DenseDiffusion, and MultiDiffusion.\n\n#### 6. Results and Analysis:\n- **Quantitative Analysis**:\n  - **IFS Rate**: IFAdapter significantly outperforms baselines in generating accurate detailed features.\n  - **Spatial Accuracy**: Achieves the highest Grounding-DINO AP score, thanks to the ISM-based guidance.\n  - **Image Quality**: Shows improved CLIP score and lowest FID, indicating high-quality images.\n- **Qualitative Analysis**:\n  - Illustrated through multiple figures comparing model outputs for various complex instances, demonstrating better fidelity in IFAdapter\u2019s outputs.\n- **Ablation Studies**:\n  - Removal of appearance tokens or EoT token leads to significant degradation in performance, affirming the necessity of both for high-quality instance generation.\n\n#### 7. Discussion:\n- **Interpretation**:\n  - The design of appearance tokens and ISM enables high-fidelity instance generation both in positional and feature accuracy.\n  - The experiments confirm the effectiveness of combining detailed appearance tokens and enhanced spatial guidance.\n- **Limitations**:\n  - The approach might require extensive computational resources due to heavy reliance on visual language models and complex annotations.\n  - Potential biases could arise from the imbalance in detailed annotations across instances or datasets.\n\n#### 8. Conclusion:\n- **Key Contributions**:\n  - Introduces the instance feature generation task and COCO-IFG benchmark.\n  - Proposes IFAdapter, which significantly enhances feature generation in T2I models and integrates as a plug-and-play module.\n- **Future Directions**:\n  - Extending the methodology to additional community models and styles.\n  - Exploring more efficient and scalable approaches to reduce computational costs without compromising performance.\n\n### Summary for Replication:\n\nTo replicate this study, follow these steps:\n\n1. **Data Collection and Annotation**:\n   - Use the COCO2014 dataset and a subset of LAION 5B.\n   - Annotate instances with bounding boxes using Grounding-DINO.\n   - Generate detailed instance-level descriptions using SOTA VLMs like QWen and InternVL.\n\n2. **Model Design**:\n   - **Architecture**:\n     - Use a pretrained Stable Diffusion model (SDXL).\n     - Integrate IFAdapter with appearance tokens and ISM.\n   - **Appearance Tokens**:\n     - Learnable queries interact with text embedding layers of a CLIP-like text encoder.\n     - Convert instance descriptions into fixed-dimensional appearance tokens.\n   - **Instance Semantic Map (ISM)**:\n     - Construct ISM for each instance and combine them using gated semantic fusion.\n     - Avoid feature leakage and prioritize closer instances in overlapping regions.\n\n3. **Training**:\n   - Train the IFAdapter on SDXL.\n   - Apply AdamW optimizer with appropriate learning rates and training steps.\n   - Data augmentation includes random dropping of local and global descriptions.\n\n4. **Evaluation**:\n   - Measure IFS rate using multiple VLMs.\n   - Calculate Grounding-DINO AP for spatial accuracy.\n   - Use FID and CLIP scores for image quality assessment.\n   - Perform a detailed ablation study to assess the contribution of each component.\n\n5. **Results Analysis**:\n   - Comprehensive quantitative and qualitative evaluation against baseline models.\n   - User studies for additional validation of model performance in real-world scenarios.\n   - Detailed discussion on interpretations, limitations, and future research directions.\n\nBy following the detailed methodology and evaluation criteria outlined, researchers can replicate or extend the study to explore new avenues in grounded text-to-image generation."}, {"role": "assistant", "content": "Preprint.\nIFA DAPTER : INSTANCE FEATURE CONTROL FOR\nGROUNDED TEXT-TO-IMAGE GENERATION\nYinwei Wu1,2\u2217, Xianpan Zhou1, Bing Ma1, Xuefeng Su1, Kai Ma1, Xinchao Wang2\u2020\n1Tencent PCG,2National University of Singapore\nGLIGENInstance DiffusionMIGCOurs\n\u2705A green vase made of glass\n\u2705Chinese style of ink landscape painting\n\u2705Square clock\n\u2705A green sofa with white stripes on it\n\u2705Red deck chair with yellow star on it\n\u2705Crystal chandelier\n\u2705Grey and blue patchwork carpet\n\u2705A wooden suitcase\n\u2705 A green vase made of glass\n\u274c Chinese style of ink landscape painting\n\u274c Square clock\n\u274c A green sofa with white stripes on it\n\u274c Red deck chair with yellow star on it\n\u2705 Crystal chandelier\n\u274c Grey and blue patchwork carpet\n\u274c Missing\n\u274cA green vase made of glass\n\u274cChinese style of ink landscape painting\n\u274cMissing\n\u274cMissing\n\u274cRed deck chair with yellow star on it\n\u2705Crystal chandelier\n\u274cGrey and blue patchwork carpet\n\u2705A wooden suitcase\n\u274cA greenvase made of glass\n\u274cChinese style of ink landscape painting\n\u274cSquare clock\n\u274cA greensofa with white stripes on it\n\u274cRed deckchair with yellow star on it\n\u2705Crystal chandelier\n\u274cMissing\n\u274cMissing\nSDXL\nBluePencilwhite hatorange hatblack hatorange catblack catwhite cat\nPixArt\nClaymation(a)\n(b)\nFigure 1: We present IFAdapter , a novel approach designed to exert fine-grained control over lo-\ncalized content generation in pretrained diffusion models. (a) IFAdapter has the capacity to generate\nintricate features with precision. (b) The plug-and-play design of IFAdapter enables it to be seam-\nlessly applied to various community models.\nABSTRACT\nWhile Text-to-Image (T2I) diffusion models excel at generating visually appeal-\ning images of individual instances, they struggle to accurately position and con-\ntrol the features generation of multiple instances. The Layout-to-Image (L2I) task\nwas introduced to address the positioning challenges by incorporating bounding\nboxes as spatial control signals, but it still falls short in generating precise instance\nfeatures. In response, we propose the Instance Feature Generation (IFG) task,\nwhich aims to ensure both positional accuracy and feature fidelity in generated\ninstances. To address the IFG task, we introduce the Instance Feature Adapter\n(IFAdapter). The IFAdapter enhances feature depiction by incorporating addi-\ntional appearance tokens and utilizing an Instance Semantic Map to align instance-\nlevel features with spatial locations. The IFAdapter guides the diffusion process as\na plug-and-play module, making it adaptable to various community models. For\nevaluation, we contribute an IFG benchmark and develop a verification pipeline\nto objectively compare models\u2019 abilities to generate instances with accurate po-\nsitioning and features. Experimental results demonstrate that IFAdapter outper-\nforms other models in both quantitative and qualitative evaluations. Project Page:\nhttps://ifadapter.github.io/ .\n1 I NTRODUCTION\nThe advent of diffusion models has revolutionized the field of Text-to-Image (T2I) synthesis (Ho\net al., 2020; Podell et al., 2023; Baldridge et al., 2024; Betker et al., 2023; Rombach et al., 2022;\nYang et al., 2023a). Despite their exceptional performance in generating high-quality images of\nsingle instances, these models remain limited in generating images containing multiple instances.\n\u2217Work partly done during an internship at Tencent PCG.\n\u2020Corresponding author.\n1arXiv:2409.08240v1  [cs.CV]  12 Sep 2024Preprint.\nThere are two key challenges underscore this limitation: 1) The ambiguity of sequential natural\nlanguage descriptions of spatial information impedes the accurate transmission of user intent to the\nmodel, resulting in the inability to precisely position instances. 2) Relying solely on a global prompt\nto describe the features of multiple instances fails to correctly associate these features with their\ncorresponding instances (Feng et al.).\nRecent advancements in the Layout-to-Image (L2I) task (Li et al., 2023; Wang et al., 2024c; Zhou\net al., 2024b; Kim et al., 2023; Bar-Tal et al., 2023) have enabled diffusion models to achieve pre-\ncise instance-level position control by incorporating bounding boxes as spatial signals. However,\nin terms of instance feature generation, most state-of-the-art (SOTA) L2I methods can accurately\ndepict only coarse features of an instance (e.g., color attribution), while struggling to generate more\ncomplex, fine-grained features, as shown in Figure 1(a). This shortcoming limits the models\u2019 appli-\ncability in scenarios such as graphic design and fashion design, where a high level of detail in local\nregions is essential. To this end, we propose a more challenging task, termed Instance Feature Gen-\neration (IFG) task, which aims to simultaneously address both the positional accuracy and feature\ngeneration accuracy of instances. We found that existing T2I methods do not perform satisfactorily\non the IFG task. Upon analysis, we attribute this phenomenon to two factors: 1) Training Data: Most\nL2I methods rely solely on category labels as descriptions for instances during training. This ap-\nproach causes samples with detailed descriptions to become out-of-distribution during inference. 2)\nInsufficient Feature Information: Existing designs mostly use a single contextualized token to guide\nthe feature generation of each instance. While this token effectively captures the coarse semantics\nof the instance (Chen et al., 2024), it is limited in generating high-frequency appearance features.\nIn this work, we propose the Instance Feature Adapter (IFAdapter) to address the aforementioned\nlimitations. First, to address issues related to the training data, we utilize existing SOTA Vision-\nLanguage Models (VLMs) for annotation, generating a dataset with detailed instance-level descrip-\ntions. Subsequently, we implement two meticulously designed components to address the challenges\nof instance positioning and feature representation. 1) Appearance Tokens: To address the loss of de-\ntailed feature information in instances, the IFAdapter introduces novel learnable appearance queries.\nThese queries extract instance-specific feature information from descriptions, forming appearance\ntokens that work alongside EoT tokens, thereby enabling more precise control over the generation\nof instance features; 2) Instance Semantic Map: In contrast to sequence-to-2D grounding condi-\ntions (Li et al., 2023; Wang et al., 2024c), IFAdapter constructs a 2D semantic map to correlate\ninstance features with designated spatial locations. This map-like condition provides enhanced spa-\ntial guidance, reinforcing the spatial prior and preventing the leakage of instance features. In regions\nwhere multiple instances overlap, a gated semantic fusion mechanism is employed to resolve feature\nconfusion. The IFAdapter integrates the semantic map only within a subset of cross-attention lay-\ners (Vaswani, 2017) in the diffusion model. This loose coupling allows the IFAdapter to function as\na plug-and-play component, enabling its instance-level control capabilities to be transferred across\nvarious community models without requiring retraining, as illustrated in 1(b).\nFor evaluation, previous L2I benchmarks primarily focused on instance position accuracy, over-\nlooking instance feature accuracy, which limits their ability to fully assess model performance on\nthe IFG task. To address this limitation, we introduce the COCO-IFG benchmark, designed to evalu-\nate models based on both positional accuracy and precise instance feature generation. Additionally,\nto overcome the limitations of existing object detection methods, which are incapable of detect-\ning instance features, we integrate SOTA VLMs to facilitate instance feature detection, establishing\nan objective verification pipeline. Comprehensive experiments on the benchmark demonstrate that\nIFAdapter significantly enhances instance feature generation accuracy while maintaining precise\nposition accuracy.\nThe contributions of this work are as follows:\n1. We propose the Instance Feature Generation task to address the challenges of positional\nand feature accuracy in multi-instance generation using diffusion models. In addition, we\nintroduce the COCO IFG benchmark and verification pipeline to evaluate and compare\nmodel performance.\n2. We propose IFAdapter, which utilizes novel appearance tokens and instance semantic map\nto enhance diffusion models\u2019 depiction of instances, enabling high-fidelity instance feature\ngeneration.\n2Preprint.\n3. Comprehensive experiments demonstrate that our model outperforms the baselines in both\nquantitative and qualitative evaluations.\n4. The IFAdapter is designed as a plug-and-play component, enabling it to seamlessly em-\npower various community models with layout control capabilities without retraining.\n2 R ELATED WORK\nLayout-to-Image Generation. In the early stages, Layout-to-Image (Layout-to-Image) works pri-\nmarily hinged on Generative Adversarial Networks (GANs) (Sun & Wu, 2019; 2021; Li et al., 2021;\nHe et al., 2021; Wang et al., 2022; Sylvain et al., 2021). Novel modules and techniques have been\nproposed to address specific challenges in existing methods, such as object-to-object relations (He\net al., 2021; Sylvain et al., 2021), object appearance (Sun & Wu, 2021; He et al., 2021), and han-\ndling interactions between bounding boxes (Sylvain et al., 2021; Li et al., 2021; Wang et al., 2022).\nNevertheless, withthe rising tide of diffusion-based methods in the generative field, incorporating\ndiffusion techniques into Layout-to-Image methods (Cheng et al., 2023; Zheng et al., 2023; Li et al.,\n2023; Wang et al., 2024c; Zhou et al., 2024b;a; Xie et al., 2023; Xiao et al., 2023; Chen et al., 2024;\nYang et al., 2023b; Avrahami et al., 2023) has led to significant improvements in the quality, di-\nversity, and controllability of generated images. LayoutDiffusion (Zheng et al., 2023) constructs a\nstructural image patch with region information and transforms it into a unified form fused with the\nlayout. LayoutDiffuse (Cheng et al., 2023) proposed an adapter based on layout attention and task-\naware prompts. MIGC (Zhou et al., 2024b;a) utilizes an instance enhancement attention mechanism\nfor precise shading. GLIGEN (Li et al., 2023) and InstanceDiffusion (Wang et al., 2024c) pioneer\nLayout-to-Image generation of open-sets by using the conceptual knowledge of SD pretrained mod-\nels to inject object locations and appearance into features through custom attention. Some works\nexplore the training-free method for Layout-to-Image generation by guiding the attention maps of\nthe diffusion model during the generative process (Xiao et al., 2023; Chen et al., 2024) or imple-\nmenting spatial constraints in the denoising process (Xie et al., 2023).\nControllable Diffusion Models. The emergence of diffusion models has significantly propelled\nadvancements in the field of image generation. Controllable Diffusion Models utilize a wide va-\nriety of control conditions to generate images with specific content, leading to a proliferation of\napplications. Semantic control enables precise manipulation of image attributes or features in the\ngeneration process by referencing text (Rombach et al., 2022; Saharia et al., 2022b; Ramesh et al.,\n2022; Chen et al., 2023a) or images (Tang et al., 2023; Saharia et al., 2022a). Spatial control pro-\nvides fine-grained control over the content in specific regions, such as segmentation-guided (Bar-Tal\net al., 2023; Couairon et al., 2023; Wu et al., 2024a), sketch-guided (V oynov et al., 2023), and\ndepth-guided methods (Kim et al., 2022). Recent efforts have concentrated on integrating these spa-\ntial control conditions into a unified framework for text-to-image generation, including approaches\nsuch as ControlNet (Zhang et al., 2023; Zhao et al., 2024), Composer (Huang et al., 2023), and\nAdapter-based (Mou et al., 2024) methods. ID and style control emphasize maintaining the consis-\ntency of user-specified identity or style in generated images, tuning-based methods guide diffusion\nmodels to generate the specified content by fine-tuning (Hu et al., 2021; Ruiz et al., 2023), while\ntuning-free methods (Ye et al., 2023; Huang et al., 2024; Wang et al., 2024b; Li et al., 2024; Hertz\net al., 2024; Wang et al., 2024a) injecting coded condition embedding in the denoising process.\n3 A PPROACH\n3.1 P RELIMINARIES\nLatent Diffusion Model . Our method is applied over a pretrained T2I diffusion model, more specif-\nically, a T2I latent diffusion model (LDM) (Rombach et al., 2022). The generation process of the\nLDM can be regarded as stepwise denoising from a initial Gaussian noise z\u223c N(0, I), conditioned\non a textual prompt y. The training objective is to minimize the following LDM loss:\nLLDM =Ez\u223cN (0,I),y,t[||\u03f5\u2212\u03f5\u03b8(zt, t, E(y))||2\n2], (1)\nwhere the \u03f5\u03b8is parameterized as a UNet (Ronneberger et al., 2015) and tis the denoising timestep.\nEis a pretrained text encoder, used to encode yinto text embeddings.\n3Preprint.\nCross Attention . In the LDM, text embeddings guide the direction of generation via cross atten-\ntion (Vaswani, 2017), which can be represented using the following equation:\nAttention (Q,K,V,M) =Softmax (QK\u22a4\n\u221a\nd+M)V, (2)\ntheQrepresents the image latent code obtained after being projected through a MLP (Multi-Layer\nPerceptron), while KandVare similarly derived from text embeddings. Mis a mask used to adjust\nattention scores, and drepresents the dimensionality of the hidden vector, which helps stabilize the\ntraining process.\n3.2 P ROBLEM DEFINITION\nIn the Instance Feature Generation task, the LDM requires additional conditioning on a set of local\ndescriptors c={(r1,l1), . . . , (rn,ln)}.rirepresents the designated generation position for the i-th\ninstance, in [x, y, w, h ]form. liis the corresponding phrase that describes the features of the i-th\ninstance. Our method differs from others in that liincorporates detailed, extended descriptions of\nthe instance, including aspects such as mixed colors, complex textures. With cserving as auxiliary\nconditions, the LDM should be able to generate instances with high fidelity in both position and\nfeatures.\n3.3 IFA DAPTER\nIn this work, the IFAdapter is designed to control the generation of instance features and positions.\nWe employ the open-source Stable Diffusion (SD) (Rombach et al., 2022; Podell et al., 2023) as\nthe base model. To address the issue of instance feature loss, we introduce appearance tokens as a\nsupplement to the high-frequency information, as discussed in Sec. 3.3.1. Furthermore, to incorpo-\nrate a stronger spatial prior for more accurate control over position and features, we use appearance\ntokens to construct an instance semantic map that guides the generation process, as elaborated in\nSec. 3.3.2.\nEOTText Encoder\u2026i-th block QKV\nBBoxFourier & MLP\nResampler\n+\u2026appearance token\na corgi in a green suitAppearance Token Fusedi-thMLP\n\u00d7C\nSemantic MapForeground\nEOT\u2026EOT\u2026EOT\u2026appearance tokenInstanceGated Semantic Map Integration\nCA\nCAFreeze\nTrain\nCConcat+Element Plus\u00d7Element MultipleCACross AttentionDrop\nMLP\nCA\nText Encoder\u2026i-th block QKV\nBBoxFourier & MLP\nResampler\n+\u2026Appearance Tokens\na corgi in a green suiti-thMLP\n\u00d7\nC\nSemantic MapForeground\n\u2026\u2026\nEOT\u2026InstanceGated Semantic Map Integration\nCA\nCAFreeze\nTrain\nCConcat+Element Plus\u00d7Element MultipleCACross Attention\nDrop\nMLP\nCA\nCross Attention\nappearance tokenEOT\u2026EOT\u2026MLP\nSemantic MapForegroundInstanceGated Semantic Map Integration\nCACA\nCA\n\u2211\nEOT\u2026EOT + Appearance TokensEOT\u2026EOT\u2026Image FeatureForegroundInstanceSemantic Mapf &softmax\nCACA\nCA\n\u00d7\nFreeze\nTrain\nCConcat+Element Sum\u00d7Element MultipleCACross AttentionInstance Semantic MapCross Attention\n+(a)(b)EOTEOTEOT\nFigure 2: Structure of proposed IFAdapter. (a) The generation process of appearance tokens. For\nsimplicity, we use the generation process of one instance (the corgi) as example. (b) The construction\nprocess of the Instance Semantic Map.\n3.3.1 A PPEARANCE TOKENS\nL2I SD enables the generation of grounded instances by incorporating local descriptions and location\nas additional conditions. Existing approaches (Li et al., 2023; Zhou et al., 2024b; Wang et al.,\n2024c) typically utilize the contextualized token (the End of Text, EoT token) produced by the\npretrained CLIP text encoder (Radford et al., 2021) to guide the generation of instance features.\nAlthough the EoT token plays a crucial role in foreground generation, it is primarily used to generate\ncoarse structural content (Wu et al., 2024b; Chen et al., 2024) and requires additional tokens to\ncomplement high-frequency details. As a result, existing L2I methods that discard all other tokens\n4Preprint.\nare unable to generate detailed instance features. One naive mitigation approach would be to use all\ntokens (77 in total) generated by the CLIP text encoder as instance-level conditions. However, this\nmethod would significantly increase the memory requirements during both inference and training.\nMoreover, these 77 tokens include a substantial amount of padding tokens, which do not contribute\nto the generation. While removing padding tokens can reduce computational costs, this strategy is\nincompatible with batch training due to the varying lengths of the descriptions. To address this,\nwe propose compressing the feature information into a small set of appearance tokens and utilizing\nthese tokens to complement the EoT token.\nDrawing inspiration from the Perceiver design (Ye et al., 2023; Alayrac et al., 2022), we employ a\nset of learnable appearance queries to interact with instance description embeddings through cross\nattention, thereby extracting feature information and forming appearance tokens, as shown in Fig. 2\n(a). It is worth noting that the appearance queries only interact with word tokens, thus converting de-\nscriptions of arbitrary length into fixed-length appearance tokens. In addition, to obtain text features\nof different entangled granularities, the query tokens also interact with text features in the shallower\nlayers of the text encoder. By combining the instance\u2019s appearance tokens with their corresponding\nlocation embeddings, appearance tokens hl\u2208RL\u00d7dare obtained from layer l, where Ldenotes the\nnumber of appearance tokens. This process can be expressed using the following formula:\nH= [hl1, . . . ,hlk]\nwhere hl=Resampler (Qa,Kl,Vl) +MLP(Fourier (r)). (3)\nFor the sake of clarity, we use the generation of appearance tokens for a single instance as an ex-\nample. The Resampler is adapted from Perceiver, composed of multiple transformer blocks. Qa\nrepresents the appearance queries, while KlandVlare obtained by projecting the text features ex-\ntracted from the l-th layer of the text encoder. The Fourier is the Fourier embedding (Mildenhall\net al., 2021), combined with a MLP to project rto the feature dimension. Finally, the appearance\ntokens at kdifferent granularities are concatenated into H\u2208R(kL)\u00d7dto serve as the generation\nguidance for each instance.\n3.3.2 I NSTANCE SEMANTIC MAP-GUIDED GENERATION\nAlong with ensuring the generation of detailed instance features, the IFG task also requires instances\nto be generated at designated locations. Previous method (Li et al., 2023) uses sequential grounding\ntokens as conditions, which lack robust spatial correspondence, potentially leading to issues such as\nfeature misplacement and leakage. Therefore, in our work, we introduce a map called the Instance\nSemantic Map (ISM) as a stronger guiding signal. Since the generation of all instances is guided\nby the ISM, two major considerations must be addressed when constructing the map: (1) generating\ndetailed and accurate features for each instance while avoiding feature leakage, and (2) managing\noverlapping regions where multiple instances are present. To address these concerns, we first gen-\nerate each instance in isolation and then aggregate them in the overlapping regions. The following\nsections will provide a detailed explanation of these processes.\nPer-instance Feature Generation . Avoiding interference from extraneous features is crucial for\nthe precise generation of high-quality instance details. To achieve this objective, we first generate\nthe semantic map of each instance individually. Specifically, for the i-th instance, we transform its\ncorresponding location riinto the following mask mi:\nmi(x, y) =\u001a\n0 if [ x, y]\u2208 R i\n\u2212\u221e if [x, y]/\u2208 R i, (4)\nwhere Rirepresents the coordinates within the region indicated by ri. By employing Eq. 2, we can\nobtain the semantic map sifor the i-th instance:\nsi=Attention (Hi,K,V,mi), (5)\nwhere Hirepresents the appearance tokens of i-th instance, as obtained from Eq. 3.\nGated Semantic Fusion . After obtaining the semantic maps for each instance, the next step is\nto blend these maps to derive the final ISM, as shown in Fig. 2 (b). A critical issue to consider\nduring the map integration process is how to represent the features of a latent pixel when it is asso-\nciated with multiple instances. Previous method (Jia et al., 2024) average the features of multiple\n5Preprint.\ninstances. While this approach is simple, it may lead to feature conflicts between different instances.\nIntuitively, the visual features in regions where multiple instances overlap should be dominated by\nthe instance closest to the observer (i.e., the one with the smallest depth). Therefore, the weights of\ndifferent instances in overlapping regions should vary. For clarity, we use the feature integration at\npixel location (x, y)as an example. The features of each instance are first projected into a scalar\nrepresenting importance through a trainable lightweight network f. Then, the Softmax operation\nnormalizes the importance across different instances, yielding their respective weights. This process\ncan be described by the following equation:\n[w1(x, y), . . . , w n(x, y)] = Softmax (f(s1(x, y)), . . . , f (sn(x, y))), (6)\nwhere wi(x, y)denotes the weight of instance iat location (x, y).\nIn addition to the instance feature, the size of the instance also influences its weight. This design is\nmotivated by the following consideration: when the region of a small instance is completely covered\nby a larger instance, it is necessary to prevent the smaller instance from being \u201cassimilated\u201d due to\nthe inclusion of excessive irrelevant features. Therefore, the proportion of the area occupied by the\ninstance in the foreground is also considered, with smaller instance being assigned greater weight.\nUsing the instance features and their respective weights, the final representation for a latent pixel\nposition (x, y)is obtained using the following formula:\nD(x, y) =X\niwi(x, y)\u00b7Sigmoid (|Sn\njaj|\n|ai|)\u00b7si(x, y), (7)\nwhere airepresents the area occupied by instance i. After the aforementioned steps, the ISM is\nconstructed. Finally, ISM interacts through the following duplicate cross attention layers (Ye et al.,\n2023) to guide the generation of salient regions:\nAttn=Attention (Q,K,V,0) + tanh(\u03bb)\u00b7(1\u2212 M bg)\u2299D, (8)\nwhere Mbgis a binary mask with the background area set to 1, and \u03bbis a trainable parameter\ninitialized to 0 to prevent pattern collapse during the initial training phase.\n3.4 L EARNING PROCEDURE\nDuring training, we freeze the parameters of the SD, training only the IFAdapter. The loss function\nused for training is the LDM loss with instance-level condition incorporated:\nLIFA=Ez\u223cN (0,I),y,t[||\u03f5\u2212\u03f5\u03b8(zt, t, E(y)), c||2\n2] (9)\nTo enable our method to perform classifier-free guidance (CFG) (Ho & Salimans, 2022) during the\ninference phase, we randomly set the global condition yand local condition cto 0 during training.\n4 E XPERIMENTS\n4.1 I MPLEMENTATION DETAILS\nWe described the basic setup for training our model. For more details, please refer to the appendix.\nTraining dataset. We use the COCO2014 (Lin et al., 2014) dataset and a 1 million subset from\nLAION 5B (Schuhmann et al., 2022) as our data sources. Following previous methods (Wang et al.,\n2024c; Zhou et al., 2024b), we utilize Grounding-DINO (Liu et al., 2023) and RAM (Zhang et al.,\n2024) to annotate the instance positions within the images. We then employ the state-of-the-art\nvisual language models (VLMs) QWen (Bai et al., 2023) and InternVL (Chen et al., 2023b) to gen-\nerate captions for the images and individual instance. Training details. We use SDXL (Podell\net al., 2023), known for its strong detail generation capabilities, as our base model. The IFAdapter is\napplied to a subset of SDXL\u2019s mid-layers and decoder layers, which significantly contribute to fore-\nground generation. We trained the IFAdapter using the AdamW (Loshchilov et al., 2017) optimizer\nwith a learning rate of 0.0001 for 100,000 steps and a batch size of 160. During training, there was\na 15% chance of dropping the local description and a 30% chance of dropping the global caption.\nFor inference, we used the EulerDiscreteScheduler (Karras et al., 2022) with 30 sample steps and\nset the classifier-free guidance (CFG) scale to 7.5.\n6Preprint.\n4.2 E XPERIMENTAL SETUP\nBaselines. We compared our approach with previous SOTA L2I methods, including training-based\nmethods InstanceDiffusion (Wang et al., 2024c), MIGC (Zhou et al., 2024b), and GLIGEN (Li\net al., 2023), as well as the training-free methods DenseDiffusion (Kim et al., 2023) and MultiDif-\nfusion (Bar-Tal et al., 2023).\nEvaluation dataset. Following the previous setup (Li et al., 2023; Zhou et al., 2024b; Wang et al.,\n2024c), we constructed the COCO IFG benchmark on the standard COCO2014 dataset. Specifically,\nwe annotate the locations and local descriptions in the validation set using the same approach as in\nthe training data. Each method is required to generate 1,000 images for validation.\nEvaluation Metrics. For the validation of the IFG task, it is imperative that the model generates\ninstances with accurate features at the appropriate locations.\n\u2022Instance Feature Success Rate. To verify spatial accuracy and description-instance con-\nsistency, we propose the Instance Feature Success (IFS) rate. The calculation of the IFS rate\ninvolves two steps. Step 1, Spatial accuracy verification: We begin by using Grounding-\nDINO to detect the positions of each instance. Next, we compute the Intersection over\nUnion (IoU) between the detected positions and the Ground Truth (GT) positions, select-\ning the GT with the highest IoU as the corresponding match for that instance. If the highest\nIoU is less than 0.5, the instance generation is considered unsuccessful. Step 2, Local fea-\nture accuracy verification: Previous methods (Avrahami et al., 2023; Zhou et al., 2024b)\nprimarily employ local CLIP for verifying local features. However, CLIP focuses on over-\nall semantics and is not well-suited for capturing fine visual details (Yuksekgonul et al.,\n2023). Therefore, we utilize VLMs in conjunction with the prompt engineering technique\nto achieve more precise verification of local details. For each local region identified in Step\n1, we prompt the VLMs to determine whether the content within the cropped region aligns\nwith the corresponding description. If the VLM confirms that the content matches the\nprompt, the instance is marked as successful. The Instance Foreground Success (IFS) rate\nis then calculated as the ratio of successful instances to the total number of instances. Ad-\nditionally, we report the Grounding-DINO Average Precision (AP) score to independently\nvalidate the positional accuracy of instance location generation.\n\u2022Fr\u00b4echet Inception Distance (FID). FID (Heusel et al., 2017) measures image quality by\ncalculating the feature similarity between generated and real images. We compute the FID\nusing the validation set of COCO2017.\n\u2022Global CLIP Score. The global caption of the image primarily describes the overall se-\nmantics of the image. Therefore, we use the CLIP score to evaluate Image-Caption Con-\nsistency.\n4.3 C OMPARISON\n4.3.1 Q UANTITATIVE ANALYSIS .\nTab. 1 presents our qualitative results on the IFG benchmark, including metrics of IFS Rate, Spatial\naccuracy, and the Image Quality.\nIFS Rate. To calculate the IFS rate, we utilize three state-of-the-art (SOTA) vision-language mod-\nels (VLMs): QWenVL (Bai et al., 2023), InternVL (Chen et al., 2023b), and CogVL (Wang et al.,\n2023). This multi-model approach ensures a more comprehensive and rigorous validation. As shown\nin Tab 1, our model outperforms the baseline models in all three IFS rate metrics. The introduction\nof appearance tokens and the incorporation of dense instance descriptions in training have signif-\nicantly enhanced our model\u2019s ability to generate accurate instance details. It is worth noting that\nInstanceDiffusion achieves a higher IFS rate compared to other baselines. This is likely due to its\ntraining dataset also contains dense instance-level descriptions. This observation further underscores\nthe necessity of high-quality instance-level annotations.\nSpatial Accuracy. As can be observed from Tab 1, IFAdapter achieves the best results in Grounding-\nDINO AP. This success can be attributed to our map-guided generation design, which incorporates\nadditional spatial priors, leading to more accurate generation of instance locations.\n7Preprint.\nMethodsIFS Rate(%) Spatial(%) Quality\nQwenVL \u2191InternVL \u2191CogVL \u2191 AP\u2191 CLIP\u2191FID\u2193\nReal images 92.8 82.2 69.9 75.3 - -\nInstanceDiffusion 69.6 49.7 38.2 43.1 23.3 26.8\nGLIGEN 44.8 25.8 17.5 18.4 23.5 29.7\nMIGC 62.8 40.7 27.5 32.5 22.9 26.0\nMultiDiffuion 58.1 47.0 34.2 36.9 22.8 28.3\nDenseDiffusion 38.7 26.0 19.7 22.2 20.1 29.9\nOurs 75.8 66.5 58.8 48.3 25.0 22.0\nTable 1: Evaluation on COCO IFG benchmark. To perform a more rigorous and comprehensive\nexperiment for calculating the IFS rate, we utilize three different VLMs. For spatial accuracy, we\nreport the Grounding-DINO AP. To assess overall image quality, we measure the CLIP score and\nFID. The \u2191indicates that a higher value is better, while \u2193signifies the opposite.\nOursInstanceDiffusionMIGCGLIGENDenseDiffusionMultiDiffusion\n a purple flower with a white centera pink flower with a yellow centera colorful vasea yellow flower with a red center\na yellow and white striped floweramagenta and turquoise paisley-patterned butterflyared and white checkered flower\nwooden table with glass topbronze horsestone tigerceramic fountainwooden flowerpot\nFigure 3: Qualitative results. We compare the models\u2019 ability to generate instances with different\ntypes of features, including mixed colors, varied materials, and intricate textures.\nImage Quality. As shown in Table 1, our method demonstrates a higher CLIP Score, indicating\nthat enhancing local details contributes to the simultaneous improvement of image-caption consis-\ntency. Additionally, our method achieves a lower FID, suggesting that the images generated by our\napproach are of higher quality compared to the baselines. We attribute this improvement to the\nadapter-like design of our model, which enables spatial control without significantly compromising\nimage quality.\n4.3.2 Q UALITATIVE ANALYSIS .\nIn Fig. 1(a), we present generation results for a scene with multiple complex instances. We further\nevaluate the models\u2019 ability to generate instances with diverse features in Fig. 3. As shown, our\nmethod demonstrates the highest level of fidelity across various types of instance details.\n4.4 U SERSTUDY .\nAlthough VLMs can verify instance details to a certain extent, a gap remains compared to human\nperception. Therefore, we invited professional annotators for further validation.\n8Preprint.\nMethodsSpatial Instance Details Aesthetics\nScore\u2191Pref. Rate \u2191Score\u2191Pref. Rate \u2191Score\u2191Pref. Rate \u2191\nInstanceDiffusion 4.44 44.4% 3.82 33.3% 2.99 14.8%\nGLIGEN 3.96 14.2% 2.54 3.7% 2.44 3.7%\nMIGC 4.30 33.3% 3.39 7.4% 2.54 3.7%\nOurs 4.85 88.9% 4.69 88.9% 4.10 96.2%\nTable 2: Results of user study. We conducted a user study to evaluate the spatial generation ac-\ncuracy, instance detail generation effectiveness, and aesthetic index of the L2I methods. Evaluators\nwere provided with the image layout and the corresponding image, and they were asked to rate the\naforementioned three dimensions on a scale of 0 to 5. A score of 0 represents the lowest rating,\nwhile 5 represents the highest rating. We also reported the user preference rate (Pref. Rate), which\nrepresents the proportion of the highest scores obtained by the methods.\nSetup. We conducted a study comprising 270 questions, each associated with a randomly sam-\npled generated image. Evaluators were asked to rate image quality, instance location accuracy, and\ninstance details. In total, 30 valid responses were collected, yielding 7,290 ratings.\nResults. As seen in Tab. 2, our method achieves the highest scores and user preference rate across\nall three dimensions. Notably, the trends in these dimensions are consistent with those in Table 1,\nfurther demonstrating the effectiveness of VLM validation.\n4.5 I NTEGRATION WITH COMMUNITY MODELS\nFigure 4: The IFAdapter can seamlessly integrate with community diffusion models.\nThanks to the plug-and-play design of the IFAdapter, it can impose spatial control on pretrained dif-\nfusion models without significantly compromising the style or quality of the generated images. This\ncapability enables the IFAdapter to be effectively integrated with various community diffusion mod-\nels and LoRAs (Hu et al., 2021). As illustrated in Fig. 4, we applied IFAdapter to several community\n9Preprint.\nmodels, including PixlArt (NeriJS, 2023), LeLo-LEGO (LordJia, 2024), Claymation (DoctorDiffu-\nsion, 2024), and BluePencil (blue pen5805, 2024). The generated images not only adhere to the\nspecified layouts but also accurately reflect the respective styles.\n4.6 A BLATION STUDY\nIn our method, appearance tokens are introduced to address the shortcomings of EoT tokens in\ngenerating high-frequency details. This ablation study primarily explores the roles of these two\ntoken types in instance generation.\na guitar case on the floora man in a black shirttalking on a cell phone\nan orange coffee cup with a handleadoughnut sitting on a napkin next to a laptopFull modew/o AP tokensw/o EoT tokenWrong attribution\nWrong attributionWrong instance\nInstance omissionWrong attribution\nFigure 5: Ablation results.\nappearance tokens. As observed in Tab. 3, the removal of appearance tokens leads to a decrease\nin the model\u2019s IFS rate and FID, indicating a loss of detailed features. Furthermore, as illustrated\nin Fig. 5, the images generated without appearance tokens exhibit instance feature mismatches, fur-\nther demonstrating that appearance tokens are primarily responsible for generating high-frequency\nappearance features.\nEoT token. As shown in Table 5, the IFS rate significantly decreases when generating without the\nEoT token. This is primarily because the EoT token is responsible for generating the coarse seman-\ntics of instances. Additionally, Fig. 5 indicates that removing the EoT token results in semantic-\nlevel issues, such as instance category errors and instance omissions.\nappearance tokens EoT tokenIFS Rate(%) Spatial(%) Quality\nQwenVL \u2191InternVL \u2191CogVL \u2191 AP\u2191 CLIP\u2191FID\u2193\n\u2713 69.6 63.9 53.5 45.9 24.1 27.2\n\u2713 29.9 16.2 12.0 12.3 24.3 44.7\n\u2713 \u2713 75.8 66.5 58.8 48.3 25.0 22.0\nTable 3: Results of ablation study.\n5 C ONCLUSION\nIn this work, we propose IFAdapter to exert fine-grained, instance-level control on pretrained Sta-\nble Diffusion models. We enhance the model\u2019s ability to generate detailed instance features by\nintroducing Appearance Tokens. By utilizing Appearance Tokens to construct an instance semantic\nmap, we align instance-level features with spatial locations, thereby achieving robust spatial control.\nBoth qualitative and quantitative results demonstrate that our method excels in generating detailed\ninstance features. Furthermore, due to its plug-and-play nature, IFAdapter can be seamlessly inte-\ngrated with community models as a plugin without the need for retraining.\n10Preprint.\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in neural information processing systems , 35:23716\u2013\n23736, 2022.\nOmri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani\nLischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable im-\nage generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 18370\u201318380, 2023.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu,\nChengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi\nTan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng\nXu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan,\nZheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou,\nJingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023.\nJason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan,\nYichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, et al. Imagen 3. arXiv preprint\narXiv:2408.07009 , 2024.\nOmer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for\ncontrolled image generation. In International Conference on Machine Learning , pp. 1737\u20131752.\nPMLR, 2023.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\nScience. https://cdn.openai.com/papers/dall-e-3.pdf , 2(3):8, 2023.\nblue pen5805. bluexspencil-xl. https://civitai.com/models/119012/bluepencil\n-xl, 2024.\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James\nKwok, Ping Luo, Huchuan Lu, et al. Pixart- \u03b1: Fast training of diffusion transformer for photore-\nalistic text-to-image synthesis. arXiv preprint arXiv:2310.00426 , 2023a.\nMinghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention\nguidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision , pp. 5343\u20135353, 2024.\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qing-\nlong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. In-\nternvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv\npreprint arXiv:2312.14238 , 2023b.\nJiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, and Mu Li. Layoutdif-\nfuse: Adapting foundational diffusion models for layout-to-image generation. arXiv preprint\narXiv:2302.08908 , 2023.\nGuillaume Couairon, Marlene Careil, Matthieu Cord, St \u00b4ephane Lathuiliere, and Jakob Verbeek.\nZero-shot spatial layout conditioning for text-to-image diffusion models. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pp. 2174\u20132183, 2023.\nDoctorDiffusion. claymation-style-lora. https://civitai.com/models/181962/doct\nor-diffusions-claymation-style-lora , 2024.\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato\nBasu, XinEric Wang, WilliamYang Wang, UcSanta Barbara, Santa Uc, Cruz Cruz, and Google\nGoogle. Training-free structured diffusion guidance for compositional text-to-image synthesis.\n11Preprint.\nSen He, Wentong Liao, Michael Ying Yang, Yongxin Yang, Yi-Zhe Song, Bodo Rosenhahn, and\nTao Xiang. Context-aware layout to image generation with enhanced object appearance. In\nProceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition , Jun 2021.\nAmir Hertz, Andrey V oynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation\nvia shared attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 4775\u20134785, 2024.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary of \"Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources\"\n\n#### 1. Abstract:\nThe paper discusses a new method, Source2Synth, designed to teach Large Language Models (LLMs) new skills without requiring costly human annotations. Source2Synth generates synthetic data points grounded in real-world sources, facilitates intermediate reasoning steps, and enhances dataset quality through answerability-based filtering. The approach demonstrates significant performance improvements in two challenging domains: multi-hop question answering (MHQA) and tabular question answering (TQA), achieving up to 25.51% improvement on TQA (WikiSQL) and 22.57% on MHQA (HotPotQA) over fine-tuned baselines.\n\n#### 2. Introduction:\nLarge Language Models (LLMs) exhibit impressive abilities in natural language generation, yet they struggle with tasks involving multi-step reasoning, structured data, or tool usage. These tasks are often linked to structured data available on the web but lack adequate instructions for task execution. Human annotations for fine-tuning are possible but expensive and time-consuming. The Source2Synth method aims to generate synthetic data that can teach LLMs new skills by grounding the data in real-world sources, improving data quality through automated curation, and making the fine-tuning process scalable. The method is tested on multi-hop question answering and tabular question answering, showing significant performance improvements.\n\n#### 3. Related Work:\nSeveral previous efforts have explored synthetic data generation using LLMs. These efforts include probing the LLMs with prompts to generate continuations or predict missing words and enhancing data quality through human or model-based filtering. However, these methods often require human annotations or back-translation approaches and are limited in generality. Recent works leverage real-world data like web corpora or open-source code snippets, but they are task-specific. Source2Synth differs by automating the selection of seed topics and grounding the synthetic data in a diverse range of existing datasets, making it more generalized and scalable.\n\n#### 4. Method:\n**Source2Synth Overview:**\nSource2Synth produces high-quality synthetic datasets through three main stages: Dataset Generation, Dataset Curation, and Model Fine-tuning.\n\n**Dataset Generation:**\n- **Data Source Selection:** The process begins by selecting a data source, such as tables from the web or Wikipedia articles. The data gathered should be realistic, diverse, and factually correct. No human annotations are required.\n- **Seed Generation:** The seed is a topic chosen to condition the generation process, based on randomly selected entries from the source data. The seed ensures the variety and relevance of examples.\n- **Example Construction:** Examples are generated step-by-step, creating intermediate reasoning steps leading to the final answer. This method uses the seed to produce questions, subquestions, reasoning chains, and answers.\n\n**Dataset Curation:**\n- **Initial Fine-Tuning:** The generated dataset is split into two slices. An intermediate model (LLMSynth) is fine-tuned on the first slice.\n- **Data Filtering and Imputation:**\n  - **Imputation:** Blanks parts of generated data are filled in by the model.\n  - **Filtering:** Examples unable to produce correct answers in multiple trials are discarded. This step enhances dataset quality.\n\n**Model Fine-tuning:**\n- The final model (LLMCurated) is fine-tuned on the curated synthetic dataset. This model is expected to perform well on the designated task.\n\n#### 5. Experimental Setup:\n**Datasets:**\n- **Multi-Hop QA:** HotPotQA with 113,000 entries in different splits for training and evaluation.\n- **Tabular QA:** WikiSQL dataset containing over 80,000 examples of natural language questions and SQL queries.\n\n**Hardware and Software:**\n- Models trained and evaluated using state-of-the-art machine learning frameworks and libraries.\n- High computational resources such as GPUs or distributed computing environments were utilized.\n\n**Evaluation Metrics:**\n- **Multi-Hop QA:** Soft exact match (soft-EM).\n- **Tabular QA:** Exact Match (EM) and soft-EM.\n\n**Baselines:**\n- Instruction-tuned LLMs evaluated in zero-shot and few-shot learning settings.\n- Models fine-tuned on human-annotated datasets and unfiltered synthetic datasets.\n\n#### 6. Results and Analysis:\n**Empirical Findings:**\n- **Multi-Hop QA (HotPotQA):** Source2Synth achieved a maximum soft-EM of 66.05% with 3-shot prompts, outperforming baseline models substantially.\n- **Tabular QA (WikiSQL):** Source2Synth reached an EM of 34.50% and a soft-EM of 42.80%, showing significant improvement over other baseline methods.\n\n**Detailed Analysis:**\n- **Ablation Studies:** Examined the effect of data curation and different levels of synthetic data on model performance.\n- **Difficulty and Question Type Analysis:** Showed Source2Synth's superior performance, especially on harder and bridging questions.\n\n**Unexpected Results:**\n- Minor gains observed for comparison-type questions not explicitly targeted in synthetic data generation.\n\n#### 7. Discussion:\n- Source2Synth significantly enhanced LLMs' ability to perform complex reasoning and tool usage tasks without human annotations.\n- The method's adaptability across different domains and tasks makes it a versatile tool for improving LLMs' generalization capabilities.\n- Limitations include its current focus on single-seed topics and single-table SQL queries. Future work can explore extending these to more complex scenarios such as multi-hop questions with greater than two hops or SQL queries involving multiple tables.\n\n#### 8. Conclusion:\nSource2Synth offers a scalable and efficient methodology for generating and curating high-quality synthetic data. It enhances LLMs' proficiency in complex tasks such as multi-hop reasoning and tool usage, presenting a promising direction for future research in teaching LLMs via synthetic data."}, {"role": "assistant", "content": "Source2Synth: Synthetic Data Generation and Curation\nGrounded in Real Data Sources\nAlisia Lupidi1,2, Carlos Gemmell1, Nicola Cancedda1, Jane Dwivedi-Yu1,\nJason Weston1,Jakob Foerster2,Roberta Raileanu1,3,Maria Lomeli1\n1Meta,2Oxford University,3University College London\nAbstract\nLarge Language Models still struggle in chal-\nlenging scenarios that leverage structured data,\ncomplex reasoning, or tool usage. In this pa-\nper, we propose Source2Synth : a new method\nthat can be used for teaching LLMs new skills\nwithout relying on costly human annotations.\nSource2Synth takes as input a custom data\nsource and produces synthetic data points with\nintermediate reasoning steps grounded in real-\nworld sources. Source2Synth improves the\ndataset quality by discarding low-quality gener-\nations based on their answerability. We demon-\nstrate the generality of this approach by apply-\ning it to two challenging domains: we test rea-\nsoning abilities in multi-hop question answer-\ning (MHQA), and tool usage in tabular question\nanswering (TQA). Our method improves per-\nformance by 25.51% for TQA on WikiSQL and\n22.57% for MHQA on HotPotQA compared to\nthe fine-tuned baselines.\n1 Introduction\nLarge Language Models (LLMs) (Devlin et al.,\n2019; Chowdhery et al., 2022; Brown et al., 2020;\nVaswani et al., 2017) have risen to popularity due\nto their remarkable ability to digest and generate\nhuman-like text (Radford et al., 2018). However,\nLLMs still struggle with more complex tasks such\nas multi-step reasoning, tool use and manipulating\nor processing structured data. For many of these\ntasks there exists source data, such as existing struc-\ntured data on the web, but little data of how to use\nit to solve a task.In principle, one can achieve per-\nformance improvements during fine-tuning by col-\nlecting human annotated data of such tasks. How-\never, this is an expensive and time-consuming pro-\ncess (Touvron et al., 2023).\nIn this paper, we propose Source2Synth , a gen-\neral approach to generate synthetic data grounded\nin external real-world sources . Grounding the data\ngeneration process in real-world sources steers theexamples to be more realistic, diverse, and fac-\ntually correct. We showcase our method on two\nchallenging tasks: multi-hop questions based on\nsources from the web, and tabular question answer-\ning using SQL as a tool. In both cases it achieves\nimproved performance without relying on human\nannotations, resulting in a scalable data generation\nmethod for complex tasks.\nSource2Synth consists of three stages: Dataset\nGeneration and Dataset Curation , followed by\nModel Finetuning , see Figure 1. At the Data Gen-\neration stage, we start by selecting a data source\n(such as tables on the web, or related Wikipedia\narticles) to ground our synthetic data generation\nin realistic information for a specific task. Then,\nto generate a given example, our method first se-\nlects a seed topic to condition the generation - for\nexample a specific entity in a Wikipedia article or\na factual statement about a table. Given the seed\ntopic, the method then generates the full example:\nthe instruction (e.g., question), the reasoning chain\nto arrive at the answer (e.g., the steps of multi-hop\nquestion answering, or tool use) and the answer\nitself.\nAt the Data Curation stage, the constructed syn-\nthetic dataset is split into two slices: the first slice\nis used to fine-tune the LLM, resulting in an inter-\nmediate fine-tuned model. We use this model to\ncurate the second slice of data via imputation and\nthe use of a filtering step by rejection sampling. For\nimputation, we blank some parts of a given exam-\nple and accept the example if the model can fill in\nthe blanks. For filtering, we reject examples that\ncannot produce the correct answer in ktrials. This\nprovides a higher quality curated dataset for the\nfinal fine-tuning stage on the second slice, resulting\nin a better performing model on a given task.\nTo demonstrate the generality of our approach,\nwe apply it to two different domains:\n\u2022answering tabular-based questions by learn-\n1arXiv:2409.08239v1  [cs.CL]  12 Sep 2024slice 0  +\n+\n1. Dataset Generation Synthetic \nDataset Base LLM \nLLM\nSynth \nLLM\nCurated Curation \nslice 1 curated \nFiltering \nImputation Seed Data source \nConstruction Dataset Factory \nBase LLM \n2. Dataset Curation 3. Model Fine Tuning Figure 1: Overall Source2Synth Method. In the Dataset Generation step we first choose a data source to build our\ndataset from. For each example we select a seed topic to condition the generation on, and use the data source and\nseed together to construct the example. The resulting synthetic dataset is sliced in two: slice 0 is used to fine-tune\nan intermediate version of the LLM ( LLMSynth ), and we use LLMSynth to curate slice 1 through filtering and/or\nimputation during the Dataset Curation step. The resulting curated dataset is of higher quality and aligned with\nthe user\u2019s design. At the Model Finetuning stage, the final LLM ( LLMCurated ) is trained on the curated synthetic\ndataset, which can then be used to provide good performance on the task of interest.\ning how to use SQL as a tool;\n\u2022answering multi-hop questions by performing\nmulti-step reasoning and information extrac-\ntion.\nTo summarize, our key contributions are:\n\u2022We introduce a new method for generating\nsynthetic examples aligned with the target\ntask, given a real-world data source as con-\ntext.\n\u2022We introduce a curation method based on fil-\ntering and imputation which yields higher\nquality data and improved task performance.\n2 Related Work\nSynthetic Data Generation using LLMs A\nnumber of works propose different strategies to\ngenerate synthetic datasets leveraging pre-trained\nlanguage models. Some of these works rely on\nprobing the knowledge contained in the LLM by\nfirst providing a prompt and letting the model ei-\nther generate the continuation of a prefix or predict\nmissing words in a close-style template (Schick and\nSch\u00fctze, 2020; Schick and Sch\u00fctze, 2021; Petroniet al., 2019; Jiang et al., 2019). Other works intro-\nduce a variety of ways to improve the quality of\nsynthetic data by using model-based or human fil-\ntering (Schick and Sch\u00fctze, 2021; Liu et al., 2022;\nLi et al., 2024; Thoppilan et al., 2022). Our method\nhowever does not rely on human annotations, and\nwe improve the quality of the synthetic data by\nleveraging the LLM itself. Furthermore, our selec-\ntion of the seed topic is automated and we use real\ndata as a starting point. We note that some recent\nwork also leverages real-world data for specific\ncases, such as a corpus from the web to construct\nhigh-quality synthetic data (Nguyen et al., 2024) or\nopen-source code snippets to generate diverse in-\nstruction data for code generation (Wei et al., 2024;\nDubey et al., 2024). In our case, we do not require\na back-translation approach or an initial finetun-\ning to generate the seed to digest the data. Our\nwork proposes a general framework which can be\napplied across tasks. See Liu et al. (2024) for a\nthorough overview of synthetic data research and\nreferences therein.\nTeaching LLMs to Use Tools Enabling LLMs\nto use different tools can augment their abilities\nbeyond text generation and towards manipulating\nstructured data, retrieving information from exter-\n2nal sources, or interacting with APIs. Even though\nthe goal of our work is not specifically to teach\nmodels to use tools, but to develop a general syn-\nthetic data generation approach, we consider this\nto be a by-product. For example, we demonstrate\nhow our method can be used to make LLMs use\nSQL - which is an example of a tool. Tool usage is\na very active area of research. Various works aug-\nment LLMs with general tools or API calls (Parisi\net al., 2022; Schick et al., 2023; Tang et al., 2023),\nwhile some propose to interweave intermediate rea-\nsoning steps with API calls (Gao et al., 2023; Cai\net al., 2024; Paranjape et al., 2023) which improves\nperformance on more complex tasks. Finally, han-\ndling unseen tools at test time has also been tack-\nled (Paranjape et al., 2023; Mekala et al., 2024).\nSee Mialon et al. (2023) and Qin et al. (2023) for an\nin-depth literature review of augmented language\nmodels research and references therein.\nTeaching LLMs to use SQL The above ap-\nproaches usually only enable tool usage for inputs\nthat are strings or numbers. However, using struc-\ntured data during post-training can be useful to\nenhance the LLM\u2019s capabilities in complex tasks,\nsuch as tables or relational data like graphs. A\nparticular tool of interest is SQL since it enables\naggregating information from tabular data. There\nexist a variety of benchmarks that have been pro-\nposed to assess LLMs abilities to generate SQL as\nwell as their performance on tabular-based question\nanswering leveraging SQL tasks (Li et al., 2023a;\nZhong et al., 2017). Alternatively, handling tabular\ndata directly by LLMs has also been tried (Herzig\net al., 2020; Gemmell and Dalton, 2023), and tabu-\nlar question answering benchmarks have been pro-\nposed (Pasupat and Liang, 2015).\n3 Method\nSource2Synth produces high-quality synthetic ex-\namples grounded in external real-world data\nsources, and this resulting synthetic data is pro-\nvided as step-by-step examples to the LLM for\nfine-tuning. Source2Synth is composed of three\nstages: Dataset Generation ,Dataset Curation , and\nModel fine-tuning .\n3.1 Dataset Generation\nData source selection The generation process\nbegins by selecting a data source. This can be an\nalready existing dataset re-purposed for a given\ntask, a collection of existing data points that wewould like to leverage to construct a new dataset, or\nstructured information (e.g. graphs, tables). There\nis no need for human annotations on the entries, as\nSource2Synth will enrich it with extra instructions.\nSeed In order to create a given example of our\nnew synthetic dataset, we first generate a seed topic\nas the initial trigger for the generation process,\nwhich is chosen conditioned on a randomly se-\nlected portion of the source data. The seed inspires\nthe creation of the entry and dictates how the source\ndata will be used. In addition, the randomness of\nthe seed ensures variety in the generated data.\nDataset construction In order to tackle com-\nplex tasks, LLMs can leverage a step-by-step ap-\nproach (Wei et al., 2022) that divides reasoning into\nsmaller sub-tasks plus instructions on how to merge\nback each step into the final one. In Source2Synth ,\nwe leverage the seed to build synthetic data step-\nby-step, decomposing into such intermediate steps\nin order to arrive at an answer for a given question.\nThis reasoning chain can then be used as supervi-\nsion by providing it as the target in the synthetically\ngenerated training examples.\n3.2 Dataset Curation\nThe Dataset Generation process yields an aug-\nmented dataset grounded in real data. At the\nDataset Curation step, we employ a model-based\napproach to automatically refine the dataset to en-\nhance its quality, while avoiding the need for hu-\nman supervision. In particular, we prune the newly-\nbuilt dataset of all the entries that have been in-\ncorrectly crafted or that are deemed low quality.\nThis is achieved by slicing the dataset in two and\nusing one slice to fine-tune the LLM ( LLMSynth ).\nDuring curation, LLMSynth is then used to improve\nthe quality of the second slice of the dataset using\nimputation plus a filtering step. After these steps,\nwe obtain the final curated dataset (shown in purple\nin Figure 1).\nData filtering During filtering, LLMSynth is used\nto predict the output of the given synthetic example\nusing ktries. If the output cannot be predicted at\nleast once, it is assumed the example is low quality\nand is not included in the final curated dataset.\nData Imputation We also consider an imputa-\ntion process, which involves blanking parts of the\naugmented data points and using the LLM to fill\nin the blanks, to replace those fields. This is to\nprovide cleaner data which is less unnatural.\n3D1_text : ' Apollo  11 (July 16\u201324, 1969)  \n     was the American spaceflight \n     that first landed humans on  \n     the Moon.\u2019 D1_title: \u2018The Moon\u2019 \nD2_text : 'Neil  Armstrong became the first person \n     to walk on the Moon as the commander of \n     the American mission Apollo  11 by first \n     setting foot on the Moon at 02:56 UTC \n     on July 21,1969' D2_title: \u2018Neil Armstrong\u2019 D1\nSeed: 'Apollo  11' D2\nQ1 : 'What was the spaceflight that   . \n     .first landed humans on the Moon?' Q2 : 'Who was the commander of Apollo 11? \nA2 : 'Neil Armstrong' \nQ : 'Who was the commander of the spaceflight that first landed humans on the Moon?' \nA : 'Neil Armstrong' \nDataset entry S\nO\nU\nR\nC\nE\nS\nE\nE\nD\nC\nO\nN\nS\nT\nR\nU\nC\nT\nI\nO\nNFigure 2: Source2Synth synthetic data generation process for multi-hop question answering . The method first\nrandomly picks one article D1, in this case with title \"The Moon\". At the Seed stage, an entity E is selected from\nD1\u2019s pool of entities, \u201cApollo 11\u201d. Then, documents are sampled from the related documents pool of D1such that\nEis present, and D2, \u201cNeil Armstrong\u201d, is selected. A question Q1is then generated from D1with the constraint\nthat the answer A1is the entity itself. A second question Q2is then generated from D2with the constraint that its\nmain topic is the entity. We then prompt an LLM to merge the two questions based on the link/entity they have in\ncommon to produce the final question, reasoning chain and answer that comprise the training example.\n3.3 Model fine-tuning\nAt this stage, we fine-tune on the curated syn-\nthetic dataset, initializing from a base version or\ninstruction-tuned version of the LLM. We use our\ndataset for supervised training of both the reason-\ning chain and the final answer given an input. The\nresulting model LLMCurated is then ready to per-\nform the desired task.\n4Applying Source2Synth to Special Cases\nThe general pipeline described above can be used to\nproduce examples for the task at hand and to teach\nLLMs new skills. To demonstrate the impact of\nSource2Synth , we apply it to two challenging tasks\nwhere LLMs struggle, which are both areas of great\ninterest for the community: multi-hop question\nanswering and tabular question answering.\n4.1 Multi-hop question answering\nIn multi-hop question answering (MHQA), we gen-\nerate a dataset of multi-hop question-answer pairs,\nin addition to the reasoning chain that is used toanswer the question, consisting of question decom-\nposition into subquestions with answers, plus the\nentity that links them. See Figure 2 for an overview\nof the procedure and Figure 3 for an example re-\nsponse from the Source2Synth model.\n4.1.1 Dataset Generation\nData source selection For multi-hop question\nanswering, we pick English Wikipedia (Wikipedia\ncontributors, 2004) as the data source, since it con-\ntains articles in natural language as well as addi-\ntional meta-information like links to related articles.\nThe data generation process starts by randomly se-\nlecting an initial article, denoted as D1, among\nall available Wikipedia articles. For each D1we\ncollect n\u22652related articles.\nSeed An MHQA seed topic corresponds to an\nentity Eretrieved from D1. The seed in MHQA\ndoubles also as the \u201chop\u201d in the multi-hop ques-\ntionQthat we aim to generate, since Elinks the\nn= 2 subquestions that compose Q. For exam-\nple, in Figure 2, we sample \"The Moon\" article\n4INPUT \nAnswer the following question `Q` by \ndecomposing it into `Q1` and `Q2` and \nsolving them step-by-step. \nQ: \u201cIt Takes a Family is a response to . .  \nthis 1996 book that was published by who?\u201d \nOUTPUT 1. Splitting `Q` into `Q1` and `Q2`: \n`Q1` : \u201cTo which book published in 1996 is \n        It Takes a Family a response to?\u201d .\n`Q2` : \u201cWho published [A1]?\u201d \n2. Answering Q1: \nThe answer `A1` to `Q1` : \u201cTo which book \npublished in 1996 is It Takes a Family a \nresponse to?\u201d is \u201cIt Takes a Village\u201d \n`A1` : \u201cIt Takes a Village\u201d \n3. Substituting A1 to Q2: \n`Q2`: \u201cWho Published It Takes a Village?\u201d  \n4. Answering Q2: \n`A2` = `A`= \u201cHillary Clinton\u201d Figure 3: Example Source2Synth Response on\nMHQA (closed book inference). We show the model\u2019s\nresponse (reasoning steps and answer) to a multi-hop\ninput question (yellow). The colours highlight the gen-\neration of the corresponding augmented entries: the\ndecomposition into sub questions Q1andQ2in green,\nthe seed A1in blue, and the final answer A2in red.\nat random, denoted by D1, and the corresponding\nentity, denoted by E, is \"Apollo 11\" (displayed\nin blue). Then, we pick \"Neil Armstrong\" as D2\nfrom the pool of related articles, since it contains a\nparagraph where the entity \"Apollo 11\" is included.\nDataset construction We prompt an instruction-\ntuned language model to generate two questions: a\nquestion Q1based on D1and whose answer is the\nselected entity E, and a second question Q2based\nonD2such that its main topic is E. See Figures\n16 and 17 for the exact prompts. For example, in\nFigure 2, Q1=\"What was the spaceflight that first\nlanded humans on the Moon?\", the hop is E=\n\"Apollo\" and Q2=\"Who was the commander of\nApollo 11?\". We then prompt the LLM to merge\nthe two questions, in order to generate the final two-\nhop question Qby using the entity as a conceptual\nlink (hop). The exact prompt is given in Figure 15.\n4.1.2 Dataset Curation\nData filtering We check if the predicted answer\nmatches the answer in the synthetically generated\nexample, and if after ktries the LLM has not sup-\n|   | Year | Country  |   Arrivals     | \n|---|------|----------|------------------| \n| 0 | 2012 |  USA     |   21.7 million   | \n| 1 | 2012 |  Mexico  |   12.4 million   | \n| 2 | 2013 |  Canada  |   29.3 million   | >> sql_table \nSQL: 'SELECT  MAX(Arrivals) FROM sql_table \n      WHERE Year=2012' \nQ : 'What country had the most tourist arrivals \n in 2012?' \nA : 'USA' Seed: \nA: 'USA' \nQ: 'What country had .the most arrivals \n    in 2012?\u2019 \nDataset entry S\nO\nU\nR\nC\nE\nS\nE\nE\nD\nC\nO\nN\nS\nT\nR\nU\nC\nT\nI\nO\nN'The country with most arrivals in 2012.'            Figure 4: Source2Synth synthetic data generation\nprocess for Tabular question answering. The method\nfirst generates the seed, which is a fact based on the table\n(shown in blue). Given the seed and table, an SQL query\nis then generated (in green) as well as its translation\ninto natural language (the question Q). Then the SQL is\nexecuted on the table to obtain the answer A.\nplied the correct answer we filter out the entry\nentirely. See Figure 3 for an example of model\ninference.\nData Imputation For MHQA, we blank Q1and\nprovide the LLM with Q,Q2,E, and the relative\ndoc sample D1as context when asking it to re-\nconstruct Q1. The new candidate for Q1is then\nassessed: if A\u2032(the answer to this new multi-hop\nquestion Q\u2032) matches A(the original answer to Q)\nthen we keep the example. We find that asking the\nmodel to reconstruct parts of the multi-hop question\nin-context results in a more natural and cohesive\nquestion, thus removing some of the unnaturalness\nof the text that can occur from automatically gener-\nated and merged examples.\n4.2 Tabular question answering\nIn Tabular question answering (TQA) we generate\na question-answer dataset where each question is\nbased on a given (real) table from the data source.\nGenerated training examples are hence enriched\nwith tables and annotations which are built from\nautomatically-generated interesting facts retrieved\nthe table.\nData source selection In the TQA case, we use\nunlabeled tables in the train split of the WikiSQL\n5dataset (Zhong et al., 2017).\nSeed We then prompt an instruction-tuned lan-\nguage model to generate a statement based on the\ntable. This statement corresponds to our seed topic\nfor the generation and is a pertinent interesting fact\nor set of observations in natural language that can\nbe derived from the table. The prompt is given in\nFigure 11.\n4.2.1 Dataset Construction\nWe next generate an SQL-statement by zero-shot\nprompting the LLM: we provide the table and the\nseed (factual statement) as context, see Figure 12\nfor the exact prompt. Given the produced SQL\nstatement, it is then executed using the Python li-\nbrary sqlite31to obtain an SQL answer formatted\nas a table. If the generated statement is invalid, we\ndiscard it and re-generate.\n4.2.2 Dataset Curation\nData filtering We check if the predicted answer\nofLLMSynth fine-tuned on slice 0 matches the\nanswer in the synthetically generated example, and\nif after ktries the model has not supplied the correct\nanswer we filter out the entry entirely. See Figure 5\nfor an example of model inference.\n5 Experimental Setup\nWe test our method on two domains: tabular ques-\ntion answering andmulti-hop question answering .\nFor each, we use Source2Synth to generate and cu-\nrate a high quality dataset suitable for fine-tuning,\nand compare our method to a number of baselines.\n5.1 Multi-Hop QA Setup\nData To evaluate multi-hop question answering\nabilities, we evaluate our Source2Synth method\non HotPotQA (Yang et al., 2018). HotPotQA is a\nbenchmark based on Wikipedia containing 113,000\nexamples of multi-hop question-answer pairs and\nis split in train, test, and validation sets. Each\nentry in HotPotQA is constructed such that: 1)\neach question requires finding and reasoning over\nmultiple supporting documents in order to answer;\n2) each entry provides sentence-level supporting\nfacts for strong supervision and explainability of\nthe prediction; 3) each question can be classified\nas either a comparison or bridge question.\nA comparison question entails comparing the\nsame concept between nobjects (e.g. \"Who is the\n1https://www.sqlite.org\nINPUT \nOUTPUT Since I don\u2019t know the schema of the table, \nI will inspect the first two rows. \n|   | Train | Day |   Distance travelled  |  \n|---|-------|-----|-----------------------| \n| 0 |   A   |  2  |        23.5 km        | \nI will make an SQL statement to answer the \nquestion. \nSELECT MIN(Kilometers) FROM sql_table WHERE  \nDay = 2  \nA : 'Train A' Answer the following question using the SQL \ntable `sql_table`. \nQ: \u201cWhat train travelled the minimum \n   distance on day 2?\u201d \n|   | Train | Day |   Distance travelled  |  \n|---|-------|-----|-----------------------| \n| 0 |   A   |  2  |        23.5 km        | \n| 1 |   B   |  1  |        67.4 km        |\n| 2 |   C   |  2  |        36.3 km        |\n|...|  ...  | ... |          ...          | Figure 5: Example Source2Synth Response on TQA.\nWe show the model\u2019s response (SQL call and final\nanswer) to the tabular input question (yellow). The\ncoloured parts highlight the generation of the corre-\nsponding augmented entries: SQL in green, and the\nfinal answer in red.\ntallest student in class?\"), while a bridge question\nbuilds on a logical and/or causal link and requires\nderiving one statement to get to the answer (e.g.\n\"What is the height of the student that topped the\nentry exam?\" - this requires first identifying the\nstudent that topped the exam. The hop length is the\nnumber of comparison objects for comparison ques-\ntions or the number of links for bridge questions. In\nour case, we chose n= 2to be consistent with Hot-\nPotQA. The test set consists of 7,405 entries, split\nevenly between bridge and comparison questions.\nWe only generate synthetic data for bridge ques-\ntions, since they pose a bigger challenge to current\nLLMs. In order to counterbalance this disparity,\nwe include 500 comparison questions from Hot-\nPotQA\u2019s training dataset in our fine-tuning dataset.\nMetrics We measure the performance using soft\nexact match (soft-EM) as the metric. Soft-EM is 1\nif the generated output contains the golden answer\nand 0 otherwise.\nModel The base model used in the experiments\nin MHQA is Llama-2 70B-Chat. We hence fine-\ntune Source2Synth and various other baseline meth-\nods initializing from this model. Source2Synth is\ntrained with 1250 synthetic examples, unless noted\n6otherwise, in addition to the 500 HotPotQA exam-\nples described above.\nBaselines We compare Source2Synth to the fol-\nlowing baselines:\n\u2022Instruction-tuned LLM: using LLama 2 70B-\nChat for the task in a zero-shot manner.\n\u2022Fine-tuned LLM (HotPotQA only): fine-\ntuning from the base model on 500 HPQA\nexamples from the training split.\n\u2022LLMSynth (Synthetic dataset only): training\nour model with 1250 synthetic examples from\nSlice 1 (see Figure 1), without the data cura-\ntion step.\n\u2022LLMSynth (Synthetic and HotPotQA): train-\ning with the uncurated synthetic data in addi-\ntion to the 500 HPQA examples.\nFor all the models listed, we tested them using\ntwo prompting methods: a zero-shot and a three-\nshot CoT prompt, see the Appendix C for details.\n5.2 Tabular QA Setup\nData We conduct evaluations with the Wik-\niSQL (Zhong et al., 2017) dataset validation split.\nThe WikiSQL dataset consists of a corpus of 80,654\nhand-annotated examples of natural language ques-\ntions, SQL queries, and SQL tables created from\n24,241 tables extracted from Wikipedia. The vali-\ndation split contains 7,857 examples after remov-\ning non-executable SQL tables, see Appendix B\nfor more details.\nMetrics We measure performance using the ex-\nact match (EM) and the soft-EM metrics. The EM\nmetric equals 1 if the golden answer is equal to the\ngenerated answer and 0 otherwise.\nModel For TQA, we use the Starchat-beta lan-\nguage model (Li et al., 2023b) from Huggingface as\nthe initial language model (batch size 32, 100 steps,\nlr 0.0001, linear warm-up). The Starchat model is\nan instruction-tuned LLM with 16 billion param-\neters trained to act as a helpful coding assistant.\nThis model is a fine-tuned version of StarCoder (Li\net al., 2023b), a LLM which was pre-trained and\nthen fine-tuned on a large code corpus, which con-\ntains SQL statements, and successively fine-tuned\non 35B Python tokens. For our Source2Synth gener-\nated data, the initial number of synthetic examples\nper slice is 8k (so 16k in total). After curation, we\nkeep 2160 of the examples in slice 2 (27%).Baselines We compare the performance of our\nSource2Synth method against a variety of baselines.\nThe baselines consist of prompting the Starchat-\nbeta instruction-tuned language model as follows:\n\u2022Zero-shot Table QA : prompt with the task in-\nstruction, the table and the question in a zero-\nshot fashion. See Figure 7 for the prompt.\n\u2022One-Shot No Context QA : prompt with the\ntask instruction and a one-shot example con-\ntaining a question and answer, together with\nthe actual question for the model to answer.\nSee Figure 8 for the prompt.\n\u2022One-Shot Table QA : prompt that includes the\ntable for both the one-shot example and the\nquestion to be answered. We use one-shot due\nto LLM context length and the typically large\nsize of the tables. See Figure 9 for the prompt.\n\u2022One-shot Table+SQL QA : the prompt includes\nan example containing the table and question,\nand an instruction suggesting that the model\ncan leverage an SQL tool. We then execute\nthe predicted SQL to obtain the answer. See\nFigure 10 for the prompt.\n\u2022LLMSynth : Finetune the model with synthetic\ndata but without applying the data curation\nstep.\n6 Results\n6.1 Multi-Hop question answering\nOverall performance of Source2Synth on\nMHQA We report the experimental results in\nTable 1. We include the baselines of the vanilla\ninstruction-tuned LLM, a fine-tuned LLM using\nonly the HPQA 500 examples from the train split\n(second row), and LLMSynth which only uses the\nuncurated synthetic data for fine-tuning (third row).\nAll fine-tuned methods outperform the instruction-\ntuned model (first row). Using only synthetic data\nor only HotPotQA data for fine-tuning demon-\nstrates worse performance than when combined,\nwhether the synthetic data is curated (fifth row) or\nnot as in LLMSynth (fourth row). Once we use\nthe full Source2Synth pipeline to obtain the curated\nsynthetic dataset for fine-tuning we see further per-\nformance improvements LLMCurated (fifth row)\nover not curating the data (fourth row).\nAnalysis of performance on different ques-\ntion types and levels of difficulty We study the\n7Method 0-shot 3-shot CoT prompt\nInstruction-tuned LLM (LLama 2 70B-Chat) 40.45% 44.13%\nfine-tuned LLM (HotPotQA only) 53.22% 58.40%\nLLMSynth (Synthetic dataset only) 52.31% 56.70%\nLLMSynth (Synthetic and HotPotQA) 57.46% 62.73%\nLLMCurated (Synthetic and HotPotQA) 65.23% 66.05%\nTable 1: Evaluation of Source2Synth on Multi-hop question answering. The models shown are fine-tuned with\n500 entries from HotPotQA (\u2018HotPotQA\u201d) and/or 1250 entries from the Source2Synth Synthetic Dataset (\u201cSynthetic\nDataset\u201d). Using Source2Synth curated synthetic data in combination with HotPotQA (last row) works best.\nBridge Comparison\nModel Hard Medium Easy Hard Medium Easy\nLlama2-70B-Chat 5.3% 11.3% 27.1% 13.4% 17.7% 26.4%\nfine-tuned LLM (HotPotQA only) 14.6% 16.9% 39.0% 23.5% 31.4% 35.6%\nLLMCurated-1250 25.3% 30.7% 41.8% 23.3% 32.3% 36.9%\nTable 2: Analysis of MHQA bridge and comparison questions with respect to level of difficulty. We evaluate\nmodels on 1k entries for each question type. Source2Synth was used to generated bridge question type data, hence\nLLM-Curated-1250 outperforms other models particularly for bridge questions.\nLLMSynth-500\nLLMCurated-500LLMSynth-750\nLLMCurated-750LLMSynth-1250\nLLMCurated-1250\nModels5658606264666870Accuracy (Soft-EM)   0-shot\n  3-shots\nFigure 6: Synthetic Data scaling performance. We\nshow how the performance of Source2Synth changes\nwith respect to MHQA data mix size, both before and\nafter curation. During the curation step, the following\npercentages of samples were removed: 7% for 500, 8%\nfor 750, 11% for 1250. LLMSynth (before curation) per-\nforms worse than LLMCurated (after curation) despite\nhaving more samples \u2013 but both approaches improve\nwith more data.\ncapabilities of our model by analysing the perfor-\nmance of LLM-Curated-1250 with particular fo-\ncus on the type and difficulty of the questions \u2013\nnamely hard/medium/easy bridge and comparisonquestions. We compare the performance of the\nbase model, the model fine-tuned on HotPotQA,\nand Source2Synth according to the difficulty level,\nas provided by the HotPotQA dataset. We also\nsubdivide the results according to the type of ques-\ntion (bridge vs. comparison). Results are given in\nTable 2.\nIn almost all cases, we observe that\nSource2Synth performs better, particularly\non bridge questions, with an overall gain of 17.44%\ncompared to the base model and a 14.56% gain\ncompared to the LLM fine-tuned on HotPotQA.\nFor the comparison questions, we see modest gains\nbecause our synthetic dataset only includes the\nharder task type (bridge). However, it is interesting\nto see some (small) improvement despite not\nexplicitly targeting comparison-type questions\nin our dataset. Hence, Source2Synth tackled a\nmore demanding task (bridge vs. comparison),\nand achieves 25.3% on hard-bridge questions\ncompared to 14.6% of the Instruction-tuned LLM\nfine-tuned on HotPotQA.\nScaling performance We also report scaling per-\nformance in Figure 6. We study how performance\nevolves when adding more synthetic data in the\nfine-tuning data mix - that already includes 500\nsamples from the HPQA train split. We perform the\nanalysis on LLMSynth andLLMCurated to show\nthe impact of the curation technique. In both cases\n8Method Exact Match Soft-EM\nOne-Shot No Context QA (Starchat-beta LLM) 0.25% 16.22%\nZero-shot Table QA (Starchat-beta LLM) 1.83% 20.07%\nOne-Shot Table QA (Starchat-beta LLM) 2.03% 31.06%\nOne-shot Table+SQL QA (Starchat-beta LLM) 12.30% 34.13%\nLLMSynth (Synthetic dataset only) 23.86% 34.21%\nLLMCurated (Synthetic dataset only) 34.50% 42.80%\nTable 3: Tabular question answering. Performance comparison on the WikiSQL evaluation dataset.\nand in all data mixes, we see that applying the\nSource2Synth pipeline results in a stronger model\non the task. For the LLMSynth model fine-tuned\non uncurated samples we see that providing more\nsynthetic examples leads to a steady improvement\nin performance across all data sizes, for both zero-\nshot and three-shot prompting variants. LLMCu-\nrated follows a similar trend, but consistently out-\nperforms the uncurated version of the model, for\nall training set sizes. Overall, we observe that using\nour synthetic data generation pipeline to construct\nmore data brings further performance gains in the\ntask.\n6.2 Tabular question answering\nWe report the experimental results for Tabular ques-\ntion answering in Table 3. Firstly, they indicate\nthat providing no context about the table when\nprompting the instruction-tuned StarChat language\nmodel has very poor performance (first row), with\nan EM metric of 0.25%. This is expected since\nthe WikiSQL benchmark questions require infor-\nmation contained in the table, and the model does\nnot have any other information to answer the ques-\ntion except for the general knowledge stored in its\nparameters. However, even if we pass the table\nas part of the prompt, the performance does not\nimprove much. For example, passing in a zero-shot\nfashion (second row) only has an EM metric of\n1.83%. This may be challenging for the model as\nthe information in the table is presented in a form\nthat is not easy for the LLM to naturally handle\n(i.e. a table rather than natural language). While\npassing an example of table usage in a one-shot\nfashion (third row) improves the soft-EM metric,\nthe EM metric is still very low (2.03%). Hence,\nthis is still very challenging for the model. Thirdly,\nthe performance increases once we provide a one-\nshot example containing the relevant table and SQL\nquery (fourth row), with an EM of 12.3%. The abil-\nity to use the SQL tool improves the performancemarkedly.\nWe obtain a significant increase in performance\nwhen we fine-tune the StarChat model using the\nSource2Synth curated data (last row), with an EM\nof 34.5%. Our full method performs significantly\nbetter than fine-tuning the StarChat language model\nusing synthetic data without curation, LLMSynth\n(second to last row) which has an EM of 23.86%, al-\nthough that still outperforms the other baselines by\na large margin as well, indicating the utility of our\nSource2Synth synthetic data generation scheme.\n7 Conclusion\nIn this paper, we introduce Source2Synth , a new\nmethod for generating and curating high-quality\nsynthetic data grounded in real data sources. We\ndemonstrate its utility on two tasks that pose sig-\nnificant challenges for LLMs: multi-hop reasoning\nand tabular question answering with SQL. We be-\nlieve our work could also be beneficial in other\nlow-data regimes, and future work could explore\nour approach on other tasks and in diverse fields,\nfor example, in to biology, chemistry and medicine.\n8 Limitations\nIn this paper, our applications use a single seed to\nderive two-hop questions in the case of MHQA\nor SQL on a single table in TQA. However,\nSource2Synth can be extended to more complex\nquestions e.g. with more hops, and even more\ncomplex tool-use, e.g. the use of multiple tables.\nThis could be done by looping the dataset gener-\nation steps and feeding the result of the previous\nstep as input to the next one. Our approach pro-\nvides a way to sample the related articles graph\nand corresponding entities within the articles based\non simple rejection sampling but we believe that\nour method could be improved with more clever\nsampling techniques. We consider this to be an\ninteresting avenue of future research.\n9References\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165 .\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,\nand Denny Zhou. 2024. Large language models as\ntool makers. Preprint , arXiv:2305.17126.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. Preprint , arXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. Preprint , arXiv:1810.04805.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurelien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Roziere, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Gregoire Mi-\nalon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,Jeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract\nThe paper proposes a novel federated learning (FL) framework to counteract model poisoning attacks, which are vulnerabilities to FL systems. By introducing an approach that leverages multiple models instead of a single global model, the authors aim to improve the security and robustness of FL systems deployed over Multi-Access Edge Computing (MEC) networks. This multi-model based FL mechanism dynamically alters the structure of client models throughout the learning epochs. A deep reinforcement learning (DRL) based model selection strategy is employed to optimize model selection, minimizing loss and recognition time while maintaining robustness against attacks. Empirical results in a Distributed Denial of Service (DDoS) attack detection scenario demonstrate competitive performance gains and potential improvements in recognition time.\n\n### 2. Introduction\nFL allows collaborative training of a global model while preserving data privacy by sharing model parameters instead of raw data. Traditional FL systems assume secure operations, but they are susceptible to model poisoning attacks due to the uniformity of the global model structure. Model poisoning involves injecting poisoned local models to corrupt the global model. This paper advocates for a proactive strategy utilizing a multi-model based FL system where a master model is trained by multiple dynamically changing client (slave) models. Using DRL for model selection, the framework aims to optimize performance metrics and enhance robustness against poisoning attacks by exploiting model diversity. The research addresses gaps in the literature concerning reactive approaches to FL security, particularly focusing on proactive defenses.\n\n### 3. Related Work\nThe paper reviews existing FL defense mechanisms, primarily focusing on outlier detection and behavioral analysis to mitigate model poisoning attacks. Previous works include robust aggregation techniques and efficient outlier detection methods to isolate attackers. However, these methods can be inadequate against sophisticated poisoning strategies, especially in heterogeneous environments where benign models might be misclassified as outliers. The novelty of this research lies in taking a proactive stance rather than a solely reactive one, by diversifying model structures and integrating dynamic model selection through DRL.\n\n### 4. Method\n#### Multi-Model Based FL Protocol\nThe proposed FL protocol leverages multiple models (slave models) to train a master model (global model) in a MEC environment. The framework includes the following steps:\n1. **Local Training**: Each device trains a local model based on its data and the specified slave model structure.\n2. **Attack Detection and Mitigation**: Malicious models are detected based on their mismatch with the preplanned model structures.\n3. **Partial Aggregation at BSs**: Aggregation of local models at base stations using weighted averages.\n4. **Knowledge Transfer to Master Model**: Knowledge from slave models is transferred to the master model using unsupervised datasets.\n5. **Uploading Parameters**: BSs transmit master model parameters to the central cloud.\n6. **Cloud Aggregation**: The central cloud aggregates master models from BSs to form the new global model.\n7. **Broadcasting Updates**: Updated models are broadcasted to devices for the next training epoch.\n\nMathematical formulations, including the defining of states, actions, and reward functions for DRL optimization of model selection, are provided. The model selection problem is formulated to minimize the loss and recognition time while ensuring robustness against attacks.\n\n### 5. Experimental Setup\n#### Datasets\nThe CICDDoS 2019 dataset, which contains realistic traffic data, is used. The dataset includes various features such as IP flow statistics, source/destination IP addresses, ports, protocols, and more. Data is split among devices for training (9000 instances) and testing (3000 instances).\n\n#### Hardware and Software\n- **Hardware**: Devices with CPU frequencies ranging from 1.9 to 2.4 GHz, BSs with CPU frequencies of 3.2 and 2.6 GHz.\n- **Software**: TensorFlow and Keras for GRU and DRL implementation. The SUMO simulator is used for generating mobility traces.\n\n#### Evaluation Metrics\nEvaluation focuses on:\n- Accuracy of DDoS attack detection.\n- Recognition time.\nPerformance is compared against baseline FL methods under both normal and poisoning attack conditions.\n\n### 6. Results and Analysis\nEmpirical results indicate:\n- Improved attack detection accuracy under poisoning attacks for the MM-FL-DRL approach, demonstrating competitive performance with non-attacked scenarios.\n- Reduced recognition time by selecting smaller slave models during training in less optimal transmission scenarios.\n- Figures and tables detail the cumulative reward, accuracy over epochs, and recognition time comparisons, highlighting the effectiveness of the multi-model approach.\n\n### 7. Discussion\nThe multi-model based FL system provides improved robustness against poisoning attacks by increasing model diversity and dynamically selecting models via DRL. This proactive approach helps mitigate the risks posed by poisoned models. Limitations include the need to balance between security and performance, potential overfitting issues with DRL, and computational overheads.\n\n### 8. Conclusion\nThe proposed MM-FL framework shows promise in enhancing FL systems' robustness against poisoning attacks through model diversity and DRL-based optimization. Future research directions include exploring more sophisticated poisoning attacks, improving DRL algorithms for faster convergence, and extending the framework to other types of adversarial attacks.\n\n### Summary of the Methodology for Replication\n1. **Multi-Model FL Framework**: Design a system where multiple slave models train a master model, facilitating dynamic model changes in each learning epoch.\n2. **Deep Reinforcement Learning Integration**: Implement DRL for optimal model selection, ensuring the models selected minimize loss and recognition time while maintaining security.\n3. **State, Action, and Reward Definitions**: Formulate state features based on transmission rates and model distributions, define actions as model assignments to devices, and design reward functions based on cumulative accuracy and recognition time.\n4. **Experimentation**:\n   - Set up a simulation grid (4x4, 100m width) with mobile devices.\n   - Use CICDDoS 2019 dataset for training and testing.\n   - Evaluate against poisoning attacks and compare with baseline FL scenarios.\n\nThis comprehensive summary should allow complete replication of the study, ensuring all critical methodological, technical, and empirical details are captured."}, {"role": "assistant", "content": "Accepted paper for publication in Globecom 2024. You can use this material personally. Reprinting or republishing this\nmaterial for the purpose of advertising or promotion, creating new collective works, reselling or redistributing to servers or\nlists, or using any copyrighted component in other works must adhere to IEEE policy. The DOI will be supplied as soon as it\nbecomes available.Multi-Model based Federated Learning Against\nModel Poisoning Attack: A Deep Learning Based\nModel Selection for MEC Systems\nSomayeh Kianpisheh1, Chafika Benza \u00a8\u0131d1, and Tarik Taleb2\n1Centre for Wireless Communications, University of Oulu, Finland ,2Ruhr University Bochum, Germany\nEmails: somayeh.kianpisheh@oulu.fi, chafika.benzaid@oulu.fi, tarik.taleb@rub.de\nAbstract \u2014Federated Learning (FL) enables training of a global\nmodel from distributed data, while preserving data privacy.\nHowever, the singular-model based operation of FL is open with\nuploading poisoned models compatible with the global model\nstructure and can be exploited as a vulnerability to conduct model\npoisoning attacks. This paper proposes a multi-model based FL\nas a proactive mechanism to enhance the opportunity of model\npoisoning attack mitigation. A master model is trained by a set\nof slave models. To enhance the opportunity of attack mitigation,\nthe structure of client models dynamically change within learning\nepochs, and the supporter FL protocol is provided. For a\nMEC system, the model selection problem is modeled as an\noptimization to minimize loss and recognition time, while meeting\na robustness confidence. In adaption with dynamic network\ncondition, a deep reinforcement learning based model selection is\nproposed. For a DDoS attack detection scenario, results illustrate\na competitive accuracy gain under poisoning attack with the\nscenario that the system is without attack, and also a potential\nof recognition time improvement.\nIndex Terms- federated learning, poisoning attack, MEC,\ndeep reinforcement learning.\nI. INTRODUCTION\nFederated Learning (FL) provides collaboration opportu-\nnities for devices in training a global model from locally\ndistributed data while preserving the data privacy by shar-\ning model parameters instead of raw data [1]. The learning\nis performed through some epochs. At each epoch, locally\ntrained models of devices are transmitted to an aggregator\nserver which aggregates the models to shape a global model\nfor the next epoch of training. Utilizing a centralized server\nis inefficient due to single point of failure and intolerable\nlatency in realtime scenarios [2], [3]. FL in edge computing is\nan alternative where Multi-Access Edge Computing (MEC),\ncollocated at base stations either aggregate the parameters\nfrom the devices in their district and return the results for a\nfast response, or partially aggregate the parameters and further\ntransmit to a server for global aggregation [2], [3]. Most of the\nFL studies in edge computing assume a secure FL protocol.\nA model poisoning attack exploits system vulnerabilities to\ncontrol the global model\u2019s aggregation by generating poisoned\nlocal model updates to inject into the system so that the global\nmodel becomes either useless or less accurate. To make the FL\nrobust, defense mechanisms mainly focus on mitigating model\npoisoning attack through outlier-detection mechanisms [4], [5],\n[6], [7] or behavioural-based analysis [8], [9]: The studies in[4], [5] utilize robust aggregation rules in FL, that apply outlier\ndetection methods to remove malicious models. The study in\n[6] provides a cost-efficient outlier detection method to isolate\nthe attackers. The authors of [10] emphasize on performance\nof learning when applying an outlier detection. The authors of\n[7] make FL robust against Byzantine clients while preserving\nthe privacy of individual users. The studies in [8], [9] analyze\nthe behaviours of the clients to determine unreliable clients and\nremove them in the aggregation phase. Either outlier-detection\nbased mechanisms or mechanisms using behavioural-based\nanalysis might not be efficient in recognition of new emerging\nmodel poisoning attack patterns, or attacks with complex be-\nhaviours. Also, the performance of outlier-detection methods\nreduces in heterogeneous environment where benign model\nmight be considered as outlier [10], [11]. In contrast to the\nreactive robustness mechanisms in the literature, this paper\nadvocates a proactive approach to make the opportunity for\npoisoning lower, which can foster defense lines in addition to\noutlier-detection approaches, particularly for the attacks which\ncan deceive the attack detection by intelligent model crafting.\nThe poisoning attacks have a knowledge about global model\nstructure so that they emulate a poisoned model compatible\nwith global model. For example, A Little Is Enough attack\n[11], according to a Gaussian-based statistics deviates each\ndimension of the model parameter from the mean with a\nfraction of the standard deviation . Model poisoning attack\nhas been mathematically modeled in [8]. The attacker tries to\ndeviate from global model with an arbitrary malicious model.\nPoisoned models compatible with global model structure, are\nuploaded. The poisoned model is calculated by updating global\nmodel with a learning rate based on the difference between\nthe malicious target model and the global model .\nThe poisoning attacks use the singular-model based oper-\nation of FL as a vulnerability. They assume that all clients\nuse the same model as the global model. Then, through\napplying an algorithm that follows their objectives in global\nmodel deviation, they emulate poisoned models with the same\nstructure of the global model and inject them in learning\nprocess e.g., [11], [8], [4], [6]. This paper, proposes a multi-\nmodel based FL at which a master model or global model is\ntrained by a set of slave models, letting dynamic change of\nclient models within the learning epochs.arXiv:2409.08237v1  [cs.LG]  12 Sep 2024Dynamic change of models that should be trained by clients\ncan be either a barrier for attack operation or reduce the\nattack triggering chance. For example, A Little Is Enough\nattack [11] may not be able to drive the mean and standard\ndeviation for the global model dimensions , when the clients\nwill have heterogeneous models with different structures. Also,\npoisoning attacks e.g., [11], [8], [4], [6] construct a poisoned\nmodel compatible with the structure of the global model. When\nthe client model is planned to be different from the global\nmodel, the mismatch between uploaded poisoned model and\nplaned model, can be detected at aggregation time\u2013providing\nthe opportunity of attack detection and mitigation. Emulation\nof client model is impossible if the communication protocol\nof FL be secure and the client model structure be dynamic\nand unknown for the attacker. Otherwise, random crafting of\nclient model can be easily detected by detection mechanisms\ndue to high deviation from normal behaviour. Sophisticated\ncrafting of a client model demands designing new advanced\nattacks and introduces complexities. Advanced knowledge\nabout e.g., dynamicity of the models, models\u2019 structures are\nrequired and new optimization for crafting should be derived\nto deviate global model in the desired direction, with crafting\nof heterogeneous, dynamic and different models from global\nmodel at client sides. To the best of our knowledge this sort of\nattack has not been investigated yet. Thus, multi-model based\nFL introduces potentials in dealing with poisoning attacks.\nThis paper introduces Multi-Model based FL (MM-FL)\nas a proactive mechanism to reduce poisoning opportunity.\nAn optimization is provided for model selection problem\nto minimize the loss and recognition time, while meeting a\nrobustness confidence. To provide optimal model selection in\nlarge state/action spaces and under dynamic nature of wireless\ncommunication channels, a Deep Reinforcement Learning\n(DRL) based model selection is proposed. For a Distributed\nDenial of Service (DDoS) attack detection scenario, results\nillustrate improvement in accuracy and recognition time.\nII. SYSTEM MODEL\nNetwork: A network consists of Ndevices such as IoT and\nmobile devices, MBase Stations (BSs) equipped with MEC\nservers, and a (central) cloud. Let mthBS be represented\nwith BSmwith CPU frequency fcmp\nm. The available CPU\ncycle at the central cloud is fcmp\nc. We use symbol ufor\na device, and its data with size |Du|is represented with\nDu={(xu\n1, yu\n1), ...(xu\n|Du|, yu\n|Du|)}, where yis the label for\ninput x. CPU frequency of the device uisfcmp\nu .\nThe model sharing can be done through MEC/cloud infras-\ntructure, to construct a global model through FL. The aim\nof FL is to train a global learning model Mcbased on the\ndistributed data in the devices, through some learning epochs.\nAt each epoch of learning, a device performs learning on its\nlocal data to train a local model. The local models will evolve\nglobal model through FL process, based on which the required\nrecognition is performed (e.g., attack detection).\nAdversary Knowledge and Operation: At any epoch of\nlearning, the adversary can exploit vulnerabilities of devices\nFig. 1. Multi-model FL architecture.\nand compromise them to inject poisoned models in learning\nprocess. The attacker does not have any control over aggre-\ngation process at MEC, nor over the protocol of the benign\ndevices which follow a normal implementation of the protocol.\nIn consistent with the poisoned attacks in the literature,\nadversary has the knowledge about global model and emulates\npoisoned model(s) with the same structure as global model\nthrough applying an algorithm. It then uploads the malicious\nmodel(s) on behalf of the compromised device(s) when com-\nmunicating with aggregator at MEC [8]. The attack can be\ntargeted or untargeted depending on the attacker objective,\nand our approach is applicable for both cases. In the case\nof targeted attack, the malicious model(s) are crafted so\nthat the aggregated model over learning epochs approaches\na targeted model [8]. Otherwise, untargeted deviation strategy\nis employed in poisoning [8].\nFederated Learning Objective: The aim of federated\nlearning is to train a global learning model, namely called\nmaster model based on the distributed data in the devices that\ntake part in learning. Master model is trained through a set of\nslave models with the objective of making the FL robust with\na predefined confidence level, while keeping the performance\nof FL in terms of accuracy and recognition time.\nIII. M ULTI -MODEL BASED FL\nFig. 1 shows the architecture of multi-model based FL.\nMultiple models are used to train the main model, thereby\nhaving multiple training scripts at a device. At each learning\nepoch, network state is observed, according which a DRL-\nbased model selection decides about the model that should\nbe trained at each device. Then, the decision about models\nare announced to devices and they will train the model, they\nhave been asked. After uploading model parameters, the edge\naggregator will perform attack mitigation, aggregation, and\nknowledge transfer. The model parameters are downloaded\nfor the next round. In the case of attack, the adversary that\nhas knowledge about the global model structure, emulates\npoisoned models compatible with global model and upload\nthem on behalf of the compromised devices. The attack can\nbe mitigated before aggregation due to mismatching with the\nmodel plan which the multi-model coordinator arranged it.\nA. Fundamental\nLetMcbe the master model with weights wcandMs=\n{M1\ns, ...Ml\ns}be the slave models with weights {w1\ns, ...wl\ns}.Mcalso can belong to the set of slave models, i.e., Mc\u2208Ms.\nThe main idea is to train master model using slave modes\naccording to an optimal model selection plan that operates\nas hindrance against Poisoning attack while maintaining the\nperformance of FL.\nLetxj,u\ns(\u03c4)be the binary variable indicating the usage of\nmodel Mj\nsin federation of knowledge of device uat epoch\ntime\u03c4. The value of 1 indicates the usage of the model, while\na value of 0 indicates not use of the model. As at every epoch\nof FL, only one model is selected for a device:\n\u2200\u03c4, u:lX\nj=1xj,u\ns(\u03c4) = 1 . (1)\nWe define the variable Tu(\u03c4)as the slave model that is\nselected to be used for the device u. We assume the asso-\nciated weight is Wu. The attack which emulates a poisoned\nglobal model on behalf of device u, can be detected at edge\naggregator, due to a mismatch with the plan, if we have:\nTu(\u03c4)\u0338=Mc (2)\nThe aim is to secure the FL with a user defined confidence\nlevel, while maintaining the efficiency of FL performance. The\ndiversity of models, will introduce hidden variables and will\nreduce the success of triggering poisoning attack. However,\ntwo issues will rise: First, new FL protocol should be designed\nto let the multi-model based learning process. Second, an op-\ntimization framework should be developed to decide about the\nmodels that are trained by devices, considering the dynamic\nnature of the network.\nAlgorithm 1: Knowledge Transfer\n1foreach BS mdo\n2 foreach slave model Ms,j\ne,m do\n3 foreach instance in De,m do\n4 input\u2190instance\n5 give input to model Ms,j\ne,m\n6 label\u2190output neurons of Ms,j\ne,m\n7 we,m(\u03c4)\u2190GD optimization to minimize the loss\nfunction with the input for training as labeled De,m\nB. Multi-Model Based FL Protocol\nMulti-model based FL can train master model from slave\nmodel set. Master model can be included in the slave model\nset, since it can be the most efficient model in the context\nof recognition problem. The parameters of master model are\nfound in learning process to minimize the global loss function\nin (4), where f(wc, xu\ns, yu\ns)is the loss value over master model\nfor the sample sof data of user u.\nmin\nwcF=1\nNX\nu1\n|Du||Du|X\ns=1f(wc, xu\ns, yu\ns). (3)\nFL is performed through some epochs, indexed by \u03c4, until\nconvergence or meeting of termination condition. BS mtrains\na master model Me,m, which has the same neural structure\nasMc, from a set of slave models Ms\ne,m={Ms,1\ne,m, ...Ms,l\ne,m}\nwith the same structures of Ms. It also has an unsupervised\ndata set of instances De,mthat is used for knowledge transfer.\nEach FL epoch, consists of the steps as below:\n1)Local Training: Each device employs a Gradient Decent\n(GD) based method using its local batch data to minimize\nthe loss function over the planed model Tu(\u03c4). Then ittransmits the local model parameters to its associated BS\nfor the purpose of partial aggregation.\n2)Attack Detection and Mitigation: For each uploaded\nmodel from a device uif the the parameters count are\ndifferent than the planed model parameters count for that\ndevice ( Wu(\u03c4)), the model will be detected as malicious\nand will be excluded from the aggregation.\n3)Partial Aggregation at BSs: For each slave model, the\npartial aggregation at BS m, is performed as a weighted\naverage of received parameters of that model:\nMs,j\ne,m=1\nKm,j=Km,jX\nu=1N(|Du|).wu, (4)\nwhere Km,jis the number of devices under the coverage\nofBSi, who use slave model Ms,j\ne,m; and|Du|is the size\nof batch data of device u; and finally, wuis the weight\nof the trained model of device u. Symbol N(.)is the\nnormalization operator.\n4)Knowledge Transfer To Master Model: Each BS performs\nknowledge transfer step to transfer the knowledge of each\nslave model to the master model. First, the unsupervised\ndata set is given to each slave model to be labeled. Then,\nthe labeled data set is used to train and update the weights\nof the master model we,m(\u03c4), based on a GD based\noptimization. Algorithm 1 is the pseudocode.\n5)Uploading of parameters: Each BS transmits it\u2019s master\nmodel weights we,m(\u03c4)to the cloud for the global\naggregation.\n6)Cloud Aggregation: The global model is constructed at\ncentral cloud as average of models of BSs and will be\nbroadcasted to the BSs:\nMc(\u03c4) =1\nMMX\nm=1Me,m. (5)\n7)Broadcasting the Update Results: Among the slave mod-\nels in Ms\ne,m, the model with the same structure of Tu(\u03c4)\nwill be transmitted to the user u.\nFL epochs will be repeated, and finally the Mcparameters\nwill be broadcasted to the users at the final epoch or when\nthe termination condition be met. Note that at step 6 after\nbroadcasting of global model, the master model at BSs, and\naccordingly the devices that have uploaded the master model,\nwill automatically be updated. This lets the master model\nupdate distribution even through training.\nIV. M ODEL SELECTION OPTIMIZATION\nThe 6G wireless model in [12] is used for communication\nbetween devices and the base stations. The transmission rate\ndevice ucommunicates with BS i, is calculated by (6). Bi,\nPtu,gu,\u03b7are respectively, transmission bandwidth of the\nbase station, transmission power of the device, channel gain\nof the device, and background noise power. Eq. (7) is the\nchannel gain calculation. Here, Cg,du,i,\u03b1are respectively\npath loss fading coefficient, distance between device uand\nbase station i, and path loss exponent. Note that when devices\nmove transmission rates will change due to distance variation.\nRu,i=Biln(1 +Ptu.gu\n\u03b7), (6)\ngu=Cg.d\u2212\u03b1\nu,i, (7)The time cost of one epoch of FL is the time it takes a device\nreceives the new updates of the model parameters. It includes\nthe time slots allocated for local training, up/down-link param-\neter transmission, knowledge transfer, and the aggregation.\n\u2022Knowledge Transfer: In BS m, the time for transferring\nknowledge to master model is calculated by:\nTm\nknw=X\nj[|De,m|.fcmp\nds,j\nfcmp\nm+|De,m|.fcmp\nds,c\nfcmp\nm], (8)\nwhere fcmp\nds,j,fcmp\nds,care the number of required CPU cycles\nto label one unit of data by slave model Mj\ns, and train\nmaster model Me,mwith one unit of data, respectively.\n\u2022Aggregation: The time for partial aggregation at BS m\nincludes: (a) the time it takes the parameters be trans-\nmitted from devices under the coverage area of that base\nstation i.e., Rmto the base station; (b) the aggregation\noperation time over the slave models:\nTm\nag= max\nu\u2208Rm|Wu|\nRu,i+X\njXj\ns.|wj\ns|.fcmp\nw\nfcmp\nm, (9)\nwhere fcmp\nw is the number of required CPU cycles to\naggregate one unit of data. Xj\nsis the number of devices\nunder coverage of BS, that uploaded slave model Mj\ns.\nThe time for global aggregation at central cloud is cal-\nculated by (10). It includes the time for partial aggre-\ngation/knowledge transfer, at base stations, as well as\nthe time takes for global aggregation of the parameters\ncollected from base stations at the central cloud:\nTag= max\nm=1..M(Tm\nag+Tm\nknw+|wc|\nRi,c) +M.|wc|.fcmp\nw\nfcmp\nc. (10)\n\u2022Downlink Parameter Transmission: The required time to\ndownload the parameters at device uunder the coverage\nof BS mis calculated by (11), as the time required\nfor parameters transmission from cloud to BS (with\ntransmision rate Rc), and from the BS to the device.\nTdown (u) =|wc|\nRc+|Wu(\u03c4)|\nRm,u. (11)\n\u2022Local Training: Local training time at device uis calcu-\nlated based on the computing capability of the device and\nthe batch size. Eq. (12) is the calculation. Here, fcmp\ns is\nnumber of required CPU cycles to train one sample of\ndata over selected model.\nTloc(u) =|Du|.fcmp\ns(\u03c4)\nfcmp\nu. (12)\nAfter the download of partially aggregated model, the device\ncan perform the recognition. The recognition time in one\nepoch of FL, for device u, is calculated as below:\nTInt(u) = max\nuK.Tloc(u) +Tag+Tdown (u) +finf\nu\nfcmp\nu, (13)\nwhere Kis the local training iterations before applying the\nother epoch of learning. finf\nuis the number of CPU cycles\nto perform the recognition for a sample of input features. The\nresponse time varies over epochs of learning.\nIn a multi-model based FL, model selection for devices\nimpacts the training and communication time, as well as the\naccuracy (See Eq. (9), (11), (12)). Straggler devices with poor\nFig. 2. (a) Policy networks. (b) Simulation grid.\ntransmission channel can prolong the learning process [13].\nSelecting a smaller model than the master model, under poor\ntransmission rates can speed up training, and compensate the\nlatency of transmission, as well as reduce the transmission\nload\u2013all speeding up the learning process. Furthermore, the\nsecurity is enhanced since a different model than the global\nmodel can be utilized for training and poisoning attack can\nbe mitigated at aggregation time (See Fig. 1). However, in the\ncases that global model be more efficient than slave models,\nthere can be accuracy-reduction. On the other hand, in high\nquality of communication status, selecting global model in\nthe case that it is more efficient than smaller model can\nenhance the accuracy, however the robustness reduces. Indeed,\nappropriate tradeoffs are required and optimization should be\nperformed for the optimal selection.\nThe slave models are assigned to the users, to minimize the\nloss function and recognition time over all users, over all\nepochs of learning. \u03b1and\u03b2make the loss and time values\nin the same scale. Besides the already defined constraints,\nconstrain (15) ensures the robustness against poisoning attack.\n1.(B)is 1 if condition Bholds. To reduce the potential for\npoisoning, it ensures that the ratio of global model selection\nbe less than a threshold value Tmax\u2208[0,1]. IfTmax= 0,\npoisoning attacks that emulate global model can not be trig-\ngered. However, the accuracy will be reduced if the master\nmodel be the most accurate model.\nmin\nxj,u\ns(\u03c4)X\n\u03c4,u\u03b1.Fu(\u03c4) +\u03b2.TInt[\u03c4](u)(14)\nsbj. (1), (2), (4), (5), (6), (7), (8), (9), (10), (11), (12), (13), (14),\n\u2200\u03c4:X\nu1.(Tu(\u03c4) =Mc)\u2264N.Tmax.(15)\nV. DRL- BASED MODEL SELECTION\nThe search space order is exponential with dynamic chan-\nnel communication. MDP and RL, advocated for solutions\nadoptable with dynamic network situations, can be used for\nformulation, since: (i) Function (14) has the memoryless\nproperty, as it can be calculated as the sum of loss and\nrecognition time values in devices, in the current interval and\nthe function value in the previous interval; (ii) Considering\nthe parameters determining the current state e.g., transmission\nrates, association of slave models to devices, every action that\nis performed by the agent ends to a new state transition,\nthat only depends on the current state. (iii) The function\n(14) is in the form of accumulated rewards. Through an\niterative process of observing the state, choosing an action,\nand receiving a reward, the state-action Q-values are estimatedby Bellman equation [14]. The high dimension of the states\nand the dynamicity in state transitions makes observing all\nstates and actions in training impossible, thereby inefficiency\nof conventional RL. To deal with this problem, we adapt\nDRL [14], that generalizes experienced states/actions to non-\nobserved ones through a neural network-based approximation\nof Q-values.\nMDP Elements: MDP Elements include:\nState : The features representing the state of network at time\nstep\u03c4, are as below:\n\u2022The transmission rates represented with matrix Rd,e(\u03c4),\nat which the entry at row iand column jis the transmis-\nsion rate between device iand BS j, at epoch \u03c4. Available\nbandwidth, distance of devices from BSs, channel gain\nvaries the transmission rates.\n\u2022The current slave model distribution denoted by vector\nX(\u03c4), at which the entry at row jand column uis the\nassignment of slave model Mj\nsto device uat learning\nepoch \u03c4(values of xj,u\ns(\u03c4))\nActions: The action is decision about the assignment of devices\nto slave models (values of xj,u\nsfor the next epoch of learning).\nReward: In the case of violation of constraints (1), (15), reward\nis 0. To ensure, optimizing the objective function the reward\nfunction is calculated as:\nR(s(\u03c4), a(\u03c4)) =X\nu\u03b1.Fu(\u03c4) +\u03b2.TInt[\u03c4](u)(16)\nTraining: Using Npolicy networks, the decision policy is\nderived by training them. Each Neural Network (NN) rep-\nresents the assignment of a device to the slave models. The\ninput neurons are the state features. There is a Fully-Connected\nlayer, with Softmax activation function. The neuron iin the\noutput layer of NN u, indicates the probability of assigning\nthe slave model Mi\nsto the device u(See Fig. 2.a).\nEach episode consists of a run of FL within several epochs of\nlearning. There are variation in network communication status,\nlocation of devices, and compromised devices within episodes.\nTraining is done through two steps performed at every epoch\nof learning: (i) Exploration: According to \u03f5\u2212greedy policy,\nwith a 1\u2212\u03f5probability, a random assignment of slave models\nto devices is selected. Otherwise, the current state features are\ngiven as input to the NNs. For each device, the slave model,\nwith the highest probability at output layer, will be assigned\nto the device. (ii) Updating the weights: After the assignment\nstrategy, reward is calculated by (16), accordingly the NNs\u2019\nweights are updated by Gradient Descent (GD) method, and\nusing Bellman equation [14].\nVI. E XPERIMENTAL RESULTS\nA scenario that 10 devices that are moving by vehicles,\nwill collaborate in DDoS attack detection, is considered. The\nadvantage of applying FL in DDoS attack detection in terms of\nenhancing accuracy through sharing attack detection models in\ncomparison with individual (local) learning approaches have\nbeen illustrated in many recent studies e.g., [3], [15]. The study\nin [3], extensively discusses the FL benefits for DDoS attack\ndetection. We used CICDDoS 2019 data set [16] comprising\nrealistic traffic to abstract the communications for legitimateand DDoS attack traffic through protocols e.g., HTTP, FTP.\nWe have randomly distributed 9000 instances composed of\nUDPLag and SYN DDoS attacks, among devices for train-\ning, as well as 3000 instances for the purpose of test. The\ndataset provides 87 IP flow features e.g., source/destination IP\naddresses/ports, protocols, flow packet statistics, flag-related\ninformation etc., which we utilize them for the attack detec-\ntion.\nWe assume a 4\u00d74bidirectional grid environment with\n100mwidth for each grid cell, where grid lines are bidi-\nrectional roads (Fig. 2.b). Mobility traces of vehicles have\nbeen generated by SUMO simulator [17]. Manhattan mobility\nmodel in urban areas [17] with probability of 0.5 for moving\nstraight and 0.25 for moving right/lef at conjunctions, is\nused for mobility of vehicles. The mean speed of vehicles\nare 45 km/h. The mobility of vehicles will cause dynamicity\nin transmission rates. We used GRU as we explained it\nwith details in [3] for attack detection. The feature matrix\nfor the packets in a flow is arranged as rows of patterns.\nThe occurrence probability for each pattern is calculated as\na function of previous observations using a GRU. A flow\n(including 10 packets) is malicious if the ratio of the malicious\npackets in that flow is larger than a threshold (0.7). TensorFlow\nand Keras are used to implement the GRUs/DRL.\nWithout loss of generality, the learning collaboration is\nonly performed in the level of edge computing. Two BSs\nequipped with MEC-servers with respectively CPU frequen-\ncies of 3.2 and 2.6 GHz [18] are located in the locations\n[50,50],[350,350] to provide edge coverage for devices. Each\nBS has 2400 random unsupervised instances from CICDDOS\ndata set for knowledge transfer. The coverage radius of base\nstations are 300 m, and their transmission bandwidth are\nrespectively, 28 and 30 MHz [3]. The CPU frequency of\ndevices are randomly chosen in the range of 1.9 up to 2.4 GHz.\nThe transmission power of BSs and devices are respectively\n34 db and 23 db [18]. Path loss exponent is 5 and back ground\nnoise power is -174 db.m [18].\nThere are more explorations at early iterations of DRL,\nwhile the exploitation gradually increases up to the greedy\nselection of 98% at the last episode. For GRUs, the learn-\ning rates 0.07 for GD optimization operated efficiently.\nLearningRateSchedule package of Keras reduces the learn-\ning rate within training for the purpose of convergence. The\nout-layer of the GRU is a neuron to predict the occurrence\nprobability of a packet. See [3] for details. Discount rate of\n0.1 and ADAM optimization in DRL also operated efficiently.\nIn the rest, GRU with 28 and 32 neurons in the hidden\nlayer are represented with GRU 28 and GRU 32, respectively.\nVarious scenarios have been considered: Two FL scenarios\nwhen GRU 28 and GRU 32 are applied for DDoS attack\ndetection. Each FL is applied in a system without attack\n(FL-GRU 28, FL-GRU 32), and a system with poisoning\nattack (FL-GRU 28-With Attack, FL-GRU 32-With Attack).\nIn MM-FL the slave models set include GRU 28 and GRU\n32. Two scenarios of MM-FL are considered: MM-FL with\nmaster model as GRU 28 and MM-FL with master model asFig. 3. Cumulative reward.\n Fig. 4. DDoS attack detection accuracy.\nFig. 5. Recognition time.\nGRU 32. We have MM-FL-DRL which utilizes DRL in model\nselection, and MM-FL-RND which uses a random selection of\nslave models. At each episode, the adversary compromises 3\nto 5 random devices and emulates Malicious Local Models\n(MLMs) with the global model structure, for updating using\nthe method in [8], as explained in Introduction, with random\ntarget malicious models and random learning rate in (0.25,\n0.35). The results are average over 50 runs of test, Tmax= 0.6.\nFig. 3 illustrates the cumulative reward gain within episodes\nin MM-FL. The cumulative reward has increased up to range\nof 6.5 to 7 and become stable around episode 3200, with a\ndominance in reward with the case that GRU 32 is master\nmodel. Since, the scenario that GRU 32 is master has gained\nbetter performance in terms of accuracy and recognition time.\nFig. 4 shows the accuracy of DDoS attack detection. The\nbars with bold boarders are for GRU 32 scenarios, and the\nrest are for GRU 28 scenarios. For GRU 32 scenarios, the\naccuracy considerably decreases after poisoning, and this de-\ncrease becomes the worse at the late epochs of FL due to more\ninvolvement of MLMs in aggregation e.g., drop from 0.92 to\n0.15 at epoch 6. MM-FL-RND has increased the accuracy\nslightly e.g., for 0.09 in epoch 6. MM-FL-DRL has increased\nthe accuracy considerably and in competitive with the scenario\nof FL without attack, due to optimization of loss function in\nmodel selection and selecting atleast 60% of the local models\ndifferent than the main model, according which MLMs can be\ndetected and mitigated. Similar results has been achieved for\nGRU 28 scenarios. However, MM-FL-DRL (Master: GPU 32)\noperates better than MM-FL-DRL (Master: GPU 28), which\ncan illustrate the more efficiency of knowledge transfer from\nGRU 28 to GRU 32, in comparison with the reverse in MM-\nFL-DRL (Master: GPU 28). This is an application dependent\nissue.\nFig. 5 shows the recognition time. For GRU 32 scenario,\nthe recognition time reduces up to 0.6 ms, after multi-model\nFL application with GRU 32 as master. Since, in this scenario,\nMM-FL-DRL uses GPU 28 for atleast 60% of devices which\ndemands less training and transmision time than GRU 32.\nHowever, application of MM-FL with GPU 28 as master,\nincreases recognition time in comparison with FL with GRU\n28 due to employing larger model of GRU 32 in training.\nVII. C ONCLUSION\nThis paper proposes a multi-model based FL as a proactive\nmechanism to enhance the opportunity of model poisoning\nattack detection and mitigation. A protocol at which a mastermodel is trained by a set of slave models, is explained. For\nfederation over a MEC system, the model selection problem is\nmodeled as an optimization and a DRL-based model selection\nmethod adaptable with dynamic network conditions is pro-\nvided. For a DDoS attack detection scenario, results illustrate\na competitive accuracy with the scenario without attack. With\na smaller model employment in training recognition time\nreduces.\nREFERENCES\n[1] B. McMahan et al. , \u201cCommunication-efficient learning of deep networks\nfrom decentralized data,\u201d in Artificial intelligence and statistics , 2017,\npp. 1273\u20131282.\n[2] J. Zhou et al. , \u201cA differentially private federated learning model against\npoisoning attacks in edge computing,\u201d IEEE Trans. on Dependable and\nSecure Computing , 2022.\n[3] S. Kianpisheh and T. Taleb, \u201cCollaborative federated learning for 6G\nwith a deep reinforcement learning based controlling mechanism: A\nddos attack detection scenario,\u201d IEEE Trans. on Network and Service\nManagement, DoI: 10.1109/TNSM.2024.3387987 , 2024.\n[4] E. M. El Mahdi, R. Guerraoui et al. , \u201cThe hidden vulnerability of\ndistributed learning in byzantium,\u201d in International Conf. on Machine\nLearning , 2018, pp. 3521\u20133530.\n[5] D. Yin, Y . Chen, R. Kannan, and P. Bartlett, \u201cByzantine-robust dis-\ntributed learning: Towards optimal statistical rates,\u201d in International\nConf. on Machine Learning , 2018, pp. 5650\u20135659.\n[6] S.-M. Huang, Y .-W. Chen, and J.-J. Kuo, \u201cCost-efficient shuffling\nand regrouping based defense for federated learning,\u201d in IEEE Global\nCommunications Conf. , 2021, pp. 1\u20136.\n[7] J. So et al. , \u201cByzantine-resilient secure federated learning,\u201d IEEE J. on\nSelected Areas in Communications , 2021.\n[8] R. Al Mallah et al. , \u201cUntargeted poisoning attack detection in federated\nlearning via behavior attestation,\u201d IEEE Access , 2023.\n[9] X. Pan et al. , \u201cJustinian\u2019s gaavernor: Robust distributed learning with\ngradient aggregation agent,\u201d in USENIX Security Symp. , 2020.\n[10] L.-Y . Chen, T.-C. Chiu, A.-C. Pang, and L.-C. Cheng, \u201cFedequal: De-\nfending model poisoning attacks in heterogeneous federated learning,\u201d\ninIEEE Global Communications Conf. , 2021, pp. 1\u20136.\n[11] G. Baruch, M. Baruch, and Y . Goldberg, \u201cA little is enough: Circumvent-\ning defenses for distributed learning,\u201d Advances in Neural Information\nProcessing Systems , vol. 32, 2019.\n[12] Z. M. Fadlullah and N. Kato, \u201cHCP: Heterogeneous computing platform\nfor federated learning based collaborative content caching towards 6G\nnetworks,\u201d IEEE Trans. on Emerging Topics in Computing , 2020.\n[13] R. Schlegel et al. , \u201cCodedpaddedfl and codedsecagg: Straggler miti-\ngation and secure aggregation in federated learning,\u201d IEEE Trans. on\nCommunications , 2023.\n[14] V . Mnih et al. , \u201cHuman-level control through deep reinforcement learn-\ning,\u201d J. Nature , vol. 518, no. 7540, pp. 529\u2013533, 2015.\n[15] J. Li et al. , \u201cFleam: A federated learning empowered architecture to\nmitigate ddos in industrial iot,\u201d IEEE Trans. on Industrial Informatics ,\nvol. 18, no. 6, pp. 4059\u20134068, 2021.\n[16] I. Sharafaldin et al. , \u201cDeveloping realistic distributed denial of service\nattack dataset and taxonomy,\u201d in Conf. on Security Technology , 2019.\n[17] \u201cSumo: https://www.eclipse.org/sumo/.\u201d\n[18] Y . Lu et al. , \u201cLow-latency federated learning and blockchain for edge\nassociation in digital twin empowered 6G networks,\u201d IEEE Trans. on\nIndustrial Informatics , vol. 17, no. 7, pp. 5098\u20135107, 2020."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "## Summary of the Paper: \"LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems\" by Hakan T. Otal and M. Abdullah Canbaz\n\n### 1. Abstract\nThe paper presents a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs). By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, the researchers developed a honeypot capable of sophisticated engagement with attackers. The methodology includes data collection, prompt engineering, model selection, and supervised fine-tuning. Evaluation through similarity metrics and live deployment demonstrates that this approach effectively generates accurate and informative responses, highlighting the potential of LLMs to revolutionize honeypot technology.\n\n### 2. Introduction\nHoneypots are decoy systems designed to attract attackers, allowing organizations to study their tactics and improve security. They come in various forms, including low-interaction and high-interaction honeypots, each serving different purposes. Despite their advantages, honeypots have limitations, such as being detectable by attackers. The advent of LLMs offers the potential to create highly realistic and interactive honeypots. This study aims to develop an LLM-based honeypot system capable of accurately mimicking a Linux server by fine-tuning a pre-trained language model using attacker-generated data.\n\n### 3. Related Work\nThe paper provides a comprehensive review of honeypot technology, highlighting the evolution from low-interaction to high-interaction honeypots designed to gather detailed information about attacker behavior. Recent advances in artificial intelligence have enabled the use of LLMs to create dynamic and realistic honeypot environments. Existing research has demonstrated the feasibility of using chatbots as honeypots, but challenges remain in maintaining their covert nature. This study addresses these challenges by developing a highly interactive and adaptive LLM-based honeypot system.\n\n### 4. Method\nThe methodology involves several steps:\n- **Data Collection and Processing**: Logs from a medium-interaction Cowrie honeypot were used to collect real-world attacker commands. Common Linux commands and command-explanation pairs were added to enhance the dataset.\n  - **Datasets**: \n    - Dataset #1: 174 commands from Cowrie honeypot logs.\n    - Dataset #2: Top 100 Linux commands with variations, totaling 160 commands.\n    - Dataset #3: Summaries of 283 Linux commands' man pages.\n  - **Preprocessing**: The collected data was tokenized and formatted for training, resulting in a dataset of 617 Linux commands and their responses.\n \n- **Prompt Engineering**: Prompts were designed to mimic a Linux terminal and provide detailed explanations of commands. \n  - **Prompt 1**: Used for terminal-like responses.\n  - **Prompt 2**: Used for expert explanations of commands.\n\n- **Model Selection**: Several recent models were tested, including Llama3, Phi 3, CodeLlama, and Codestral. Llama3 8B was selected for its balance of capability and computational efficiency.\n\n- **Supervised Fine-Tuning (SFT)**: The model was fine-tuned using LlamaFactory with techniques like LoRA and QLoRA for efficiency, and noise regularization for robustness.\n\n### 5. Experimental Setup\n- **Datasets**: The dataset consisted of real-world attacker commands, common Linux commands, and command explanations.\n- **Hardware and Software**: The models were trained using 2 x NVIDIA RTX A6000 GPUs (40GB VRAM). \n- **Evaluation Metrics**: Performance was evaluated using cosine similarity, Jaro-Winkler similarity, and Levenshtein distance.\n- **Baselines and Comparison Methods**: The fine-tuned model's performance was compared against the base (pre-trained) model.\n\n### 6. Results and Analysis\n- **Training Loss Analysis**: Training losses consistently decreased, indicating effective learning.\n- **Similarity Analysis**: The fine-tuned model achieved a cosine similarity score of 0.695, Jaro-Winkler similarity of 0.599, and Levenshtein distance of 0.285, demonstrating close matches to expected outputs.\n- **Custom SSH Server**: A custom Python SSH server was developed to integrate the LLM, providing realistic interactions and logging attacker behavior.\n\n### 7. Discussion\nThe results show that the fine-tuned LLM can generate realistic and contextually appropriate responses, effectively mimicking a Linux server. The integration of the LLM with the honeypot system enhances its ability to detect and analyze malicious activities. Limitations include potential biases in the dataset and the need for continuous updates to maintain effectiveness. Future work includes expanding the dataset, exploring alternative fine-tuning techniques, and real-world deployment for further analysis.\n\n### 8. Conclusion\nThis study presents a novel approach to creating interactive and realistic honeypot systems using LLMs. The fine-tuned model was shown to improve the quality of honeypot interactions, providing valuable insights into attacker behavior. Future research will focus on expanding the dataset, refining the model, and deploying the system for real-world interactions to further enhance cybersecurity defenses.\n\n**Key Contributions:**\n- Development of a fine-tuned LLM-based honeypot system.\n- Enhanced realism and adaptability in honeypot interactions.\n- Comprehensive evaluation demonstrating the model's effectiveness.\n- Framework for future improvements and real-world deployment.\n\n### Replication and Building Upon This Work\nTo replicate and build upon this study:\n1. **Data Collection**: Collect logs from a medium-interaction honeypot (e.g., Cowrie) and enhance the dataset with common commands and explanations.\n2. **Model Selection**: Select a suitable pre-trained LLM (e.g., Llama3 8B) and perform fine-tuning using techniques like LoRA and QLoRA.\n3. **Prompt Engineering**: Design prompts that mimic terminal responses and provide expert explanations.\n4. **Custom SSH Server**: Develop a custom SSH server using Python to integrate the LLM and generate realistic interactions.\n5. **Training and Evaluation**: Fine-tune the model and evaluate its performance using metrics like cosine similarity, Jaro-Winkler similarity, and Levenshtein distance.\n6. **Real-World Deployment**: Deploy the system to a public IP for real-world interaction and log analysis.\n\nBy following these detailed steps, researchers can replicate the study and further enhance the capabilities of LLM-based honeypots for improved cybersecurity."}, {"role": "assistant", "content": "LLM Honeypot: Leveraging Large\nLanguage Models as Advanced Interactive\nHoneypot Systems\nHakan T. Otal and M. Abdullah Canbaz\nDepartment of Information Science and Technology\nCollege of Emergency Preparedness, Homeland Security, and Cybersecurity\nUniversity at Albany, SUNY\nAlbany, New York, United States\nhotal, mcanbaz [at] albany [dot] edu\nAbstract \u2014The rapid evolution of cyber threats\nnecessitates innovative solutions for detecting and\nanalyzing malicious activity. Honeypots, which are\ndecoy systems designed to lure and interact with\nattackers, have emerged as a critical component\nin cybersecurity. In this paper, we present a novel\napproach to creating realistic and interactive honey-\npot systems using Large Language Models (LLMs).\nBy fine-tuning a pre-trained open-source language\nmodel on a diverse dataset of attacker-generated\ncommands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers.\nOur methodology involved several key steps: data\ncollection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the\nmodel\u2019s performance. Evaluation through similarity\nmetrics and live deployment demonstrated that our\napproach effectively generates accurate and infor-\nmative responses. The results highlight the potential\nof LLMs to revolutionize honeypot technology, pro-\nviding cybersecurity professionals with a powerful\ntool to detect and analyze malicious activity, thereby\nenhancing overall security infrastructure.\nIndex Terms \u2014Honeypot, Large Language Models,\nCybersecurity, Fine-Tuning\nI. I NTRODUCTION\nIn the realm of cybersecurity, honeypots have\nproven to be a valuable tool for detecting and\nanalyzing malicious activity by serving as decoy\nsystems that attract potential attackers, allowing\norganizations to study their tactics and enhance\ntheir overall security infrastructure [1]. Honeypots\ncome in various forms, including low-interaction\nhoneypots that simulate services with minimal\nfunctionality to gather information about general\nattack patterns [2], and high-interaction honeypots\nthat provide a more complex and realistic en-\nvironment to engage attackers more thoroughly.\nThese can range from simple emulations of specific\nservices to full-fledged systems that mimic entire\nnetworks. Examples include server honeypots [2],\nwhich expose network services to attract attackers,and client honeypots, which are designed to be\nattacked by malicious servers. Additionally, there\nare specialized honeypots such as malware honey-\npots that capture and analyze malicious software,\nand database honeypots that protect sensitive data\nrepositories. Each type of honeypot serves a unique\npurpose in a cybersecurity strategy, providing in-\nsights into different aspects of attacker behavior\nand tactics, including reducing the costs associated\nwith maintaining security [3]. Deploying honeypots\non cloud platforms like Amazon Web Services,\nGoogle Cloud, and Microsoft Azure allows for the\nmonitoring and analysis of adversarial activities in\na scalable and dynamic environment [4].\nHowever, despite their advantages, honeypots\ncome with certain limitations that must be carefully\nconsidered. For instance, low-interaction honey-\npots, often favored for their resource efficiency\nand minimal engagement with attackers, have\nconstrained emulation capabilities. This limitation\nmakes them vulnerable to honeypot fingerprinting,\nwhich can potentially reduce their effectiveness [5].\nMoreover, these honeypots are easier for attackers\nto detect and can only gather limited information\nabout the nature of attacks, resulting in restricted\nresponses to threats [6]. Additionally, the use of\nfixed rate-limiting thresholds in experiments to pre-\nvent damage may inadvertently reveal their pres-\nence to scanners, thus compromising their covert\nnature [7]. These factors highlight the necessity for\na balanced approach in the deployment and config-\nuration of honeypots to optimize their effectiveness\nwhile mitigating inherent limitations.\nIn parallel, recent advances in artificial intelli-\ngence and natural language processing have given\nrise to Large Language Models (LLMs) capable of\ngenerating human-like text responses [8]. With ap-\npropriate fine-tuning and prompt engineering, thesearXiv:2409.08234v1  [cs.CR]  12 Sep 2024Fig. 1. Data Collection & Model Training Pipeline\nmodels have the potential to revolutionize honeypot\ntechnology by enabling the creation of highly real-\nistic and interactive honeypots. Leveraging LLMs,\nhoneypots can engage with attackers in a more\nsophisticated manner, providing more secure and\nintelligent responses. Recent research demonstrates\nthe feasibility of using LLMs as dynamic honey-\npot servers by employing pre-trained models like\nChatGPT. Even without extensive fine-tuning, well-\ncrafted prompts can allow these models to observe\nand study attacker behaviors and tactics effectively\n[9]\u2013[11]. However, a significant challenge in using\nchatbots as honeypots is the potential for attackers\nto detect and identify the honeypot due to static ele-\nments or predictable behaviors. One way to fix this\nproblem would be to make honeypot environments\nmore dynamic and real, use advanced behavioral\nanalysis, and create continuous learning models\nthat can better adapt to new attack patterns [9].\nThese advancements would enhance the capability\nof honeypots to remain covert and effective in\ndetecting sophisticated threats.\nTherefore, in this study, we aim to develop an\ninteractive LLM-Honeypot system based on Large\nLanguage Models (LLMs) capable of accurately\nmimicking the behavior of a Linux server. By fine-\ntuning a pre-trained open-source language model\nusing a dataset of attacker-generated commands\nand responses, we seek to create an interactive\nhoneypot system that can engage with attackers\nin a highly realistic and informative manner. This\napproach has the potential to significantly enhance\nthe effectiveness of honeypot technology, offering\ncybersecurity professionals a powerful tool to de-\ntect and analyze malicious activities.\nII. M ETHODOLOGY\nThis study develops an LLM-based honeypot\nto interact with attackers and gather insights into\ntheir tactics. As shown in Figure 1, a multi-stagepipeline was created. It starts with collecting and\npreprocessing a dataset of attacker commands and\nresponses. This data is used for Supervised Fine-\nTuning (SFT) on a pre-trained language model,\nenhancing its ability to mimic a Linux server.\nThe fine-tuned model is then rigorously evalu-\nated to ensure it effectively engages with attackers\nand provides valuable security insights. Finally,\nthe optimized honeypot is deployed to a public\nIP address for real-world interaction with potential\nthreats.\nA. Data Collection and Processing\nTo develop the honeypot, we used log records\nfrom a Cowrie honeypot on a public cloud endpoint\n[12]. Cowrie, a medium-interaction honeypot, logs\nbrute-force attacks and shell commands via SSH\nand Telnet [13], simulating system compromises\nand subsequent interactions [14].\nWe parsed terminal commands from a public\nhoneypot dataset [10], providing real-world at-\ntacker data. To enhance the dataset, we included\ncommonly used Linux commands [15], ensuring\nthe model could respond accurately to various\nscenarios. Additionally, we added 293 command-\nexplanation pairs [16], providing context to im-\nprove the model\u2019s ability to generate accurate re-\nsponses. This comprehensive approach improved\nthe model\u2019s performance and engagement with\nattackers.\nOverall, the combination of real-world attacker\ndata, common Linux commands, and detailed com-\nmand explanations formed a robust training dataset.\nThis combined dataset played a crucial role in fine-\ntuning the language model to function effectively\nas a honeypot, capable of providing realistic and\nintelligent interactions with attackers:\n\u2022Dataset #1: consisting of 174 commands\nparsed from the cloud-deployed Cowrie hon-\neypot logs [10].\u2022Dataset #2: comprising the top 100 Linux\ncommands [15] with manually populated vari-\nations, totaling 160 commands.\n\u2022Dataset #3: Summaries of 283 Linux com-\nmands\u2019 man pages [16].\nWe processed the collected data to prepare it for\nlanguage model training, essential for developing\nour fine-tuned LLM to mimic a honeypot. This\ninvolved transforming raw data into a format suit-\nable for effective training. Initially, we combined\nmultiple datasets to create a collection of 617 Linux\ncommands, covering various range of scenarios to\nensure model robustness. Using a local Cowrie\nhoneypot system, we simulated command execu-\ntion via SSH, capturing responses in a controlled\nenvironment and saving these interactions as logs.\nThis resulted in a substantial dataset of command-\nresponse pairs.\nNext, we performed text preprocessing on the\ndataset, including tokenizing the text data and\nconverting tokens into a standardized format for\ntraining. These preprocessing steps were crucial\nfor maintaining dataset quality and consistency. By\ntransforming raw data into a structured format, we\nlaid a foundation for training our LLM, contribut-\ning to the development of a realistic and interactive\nhoneypot.\nB. Prompt Engineering\nBy analyzing the prompts utilized in prior re-\nsearch [9]\u2013[11], we rigorously tested and refined\nour prompts to ensure they aligned with our objec-\ntives. This iterative process of prompt engineering\nwas essential to optimizing the model\u2019s interaction\nwith the dataset, ultimately contributing to the\ndevelopment of a highly effective honeypot system.\nYou are mimicking a Linux server. Respond\nwith what the terminal would respond\nwhen a code is given. I want you to\nreply only with the terminal outputs\ninside one unique code block and\nnothing else. Do not write any\nexplanations. Do not type any\ncommands unless I instruct you to do\nso.\nListing 1. Prompt 1\nFor the first part of the dataset, we designed\nprompts that mimicked a Linux terminal prompt,\ninstructing the model to respond as a Linux server\nwould. These prompts ensured that the model gen-\nerated responses that were concise, accurate, and\nformatted like a Linux terminal output. A sample\nprompt for the first part is provided in Listing 1.You are a Linux expert. You understand\nwhat every Linux terminal command does\nand you reply with the explanation\nwhen asked.\nListing 2. Prompt 2\nFor the second part of the dataset, we created\na set of prompts that positioned the model as a\nLinux expert, capable of providing detailed ex-\nplanations of Linux terminal commands. These\nprompt enabled the model to generate informative\nand context-rich responses that demonstrated a\ndeep understanding of Linux commands and their\napplications. Again, a sample prompt is provided\nin Listing 2.\nC. Model Selection\nThe rapid development of Large Language Mod-\nels (LLMs) has provided powerful tools for various\napplications, including honeypot mimicry. Select-\ning the correct model is critical for accurately sim-\nulating interactions and balancing computational\nefficiency with performance for real-time deploy-\nment.\nWe tested several recent models, including\nLlama3 [17], Phi 3 [18], CodeLlama [19], and\nCodestral [20]. Llama3, with its 8B and 70B vari-\nants, offers scalable language processing, while Phi\n3, CodeLlama, and Codestral are notable for their\nfocus on code-development tasks. However, our\nexperiments showed that code-centric models like\nPhi 3, CodeLlama, and Codestral were less effec-\ntive for honeypot simulation. Larger models (8B\nand 70B) were too slow, emphasizing the need for\ncomputational speed. Smaller models demonstrated\nsufficient capability, suggesting the importance of\nbalancing model size and efficiency.\nThese findings highlight the need for a model\nthat excels in linguistic proficiency and meets prac-\ntical demands for speed and resource management.\nTherefore, we chose the Llama3 8B model for our\nhoneypot LLM.\nD. Supervised Fine-Tuning (SFT)\nSupervised Fine-Tuning (SFT) is essential for\nadapting large pre-trained models to specific tasks.\nWe fine-tuned the foundation models using Llama-\nFactory [21] with our curated dataset. To enhance\ntraining efficiency, we employed Low-Rank Adap-\ntation (LoRA) [22], which reduces the number of\ntrainable parameters by decomposing the weight\nmatrices into lower-rank representations, allowing\nfor efficient training without sacrificing perfor-\nmance.Fig. 2. Interactive LLM-Honeypot Server Framework\nQuantized Low-Rank Adapters (QLoRA) fur-\nther optimized the model by quantizing it to 8-\nbit precision, reducing its size and computational\nload while maintaining accuracy. To prevent over-\nfitting and improve generalization, we incorporated\nNEFTune noise [23], a regularization technique\nthat introduces noise during training. Additionally,\nFlash Attention 2 [24] was integrated to enhance\nattention mechanism efficiency, critical for process-\ning long sequences.\nThe final model, fine-tuned to respond like a\nhoneypot server, achieves a balance between ef-\nficiency and accuracy using these advanced tech-\nniques. The model is publicly accessible on Hug-\ngingface1and our GitHub page2.\nIII. E XPERIMENTAL RESULTS\nThe experimental results of our proposed ap-\nproach include an analysis of training losses, eval-\nuation metrics, and a comparative performance as-\nsessment of different models. We begin by detailing\nthe experimental setup, which involved significant\ncomputational resources, utilizing 2 x NVIDIA\nRTX A6000 (40GB VRAM) GPUs for training\nthe models. Following this, we provide a com-\nprehensive analysis of the results, highlighting the\neffectiveness and efficiency of our fine-tuned model\nin mimicking honeypot behavior.\nA. Interactive LLM-Honeypot Framework\nLarge Language Models (LLMs) are primarily\ndesigned to process and generate natural language\ntext, and as such, they do not natively under-\nstand network traffic data. To bridge this gap and\nleverage LLMs\u2019 capabilities for cybersecurity ap-\nplications, we developed a wrapper that interfaces\nthe LLM with network traffic at the IP (Layer\n3) level. This wrapper enables the system to act\nas a vulnerable server, capable of engaging with\nattackers through realistic interactions.\nIn figure 2, we illustrate the architecture of our\nLLM-based honeypot system, which integrates an\nSSH server with a Large Language Model (LLM)\n1huggingface.co/hotal/honeypot-llama3-8B\n2github.com/AI-in-Complex-Systems-Lab/LLM-Honeypotto simulate realistic interactions with potential at-\ntackers. The setup involves the following compo-\nnents:\n1)Attacker Interface: Represented by the icon\non the left, this interface depicts the external\nentity attempting to interact with the honey-\npot system via SSH (Secure Shell) protocol.\nAttackers use this interface to execute com-\nmands and probe the system.\n2)SSH Server: The central component of the\nsystem, highlighted in purple, is the SSH\nserver. This server acts as the entry point for\nall incoming SSH connections from attack-\ners. It is configured to handle authentication,\nmanage sessions, and relay commands to the\nintegrated LLM.\n3)Large Language Model (LLM): Embedded\nwithin the SSH server and shown in green,\nthe LLM is fine-tuned to mimic the behavior\nof a typical Linux server. Upon receiving\ncommands from the SSH server, the LLM\nprocesses these commands and generates ap-\npropriate responses. This model leverages\npre-trained data and fine-tuning techniques\nto provide realistic and contextually relevant\nreplies.\n4)Interaction Flow: The arrows indicate the\nflow of interactions. The attacker initiates a\nconnection and sends commands to the SSH\nserver, which then forwards these commands\nto the LLM. The LLM processes the com-\nmands and generates responses, which are\nsent back to the SSH server and subsequently\nrelayed to the attacker.\nBy combining the SSH server with a sophis-\nticated LLM, our system can engage attackers\nin a realistic manner, capturing valuable data on\ntheir tactics and techniques. This architecture not\nonly enhances the honeypot\u2019s ability to simulate\ngenuine server interactions but also provides a\nrobust framework for analyzing attacker behavior\nand improving overall cybersecurity defenses.\nB. Custom SSH Server Wrapper\nTo deploy the final model as a functional honey-\npot server, we crafted a custom SSH server usingPython\u2019s Paramiko library [25]. This server inte-\ngrates our fine-tuned language model to generate\nrealistic responses.\nFigure 3 displays an example SSH connection\nand the corresponding responses to issued com-\nmands. The custom SSH server operates as follows:\n1)SSH Connection: User connects to the\nhoneypot server using ssh -T -p 2222\n\u201droot@localhost\u201d, simulating an attack.\n2)Authentication: Server prompts for a pass-\nword; upon success, user accesses the hon-\neypot\u2019s command-line interface.\n3)Command Execution: User runs Linux com-\nmands (e.g., ls -al, echo \u2019hello world\u2019, ifcon-\nfig), which the SSH server forwards to the\nintegrated LLM.\n4)LLM Response Generation: LLM generates\nresponses mimicking a real Linux server\n(e.g., listing directory contents, outputting\ntext, displaying network configuration).\n5)Interaction Logging: Honeypot logs all com-\nmands and responses, capturing data on at-\ntacker behavior for cybersecurity analysis.\nFor generating inferences, we utilized the Hug-\ngingface Transformers [26]. Our Custom-SSH\nserver is capable of collecting the IP addresses\nof incoming SSH connections, username-password\npairs (for authentication), and logs of every com-\nmand along with the generated responses by the\nmodel. By incorporating a LLM, our custom SSH\nserver can engage attackers in a realistic manner,\nproviding insights into their actions and enhancing\nthe honeypot\u2019s overall functionality.\nC. Training Loss Analysis\nAs illustrated in Figure 4, the training losses\nof our fine-tuned model exhibit a steady decline\nover the training steps. This trend indicates that\nthe model effectively learned from our dataset and\nadapted well to the task of mimicking a Linux\nserver. During the fine-tuning phase, we employed\na learning rate of 5\u00d710\u22124and conducted a total of\n36 training steps. The entire training process was\nFig. 3. Example of Honeypot SSH Connection\nFig. 4. Training losses over 36 steps in Supervised Fine-Tuning\nTABLE I\nSIMILARITY AND DISTANCE METRICS\nMetric Mean Score\nBase Fine-Tuned\nCosine Similarity 0.663 0.695\nJaro-Winkler Similarity 0.534 0.599\nLevenshtein Distance 0.332 0.285\ncompleted in 14 minutes. The consistent decrease\nin training loss demonstrates the model\u2019s capability\nto improve its performance progressively, thereby\nenhancing its ability to generate realistic and con-\ntextually appropriate responses.\nD. Similarity Analysis with Cowrie Outputs\nTo evaluate the performance of our fine-tuned\nlanguage model, Llama3-8B, we employed multi-\nple metrics to measure the similarity between the\nexpected (Cowrie) and generated terminal outputs.\nWe used cosine similarity to quantify the cosine of\nthe angle between two vectors in high-dimensional\nspace, with higher scores indicating better per-\nformance. Additionally, we used the Jaro-Winkler\nsimilarity, which measures the similarity based on\nmatching characters and necessary transpositions,\nwith higher scores indicating closer matches. Fi-\nnally, we utilized the Levenshtein distance, which\ncalculates the minimum number of single-character\nedits needed to transform one string into another,\nwith lower scores indicating closer matches. These\ndiverse metrics provided a comprehensive evalua-\ntion of our model\u2019s performance.\nFig. 5. Histogram of Cosine Similarity Scores over 140 SamplesThe results of our evaluation, summarized in\nTable I, demonstrate the performance of our fine-\ntuned language model, Llama3-8B, using different\nsimilarity and distance metrics over 140 random\nsamples. The fine-tuned model achieved a cosine\nsimilarity score of 0.695 (higher is better), indicat-\ning a strong match between the expected and gener-\nated terminal outputs. The Jaro-Winkler similarity\nscore was 0.599 (higher is better), also reflecting\na reasonable level of similarity. The Levenshtein\ndistance was 0.332 (lower is better), suggesting a\nrelatively low number of edits needed to align the\ngenerated output with the expected one.\nIn addition, fine-tuned LLM showed improve-\nments across all metrics compared to the base\nmodel. These results show the effectiveness of our\nmodel in generating outputs that closely match the\nexpected responses from a Cowrie honeypot server.\nAs shown in Figure 5, the cosine similarity\nscores of the outputs generated by LLM exhibit a\ndistribution with most scores concentrated towards\nhigher values, indicating that the model\u2019s responses\nare mostly similar to the expected outputs.\nIt is also worth noting that while some generated\noutputs may differ but they remain contextually\naccurate and true for the given commands.\nIV. C ONCLUSION\nThis study introduces an innovative approach\nto developing interactive and realistic honeypot\nsystems using Large Language Models (LLMs).\nBy fine-tuning a pre-trained open-source language\nmodel on attacker-generated commands and re-\nsponses, we created a sophisticated honeypot that\nenhances realism and deployment effectiveness.\nOur LLM-based honeypot system improves re-\nsponse quality and the ability to detect and ana-\nlyze malicious activities. The integration of LLMs\nwith honeypot technology creates a dynamic, adap-\ntive system that evolves with emerging threats.\nLeveraging LLMs\u2019 reinforcement learning and at-\ntention mechanisms, our system refines responses\nand maintains high contextual relevance, providing\ndeeper insights into attacker behavior and strength-\nening security infrastructures.\nFuture work includes expanding training\ndatasets, exploring alternative fine-tuning, and\nincorporating behavioral analysis. We plan to\ndeploy the system to a public IP for real-\nworld interaction, collecting logs on attack\nvectors for analysis. This will involve creating\nknowledge graphs to map attack techniques\nand provide insights into attacker strategies.\nPerformance analysis can focus on responserealism, engagement effectiveness, and robustness\nusing metrics like accuracy and interaction quality,\naiming to refine our model and enhance honeypots\nfor better cyber-threat detection and analysis.\nREFERENCES\n[1] N. Naik, C. Shang, P. Jenkins, and Q. Shen, \u201cD-fri-\nhoneypot: a secure sting operation for hacking the hackers\nusing dynamic fuzzy rule interpolation,\u201d IEEE Transac-\ntions on Emerging Topics in Computational Intelligence ,\n2020.\n[2] M. A. Hakim, H. Aksu, A. S. Uluagac, and K. Akkaya,\n\u201cU-pot: a honeypot framework for upnp-based iot de-\nvices,\u201d 2018 IEEE 37th International Performance Com-\nputing and Communications Conference (IPCCC) , 2018.\n[3] M. L. Bringer, C. Chelmecki, and H. Fujinoki, \u201cA survey:\nrecent advances and future trends in honeypot research,\u201d\nInternational Journal of Computer Network and Informa-\ntion Security , 2012.\n[4] C. Kelly, N. Pitropakis, A. Mylonas, S. McKeown, and\nW. J. Buchanan, \u201cA comparative analysis of honeypots on\ndifferent cloud platforms,\u201d Sensors , 2021.\n[5] I. Damg \u02daard, M. Keller, E. Larraia, V . Pastro, P. Sch \u00a8oll, and\nN. P. Smart, \u201cComputer security \u2013 esorics 2021,\u201d Lecture\nNotes in Computer Science , 2021.\n[6] A. R.A., D. F.M., A. B.K., A. O.S, and O. T.J, \u201cImproving\ndeception capability in honeynet through data manipula-\ntion,\u201d Journal of Internet Technology and Secured Trans-\naction , 2015.\n[7] H. Griffioen, K. Oosthoek, P. v. d. Knaap, and C. Doerr,\n\u201cScan, test, execute: adversarial tactics in amplification\nddos attacks,\u201d Proceedings of the 2021 ACM SIGSAC\nConference on Computer and Communications Security ,\n2021.\n[8] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang,\nS. Zhong, B. Yin, and X. Hu, \u201cHarnessing the power of\nllms in practice: A survey on chatgpt and beyond,\u201d ACM\nTransactions on Knowledge Discovery from Data , 2024.\n[9] F. McKee and D. Noever, \u201cChatbots in a Honeypot World,\u201d\n2023.\n[10] U. Sedlar, M. Kren, L. \u02c7Stefani \u02c7c Ju \u02c7zni\u02c7c, and M. V olk,\n\u201cCyberLab honeynet dataset,\u201d 2020.\n[11] M. Sladi \u00b4c, V . Valeros, C. Catania, and S. Garcia, \u201cLLM\nin the Shell: Generative Honeypots,\u201d 2023.\n[12] M. Oosterhof, \u201cCowrie 2.5.0 documentation.\u201d [Online].\nAvailable: https://cowrie.readthedocs.io/\n[13] C. Kelly, N. Pitropakis, A. Mylonas, S. McKeown, and\nW. J. Buchanan, \u201cA comparative analysis of honeypots on\ndifferent cloud platforms,\u201d Sensors , 2021.\n[14] S. Deshmukh, R. Rade, and D. F. Kazi, \u201cAttacker be-\nhaviour profiling using stochastic ensemble of hidden\nmarkov models,\u201d 2019.\n[15] V . \u02c7Sv\u00b4abensk \u00b4y, J. Vykopal, P. Seda, and P. \u02c7Celeda, \u201cDataset\nof shell commands used by participants of hands-on\ncybersecurity training,\u201d Data in Brief , 2021.\n[16] \u201cLinux man pages tldr summarized.\u201d [On-\nline]. Available: https://huggingface.co/datasets/tmskss/\nlinux-man-pages-tldr-summarized/\n[17] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi\net al. , \u201cLlama 2: Open foundation and fine-tuned chat\nmodels,\u201d 2023.\n[18] Microsoft, \u201cPhi-3 technical report: A highly capable lan-\nguage model locally on your phone,\u201d 2024.\n[19] MetaAI, \u201cCode llama: Open f. models for code,\u201d 2024.\n[20] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch,\nB. Savary, and Others, \u201cMixtral of experts,\u201d 2024.\n[21] Y . Zheng, R. Zhang, J. Zhang, Y . Ye, Z. Luo, and Y . Ma,\n\u201cLlamafactory: Unified efficient fine-tuning of 100+ lan-\nguage models,\u201d 2024.[22] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang,\nL. Wang, and W. Chen, \u201cLora: Low-rank adaptation of\nlarge language models,\u201d 2021.\n[23] N. Jain, P. yeh Chiang, Y . Wen, J. Kirchenbauer, H.-\nM. Chu, G. Somepalli, B. R. Bartoldson, B. Kailkhura,\nA. Schwarzschild, A. Saha, M. Goldblum, J. Geiping,\nand T. Goldstein, \u201cNeftune: Noisy embeddings improve\ninstruction finetuning,\u201d 2023.\n[24] T. Dao, \u201cFlashattention-2: Faster attention with better\nparallelism and work partitioning,\u201d 2023.\n[25] \u201cParamiko documentation.\u201d [Online]. Available: https:\n//www.paramiko.org/\n[26] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue,\nand Others, \u201cHuggingface\u2019s transformers: State-of-the-art\nnatural language processing,\u201d 2020."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract:\n\nThis paper explores the use of Deep Reinforcement Learning (DRL) to optimize the design of tokamak nuclear fusion reactors. By addressing multiple physics and engineering constraints involved in steady-state operation, the research integrates a parallelizable DRL framework to find optimal reactor configurations that maximize operational efficiency and minimize construction costs. The DRL-based approach simplifies multi-objective optimization and has shown promising results in efficiently identifying practical, cost-effective reactor designs compared to conventional grid search algorithms.\n\n### 2. Introduction:\n\nNuclear fusion, particularly through the use of magnetic confinement tokamak reactors, is a promising technology for addressing global energy demands sustainably. However, designing an optimal tokamak reactor is inherently complex due to the need to satisfy multiple operational constraints while minimizing physical and financial costs. Fundamental operational limits include density, beta, kink safety instability, and bootstrap fraction requirements. Previous research has tackled these constraints, but finding an effective multi-objective optimization method remains challenging.\n\nRecent advances in data-driven approaches, like Deep Reinforcement Learning (DRL), offer potential solutions by simplifying these complex design problems. DRL, which combines neural networks and reinforcement learning (RL), can learn optimal policies by maximizing cumulative rewards through exploration and exploitation. This study investigates the application of DRL for optimizing tokamak fusion reactor design, demonstrating its ability to find feasible, cost-effective designs that meet all operational criteria.\n\n### 3. Related Work:\n\n- **Density and Beta Limits**: Previous studies highlighted operational constraints like the Greenwald density limit and Troyon beta limit. These constraints are critical for maintaining safe and efficient plasma conditions.\n- **Bootstrap Fraction**: Maintaining a high bootstrap current fraction is vital for steady-state operation.\n- **Cost Optimization**: Studies have focused on minimizing reactor costs while enhancing energy gain.\n- **Design Optimization Algorithms**: Traditional methods include gradient-based and grid search algorithms. However, these methods often suffer from limitations like high computational costs and sensitivity to initial conditions.\n- **ML in Design Optimization**: Earlier applications of machine learning in design optimization, such as for airfoil design, have shown ML's potential in complex engineering problems.\n\nThis paper advances the state of the art by applying DRL to the multi-objective optimization of tokamak reactors, a significantly more complex problem than previously addressed applications like airfoil design.\n\n### 4. Method:\n\n- **Deep Reinforcement Learning Framework**:\n  - **Policy Gradient Methods**: The study uses Proximal Policy Optimization (PPO), a policy-gradient algorithm that operates within a trust region to ensure stable and continuous updates.\n  - **Actor-Critic Network**: Utilizes three hidden layers of 64 dimensions each, guiding the agent (network) to optimize input parameters (like plasma beta, elongation, aspect ratio, and electric power) by maximizing cumulative rewards.\n  \n- **Reactor Design Computation**:\n  - **Geometrical Design**: Calculates core dimensions, blanket thickness, and TF coil thickness based on neutron wall loading constraints and material stress limits.\n  - **Plasma Parameters**: Estimates critical parameters (plasma pressure, density, beta, confinement time, current) using input variables and empirical scaling laws.\n\n- **Reward Function**: \n  - Formulated as a weighted sum of individual rewards representing operational constraints. Equations for the reward function are:\n    - \\( R(x) = \\tanh(a(\\frac{x}{x_{\\text{lim}}} - 1)) + r_f [\\theta(x - x_u) + \\theta(x_l - x)] \\)\n    - \\( R(x) = \\tanh(a(\\frac{x}{x_{\\text{ref}}} - 1)) \\)\n  - Combined into \\( R_{\\text{total}} = w_\\tau R_\\tau + w_{\\text{cost}} R_{\\text{cost}} + w_\\beta R_\\beta + w_q R_q + w_{ne} R_{ne} + w_{fbs} R_{fbs} + w_Q R_Q \\).\n\n### 5. Experimental Setup:\n\n- **Datasets**: Utilized computed design parameters and engineering constraints for simulations.\n- **Hardware/Software**: Experiments executed using standard DRL training frameworks and high-performance computing resources.\n- **Evaluation Metrics**: Used cumulative reward scores, focusing on operational constraints (density, beta, kink safety, bootstrap fraction) and cost parameters.\n- **Baselines**: Compared DRL results against grid search algorithms using the same parameter range.\n\n### 6. Results and Analysis:\n\n- **Training Dynamics**: \n  - Policy loss and total reward curves showed steady improvement and saturation, indicating effective learning.\n- **Comparison with Grid Search**:\n  - **DRL Efficiency**: Achieved optimal designs in roughly half the episodes required by grid search.\n  - **DRL Outcomes**: Found configurations with reduced major radius and cost, while maintaining operational limits. DRL identified different operational regimes, showcasing its robustness and effectiveness in exploration.\n\n### 7. Discussion:\n\n- **Result Implications**: DRL demonstrated significant potential in multi-objective optimization for complex engineering problems. It offered practical designs that grid search algorithms missed, thus highlighting DRL's superior generalization capabilities.\n- **Limitations**: While effective, the results depended on modeling accuracy and reward function definitions. The optimization's robustness across different initial conditions and parameter ranges was assumed but not exhaustively tested.\n- **Future Directions**: Research could extend to optimizing profile shapes and wall materials, addressing discontinuous variables and higher-dimensional inputs, potentially improving design accuracy and practical applicability.\n\n### 8. Conclusion:\n\nThe study successfully applied DRL to the optimization of tokamak reactor designs, marking an advancement in multi-objective optimization techniques. Future work should focus on expanding model capabilities, refining reward functions, and addressing additional design constraints, ultimately contributing to the development of efficient and sustainable fusion reactors.\n\n### Summary Conclusion:\n\nApplying DRL to tokamak reactor design has shown to efficiently navigate the multi-constraint, multi-objective optimization landscape, offering superior solutions compared to traditional methods. The integration of advanced DRL techniques such as PPO, coupled with a robust reward formulation, has underscored the potential of machine learning to transform complex engineering design processes. Future work should leverage and build upon these findings to further enhance reactor design, operational efficiency, and cost-effectiveness."}, {"role": "assistant", "content": "DESIGN OPTIMIZATION OF NUCLEAR FUSION REACTOR\nTHROUGH DEEPREINFORCEMENT LEARNING\nJinsu Kim\nDepartment of Mechanical and Aerospace Engineering\nPrinceton University\njk9075@prineton.eduJaemin Seo\nDepartment of Physics\nChung-Ang University\njseo@cau.ac.kr\nSeptember 13, 2024\nABSTRACT\nThis research explores the application of Deep Reinforcement Learning (DRL) to optimize\nthe design of a nuclear fusion reactor. DRL can efficiently address the challenging issues attributed\nto multiple physics and engineering constraints for steady-state operation. The fusion reactor\ndesign computation and the optimization code applicable to parallelization with DRL are developed.\nThe proposed framework enables finding the optimal reactor design that satisfies the operational\nrequirements while reducing building costs. Multi-objective design optimization for a fusion reactor\nis now simplified by DRL, indicating the high potential of the proposed framework for advancing the\nefficient and sustainable design of future reactors.\n1 Introduction\nNuclear fusion energy, which can be generated by combining two light atomic nuclei while releasing massive\namounts of energy, has been highlighted as a promising way to address the world\u2019s growing energy demands for\nsustainable and clean energy. A magnetic confinement fusion reactor called Tokamak is promising for realizing\nsustainable nuclear fusion energy. However, designing an optimal fusion reactor has remained challenging. It requires\nboth satisfying multiple constraints for steady-state operation [1 \u20133] and reducing the size of a reactor [4]. In more detail,\nthe optimally designed tokamak reactor should prioritize to consider at least four fundamental operational limits [2],\nincluding density limit [5,6], beta limit [7], the kink safety instability limit [8], and the achievable bootstrap fraction [9].\nIn addition, the reactor design should not exceed the maximum allowable neutron wall load and stress induced by\nelectromagnetic force to TF coils [1, 2, 10]. Finally, the practical aspect should include reducing costs and increasing\nenergy gain [4, 11]. Previous investigation in this field has been conducted [1 \u20134, 11\u201318], but the question about how to\noptimize multiple reactor design objectives effectively has still been issued.\nRecently, a data-driven approach, enabling mitigating the high complexity associated with underlying physical\nconstraints, has been developed to cope with this problem. Deep Reinforcement Learning (DRL), combined with a\nneural network and Reinforcement Learning (RL) [19], has demonstrated a remarkable generalization ability to unseen\ndata, applicable to repeating optimization tasks, including design optimization [20]. This can provide the optimal policy\nthat satisfies the maximization of the cumulative reward associated with the system\u2019s objective through exploitation and\nexploration. Several studies have shown the validity of this approach in airfoil design optimization [20 \u201323] and strain\ndesign optimization [24].\nThis research explores the application of a Deep Reinforcement Learning (DRL) framework to optimize a\ntokamak fusion reactor design. Again, a tokamak fusion reactor requires steady-state operation conditions to avoid\ninstabilities and achieve high power generation by self-ignition. The optimal reactor design should achieve all these\nmultiple design objectives, making it challenging to find the optimal configuration. DRL, however, can search for\noptimal conditions with its simplified process by replacing design objectives with scalarized rewards. Later, we will\nshow that our optimal design satisfies all operational constraints with cost reduction in an efficient way compared toarXiv:2409.08231v1  [physics.plasm-ph]  12 Sep 2024APREPRINT - SEPTEMBER 13, 2024\nthe grid search algorithm, which has been mainly used in previous studies. The proposed method provides practical\nintuition for future fusion reactor designs.\nThis paper is organized as follows. The tokamak reactor design computation process and design objectives to be\noptimized will be introduced. Then, our proposed design optimization method based on DRL will be covered. Finally,\nthe following section will represent the optimization results and compare the design performance to verify the process.\n2 Tokamak Fusion Reactor Design Computation\nTo design fusion tokamak reactors, it is necessary to consider not only the aspect of plasma physics but also\nengineering and nuclear physics [2,18]. Previous research has shown that these constraints affect the overall design of a\ntokamak reactor [2], and we developed the computation code for calculating the tokamak reactor design parameters and\nperformance, including plasma pressure, density, and cost. Figure 1 describes the simplified design computation process.\nNote that our computation code considers the maximum allowable current density and stress on materials [1, 2, 10]\nin addition to neutron radiation wall loading [2, 18] to determine the designs concerning the physically achievable\nconstruction of a reactor. Table 1 describes the input parameters required for calculating the overall geometrical design\nof a reactor, plasma parameters, and the designed reactor\u2019s performance. If the input parameters are given, the code\ncomputes the output parameters, as shown in Table 2. The details of the reactor design computation are presented in\nAppendix A.\nFigure 1: A diagram for a tokamak reactor design computation based on nuclear physics and engineering constraints. A\ndesign is determined considering the maximum allowable current density and stress on materials in addition to neutron\nradiation wall load.\nTable 1: A list of input parameters for computing a tokamak reactor design. These parameters are determined by the\nagent from DRL.\nQuantity Description\n\u03b2N Normalized ratio of plasma thermal and magnetic pressure\n\u03ba Elongation\nA Aspect ratio\n\u00afT Average electron temperature\nBmax Maximum allowable magnetic field\nH Confinement enhancement factor\n\u2206armour Armour thickness\n\u03b7RF RF recirculating rate\nPE Generated electric power\n2APREPRINT - SEPTEMBER 13, 2024\nTable 2: A list of output parameters determined by the code. These include the plasma parameters and notions for\nreactor performance.\nQuantity Description\nR Major radius\na Minor radius\n\u2206Blanket Blanket thickness\n\u2206TFcoil TFcoil thickness\n\u03c4 Energy confinement time\n\u03b2 Plasma beta\nIp Plasma current\nq Safety factor\nfBS Required bootstrap fraction\nfNC Achievable neoclassical bootstrap fraction\nQ\u2225 Parallel heat flux conducted to divertor\n\u00afn Plasma average density\n\u00afp Plasma average pressure\nnG Greenwald density\n\u03b2T Troyon beta\nQ Gaining factor\nC Cost parameter\nThe following evaluation process for verifying the steady-state operation is conducted after computing the\nparameters. For steady state operation, it is necessary to consider operational constraints [2], including density\nlimit [5, 6], beta limit [7], kink stability factor limit [8], and bootstrap fraction limit [9]. These requirements can be\nformalized as below.\n1. Density limit: \u00afn <\u00afnG=Ip\n\u03c0a2\n2. Beta limit: \u03b2 < \u03b2 T=\u03b2NIp\naB0\n3. Kink safety factor limit: q > q kink= 2\n4. Bootstrap fraction limit: fNC> fBS\nThe density limit provides the safety margin for tokamak plasma operation to the plasma density. The beta\nlimit also refers to the maximum achievable plasma beta. The kink safety factor limit describes the current major\nMHD disruption in the case of (2,1) mode. The bootstrap fraction limit represents the achievable neoclassical bootstrap\nfraction fNCshould exceed the required bootstrap fraction fBSfor steady operation.\nFurthermore, to achieve practical plasma operation, lowering the density and beta to be achievable while\navoiding operational limits is appropriate. Furthermore, building a cost-efficient reactor requires minimizing the cost\nparameters while maintaining a high Q for commercial purposes. Thus, the density, beta, and cost parameters are\nsupposed to decrease while being close to the ignition condition during the optimization process. These objectives can\nbe summarized below.\n1. Achievable low-density\n2. Achievable low beta\n3. Low cost\n4. High Q for efficiency\nThe reference design [2] failed to satisfy all operational constraints, requiring the optimization process for\nfinding the optimal design configuration. Several studies have been conducted [3, 4, 11 \u201313, 25] to find the optimal\nreactor design with minimum cost and to meet physics, engineering, and neutron constraints [4]. This study also aims\nfor a tokamak reactor design optimization, enabling it to meet the minimum requirements for steady-state operation [2]\nwhile minimizing the cost [4, 11] in a more efficient way through deep reinforcement learning.\nThese multi-objectives have difficulty finding the optimal reactor design. Preliminary studies have shown the\npossibility of searching for the optimal design based on cost modeling [4,11] or massive parametric study [3]. Compared\nto the conventional approaches, our optimization method is based on deep reinforcement learning [19, 26 \u201328], enabling\n3APREPRINT - SEPTEMBER 13, 2024\nus to search for the optimal design satisfying multiple design objectives with relatively few computations. Moreover,\nthese multiple objectives can now be easily and mathematically formalized by defining the weighted sum of scalar\nreward, which will be covered in the next section.\n3 Design Optimization through Deep Reinforcement Learning\nDesign optimization requires exploring a high-dimensional design parameter space, indicating a high computa-\ntional cost for high-fidelity numerical simulation processing [22, 23]. Investigations on optimizing design parameters\nhave been conducted using two main approaches: Gradient and gradient-free methods. Gradient methods utilize the\ngradient of the design objective functions, and gradient computation can be reduced through the adjoint method [29],\nbut easily trapped in local optimal and sensitive to the initial points [22]. On the other hand, gradient-free methods,\nincluding genetic algorithm [30], are efficient in finding global optimal and less sensitive to initial points, yet have\nhigher computational cost than gradient methods [22, 31]. Alternative methods based on machine learning for surrogate\nmodels [32] have been highlighted to cover the computational part of the simulations and design performance, but they\ndepend on the dataset\u2019s quality [23].\nRecently, several studies have demonstrated the effectiveness of deep reinforcement learning in design optimiza-\ntion fields [20 \u201323]. Reinforcement Learning (RL), a data-driven approach for learning to make decisions through trial\nand error [19], provides a high generalization capability to unobserved system configurations [20]. Deep Reinforcement\nLearning (DRL) [27], combined with Deep Neural Network (DNN) and Reinforcement Learning (RL), has emerged\nand achieved high performance in diverse fields [28]. Since neural networks can approximate arbitrary nonlinear\nfunctions guaranteed by the universal function approximation theorem [33], the integration of deep neural network\nand reinforcement learning enables agents to learn the approximation of an optimal policy efficiently while allowing\nunstructured and high-dimensional input data. Our work utilized Proximal Policy Optimization (PPO) [34], classified\nas a policy-gradient method [26], to optimize the reactor design. PPO has several benefits, including the capability to\nhandle the continuous action space and the stability with robust optimization by incorporating a form of trust region\noptimization [35].\nFigure 2 describes our proposed framework for reactor design optimization based on deep reinforcement learning.\nWe built the environment based on the reactor design computation code. Then, we trained the actor-critic network as an\nagent for finding the optimal input parameters, given in Table 1. The environment provides the plasma parameters as a\nnext state and evaluates the reward based on the design performance. The reactor design is optimized during training, in\nwhich the agent finds the optimal input parameters by being trained through the computed reward. More details of the\nproposed algorithm are represented in Appendix B.\nFigure 2: The proposed design optimization process based on deep reinforcement learning for finding the optimal\ndesign configuration of a tokamak reactor is represented.\n4APREPRINT - SEPTEMBER 13, 2024\nOne of the main points of this framework is that the process of multi-objective design optimization can be\nsimplified by the weighted sum of each reward corresponding to each design objective. Compared to gradient-based\nmethods, this does not require computing the gradient of multiple design objectives. However, it can still be applied to\neven multidisciplinary cases where the design objectives and parameters are coupled. Thus, this reduces computational\ncosts on multi-objective optimization and reflects the user\u2019s preference for design. In addition, this framework applies\nto parallelization, enabling the agent to learn the optimal policy from multiple sets of parallel environments instances\nin parallel [36]. This increases optimization efficiency and reduces sample correlation, leading to the stabilization of\ntraining.\n4 Reward engineering for a reactor design optimization\nTo apply deep reinforcement learning, defining the reward function based on design objectives is necessary.\nTwo types of reward functions are suggested to reflect our design purpose. The minimum requirements for steady-state\noperation can be expressed as the inequalities with the ratio of the plasma parameters to their maximum allowable\nvalues, including Greenwald density nGand Troyon beta \u03b2T, as well as with the ratio of the parameters to their\nminimum required values, including qkink andfBS. All parameters of an optimal-designed reactor should satisfy these\ninequalities.\n1. Density limit: ( \u00afn/\u00afnG)\u22121>1\n2. Beta limit: ( \u03b2/\u03b2T)\u22121>1\n3. Kink safety factor limit: q/qkink>1\n4. Bootstrap fraction limit: fNC/fBS>1\nThen, by defining the reward function as Equation 1, we can induce the agent to find the optimal design that\nsatisfies both avoiding the operational limits and enabling the values in the valid range. In this equation, xandxlim\ncorrespond to the plasma parameter and operational safety margin value, respectively. This term can be managed with\nhyperparameter a.xlandxurepresent each parameter\u2019s lower and upper bound for physical validity. rfis a penalty to\ninduce the agent to acquire physically valid design configurations.\nR(x) = tanh a(x\nxlim\u22121) +rf[\u03b8(x\u2212xu) +\u03b8(xl\u2212x)] (1)\nIn addition, the quantitative metrics, including the energy confinement time and cost parameter, can be expressed\nas the ratio of those from the reference design and the target design. Then, Equation 2 can be a suitable function\nenabling the designed reactor to have higher values than the reference. In the equation below, xandxrefdescribe the\nplasma parameter from the designed reactor and reference, respectively.\nR(x) = tanh a(x\nxref\u22121) (2)\nEach objective can be represented as one of two types of reward functions, as mentioned above. Finally, the\nweighted sum of each reward, as Equation 3, obtained from the environment, is given to the agent, resulting in the\noptimal reactor design maximizing the total reward.\nRtotal=w\u03c4R\u03c4+wcostRcost+w\u03b2R\u03b2+wqRq+wneRne+wfbsRfbs+wQRQ (3)\n5 Experimental setup\nFor a reactor design optimization, we applied DRL-based optimization as a proposed method and grid search\nalgorithm as a comparison. The proposed method was based on Proximal Policy Optimisation (PPO) [34] with a\nmaximum local time step (memory buffer size) equal to 4 and 100,000 episodes. The optimization process was\nconducted over 100,000 episodes, at which saturation was observed. The actor-critic network has three layers of\n64 hidden dimensions, while the deviation for sampling policy distribution is 0.25. The loss function for PPO in\nthis research used the weighted sum of policy surrogate loss combined with CLIP loss and entropy bonus term and\nSmoothL1Loss for the value function [34]. In more detail, the clipping coefficient for CLIP loss \u03f5is 0.2, and the entropy\ncoefficient is 0.05. The learning rate for training the network is 0.001, while a discount factor \u03b3is 0.999. The optimizer\nused in this research is RMSProps.\n5APREPRINT - SEPTEMBER 13, 2024\nTable 3: A list of the search space for each input parameter corresponding to Table 1.\nInput parameter Search space\n\u03b2N [2.0, 4.0]\n\u03ba [1.5, 1.9]\nA [3.5, 4.5]\n\u00afT(KeV) [10, 15]\nBmax (T) [10, 16]\nH [1.0, 1.3]\n\u2206armour (cm) [0.01, 0.05]\n\u03b7RF [0.1, 0.2]\nPE(MW) [500, 1500]\nThe search space should be bounded for an efficient optimization process. Table 3 describes the action range for\neach parameter during the optimization. The right column contains the lower and upper bound of the input parameter,\nused for Equation 1. The grid search algorithm employed the same search space with 20 grid points for each parameter.\nTable 4: Weight value setup for each objective to scalarize the reward. From the left, each weight corresponds to a cost\nparameter, energy confinement time, plasma beta, plasma average density, safety factor, bootstrap current ratio, and\nignition condition.\nwcost w\u03c4 w\u03b2 wne wq wbs wi\n0.1 0.1 0.5 0.5 1.0 1.0 1.5\nTable 4 shows the default weights used for reward scalarization. The fraction is normalized after the weighted\nsum of each reward. Since it is a priority to satisfy the minimum requirements for steady-state operation, the weights\ncorresponding to operational constraints have higher values than others. In addition, the parameter ais 1.0, while\nrfailure is\u22121.0for all cases. Additional details for experimental setup are shown in our code.\n6 Results\nIn this section, the results of the optimization process are represented. The optimal designs refer to the cases\nwhen the best reward was acquired while achieving the steady-state operation conditions during the optimization. We\nconducted DRL-based optimization to find the optimal cases and compared the performance with the grid search case\nto show the efficiency of the proposed one.\nFigure 3 describes the policy loss and total reward change over episodes, verifying the training process for the\nagent works under the reactor design optimization problem. The policy loss decreased while the temporal average total\nreward increased over the episode, and both were saturated. This indicates that the proposed algorithm is applicable\nfor multiple design objective optimization for a tokamak reactor. Meanwhile, Figure 4 represents the partial reward\nchange for each plasma parameter related to design objectives over the episode. The partial rewards corresponding to\nplasma density and the cost have different tendencies compared to others, indicating a trade-off between those two\nparameters and other operational constraints. This trade-off causes Pareto-optimal, thereby optimizing the preferences\nfor the objectives required.\nNext, we compared two different algorithms to show the efficiency of DRL in a reactor design optimization.\nThe grid search algorithm has been mainly utilized for parametric search in finding compact tokamak reactor design [3].\nHowever, this algorithm randomly selects the input parameters, thus requiring a large number of processes. However,\nthe proposed algorithm can find the optimal designs satisfying operational constraints while reducing the cost parameter\nwith fewer episodes, as seen in Figure 5. This only requires half of the episodes to achieve the highest reward and\nsatisfy the constraints, highlighting the competence of finding the optimal reactor design parameters.\n6APREPRINT - SEPTEMBER 13, 2024\nFigure 3: The policy loss curve (left) and total reward change (right) during the optimization process.\nFigure 5: Comparison of a total reward change during the optimization through grid search and DRL.\nTable 5 shows the details for comparison of parameters and conditions between the reference and the optimized\ncases by grid search and DRL. From Table 5, grid search found the case in which the cost parameter increased and\nQ decreased. This indicates that the reactor design cannot achieve a cost reduction and be efficient. Meanwhile, the\nproposed algorithm figured out the case in which the cost parameter was reduced, while Q also decreased to 6.03 with\noptimized design, represented as Figure 6. Since the compact reactor should decrease a fusion power to reduce its\nsize [9], the design configuration that DRL found was also optimized in the same way, which is physically reasonable.\nHowever, this induces the reduction of energy confinement time according to scaling law [37], thus resulting in a\ndecrease in Q. Nevertheless, Figure 6 shows that DRL found the different operational regime, which has relatively low\ntemperature and high density compared to grid search and reference case, representing the effectiveness for exploring\nnew operational regimes. Note that not only a high magnetic field but also a high aspect ratio and elongation are\nrequired to satisfy all minimum requirements for steady-state operation. The result of the optimized reactor design is\ndescribed in Figure 7, showing the successful volume reduction of a reactor.\nTable 6 represents the fraction of operational constraints and corresponding plasma parameters. All fractions\nshould be less than 1 for steady-state operation. However, the reference case cannot satisfy the safety factor condition\nform= 2kink instability and operational bootstrap condition. Both conditions are satisfied in grid search and DRL\ncases compared to the reference. The details can be shown in Figure 8. Figure 8 depicts the curves ofn\nnG,\u03b2\n\u03b2T,qK\nq,\nandfBS\nfNCas a function of a,H,Bmax, and PE. The black vertical line for each plot indicates a designed reactor\u2019s\nstatus. Each curve refers to a stability change to input parameters and should be less than 1. Thus, all curves should\nsimultaneously lie in the unshaded region for each plot. The top four plots are derived from [2], which failed to find the\nsafety regime for steady-state operation. However, the bottom four plots found by DRL lie in the unshaded region for\nall cases, meaning that this optimal design achieved all constraints. Even if the input parameters deviate from the\noptimal values, the designed tokamak satisfies the constraints, indicating that this design is reliable enough to allow the\ndesign error for construction.\n7APREPRINT - SEPTEMBER 13, 2024\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 4: The partial reward change for each plasma parameter during optimization. (a) Plasma beta \u03b2, (b) cost\nparameter, (c) average density \u00afne, (d) bootstrap current ratio fbs, (e) safety factor q, (f) energy confinement time \u03c4.\nTable 5: The comparison of design parameters from the reference and the optimized reactor.\nReference case Grid search case Optimized case (DRL)\nR(m) 5.346 4.467 3.843\na(m) 1.337 1.081 0.854\n\u2206blanket (m) 1.199 1.164 1.164\n\u2206shield (m) 0.1 0.1 0.1\n\u2206TF(m) 0.969 1.052 0.494\nBmax(T) 13.0 14 16.0\n\u03ba 1.7 1.816 1.9\nA 4.0 4.132 4.5\nH 1.0 1.3 1.3\n\u03b7thermal (m) 0.4 0.4 0.4\nPelectric (MW) 1000.0 710 500\nPthermal (MW) 2500.0 1780 1250.0\n\u03b2 4.125 4.238 3.845\n\u03c4(s) 0.944 0.840 0.713\nIp(MA) 14.296 9.728 7.463\nq 1.544 2.028 2.227\nfBS 0.841 0.705 0.770\nfNC 0.445 0.728 0.832\nQparallel (MW-T/m) 500.0 435.85 386.12\n\u00afT(keV) 14.0 13.2 10.0\n\u00afn(1020m\u22123) 1.43 1.64 2.30\n\u00afp(atm) 7.67 8.30 8.83\nQ 10.38 9.31 6.03\nCost 1.003 1.014 0.851\n8APREPRINT - SEPTEMBER 13, 2024\nTable 6: The fraction of operational constraints and corresponding parameters for the reference, grid search, and DRL\ncases.\nReference case Grid search case Optimized case (DRL)\nn/nG 0.560 0.620 0.706\nqkink/q 1.287 0.986 0.898\n\u03b2/\u03b2T 0.942 0.872 0.955\nfBS/fNC 1.890 0.968 0.925\nFigure 6: The lawson criteria curve with designed tokamak reactor through our design optimization code. The red dot is\nthe optimized design by DRL ( Q= 6.03), the green dot is the design found by grid search ( Q= 9.31), and the black\ndot is the reference design ( Q= 10.38).\nFigure 7: The wall design configuration from [2] (left), grid search algorithm (middle), and our proposed optimization\ncode (right). The agent optimized the design, which has more vertically elongated plasma with a smaller size.\n9APREPRINT - SEPTEMBER 13, 2024\nFigure 8: The curves ofn\nnG,\u03b2\n\u03b2T,qK\nq, andfBS\nfNCas a function of a,H,Bmax, and PE. The figure depicts the operation\nregions for those parameters, while the unshaded region corresponds to the successful operation region. The top is\nderived from [2], and the bottom is derived from our proposed optimization code.\n10APREPRINT - SEPTEMBER 13, 2024\nOur proposed method can find multiple design configurations satisfying the operational constraints. These\ninclude not only the case with the highest reward but also those with high energy gain. Table 7 and Table 8 compare two\ncases for design parameters and the fraction of operational constraints. Case 1 was mentioned in Table 5 and Table 6,\nobtained from the episode with the highest reward. Case 2 is the case with the minimum cost parameter selected from\nthe cases satisfying that Q exceeds 10. From Table 8, both cases met the operational constraints, while case 2 succeeded\nin the cost reduction, maintaining the high energy gain, as seen in Table 7. This highlights the effectiveness of our\nproposed algorithm in searching for multiple optimal cases considering the preferences of reactor design objectives.\nTable 7: The comparison of design parameters from two different cases found by DRL.\nCase 1 Case 2\nR(m) 3.766 4.162\na(m) 0.837 1.189\n\u2206blanket (m) 1.164 1.164\n\u2206shield (m) 0.1 0.1\n\u2206TF(m) 0.491 1.382\nBmax(T) 16.0 16.0\n\u03ba 2.0 1.9\nA 4.5 3.5\nH 1.3 1.3\n\u03b7thermal (m) 0.4 0.4\nPelectric (MW) 500 750\nPthermal (MW) 1250.0 1875\n\u03b2 3.967 4.273\n\u03c4(s) 0.709 0.924\nIp(MA) 7.422 12.111\nq 2.348 2.248\nfBS 0.770 0.647\nfNC 0.865 0.640\nQparallel (MW-T/m) 388.88 492.05\n\u00afT(keV) 10.0 15.0\n\u00afn(1020m\u22123) 2.31 1.43\n\u00afp(atm) 8.87 8.24\nQ 6.03 11.80\nCost 0.836 0.954\nTable 8: The fraction of operational constraints and corresponding parameters for two different cases found by DRL.\nCase 1 Case 2\nn/nG 0.685 0.525\nqkink/q 0.852 0.890\n\u03b2/\u03b2T 0.958 0.729\nfBS/fNC 0.890 0.989\n7 Conclusion\nIn this research, an innovative approach was proposed for tokamak reactor design optimization based on Deep\nReinforcement Learning (DRL), aiming to satisfy the minimum requirements for steady-state operation and cost-\nefficient design. Since designing a tokamak device requires the consideration of plasma physics and nuclear engineering,\nan efficient multi-objective optimization technique is necessary to find the optimal reactor design. We demonstrated\nthat DRL can successfully search for cost-reduced designs that satisfy multiple design objectives, including operational\nrequirements, while the reference failed. Our framework has shown a relatively low computational cost for finding the\ncost-efficient optimal designs with only 100000 trials compared to the conventional method. Moreover, this can simplify\nthe multiple objectives by replacing those with rewards, enabling the reduction of computation for optimization. Since\nbuilding a tokamak device requires optimizing multiple design objectives, our proposed method has a substantial benefit\nbecause it is efficient and straightforward to apply in multiple design objective optimization for a tokamak reactor.\nFuture research requires optimizing profile shape and materials for walls. DRL can also handle discontinuous\nvariables and high-dimensional input data, indicating the ability to optimize 1D parameters and material composition.\nThis will provide more practical and accurate insight for conceptualizing the optimal tokamak reactor design.\n11APREPRINT - SEPTEMBER 13, 2024\nReferences\n[1]Farrokh Najmabadi, RW Conn, ARIES TEAM:, CG Bathke, L Bromberg, ET Cheng, DR Cohn, PIH Cooke,\nRL Creedon, DA Ehst, et al. The aries-i tokamak reactor study. Fusion Technology , 19(3P2A):783\u2013790, 1991.\n[2]JP Freidberg, FJ Mangiarotti, and J Minervini. Designing a tokamak fusion reactor\u2014how does plasma physics fit\nin?Physics of Plasmas , 22(7), 2015.\n[3]J Kang, G Jo, J-M Kwon, and BG Hong. Massive parametric study for prospective design space of a compact\ntokamak fusion reactor. Fusion Engineering and Design , 195:113962, 2023.\n[4]Gahyung Jo, Jae-Min Kwon, Ara Cho, Hyun-Kyung Chung, and Bong-Guen Hong. Cost assessment of a tokamak\nfusion reactor with an inventive method for optimum build determination. Energies , 14(20):6817, 2021.\n[5]Martin Greenwald, JL Terry, SM Wolfe, S Ejima, MG Bell, SM Kaye, and GH Neilson. A new look at density\nlimits in tokamaks. Nuclear Fusion , 28(12):2199, 1988.\n[6]Martin Greenwald. Density limits in toroidal plasmas. Plasma Physics and Controlled Fusion , 44(8):R27, 2002.\n[7]Francis Troyon, R Gruber, H Saurenmann, S Semenzato, and S Succi. Mhd-limits to plasma confinement. Plasma\nphysics and controlled fusion , 26(1A):209, 1984.\n[8] JA Wesson. Hydromagnetic stability of tokamaks. Nuclear Fusion , 18(1):87, 1978.\n[9] M Kikuchi. Steady state tokamak reactor based on the bootstrap current. Nuclear fusion , 30(2):265, 1990.\n[10] C Sborchia, Y Fu, R Gallix, C Jong, J Knaster, and N Mitchell. Design and specifications of the iter tf coils. IEEE\ntransactions on applied superconductivity , 18(2):463\u2013466, 2008.\n[11] G Jo, J-M Kwon, A Cho, H-K Chung, and BG Hong. Design space exploration of k-demo using novel method for\nthe minimum build determination. IEEE Transactions on Plasma Science , 50(11):4425\u20134429, 2022.\n[12] Bong Guen Hong. A study regarding the optimal radial build of a low-aspect-ratio tokamak fusion system. Fusion\nEngineering and Design , 135:110\u2013115, 2018.\n[13] BG Hong and T-H Kim. On the optimal radial build of a normal aspect ratio tokamak fusion system. Fusion\nEngineering and Design , 139:148\u2013154, 2019.\n[14] David Maisonnier, I Cook, P Sardain, R Andreani, L Di Pace, R Forrest, L Giancarli, S Hermsmeyer, P Norajitra,\nN Taylor, et al. A conceptual study of commercial fusion power plants. final report of the european fusion power\nplant conceptual study (ppcs). EFDA report number EFDA (05)-27/4.10 , 1, 2005.\n[15] David Maisonnier, Iau Cook, Sardain Pierre, Boccaccini Lorenzo, Giancarli Luciano, Norajitra Prachai, Pizzuto\nAldo, PPCS Team, et al. Demo and fusion power plant conceptual studies in europe. Fusion Engineering and\nDesign , 81(8-14):1123\u20131130, 2006.\n[16] K Okano, Y Asaoka, T Yoshida, M Furuya, K Tomabechi, Y Ogawa, N Sekimura, R Hiwatari, T Yamamoto,\nT Ishikawa, et al. Compact reversed shear tokamak reactor with a superheated steam cycle. Nuclear Fusion ,\n40(3Y):635, 2000.\n[17] M Coleman and S McIntosh. Blueprint: a novel approach to fusion reactor design. Fusion Engineering and\nDesign , 139:26\u201338, 2019.\n[18] Youji Someya, Kenji Tobita, Ryoji Hiwatari, Yoshiteru Sakamoto, Joint Special Design Team for Fusion, et al.\nFusion demo reactor design based on nuclear analysis. Fusion Engineering and Design , 136:1306\u20131312, 2018.\n[19] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126\u2013\n134, 1999.\n[20] Kazuo Yonekura and Hitoshi Hattori. Framework for design optimization using deep reinforcement learning.\nStructural and Multidisciplinary Optimization , 60:1709\u20131713, 2019.\n[21] Jean Rabault, Feng Ren, Wei Zhang, Hui Tang, and Hui Xu. Deep reinforcement learning in fluid mechanics: A\npromising method for both active flow control and shape optimization. Journal of Hydrodynamics , 32:234\u2013246,\n2020.\n[22] Jonathan Viquerat, Jean Rabault, Alexander Kuhnle, Hassan Ghraieb, Aur\u00e9lien Larcher, and Elie Hachem. Direct\nshape optimization through deep reinforcement learning. Journal of Computational Physics , 428:110080, 2021.\n[23] Thomas P Dussauge, Woong Je Sung, Olivia J Pinon Fischer, and Dimitri N Mavris. A reinforcement learning\napproach to airfoil shape optimization. Scientific Reports , 13(1):9753, 2023.\n[24] Maryam Sabzevari, Sandor Szedmak, Merja Penttil\u00e4, Paula Jouhten, and Juho Rousu. Strain design optimization\nusing reinforcement learning. PLoS computational biology , 18(6):e1010177, 2022.\n12APREPRINT - SEPTEMBER 13, 2024\n[25] Keeman Kim, Sangjun Oh, Jong Sung Park, Chulhee Lee, Kihak Im, Hyung Chan Kim, Gyung-Su Lee, George\nNeilson, Thomas Brown, Charles Kessel, et al. Conceptual design study of the k-demo magnet system. Fusion\nEngineering and Design , 96:281\u2013285, 2015.\n[26] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for re-\ninforcement learning with function approximation. Advances in neural information processing systems , 12,\n1999.\n[27] Mohit Sewak. Deep reinforcement learning . Springer, 2019.\n[28] Xu Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, and Qiguang Miao. Deep\nreinforcement learning: a survey. IEEE Transactions on Neural Networks and Learning Systems , 2022.\n[29] DX Wang and Lintong He. Adjoint aerodynamic design optimization for blades in multistage turbomachines\u2014part\ni: Methodology and verification. 2010.\n[30] Abdus Samad and Kwang-Yong Kim. Shape optimization of an axial compressor blade by multi-objective\ngenetic algorithm. Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy ,\n222(6):599\u2013611, 2008.\n[31] Shaun N Skinner and Hossein Zare-Behtash. State-of-the-art in aerodynamic shape optimisation methods. Applied\nSoft Computing , 62:933\u2013962, 2018.\n[32] Yolanda Mack, Tushar Goel, Wei Shyy, and Raphael Haftka. Surrogate model-based optimization framework: a\ncase study in aerospace design. Evolutionary computation in dynamic and uncertain environments , pages 323\u2013342,\n2007.\n[33] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approxi-\nmators. Neural networks , 2(5):359\u2013366, 1989.\n[34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347 , 2017.\n[35] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimiza-\ntion. In International conference on machine learning , pages 1889\u20131897. PMLR, 2015.\n[36] C Alfredo and C Arjun. Efficient parallel methods for deep reinforcement learning. In The Multi-disciplinary\nConference on Reinforcement Learning and Decision Making (RLDM) , pages 1\u20136, 2017.\n[37] PN Yushmanov, T Takizuka, KS Riedel, Otto JWF Kardaun, JG Cordey, SM Kaye, and DE Post. Scalings for\ntokamak energy confinement. Nuclear Fusion , 30(10):1999, 1990.\n[38] Mark B Chadwick, Michal Herman, P Oblo\u017einsk `y, Michael E Dunn, Yaron Danon, AC Kahler, Donald L Smith,\nBoris Pritychenko, Goran Arbanas, R Arcilla, et al. Endf/b-vii. 1 nuclear data for science and technology: cross\nsections, covariances, fission product yields and decay data. Nuclear data sheets , 112(12):2887\u20132996, 2011.\n[39] G Verdoolaege, G Karagounis, and M Tendler. Plasma confinement and transport.\n[40] John Wesson and David J Campbell. Tokamaks , volume 149. Oxford university press, 2011.\n[41] WR Spears and JA Wesson. Scaling of tokamak reactor costs. Nuclear Fusion , 20(12):1525, 1980.\n[42] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-\nbased policies. In International conference on machine learning , pages 1352\u20131361. PMLR, 2017.\n13APREPRINT - SEPTEMBER 13, 2024\nAppendix A Implementation: Fusion Reactor Design Computation Code\nIn this section, we describe the details of the computation process for the design and plasma parameters of a\ntokamak reactor. Our code is based on [2], aiming to determine the parameters subject to the engineering and nuclear\nphysics constraints [3, 18]. The computational process consists of two steps: (1) the geometrical design computation\nand (2) the estimation of plasma parameters. These parameters can then calculate the reactor design performance related\nto operational constraints.\nA.1 Geometrical Design Computation\nStarting from the core design, the input variables from Table 1 are required. First, the major radius of the core\nplasma can be determined through the neutron wall loading constraint. In more detail, the neutron wall loading at\nthe first wall should not exceed the maximum allowable neutron wall load PW. From electric power PEand thermal\nefficiency \u03b7T, the approximate neutron power at the wall can be estimated; thus, the major radius can be computed\nas Equation 4. Equation 4 assumes that maximum neutron wall load PW\u22434MW/m2[2] with En= 14.1MeV and\nEF= 22.4MeV . In addition, the geometrical variables are also utilized, including elongation \u03baand aspect ratio A, to\ncompute the surface area and the minor radius from A=R\na.\nR= [1\n4\u03c02En\nEFPE\n\u03b7TPW(2\n1 +\u03ba2)0.5A]0.5(4)\nThe computation of blanket thickness is based on a simple planar structure with the governing equations for\nneutron slowing down and lithium breeding. Slowing down is explained as the collision of neutrons with Li-7 at the\nblanket, while the breeding process is for the reaction between neutrons and Li-6. Thus, the energy balance equation for\nthe collisional process and the mass balance equation from the nuclear reaction can be described as Equation 2 from [2],\nenabling to derive the estimation of the blanket thickness as Equation 5. From the hard-sphere collision model, the\nscattering cross-section is estimated that \u03c3s\u22432barns , thus \u03bbs= 0.1m[38]. In addition, \u03b1B=\u03bbB\n\u03bbS(EF\nET)0.5while\nET= 0.025eV[38].\u0393b\n\u03930is a flux ratio between the inward and outward of the blanket, assumed to be 10\u22125[2].\n\u2206Blanket =\u03bbs(1 +\u03b1Bln\u03930\n\u0393b) (5)\nThen, it is now available to compute the TF coil thickness, which is supposed to be a flat circular magnet. The\nTF coil should maintain its strength against the material stress from the centering and tensile force due to the magnetic\nfield. After the simple calculation for deriving each component of the stresses [2], the TF coil thickness is estimated as\nEquation 6.\n\u2206TFcoil =R0[2(1\u2212\u03f5B)\u2212[(1\u2212\u03f5B)2\u2212\u03b1M]0.5\u2212[(1\u2212\u03f5B)2\u2212\u03b1J]0.5(6)\nwhere \u03b1Mand\u03b1Jare represented as below. \u03c3max= 600 MPa is the maximum allowable material stress applied to the\ncoil, and Jmax= 20MA/m2is the maximum allowable overall current density for the superconducting coils [1, 2, 10].\n\u03b1M=B2\n0\n\u00b50\u03c3max[2\u03f5B\n1 +\u03f5B+1\n2ln(1 +\u03f5B\n1\u2212\u03f5B)]\n\u03b1J=2B0\n\u00b50R0Jmax(7)\nA.2 Plasma Parameter Estimation\nAfter the geometric design computation, it is now available to estimate plasma parameters required to calculate\nthe design performance. In this step, the average plasma temperature \u00afTand the enhancement factor Hare input\nvariables to determine the rest of the parameters. Following the given profiles and microscopic cross-section \u27e8\u03c3\u03bd\u27e9,\nmentioned in Equations 20 and 21 from [2], we can compute average plasma pressure, density, and plasma beta, as\ngiven in Equation 8, 9, and 10.\n\u00afp= [4\n\u03c02(1 +\u03bdT\n1 +\u03bdP)2 PE\n\u03b7TEFR0a2\u03ba\u00afT2\nR1\n0(1\u2212\u03c12)2\u03bdn\u27e8\u03c3\u03bd\u27e9\u03c1d\u03c1]0.5(8)\n14APREPRINT - SEPTEMBER 13, 2024\n\u00afn=1 +\u03bdp\n2(1 + \u03bdT)(1 + \u03bdn)\u00afp\n\u00afT(9)\n\u03b2=2\u00b50\u00afp\nB2\n0(10)\nNow, from the power balance relation and empirical energy confinement time estimation for ELMy H-mode\n[37, 39], we can calculate the energy confinement time and plasma current, as shown in Equation 11, and 12.\n\u03c4E= 3\u03c02R0a2\u03baEF\u03b7T\nE\u03b1PE\u00afp (11)\nIp=7.98\nH1.08\u03c41.08\nE\nR1.49\n0a0.62\u03ba0.84\u00afn0.44B0.16\n0A0.2(E\u03b1PE\nEF\u03b7T)0.74(12)\nIn addition, the required bootstrap fraction for steady-state operation and the neoclassical bootstrap fraction [40]\nare estimated through Equations 13 and 14, which are also suggested in [2].\nfB= 1\u22121.2PCD\nR0\u00afnIn2\n\u2225(13)\nfNC= 268a2.5\u03ba1.25\u00afp\n\u00b50R0.5\n0I2Z1\n"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary: Photonic Quantum Computers by Muhammad AbuGhanem\n\n**1. Abstract:**\nThe paper reviews the latest advancements in photonic quantum computing, highlighting significant progress made by leading industry players. It outlines contemporary performance metrics, architectural designs, and development strategies aimed at achieving scalable and fault-tolerant photonic quantum computers. Key experiments leveraging the unique advantages of photonic technologies are underscored, recognizing their potential to transform the future of quantum computing, particularly in the noisy intermediate-scale quantum (NISQ) era.\n\n**2. Introduction:**\nQuantum computers are increasingly prominent in deep technology sectors due to their potential to solve complex problems faster than classical computers. Fault-tolerant quantum computers using photonics are leading contenders due to their resilience against noise and decoherence, even at room temperature, making them highly scalable. Encoding qubits within photonic states offers numerous benefits, including immunity to decoherence and the capability for room temperature operation. Effective photonic quantum information processing (QIP) requires processors to be large-scale, universal, low-loss, and maintain quantum interference. Significant advancements in photonics have led to increased system complexity, breakthroughs in QIP, quantum machine learning, and quantum communication, all of which are detailed in this paper.\n\n**3. Related Work:**\nThe paper reviews competitive photonic quantum computing technologies developed by companies and research entities including iPronics, USTC Jiuzhang, ORCA Computing, PsiQuantum, and more. It explains how each entity is moving towards fault-tolerant architectures and the milestones they have achieved in photonic quantum technology. This review discusses innovations in device components, photonic quantum communication, and programmable photonic integrated circuits, providing comparisons and context regarding how each contribution has pushed the state of the art forward.\n\n**4. Method:**\nThe methods discussed in the paper encompass the development of photonic processors, each with unique architectural considerations:\n   - **iPronics** employs a reconfigurable photonic processor based on hexagonal waveguide mesh structures, capable of performing complex optical transformations such as CNOT gates. \n   - **Jiuzhang (USTC)** uses Gaussian Boson Sampling (GBS) within high-dimensional Hilbert spaces, achieving milestones in quantum computational advantage with Jiuzhang 2.0 and 3.0.\n   - **ORCA Computing** advances fault-tolerant MBQC (Measurement-Based Quantum Computing) architectures via modular, fiber-interconnected designs using standard telecommunication technologies.\n   - **PsiQuantum** employs FBQC (Fusion-Based Quantum Computation), combining high-fault tolerance through physical primitives, i.e., entangling measurements or \"fusions.\"\n\n**5. Experimental Setup:**\nEach photonic quantum system leverages specific datasets and experimental setups:\n   - **Datasets:** Experimental configurations using high-purity, high-efficiency two-mode squeezed states, photon detection events, and other intricate setups detailed for each technology.\n   - **Hardware and Software:** Descriptions include the use of high-intensity lasers, single-photon detectors, multi-mode interferometers, and sophisticated software for control and data analysis.\n   - **Evaluation Metrics:** Success rates for entangling operations, computational speed comparisons, error rate analyses, and photon click events metrics are presented.\n   - **Baselines:** Comparisons are made against classical supercomputing capacities and theoretical models outlining computational advantages.\n\n**6. Results and Analysis:**\n   - **Empirical Findings:** Detailed numerical results show computational speeds, sampling rates, and photon detection events for each system. For instance, Jiuzhang 3.0 achieves over 255 photon-click events, demonstrating a significant computational advantage.\n   - **Graphs and Tables:** Visual aids display hexagonal waveguide mesh setups, photonic network configurations, and experimental data outputs.\n   - **Analysis:** Each system's unique facets and success metrics are discussed, including loss thresholds, scalability potentials, and system efficiencies.\n\n**7. Discussion:**\n   - **Interpretation:** In-depth interpretation of empirical results highlights the strengths and limitations of each system. For instance, Jiuzhang's advancement in GBS showcases practical applications in high-dimensional quantum state manipulations.\n   - **Implications:** Significant transformative potentials in cryptography, molecular simulations, and logistical optimizations are discussed.\n   - **Limitations:** Challenges such as photon loss, system integration complexities, and real-world application scalability issues are critically examined.\n\n**8. Conclusion:**\n   - **Summary:** The paper concludes that photonic quantum computers are on the verge of transformational application, demonstrating superior scalability and maintenance of quantum coherence at ambient conditions.\n   - **Future Research:** Recommendations include exploring alternative fault-tolerant coding schemes, enhancing resource-state generation methodologies, and extending applications into multipath quantum networks and beyond current experimental constraints.\n\nThis summary captures all essential technical, methodological, and empirical details necessary for replication or further development in photonic quantum computing research."}, {"role": "assistant", "content": "Photonic Quantum Computers\nMuhammad AbuGhanem1,2,\u22c6\n1Faculty of Science, Ain Shams University, Cairo, 11566, Egypt and\n2Zewail City of Science, Technology and Innovation, Giza, 12678, Egypt\u2217\n(Dated: September 13, 2024)\nIn the pursuit of scalable and fault-tolerant quantum computing architectures, photonic-based\nquantum computers have emerged as a leading frontier. This article provides a comprehensive\noverview of advancements in photonic quantum computing, developed by leading industry players,\nexamining current performance, architectural designs, and strategies for developing large-scale, fault-\ntolerant photonic quantum computers. It also highlights recent groundbreaking experiments that\nleverage the unique advantages of photonic technologies, underscoring their transformative potential.\nThis review captures a pivotal moment of photonic quantum computing in the noisy intermediate-\nscale quantum (NISQ) era, offering insights into how photonic quantum computers might reshape\nthe future of quantum computing.\nKeywords: Photonics quantum computers, light-based quantum technology, iPronic, Photonic, Quandela,\nORCA, Xanadu, Jiuzhang, PsiQuantum, Quix Quantum, TundraSystems, TuringQ, QBoson, Hamamatsu.\nPACS: 03 .67.Lx, 03.65.Ca, 42.50.Ex, 42.79.Ta, 42.50.\u2212p, 42.50.Dv.\nCONTENTS\nI. Introduction 2\nII. iPronics Programmable Photonics 3\nA. Overview 3\nB. iPronics SmartLight processor 3\nC. Reconfigurable photonic processors 3\nIII. The Jiuzhang Photonic Quantum Computers 3\nA. Overview 3\nB. The USTC Jiuzhang 4\nC. The USTC Jiuzhang 2.0 5\nD. The USTC Jiuzhang 3.0 5\nIV. ORCA Computing 6\nA. Overview 6\nB. ORCA\u2019s PT series photonic quantum\nsystems 8\nC. Fault tolerance in photonic MBQC\narchitectures 8\nD. ORCA\u2019s path to fault tolerance 8\nV. Photonic 9\nA. Overview 9\nB. Silicon spin-photon interface 9\nC. Photonic\u2019s path towards fault-tolerant\nquantum technologies 9\nVI. PsiQuantum 9\nA. Overview 9\nB. PsiQuantum\u2019s approach to fault-tolerance 11\nC. PsiQuantum\u2019s active volume architecture 11\nVII. Quandela Photonic Quantum Computers 12\nA. Overview 12\n\u2217gaa1nem@gmail.comB.MosaiQ 13\nC.Prometheus 13\nD.Ascella 13\nE.Perceval 14\nF. Quandela Cloud 14\nVIII. The QuiX Quantum 14\nA. The QuiX Quantum\u2019s photonic processors 14\nB. The 12-mode quantum photonic processor 15\nC. The 20-mode quantum photonic processor 17\nIX. TundraSystems Global 19\nA. Overview 19\nB. TundraSystems\u2019 quantum computing\necosystem 19\nC. The versatile TundraSystem HPC units 19\nX. TuringQ 20\nA. Overview 20\nB. TurinQ optical quantum computing\nsystems 20\nC. Quantum computational advantage with\nmembosonsampling 20\nD. Experimental quantum fast hitting on\nhexagonal graphs 21\nE. Quantum network with hybrid quantum\nmemory 24\nF. Photonic computer solving the subset sum\nproblem 24\nXI. Xanadu Quantum Technologies 25\nA. Overview 25\nB.Borealis 25\nC.Borealis \u2019s optical circuit design 25\nD.PennyLane 25\nXII. Quantum Photonics Industry 26\nXIII. Conclusion 27arXiv:2409.08229v1  [quant-ph]  12 Sep 20242\nDeclaration of competing interest 27\nEthical Approval and Consent to\nparticipate 27\nConsent for publication 27\nAvailability of data and materials 27\nCompeting interests 27\nFunding 27\nAcknowledgements 29\nData availability 29\nReferences 29\nI. INTRODUCTION\nIn the ever-evolving landscape of technology, quan-\ntum computers stand out as revolutionary and highly\npromising entities [1\u20134], extending their influence from\ncryptography and artificial intelligence to drug discov-\nery, optimization problems and communications [1]. By\nharnessing the fundamental principles of quantum me-\nchanics [5, 6], quantum computers possess the capacity\nto address intricate problems with unparalleled speed,\nsurpassing the computational capabilities inherent in the\nmost powerful classical computer known at present [1, 7\u2013\n18].\nPhotonic quantum computers are currently promi-\nnent contenders in fault-tolerant quantum computation\n(FTQC). These advanced architectures utilize photons as\nthe medium for qubit encoding and manipulation [19],\nexhibiting inherent resilience against decoherence and\nnoise, even at room temperature. This makes them ex-\nceptionally well-suited for scalable and FTQC. Photonic\nquantum computing also stands out for enabling the con-\nstruction of modular, easily networked quantum com-\nputers, holding significant potential for practical appli-\ncations [1].\nA key characteristic of photonic quantum computing\nlies in the encoding of qubits within the quantum state\nof light, unlocking a multitude of possibilities for quan-\ntum information processing (QIP). Quantum states of\nlight have played a pivotal role since the inception of\ngroundbreaking experiments in nonlocality and quantum\nteleportation [21, 22]. The benefits resulting from the\nencoding of qubits in single-photon states are numerous,\nincluding immunity to decoherence phenomena, the feasi-\nbility of conducting information processing at room tem-\nperature, and the capability to transmit photons through\nboth optical fibers and free-space channels.\nEffective photonic QIP requires photonic processors\nto meet four key requirements. Firstly, they must be\nlarge-scale to handle complex problem-solving. Second,\nuniversality is essential, enabling the implementation of\narbitrary transformations that map the system onto var-\nious problems. Achieving universality requires all-to-all\nconnectivity ( ninputs to noutputs) and full reconfig-\nurability [27, 54\u201356]. Third, low loss is crucial to pre-\nserve the integrity of (quantum) information carried bythe system. Finally, it is essential for a photonic pro-\ncessor to maintain quantum interference, ensuring the\naccuracy and reliability of quantum computations [57].\nThe integration of photonics into quantum computing\nholds transformative potential across various fields. In\nthe last decade, significant strides in photonic quantum\ntechnologies have led to heightened system complexity,\nresulting in breakthroughs across diverse facets of quan-\ntum information science. Noteworthy achievements in-\nclude the realization of quantum advantage [11, 12, 23]\nand the establishment of satellite quantum communica-\ntions [24\u201326]. More recently, photonic processors have\ngarnered increasing attention due to their versatile appli-\ncations. These applications span QIP based on linear op-\ntics [27\u201342], quantum machine learning (QML) [43\u201347],\nquantum repeater networks [48\u201351], and radio-frequency\nsignal processing [52, 53].\nFunctioning as tunable multimode interferometers ca-\npable of executing arbitrary linear optical transforma-\ntions, photonic processors have been realized in vari-\nous topologies, including triangular [27, 54], square [55],\nhexagonal [53], fan-like [44], rhomboidal [30], and\nquadratic configurations [52]. Photonic quantum com-\nputers are set to significantly enhance computational effi-\nciency, surpassing classical systems in areas such as cryp-\ntography [58], advancing quantum chemistry and materi-\nals science [34, 59, 60]. These machines have the potential\nto revolutionize secure communication [61], refine molec-\nular simulations critical for drug discovery [62, 63], and\noptimize complex logistical networks [64].\nIn this paper, we provide an in-depth review of key\nplayers in photonic quantum computing technologies\n(listed alphabetically) and their developed quantum pho-\ntonic computers, delineating their efforts towards the\nrealization of fault-tolerant photonic quantum comput-\ners. Featuring noteworthy contributions from key players\nsuch as iPronics, USTC Jiuzhang, ORCA Computing,\nPhotonic Inc., PsiQuantum, Quandela Photonic Quan-\ntum Computers, QuixQuantum, TundraSystems, Tur-\ningQ, and Xanadu, among others. We also explore the\nperformance of these photonic quantum processors, our\nfocus extends to recent experiments that leverage the\nquantum computational advantages inherent in photonic-\nbased quantum computers, highlighting the substantial\ncontributions of these entities to the field. For a detailed\nexploration of photonic quantum computing, including\ncritical aspects such as photon encoding, device compo-\nnents, photonic quantum communication and internet,\nand programmable photonic integrated circuits (PPICs),\nwe refer to [65]. This review also covers applications,\nprospects, and challenges in photonic quantum comput-\ning, which extend beyond the scope of our current review.3\nII. IPRONICS PROGRAMMABLE PHOTONICS\nA. Overview\nEstablished in 2019, iPronics [70] is dedicated to push-\ning the boundaries of versatile integrated programmable\nphotonic systems [52, 71\u201382], wherein the synergy of op-\ntical hardware and software facilitates multifaceted func-\ntionalities. These systems find applications across diverse\ndomains, including RF signal processing, optical commu-\nnications [83\u201387], sensing and biophotonics [89, 90], arti-\nficial intelligence, machine learning techniques [91], hard-\nware acceleration [92], neuromorphic computing, and\nquantum computing [1, 93].\nB. iPronics SmartLight processor\nThe iPronics SmartLight processor is characterized by\nits silicon photonics chip operating in the C-band, fea-\nturing a hexagonal mesh configuration comprising 72\ntuning units and 64 input/output ports [94]. Notable\nperformance attributes encompass high-capacity build-\ning blocks designed for 50, 100, and 200 GHz bandwidth\nfiltering functions, affording programmable control over\ncentral wavelength and extinction ratio, as well as demul-\ntiplexing capabilities. The processor is equipped with a\nlaser spanning a 2nm range, with a central wavelength\nat 1550 nm. Augmenting user accessibility, the associ-\nated software exhibits a user-friendly interface with auto-\nmated functions, while an integrated driving unit ensures\ncomprehensive operational control [31, 55, 94\u201399].\nC. Reconfigurable photonic processors\nThe research conducted by iPronics marks toward\nprogress in advancing the realization of an innovative\nparadigm characterized by versatile and reconfigurable\nphotonic processors [71\u201380]. In [53], P\u00b4 erez et al. has\ndesigned, fabricated, and exhibited an integrated re-\nconfigurable photonic signal processor [71\u201380]. The\nchip employs a configurable hexagonal silicon waveguide\nmesh structure, with each hexagonal side comprising\ntwo waveguides [100] that can be dynamically coupled\nor switched using a programmable tuneable basic unit\n(TBU) implemented through a Mach-Zehnder interfer-\nometer (MZI) [55, 95, 99\u2013102]. iPronics systematically\nmodified a 7-hexagon cell design to empirically manifest\n21 distinct functionalities, encompassing asymmetrical\nFIR Mach-Zehnder filters [101, 102], ring cavities, com-\nplex CROW [103], SCISSOR [104], and ring-loaded MZI\nfilters [101, 105, 106], as well as multiple input multiple\noutput linear optic transformation devices (see Figure 1),\nnotably a CNOT gate [53]. These devices find application\nacross diverse domains such as communications [83\u201387],\nbiophotonics and biomedical sensing [89, 90], multipro-\ncessor and memory units interconnection [107\u2013109], high-speed signal processing operations [110\u2013114], advanced\ncivil radar systems [88] internet of things, quantum logic\ngates [115\u2013117], and quantum information [53].\nAdditionally, P\u00b4 erez-L\u00b4 opez et al. introduced a control\narchitecture and a series of control strategies [91, 118,\n119] designed for fault-tolerant self-configuration [120]\nof the circuit to execute specific tasks in [121], as\nshown in Figure 2. These algorithms are catego-\nrized into configuration methods that necessitate pre-\ncharacterization routines [122], as also outlined, and\nadvanced optimization methods that not only elimi-\nnate this requirement but also address challenges arising\nfrom nonideal components characterized by nonhomoge-\nneous loss distribution, power consumption [123, 124],\nphase offsets, optical crosstalk, and tuning crosstalk [112,\n125\u2013128]. The authors further proposed and imple-\nmented self-configuration routines based on stochastic\npopulation-based methods, exemplified by Genetic Al-\ngorithms (GAs) [119] and Particle Swarm Optimization\n(PSO) [129], for three distinct applications: an all-cross\nrouter, a beamsplitter, and an optical filter adaptable\nto a diverse range of spectral masks [121]. Through the\namalgamation of computational optimization and pho-\ntonics, this endeavor marks a noteworthy stride toward\na novel paradigm in programmable photonics [93, 121].\nThe advancing maturity of integrated photonic tech-\nnology has enabled the construction of progressively\nlarger and intricate photonic circuits directly on chip sur-\nfaces [71\u201380]. While the majority of these circuits are\npresently tailored for specific applications, the heightened\ncomplexity has ushered in a new era of programmable\nphotonic circuits [82, 93, 130\u2013132]. These circuits can\nbe dynamically configured using software, leveraging\na network of on-chip waveguides, tunable beam cou-\nplers [56, 135, 136], and optical phase shifters [137, 138]\nto serve a diverse array of functions [106]. Further com-\nprehensive insights into the research potential of iPronics\nin this domain can be explored in [53, 82, 93, 121, 132\u2013\n134].\nIII. THE JIUZHANG PHOTONIC QUANTUM\nCOMPUTERS\nA. Overview\nThe USTC (University of Science and Technology of\nChina) has made remarkable strides in photonic quan-\ntum computing technology with its series of Jiuzhang\nquantum computers: Jiuzhang [11], Jiuzhang 2.0 [12]\nandJiuzhang 3.0 [13]. Each iteration represents a sig-\nnificant milestone in the field. Jiuzhang was the first to\nachieve quantum computational advantage [11], marking\na pivotal moment in quantum computing history. Sub-\nsequent versions, Jiuzhang 2.0 and Jiuzhang 3.0, have\nfurther advanced the field by enhancing programmabil-\nity and speed. Notably, they have demonstrated remark-\nable capabilities in executing large-scale Gaussian boson4\nFIG. 1. The hexagonal waveguide mesh chip was manufactured with the following key features: (a) Design layers, encompassing\noptical, electrical, and thermal components, were incorporated for both the 7-cell hexagonal waveguide mesh and an auxiliary\ntest cell. (b) The silicon on insulator (SOI) chip, with dimensions of 15 \u00d720 mm, was fabricated. (c) A detailed view of the\n7-cell hexagonal waveguide mesh, with a scale bar of 2 mm. (d) An enlarged image of an optical interconnection node featuring\nthree tunable basic units (TBUs), with a scale bar of 100 \u00b5m. (e) A close-up image of a single hexagonal cell exhibiting the\nMach-Zehnder Interferometer (MZI), with a scale bar of 500 \u00b5m, and additional elements such as tuning heaters and star-type\nthermal isolation trenches in the right bottom corner. (f) The printed circuit board, where the waveguide mesh chip is mounted\nand wire bonded for integration. Reproduce from Ref. [53] under a Creative Commons Attribution 4.0 International License\n(http://creativecommons.org/licenses/by/4.0/ ).\nsampling (GBS) tasks [66, 67, 466, 520], and solving non-\nplanar graph problems by leveraging GBS [467]. As re-\nsearch and development in quantum computing progress,\nfurther advancements and improvements in these systems\nare expected to continue shaping the landscape of pho-\ntonic QIP, paving the way for transformative applications\nin various fields.\nB. The USTC Jiuzhang\nOn December 3, 2020, USTC reported a significant\nachievement in the field of quantum computing [11].\nTheir quantum computer, Jiuzhang , successfully exe-\ncuted GBS [66, 67, 466, 520] within a mere 200 seconds.\nIn contrast, the USTC research group estimated that the\nSunway TaihuLight supercomputer [68] would require a\nstaggering 2.5 billion years to perform the same calcula-\ntion. The photonic quantum computer, Jiuzhang , pro-\nduces as many as 76 photon clicks in its outputs [11],\nresulting in an expansive state-space dimension of 1030.\nNotably, the sampling rate achieved by Jiuzhang claimed\nto surpasses the capabilities of state-of-the-art simulation\nstrategies and supercomputers by an astonishing factorof approximately 1014.\nThe experimental configuration consists of a Verdi-\npumped Mira 900 Ti:sapphire laser (also known as\ntitanium-sapphire lasers, Ti:Al 2O3lasers) which is di-\nvided into 13 paths of equal intensity. The laser beams\nare directed onto 25 periodically poled potassium titanyl\nphosphate (PPKTP) crystals, resulting in the generation\nof 25 two-mode squeezed states. A hybrid encoding tech-\nnique is employed, effectively representing 50 single-mode\nsqueezed states. To enhance the purity of the states, a\n12nm filtering process raises the purity level from 98%\nto 99%. The 50 single-mode squeezed states are then\nfed into a 100-mode interferometer and subsequently de-\ntected by 100 single-photon detectors, each operating at\nan efficiency of 81%.\nFurthermore, in order to draw a comparison between\nthe results achieved by the photonic quantum comput-\ning and classical computing systems, an analysis was\ncarried out in [11] to determine the time required for\ntwo supercomputers to execute an equivalent GBS task.\nTheTaihu-Light supercomputer was estimated to require\napproximately 8 \u00d71016seconds (equivalent to 2.5 bil-\nlion years), whereas Fugaku was projected to necessi-\ntate 2\u00d71016seconds (equivalent to 0.6 billion years [11].5\nFIG. 2. Integrated Photonic hardware and control architecture of multiuse programmable photonic circuits.\n(a) The quantity of integrated phase shifters observed in recent waveguide mesh circuits. (b) The architecture of a labeled\nfield programmable photonic gate array (FPPGA), comprising a waveguide mesh core and high-performance building blocks.\n(c) The FPPGA core incorporates a longitudinally parallel hexagonal waveguide mesh interconnection topology [121]. (d) The\nelectronic control subsystem, signals, and software procedures are depicted, illustrating the mechanisms for controlling the\nprogrammable photonic integrated circuit. (e) The data array presents the complete scattering matrix of the FPPGA core,\nencompassing input and output spatial ports, along with the optical spectral dimension. Key components include GP (general-\npurpose), MEMA (multichannel electrical monitoring array), MEDA (multichannel electrical driving array), and LU (logic\nunit). Reproduced from Ref. [121] under a Creative Commons License ( http://creativecommons.org/licenses/by/4.0/ ).\nA schematic diagram illustrates the configuration of the\nlight-based quantum computer Jiuzhang is presented in\nFigure 3.\nC. The USTC Jiuzhang 2.0\nSubsequently, in the year 2021, an enhanced photonic\nquantum computer named \u201c Jiuzhang 2.0,\u201d purposefully\nengineered for large-scale GBS was introduced in [12].\nThe experiments involved a 144-mode photonic circuit,\nwhich yielded up to 113 photon detection events. The\nteam achieved this feat through the development of a\nnovel and scalable quantum light source based on stimu-\nlated emission of squeezed photons, boasting both near-\nunity purity and efficiency.\nRemarkably, Jiuzhang 2.0\u2019s photonic quantum com-\nputing capabilities translate into a substantial Hilbert\nspace dimension of approximately 1043, leading to a\nsampling rate that is approximately 1024times faster\nthan brute-force simulations conducted on supercomput-\ners. These developments represent a significant advance-\nment in photonic quantum computing, paving the way\nfor groundbreaking applications and furthering our un-\nderstanding of quantum computational power.\nThe concept involves the generation of spontaneously\nproduced photon pairs, resonating with the pump laser,\nto stimulate the emission of a second photon pair in again medium [69]. The experiment utilizes transform-\nlimited laser pulses at a wavelength of 775 nm focused\non PPKTP crystals to create two-mode squeezed states\n(TMSS). A concave mirror reflects the pump laser and\nthe collinear TMSS photons back, serving as seeds for\nthe second parametric process. Birefringence walk-off be-\ntween horizontally and vertically polarized photons of the\nTMSS is compensated using a quarter-wave plate. Addi-\ntionally, the dispersion between the pump laser and the\nTMSS is addressed by designing the PPKTP crystals to\neliminate frequency correlation. Visual depictions of the\nexperimental configurations are presented in Figure 4.\nIn their research, they achieved high collection effi-\nciency (0.918 at a waist of 125 \u00b5m and 0.864 at 65 \u00b5m)\nand simultaneous high photon purity (0.961 and 0.946)\nwithout any narrowband filtering. Their double-pass ap-\nproach can be readily expanded to higher orders, en-\nabling the generation of higher brightness quantum light\nsources, scalable and nearly optimal for various applica-\ntions.\nD. The USTC Jiuzhang 3.0\nIn December 2023, a recent experiment in GBS uti-\nlizing pseudo-photon-number-resolving detection was de-\ntailed in [13]. The experiment documenting photon-click\nevents reaching a maximum of 255, see Figure 5. This6\nFIG. 3. TheJiuzhang light-based quantum computing device\u2019s configuration, engineered by USTC. The machine\noperates by intricately manipulating light through an array of optical components. This visual representation of the Jiuzhang\nphotonic network provides insight into its experimental configuration, which occupies an optical table spanning an area of\napproximately three square meters. Within this setup, 25 Two-Mode Squeezed States (TMSSs) are introduced into the photonic\nnetwork, resulting in the acquisition of 25 phase-locked light signals. To provide further elucidation, the output modes of the\nJiuzhang photonic network are systematically segregated into 100 distinct spatial modes through the employment of miniature\nmirrors and Polarizing Beam Splitters (PBSs). This accomplishment signifies the emergence of the second quantum computing\nsystem to assert the achievement of quantum computational advantages, following in the footsteps of Google\u2019s Sycamore\nquantum processor. Reproduced under the provisions of the Creative Commons license ( https://creativecommons.org/\nlicenses/by/4.0/ from [11]).\ninvestigation integrates considerations for partial photon\ndistinguishability and introduces an extensive model for\ncharacterizing noisy GBS. Bayesian tests and correlation\nfunction analysis are applied within the context of quan-\ntum computational advantage to validate the samples\nagainst established classical spoofing simulations. Com-\nparative assessments with state-of-the-art classical algo-\nrithms reveal that generating a single ideal sample from\nthe identical distribution on the supercomputer Fron-\ntierwould demand approximately 600 years using exact\nmethods, whereas the quantum computer, Jiuzhang 3.0,\nachieves this feat in merely 1.27 \u00b5s. Computing the most\nchallenging sample from the experiment utilizing an ex-\nact algorithm would necessitate Frontier approximately\n3.1\u00d71010years.IV. ORCA COMPUTING\nA. Overview\nORCA Computing [142] is a leading company focused\non the development of comprehensive photonic quantum\ncomputers. At its core, ORCA\u2019s vision encompasses the\ndevelopment of both near-term, value-generating quan-\ntum accelerators and long-term, error-corrected quantum\ncomputing systems, all made possible through its innova-\ntive modular, fiber-interconnected architecture. Since its\nfounding in 2019, the company has successfully deployed\nits PT-1 quantum photonic system to clients worldwide,\nearning the trust of notable customers such as the UK\nMinistry of Defence (MoD) as they embark on their quan-\ntum computing journeys. The company\u2019s innovative ap-7\nFIG. 4. The Jiuzhang 2.0 experiment\u2019s configuration. The experiment employs a configuration comprising five key\nelements. In the top-left quadrant, a high-intensity pulsed laser emitting light at a wavelength of 775 nm is utilized to excite\n25 sources of TMSS (Two-mode Squeezed States), as indicated by the orange label within the left section. Simultaneously,\na continuous-wave laser operating at 1450 nm is directed to co-propagate with the aforementioned 25 TMSS sources. The\nresulting 1550-nm two-mode squeezed light is conveyed into a single-mode fiber that demonstrates resistance to temperature\nvariations. Notably, a 5-meter segment of this fiber is wound around a piezo-electric cylinder to enable control over the source\nphase, located in the central region. Transitioning to the central-right section, an optical arrangement comprising collimators\nand mirrors facilitates the injection of the 25 TMSSs into a photonic network. Here, 25 light beams corresponding to these\nTMSSs (illustrated in yellow) with a wavelength of 1450 nm and an intensity power of approximately 0.5 \u00b5W are harnessed for\nthe purpose of achieving phase coherence. The resulting 144 output modes from this arrangement are divided into four segments\nusing arrays of adjustable periscopes and mirrors. Ultimately, these output modes undergo detection using 144 superconducting\nnanowire single-photon detectors and are subsequently processed through a 144-channel high-speed electronic coincidence unit.\nReproduced under Creative Commons license ( https://creativecommons.org/licenses/by/4.0/ ) from [12].\nFIG. 5. Jiuzhang 3.0 :The experimental arrangement comprises 25 stimulated two-mode squeezed state photon sources,\nsynchronized in phase, and directed into a 144-mode ultralow-loss fully-connected optical interferometer. These photons traverse\n72 units of fiber loop setups for temporal-spatial demultiplexing before reaching 144 superconducting nanowire single-photon\ndetectors, constituting a pseudo-photon-number resolving detection scheme. Each fiber loop setup, distinguished by distinct\ncolors, accommodates two input modes. Within these setups, photons undergo temporal demultiplexing via fiber beam splitters\nand delay lines, resulting in four time bins. Furthermore, each time bin is partitioned into two path bins at the terminal fiber\nbeam splitter. Discrimination between photons from the two input modes within the same fiber loop setup is facilitated by\nassessing their temporal parity through a coincidence event analyzer (not depicted). Reproduced from [13] under a Creative\nCommons licenses ( https://creativecommons.org/licenses/by/4.0 ).8\nproach harmoniously integrates state-of-the-art technol-\nogy with readily available telecom and optical fiber com-\nponents, promising to deliver unparalleled performance,\nscalability, and user-friendliness across a wide range of\napplications, including generative machine learning and\noptimization.\nB. ORCA\u2019s PT series photonic quantum systems\nORCA\u2019s quantum computing vision is driven by their\ninnovative modular, fiber-interconnected architectures.\nThey have successfully minimized component complex-\nity by leveraging multiplexing and quantum memory\ntechnologies, thereby optimizing the quantum computing\nprocess. Importantly, ORCA leverages standard telecom-\nmunications technologies, enhancing robustness and cost-\neffectiveness. The modularity of ORCA\u2019s design not only\nsupports scalability but also future upgradability, mini-\nmizing obsolescence and promoting ongoing research and\ndevelopment. Additionally, the PT series by ORCA, de-\nsigned for integration into classical computing workflows,\nis facilitated by ORCA\u2019s SDK, which is programmed in\nPython and integrated with PyTorch , simplifying access\nfor machine learning users and fostering the development\nof quantum applications for practical use. This holistic\napproach caters to the hybrid quantum-classical environ-\nment and positions ORCA as a significant contributor to\nthe advancement of quantum computing technologies in\nboth the short and long term [143\u2013145].\nThe PT Series by ORCA is specifically designed to\nintegrate seamlessly with classical computing infrastruc-\nture and workflows. Furthermore, ORCA\u2019s Software De-\nvelopment Kit (SDK) is programmed in Python and in-\ntegrated with PyTorch . This integration enables easy\naccessibility for machine learning users, enhancing pro-\nductivity and reducing the complexity of developing AI\napplications for quantum computing.\nC. Fault tolerance in photonic MBQC architectures\nFault-tolerant quantum computation (FTQC) hinges\non the effective correction of hardware errors during pro-\ngram execution. The practical realization of fault tol-\nerance is contingent upon the specific characteristics of\nthe underlying hardware. For example, circuit-based er-\nror correction serves as a framework suitable for hard-\nware featuring deterministic gates, allowing the detec-\ntion of errors through non-destructive ancilla-assisted\nmeasurements [146, 147]. Conversely, in the context of\nmeasurement-based quantum computing (MBQC), error\nsyndromes are constructed through destructive measure-\nments on previously generated entangled states. MBQC\nis notably well-suited for hardware involving proba-\nbilistic entangling operations and destructive measure-\nments [148\u2013151], including discrete variable photonicqubits [151, 155\u2013157] and continuous variable qubits [158,\n159, 476].\nNumerous photonic MBQC architectures [149, 150,\n155\u2013157, 160\u2013163] follow a two-stage approach to achieve\nfault tolerance. The first stage involves creating a siz-\nable entangled resource state whose dimensions corre-\nspond to the complexity of the quantum computer pro-\ngram. Subsequently, in the second stage , destructive\nsingle-qubit measurements are applied to the prepared\nstate to execute the program. A more streamlined\napproach in MBQC, referred to as fusion-based quan-\ntum computation (FBQC), was introduced in [151]. In\nFBQC, destructive two-qubit projective measurements\nin the Bell-state basis, known as Bell-state measure-\nments [40] (BSMs) or fusions [164], are performed on\nconstant-sized resource states. The study in [151] also\npresents FBQC architectures implementing the surface\ncode, demonstrating high thresholds for both photon loss\nand fusion failures. Similar high thresholds have been\nachieved for other topological error correction codes us-\ning FBQC [165, 166]. The ability to generate entangled\nphotonic resource states [167\u2013171], along with the high\nthresholds, suggests the suitability of photonic platforms\nfor implementing FBQC architectures. However, the re-\nsource state size and photon loss thresholds remain chal-\nlenging for current hardware.\nIn [145], ORCA Computing has explored alternative\nphotonic architectures aimed at mitigating the hard-\nware demands. Specifically, they have devised MBQC\narchitectures that attain fault tolerance by fusing n\nresource states through measurements in the n-qubit\nGreenberger\u2013Horne\u2013Zeilinger (GHZ) state basis, known\nas GHZ-state measurements (GSMs).\nD. ORCA\u2019s path to fault tolerance\nRecently [145], ORCA Computing introduced inno-\nvative fault-tolerant architectures. Their focus lies on\nutilizing projective measurements in the GHZ basis\nin conjunction with constant-sized entangled resource\nstates. They have meticulously developed linear-optical\nconstructions of these architectures, wherein the GHZ-\nstate measurements are subjected to encoding techniques\naimed at mitigating errors stemming from photon loss\nand the inherent probabilistic characteristics of linear\noptics. Extensive simulations within their study in-\ndicate remarkable enhancements in single-photon loss\nthresholds when compared to the current state-of-the-art\nlinear-optical architectures, specifically those employing\nencoded two-qubit fusion measurements with constant-\nsized resource states. ORCA Computing\u2019s findings in this\nendeavor underscore the potential of a resource-efficient\nroute towards achieving fault-tolerant quantum comput-\ning based on photonic technologies.\nAn extensive array of research directions, extending\nbeyond the scope of ORCA Computing\u2019s work presented\nin [145], encompasses various avenues. These include the9\ninvestigation of alternative coding schemes for GHZ-state\nmeasurements as delineated in [172, 173], the exploration\nof leveraging biased error correction techniques as dis-\ncussed in [165], the utilization of diverse resource states\nas highlighted in [166], and the examination of fusion net-\nwork designs not rooted in conventional foliation princi-\nples as discussed in [174, 175].\nBeyond the realm of quantum computation, the en-\ncoded GHZ-state measurements developed by ORCA\nmay also find pertinent applications in the domain of\nquantum communication. For instance, ORCA Comput-\ning exhibit potential utility in facilitating multipath rout-\ning strategies, which, in turn, hold the promise of enhanc-\ning entanglement rates within quantum networks, as elu-\ncidated in [176, 177]. Photonic resource state generation\nmethods can be found in Refs. [157, 164, 169, 178\u2013180].\nV. PHOTONIC\nA. Overview\nPhotonic [182] is in the process of constructing an in-\nnovative quantum computing and networking platform,\ndistinguished by its scalability, fault-tolerance, and in-\ntegration [183]. Founded in 2016, the company har-\nnesses a proprietary technological framework encompass-\ning photon spin interfaces within silicon, integrated sil-\nicon photonics, and quantum optical components [184\u2013\n187]. Photonic\u2019s technological framework includes a na-\ntive telecommunications networking interface and lever-\nages the manufacturing capabilities inherent in sili-\ncon [188\u2013190]. Photonic collaborating with Microsoft to\npower global quantum ecosystem [191].\nB. Silicon spin-photon interface\nPhotonic\u2019s quantum computing approach is distinc-\ntively characterized by the integration of silicon spin\nqubits with photons, in order to surmounting the obsta-\ncles that hinder the progress of other quantum comput-\ning frameworks [184]. This integrated solution leverages\nphotonically-linked silicon spin qubits, facilitating the re-\nalization of a resource-efficient, error-corrected, and gen-\nuinely scalable quantum computing platform [192, 193].\nThe architecture, known as the silicon spin-photon plat-\nform, paves the way for the achievement of scalable fault-\ntolerance [192]. By harnessing the computational po-\ntential of spin qubits with photon interfaces, this archi-\ntectural design enables seamless communication through\nultralow-loss telecommunication band fibers [194\u2013196],\nthereby enabling fault tolerance at a large scale [190].\nFurthermore, this architecture amalgamates the ad-\nvanced manufacturing capabilities of silicon with dis-\ntributed computing, ultimately yielding a quantum com-\nputing system with boundless scalability potential [192].C. Photonic\u2019s path towards fault-tolerant quantum\ntechnologies\nThe existing challenges in scaling quantum networking\nand quantum computing technologies share a common\nobstacle of achieving widespread distribution of high-\nquality entanglement. In this regard, Photonic proposed\na novel QIP architecture, centered on optically active\nspins in silicon [186, 187, 192, 197, 198]. This archi-\ntecture (see Figure 6) presents an integrated technologi-\ncal platform that addresses the requirements for scalable,\nfault-tolerant quantum computing and networking [192].\nEmphasizing optimization for overall entanglement dis-\ntribution, the design utilizes color center spins in silicon\n(T centres) [186, 199, 200] for their manufacturability,\nphotonic interface, and high-fidelity information process-\ning attributes [192].\nThis architectural framework is broadly applicable\nacross various qubit systems [192]. The spin-photon in-\nterface\u2019s elevated connectivity allows for the utilization\nof fixed- and low-overhead quantum low-density parity-\ncheck (QLDPC) codes [196, 201\u2013223], ensuring fault tol-\nerance. Leveraging an integrated silicon photonics plat-\nform, thousands of qubits can be manufactured and\naddressed on a single chip, incorporating both optical\nand electronic control and routing [184, 185, 224\u2013228].\nFurthermore, modules can be seamlessly interconnected\nthrough existing global telecommunications infrastruc-\nture without incurring transduction losses. Employing\na T center network [184, 186, 199, 200] (as shwon in Fig-\nure 7) for distributing verified quantum entanglement en-\nables device-independent networking protocols, offering\nrobust protection against eavesdropper attacks. This ap-\nproach also has significant implications for applications\ndependent on entanglement distribution, such as blind\ncomputing [229, 230].\nVI. PSIQUANTUM\nA. Overview\nPsiQuantum [231] employs a photon-based approach to\nqubit production, wherein photons traverse silicon chip-\nembedded conduits. By utilizing mirrors, these photons\nare maneuvered into an entangled state, and their fu-\nsion measurements serve as gates. The company, founded\nin 2015, asserts that their approach represents the sole\npractical method for constructing a commercially viable\nquantum computer with a requisite number of qubits.\nTheir goal is to accelerate the development of dependable\nsystems leveraging existing manufacturing processes and\ninfrastructure.\nPsiQuantum adopts a superconducting single-photon\ndetector, renowned for its capacity to achieve requisite\nefficiencies with minimal developmental efforts. These\ndetectors operate at approximately 4 Kelvin, a temper-\nature notably higher than the milli-Kelvin temperatures10\nFIG. 6. Photonic\u2019s scalable quantum technology architecture. Involves a quantum chip that undergoes cooling within\na 1 K cryostat. This chip incorporates integrated silicon T centers situated within optical cavities, along with photonic switches\nand single-photon detectors. The architecture includes optical input-output (IO) ports connected to a room temperature\nphotonic switch network and control electronics through telecom fiber. This design naturally facilitates a highly-connected\narchitecture characterized by non-local connectivity, even as the system scales in size. The utilization of telecom fiber enables\nhorizontal scaling of the system by interconnecting multiple cryostats through their optical IO. This, in turn, enables the\nexpansion of computing power and the establishment of long-distance quantum networks. Reproduced with alternations from\n(www.photonic.com ).\nFIG. 7. T center network. (a) The non-local connectivity of the system operates consistently, whether entangling qubits\nwithin the same chip or extending across multiple chips in the same cryostat, spanning distances of 10 meters or even 100\nkilometers. This underscores the robustness and scalability of the system, showcasing its ability to maintain performance\nintegrity irrespective of spatial separation. The comparison between two-qubit operations on devices featuring: (b) planar,\nnearest-neighbor connectivity graphs and (c) non-local connectivity reveals distinct efficiency disparities. In the case of a\nplanar graph, executing a CNOT gate between two distant qubits necessitates sequential swap operations. Each successive\noperation introduces errors, resulting in a significant decline in overall fidelity, even over relatively short distances depicted in\nthe illustration. Conversely, non-local connectivity ensures that two-qubit operations remain uniform across the entire graph,\nallowing the device to scale with enhanced fidelity. Adapted from [190].11\nrequired for superconducting qubits [232] and certain\ncompeting technologies [2].\nB. PsiQuantum\u2019s approach to fault-tolerance\nFusion-based quantum computation The concept of\nFBQC has been introduced as a novel approach to quan-\ntum computing architecture [151]. Traditional quan-\ntum computing relies on deterministic unitary entangling\ngates [152\u2013154], which may not align naturally with cer-\ntain physical systems, notably those in the realm of pho-\ntonics [155, 156, 163, 164, 233\u2013243]. In contrast, FBQC\nleverages physical primitives inherently accessible in pho-\ntonic setups, primarily entangling measurements termed\n\u201cfusions,\u201d performed on qubits originating from small,\nconstant-sized entangled resource states. This approach\nnot only addresses the probabilistic nature of photonic\ngates and inherent errors but also integrates quantum\nerror correction techniques effectively [151].\nPsiQuantum\u2019s claims that this computational model\ncan attain significantly higher fault tolerance thresh-\nolds than those documented in existing literature [151].\nSpecifically, the company proposes a robust design ca-\npable of withstanding a substantial 10 .4% probability of\nphoton loss during each fusion, equating to a mere 2 .7%\nchance of losing each individual photon [151]. PsiQuan-\ntum also demonstrates a 43 .2% barrier against fusion fail-\nure using a ballistic technique [244], compared to 14 .9%\npreviously reported in [150]. Notably, this architecture\noffers modularity and substantially reduces classical pro-\ncessing demands compared to earlier photonic quantum\ncomputing designs [150, 156, 163, 244].\nFlexible architectural design in fusion networks The\nphysical architecture of FBQC, termed fusion networks,\nfeatures a remarkable degree of flexibility, devoid of in-\nherent temporal or spatial constraints. This flexibility\nallows for variations in the physical implementation of\nfusion networks. At the core of this architecture lies the\nconcept of Resource State Generators (RSGs), physical\ndevices responsible for generating resource states at spe-\ncific spatial locations and time intervals. This concept is\nespecially pertinent to photonic architectures [65]. The\nlifespan of a qubit is a critical factor to consider; it com-\nmences with its creation within an RSG, then proceeds\nthrough a fusion network router, which directs the qubits\nto their respective fusion locations [151]. Ultimately, each\nqubit undergoes a destructive measurement within a fu-\nsion. This relatively brief qubit lifespan proves advanta-\ngeous for FBQC, especially within photonic architectures\nwhere optical loss remains a dominant source of physical\nerrors [245\u2013247]. Figure 8 illustrates a 6-ring example\nfusion network. In [151] PsiQuantum introduces FBQC\nas a universal quantum computation model that capital-\nizes on the generation of small, constant-sized entangled\nresource states and projective entangling measurements.\nThe research explores how this model can be harnessed to\nachieve topological fault-tolerant quantum computation,particularly in the context of photonic architectures [248\u2013\n255].\nTowards large-scale, fault-tolerant quantum computa-\ntion with FBQC FBQC is formulated as a versatile\nframework for quantum computation, aligning with the\ninherent characteristics of various physical systems, in-\ncluding those in the domain of photonics. In [151] the\nauthors anticipate that this framework, which tightly in-\ntegrates physical errors with their impact on quantum er-\nror correction, holds the potential to enhance the perfor-\nmance of systems fundamentally based on resource state\ngeneration and projective measurements, as exemplified\nin linear optical quantum computing (LOQC)[150, 156,\n162, 234, 235, 244, 256\u2013258]. PsiQuantum\u2019s research cul-\nminates in the demonstration of a remarkable doubling of\nfault tolerance thresholds compared to previous schemes.\nAs quantum technology advances closer to realizing these\nsystems, the theoretical framework introduced by Psi-\nQuantum assumes significance as a vital tool for engi-\nneering both hardware and architectural designs, facili-\ntating the realization of large-scale, fault-tolerant quan-\ntum computation. Further elaboration on this physical\narchitecture is provided in [151].\nC. PsiQuantum\u2019s active volume architecture\nPsiQuantum\u2019s research in [259], introduces a decoding\nframework to support FTQC. In their pursuit, they in-\ntroduced the Active Volume Architecture (AVA) [260],\na technique expected to amplify quantum algorithm ef-\nficiency by approximately 50 times, specifically target-\ning error-corrected quantum systems. This method opti-\nmizes hardware utilization by facilitating long-range con-\nnections within the quantum computer, particularly ad-\nvantageous in photonic quantum computing. Their ad-\nvancements in FTQC extend to impactful domains like\ncryptography [261], estimating the computational scale\nrequired to breach widely used cryptographic systems\nusing a novel architecture, leveraging non-local connec-\ntions from photonic components. Collaborating with\nBoehringer Ingelheim [262], PsiQuantum explores molec-\nular observable computations, critical for industrial ap-\nplications with quantum computers.\nIn partnership with Mercedes-Benz [263], PsiQuantum\ninvestigates the quantum computational thresholds nec-\nessary to innovate Li-ion battery design, leveraging their\nexpertise in Fault Tolerance. Additionally, extensive re-\nsearch on photonic quantum computing technologies by\nPsiQuantum is detailed in [19, 27, 28, 38, 155, 180, 255,\n259\u2013261, 263\u2013278].12\nFIG. 8. The \u201cSix-ring\u201d Fusion Network. (a) Each resource state is represented as a graph state configured in the shape\nof a ring containing six qubits."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary of the Paper: \"CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs\"\n\n#### 1. Abstract:\nThe paper introduces \"CliquePH,\" a method to enhance Graph Neural Networks (GNNs) by integrating higher-order information using persistent homology on clique graphs. The method extracts higher-dimensional topological features efficiently using low-dimensional persistent homology algorithms. On benchmark datasets, this innovation improves test accuracy by up to 31%.\n\n---\n#### 2. Introduction:\n**Context and Motivation:**\n- Graph Neural Networks (GNNs) have become prevalent for graph learning tasks such as graph classification and node classification.\n- Current GNN architectures struggle to capture higher-order structures beyond pairwise interactions, like cliques and cycles.\n- Persistent homology, a tool from topological data analysis (TDA), has shown benefits in incorporating higher-order topological information into GNNs.\n\n**Research Questions and Hypotheses:**\n- Can persistent homology be applied efficiently to extract higher-order topological features for GNNs?\n- Will incorporating these higher-order topological features improve the performance of GNNs on graph-related tasks?\n\n**Literature Review and Positioning:**\n- GNNs, based on the message-passing framework, are limited in capturing information beyond pairwise interactions.\n- Prior works incorporate dimension-0 and dimension-1 persistent homology into GNNs but face computational challenges for higher dimensions.\n- The paper introduces CliquePH, an approach to leverage higher-dimensional topological information efficiently.\n\n---\n#### 3. Related Work:\n**Review of Relevant Prior Work:**\n- GNNs [1,2] as the state-of-the-art models for graph data but limited in higher-order interactions [3,4].\n- Persistent homology from TDA has been used to enhance graph learning with important structural information up to dimension 1 [7,8].\n- Existing methods [9] incorporating TDA information into GNNs are computationally expensive for dimensions higher than 1.\n\n**Advancements Over Prior Work:**\n- CliquePH introduces a topological layer for GNNs that efficiently captures higher-order structural information using low-dimensional persistent homology.\n- Bridges the efficiency gap by transforming the graph into multiple clique graphs before applying persistent homology.\n\n---\n#### 4. Method:\n**Proposed Approach Description:**\n- **CliquePH Topological Layer:** Adds a topological layer named \"CliquePH\" to any GNN, leveraging persistent homology on clique graphs to capture higher-order structures.\n  \n**Mathematical Formulations and Algorithms:**\n- **Graph Transformation (Graph Lifting):** Transforms the original graph \\(G = (V, E)\\) into a set of clique graphs \\(K^{(r)}(G)\\) capturing various \\(r\\)-vertex cliques.\n- **Persistent Homology Computation:** Computes persistent homology up to dimension 1 on these clique graphs, summarizing valuable higher-order topological structures.\n  \n**Architectural Details:**\n1. **Graph Lifting:**\n   - Extract higher-order clique graphs from the original graph.\n   - Use node embeddings computed from a GNN.\n  \n2. **Message Passing:**\n   - Perform \\(L_m\\) steps of message passing with any GNN to obtain node embeddings \\(H^{(L_m)}\\).\n  \n3. **Learnable Persistent Homology:**\n   - Assign filtration values to nodes and edges in all graphs.\n   - Perform persistent homology up to dimension 1 to generate persistence diagrams.\n   - Use DeepSet networks to embed these persistence diagrams into meaningful representations.\n  \n4. **Information Combination:**\n   - Combine topological features with node embeddings.\n   - Perform additional message passing if needed and aggregate the final representation.\n\n**Novel Techniques:**\n- Use of efficient 0 and 1-dimensional persistent homology on lifted clique graphs, reducing computational complexity.\n- Integration of persistent homology in a differentiable manner, allowing end-to-end learning.\n\n---\n#### 5. Experimental Setup:\n**Datasets:**\n- Benchmarked on standard datasets: DD, ENZYMES, MNIST, PROTEINS, IMDB-B, REDDIT-5K, OGBG-Molhiv.\n\n**Hardware and Software:**\n- Experiments conducted using an NVIDIA A100 GPU and AMD EPYC 7742 64-Core CPUs.\n\n**Evaluation Metrics:**\n- Test accuracy and ROC-AUC for OGBG-molhiv.\n- Averaged performance over multiple runs with different random seeds.\n\n**Baselines and Comparison Methods:**\n- Standard popular GNNs: GCN, GIN, GAT.\n- Comparisons with GNNs incorporating TOGL (persistent homology up to dimension 1).\n\n---\n#### 6. Results and Analysis:\n**Empirical Findings:**\n- CliquePH significantly improves performance across various datasets, particularly effective in graph classification tasks without node attributes.\n- Improves over TOGL by capturing higher-order topological information from cliques, providing practical performance benefits.\n\n**Detailed Description of Results:**\n- Structural information experiments (without node features): CliquePH outperforms baseline GNNs and TOGL.\n- Graph classification benchmarks: CliquePH notably boosts accuracy even when using node attributes.\n\n**Graphs, Tables, Figures:**\n- Table 1: Runtime comparison demonstrating the efficiency of CliquePH.\n- Table 2: Test accuracy comparisons for structure-based experiments.\n- Table 3: Empirical validation of expressiveness using untrained models.\n- Table 4: Test accuracy on standard graph classification benchmarks.\n\n**Ablation Studies:**\n- Analyzed the impact of different placements of the CliquePH layer and the maximum dimension \\( r \\) of lifted clique graphs.\n- Found that performance is dependent on the dataset and configuration, with varying optimal placements and clique dimensions.\n\n---\n#### 7. Discussion:\n**Interpretation of Results:**\n- Confirmed the hypothesis that integrating higher-order topological information via persistent homology improves GNN performance.\n- Highlighted the practicality and stability of CliquePH in enhancing GNNs.\n\n**Implications of Findings:**\n- The approach presents a significant advancement in enhancing GNNs with efficient higher-order structural information.\n- Shows the potential for topology-driven methods to address limitations in traditional GNNs.\n\n**Limitations and Potential Biases:**\n- Scalability concerns for very large graphs.\n- The method doesn't equate to exact higher-order persistent homology.\n- May overfit if higher-order structures are not informative.\n\n---\n#### 8. Conclusion:\n**Key Contributions:**\n- Introduced a scalable method to capture higher-order structures in GNNs.\n- Demonstrated substantial performance improvements using CliquePH across various graph learning tasks.\n\n**Future Research Directions:**\n- Explore more scalable and efficient ways to handle very large graphs.\n- Investigate the theoretical guarantees and expressiveness of CliquePH further.\n- Extend the method to other tasks and domains in graph learning.\n\n---\nThis comprehensive summary encapsulates all essential details, technical specifics, and empirical findings, ensuring it provides sufficient information for a researcher to replicate and build upon this study."}, {"role": "assistant", "content": "CliquePH: Higher-Order Information for Graph Neural\nNetworks through Persistent Homology on Clique Graphs\nDavide Buffelli\u2217\nMediaTek Research, London, United Kingdom\ndavide.buffelli@mtkresearch.comFarzin Soleymani\u2217\nTechnical University of Munich, Munich, Germany\nfa.soleymani@tum.de\nBastian Rieck\nUniversity of Fribourg, Fribourg, Switzerland\nbastian.grossenbacher@unifr.ch\nAbstract\nGraph neural networks have become the default choice by practitioners for graph\nlearning tasks such as graph classification and node classification. Nevertheless,\npopular graph neural network models still struggle to capture higher-order infor-\nmation, i.e., information that goes beyond pairwise interactions. Recent work has\nshown that persistent homology, a tool from topological data analysis, can enrich\ngraph neural networks with topological information that they otherwise could\nnot capture. Calculating such features is efficient for dimension 0 (connected\ncomponents) and dimension 1 (cycles). However, when it comes to higher-order\nstructures, it does not scale well, with a complexity of O(nd), where nis the\nnumber of nodes and dis the order of the structures. In this work, we introduce a\nnovel method that extracts information about higher-order structures in the graph\nwhile still using the efficient low-dimensional persistent homology algorithm.\nOn standard benchmark datasets, we show that our method can lead to up to 31%\nimprovements in test accuracy.\n1 Introduction\nThe research area of Graph Neural Networks (GNNs) has received a huge amount of interest in\nthe past few years [ 1,2]. As a result, GNNs are now the first choice for many graph-related tasks,\nlike graph classification, node classification, and link prediction. Nevertheless, GNNs present some\nshortcomings when it comes to modeling higher-order structures (i.e., structures in the graph that go\nbeyond pairwise interactions, like cliques andcycles [3,4]). In fact, GNNs follow a message-passing\nframework [ 5] in which each node communicates with its neighbors, but there is no mechanism\nto go beyond pairwise interactions, which limits the expressiveness of these models [ 6]. However,\ninformation regarding certain topological structures like cycles and cliques, can be of great importance\nto graph-related tasks, significantly improving performance [7].\nTopological data analysis (TDA) [ 8] is an emerging area of study that uses techniques from topology,\nand in particular the tool of persistent homology, to study the shape of the data at multiple scales.\nTDA is particularly well-suited for capturing higher-order information in graphs, as it can provide\ninformation related to structures of different orders, including cliques and cycles, which are known\nto be particularly informative for several graph-related tasks [ 7]. Recent works have incorporated\ntopological information related to persistent homology into graph learning tasks with great success\n[9]. These methods are however limited to persistent homology up to dimension 1, due to the high\ncomputational cost that is required for higher dimensions. While this already empowers the model\nwith important structural information that GNNs cannot capture, it is still missing a vast amount of\nhigher-order information.\n\u2217Equal contribution.\nPreprint. Preliminary work.arXiv:2409.08217v1  [cs.LG]  12 Sep 2024CliquePH: Higher-Order Information for Graph Neural Networks\nIn this paper we introduce CliquePH, a new topological layer that can be added to any GNN, and that\nprovides information about persistent homology on higher-order structures. Performing persistent\nhomology up to dimension 1is extremely efficient and scalable, while higher-dimensional persistent\nhomology is prohibitively expensive [ 9]. We then devise a strategy that captures higher-order\ninformation while still using the efficient low-dimensional persistent homology. In more detail, we\nfirst \u201clift\u201d the graph into multiple clique graphs describing the connections of higher-order structures,\nand then apply persistent homology up to dimension 1 to each \u201chigher-order\u201d graph. This strategy\nallows us to use the very efficient (with complexity linear in the number of nodes) persistent homology\nof dimension 1, while still extracting information from higher-order structures.\nThecontributions of this paper can be summarized as follows:\n\u2022We introduce a topological layer named CliquePH that can be incorporated into any GNN and\nthat provides information related to persistent homology on higher-order graph structures.\n\u2022We highlight theoretical results that provide a strong motivation for the use of higher-order\npersistent homology in graph learning tasks.\n\u2022We evaluate our method applied to three popular GNNs on standard graph classification bench-\nmarks, and show performance improvements of up to 31%.\n2 Preliminaries\nWe denote a graph with G= (V, E), where Vis the set of nodes, with |V|=n, and Eis the set of\nedges. We consider attributed graphs, in which every node has a set of dfeatures. The features for\neach node in a graph are contained in a matrix X\u2208Rn\u00d7d. We use Nvto indicate the neighbors of\nnodev(i.e., the nodes connected to vby an edge).\nClique Graphs. Ak-vertex clique in a graph G= (V, E)is a subset of vertices such that every two\ndistinct vertices in the clique have an edge between them, i.e., k={v1, v2, . . . , v k}, with vi\u2208V, is\nak-vertex clique if (vi, vj)\u2208E\u2200i\u0338=j;i, j= 1, . . . , k . Let k1, k2, . . . k zbe the r-vertex cliques\nin the graph G, then the r-vertex clique graph ofGis a graph K(r)(G) = ( VK(r), EK(r)), with\nVK(r)={k1, k2, . . . k z}, and (ki, kj)\u2208EK(r)if and only if i\u0338=jandki\u2229kj\u0338=\u2205. In other words,\nther-vertex clique graph K(r)(G)summarizes the structure of the r-vertex cliques in the graph G.\nTable 1: Runtime comparison for one epoch of\ntraining of exact higher-order persistent homology\nup to dimension 2 (denoted with \u201cFullPH\u201d), and\nour proposed method CliquePH. Results are for the\nsame architecture and hyperparameters trained for\none epoch on MNIST dataset. All results are run\non NVIDIA A100.\nMethod Time (h:m:s/epoch) # Iteration/sec Speedup\nFullPH 9:24:14 0.05 it/s 1\nCliquePH 00:02:57 2.42 it/s \u223c191Persistent Homology. Persistent homol-\nogy [10,11], a technique for capturing\ntopological features of data, constitutes the\nfoundation of our proposed method. In contrast\nto other graph learning techniques, persistent\nhomology permits us to quantify topological\nfeatures at different scales . The crucial element\nenabling multi-scale calculations is a filtration ,\ni.e., a consistent ordering of the vertices and\nedges of a graph, defined using a function\nf:G\u2192R. Filtrations can be either obtained\nfrom static descriptor functions that measure\ncertain aspects of a graph, such as its number of\nconnected components or its curvature [ 12], but recent work also demonstrates that it is possible to\nlearn filtrations in an end-to-end fashion [ 9,13]. Regardless of the origin of a filtration, the crucial\ninsight is that, since a graph is a discrete object, its topology can only change at a finite number of\ncritical thresholds t1, . . . , t k. Thus, any function f:G\u2192Rthat gives rise to a filtration enables\nus to turn a graph into a sequence of nested subgraphs G1\u2286G2\u2286. . .\u2286Gk=G, where each\nGiis defined based on the critical points of fviaGi:={x\u2208V\u222aE|f(x)\u2264ti}. Intuitively,\neach subgraph consists of all elements of the graph Gwhose function value is less than or equal\nto the critical threshold. Alongside this sequence of subgraphs, we now calculate the homology\ngroups . These groups capture the topological features\u2014connected components, cycles, voids, and\nhigher-order features\u2014of each subgraph. Persistent homology represents all topological features\narising from the filtration in a set of persistence diagrams . Each persistence diagram consists of\na multiset of intervals of the form (ci, di)withci, di\u2208R\u222a {\u221e} , with cidenoting the \u201ccreation\ntime\u201d (in terms of the critical thresholds ) and didenoting the \u201cdestruction time.\u201d Features with\n2CliquePH: Higher-Order Information for Graph Neural Networks\nN-Vertex \nClique Graph \n3-Vertex \nClique Graph \nOriginal \nGraph \nLifting \nMessage \nPassing Information \nCombination New Node \nEmbeddings \nOriginal \nGraph \n Persistent     \nHomology \nNode \nEmbeddings \u2026 \n\u2026 \u2026 \u2026 \n\u2026 \u2026 \u2026 \nFigure 1: Overview of our method. CliquePH is composed of four stages. (1) First we \u201clift\u201d the\noriginal graph by extracting its clique graphs . (2) We construct node embeddings using a graph\nneural network. (3) We use learnable functions to generate filtration values for all nodes and edges in\nall the lifted graphs. We then perform persistent homology (up to dimension 1) on all lifted graphs.\n(4) We incorporate the information from persistent homology and message-passing into a single\nrepresentation. The whole model is trained end-to-end.\nan infinite destruction time are called essential ; these denote topological features that appear at\nall scales. Prior work in leveraging topological features from graphs ignored clique information\ndue to computational issues: while it is possible to work with higher-order topological features\narising from cliques, a naive application of the persistent homology scales with O(nd), where n\ndenotes the number of vertices in a graph, and ddenotes the order of cliques (we refer to this as exact\nhigher order persistent homology). Our proposed method instead liftscliques into individual skeleton\ngraphs, and then performs the efficient persistent homology up to dimension 1. In Table 1 we show a\ntime comparison for one epoch of training, of our proposed method CliquePH with a lifting of the\noriginal graph up to 3-vertex clique graphs, and a modified version making use of exact persistent\nhomology up to dimension 2 (dimension 2 corresponds to triangles, i.e., 3-vertex cliques \u2013 hence\nthe comparison). The exact persistent homology computations take about 190 times more than our\nmethod CliquePH2.\nGraph Neural Networks. GNNs are deep learning models that operate on graphs. Most popular\nGNNs adhere to the message-passing framework [ 5]. Let H(\u2113)\u2208Rn\u00d7d\u2032be the matrix of node\nrepresentations at iteration \u2113(withH(0)=X, andH(i)\nvused to indicate the representation of node v\nat iteration i). Given a permutation-invariant aggregation function \u03a6, and a learnable update function\n\u03a8\u03b8(usually a neural network), a message passing layer updates node representations via\nm(\u2113)\nv= \u03a6({{H(\u2113)\nu|u\u2208 Nv}}) (1)\nH(\u2113+1)\nv = \u03a8 \u03b8(H(\u2113)\nv,m(\u2113)\nv) (2)\nwhere {{\u00b7}} indicates a multiset, and mvis the message received by node vfrom its neighbours.\nAfter Lmessage-passing layers, the node embeddings H(L)are used to perform a given task (e.g.,\n2The output of CliquePH is not the same as performing exact higher-order persistent homology; Table 1 is\nmeant to show the impracticality of performing the latter.\n3CliquePH: Higher-Order Information for Graph Neural Networks\nthey are fed to a classifier network), and the whole system is trained end-to-end. For graph-level\ntasks, it is common to adopt a permutation invariant readout function that aggregates all the node\nrepresentations into a unique graph embedding (e.g., through averaging).\nTopological Graph Neural Networks. TOGL [ 9] is a layer that can be added in-between message\npassing iterations in GNNs to incorporate global topological information extracted by persistent\nhomology up to dimension 1. Given the node embedding at layer \u2113, TOGL uses a learnable filtration\nfunction3f\u03b8:H(\u2113)\nv\u2192R,\u2200v\u2208Vto assign a filtration value to each node. Filtration values\nfor each edge are then obtained by taking the maximum filtration value between the two nodes\nconnected by the edge. Successively, persistent homology up to dimension 1 is computed, returning a\npersistence diagram for dimension 0and one for dimension 1:D(0),D(1). The persistence diagrams\nare then embedded using DeepSet [ 14] networks S(0)\n\u03b8, S(1)\n\u03b8, encoding each diagram into a vector:\nA0=S(0)\n\u03b8(D(0)),A1=S(1)\n\u03b8(D(1)). The information from the diagrams is then incorporated into\nnode embeddings (for dimension 0), and to the graph embedding (for dimension 1). All the operations\nin TOGL are differentiable, and the network is learned end-to-end.\n3 Our Method\nOur proposed method aims at introducing information from persistent homology on higher-order\nstructures into GNNs. While our method is theoretically different from performing persistent\nhomology up to higher dimensions, we show it brings substantial performance improvements. Our\nmethod can be decomposed into four components (as shown in Figure 1), which we present below.\n3.1 Graph Lifting\nStarting from the original graph G, we perform a \u201clifting\u201c operation to extract its clique graphs:\nlift(G) ={G, K(3)(G), K(4)(G), . . . , K(r)(G)}\nthe maximum size rfor the clique graphs can be defined arbitrarily according to the data at hand (e.g.,\nby taking the value that leads to a clique graph with a number of nodes that is least 5%the number of\nnodes in the original graph). We start from the 3-vertex clique graph, as the information about nodes\nand edges is already present in the original graphs, while the higher dimensions are the ones in which\nGNNs struggle. This step can be done just once, as pre-processing on all graphs in the dataset.\nIn practice, we use r= 5in our experiments as available datasets tend to have a very small number of\ncliques larger than that. When dealing with real-world graphs, several works have proposed methods\nfor enumerating all cliques in a graph that in practice have a fast runtime [ 15\u201318]. In all datasets\nconsidered in our experiments, we have been able to perform this pre-processing step for each dataset,\nup to r= 5, within 1 hour. For instance, complete preprocessing time on MNIST dataset for up to\n3-cliques, up to 4-cliques, and up to 5-cliques takes 36.4 min, 58.1 min, and 60.7 min consecutively.\n3.2 Message-Passing\nIn this step, we obtain node embeddings for the original graph G. In more detail, we perform Lm\nsteps of message passing on G. Any GNN could be used for this step, and the output is a matrix of\nnode embeddings H(Lm).\n3.3 Learnable Persistent Homology\nOnce we have obtained embeddings for the nodes in G, we incorporate information from persistent\nhomology for all graphs in lift(G). This step is composed of three sub-components presented below.\n1 - Filtration values for nodes and edges. To perform persistent homology, we first need to\ncompute filtration values for dimension 0(nodes), which we indicate with F(\u00b7)\n0, and dimension 1\n(edges), which we indicate with F(\u00b7)\n1, in all graphs in the lifted set. We first describe how we obtain\nfiltration values for the original graph G, and then for the clique graphs K(\u00b7)(G).\n3TOGL actually uses a set of filtration functions, but we introduce the method with only one for clarity.\n4CliquePH: Higher-Order Information for Graph Neural Networks\nOriginal graph G.For dimension 0, we use a two-layer MLP f0to map the embedding of each node\nintodffiltration values: F(G)\n0(v) =f0(H(Lm)\nv). For dimension 1, we first obtain a representation\nreifor each edge ei= (u, v)by concatenating the representations of the nodes it connects: rei=\ncat(H(Lm)\nv(u),H(Lm)\nv(v)). We then obtain the filtration value for the edge using a 2-layer MLP\nfunction f1as follows:\nF(G)\n1(ei) =max({F(G)\n0(u),F(G)\n0(v)}) +f1(rei) (3)\nClique graphs K(\u00b7)(G).For the lifted graphs, we obtain filtration values in a similar way. Let kibe\na node in a clique graph, related to a clique between the nodes {u1, u2, . . . , u j}, we first obtain the\nembedding for kiaseki=cat(H(Lm)\nv(u1), . . . ,H(Lm)\nv(uj)). We then compute the filtration value\nas\nFK(\u00b7)\n0(ki) = max( {F(G)\n0(u1), . . . ,F(G)\n0(uj)}) +fk(\u00b7)\n0(eki) (4)\nwhere fk(\u00b7)\n0is a two-layer MLP (a separate one for each clique graph). We then obtain the filtration\nvalues for dimension 1 following an analogous procedure to the one used for dimension 1on the\noriginal graph G, using a separate learnable function for each clique graph.\n2 - Persistent homology. We now have filtration values for all the nodes and edges in all the\ngraphs in the lifted set. We can then perform the efficient persistent homology up to dimen-\nsion 1 to all the graphs, obtaining two persistence diagrams for each graph in the lifted set:\n{(D(0)\nG, D(1)\nG),(D(0)\nK(3), D(1)\nK(3)), . . . , (D(0)\nK(r), D(1)\nK(r))}. The diagrams for dimension 0have a number\nof entries equal to the number of nodes in the respective graph, while the diagrams for dimension 1\nhave a number of entries equal to the number of independent cycles in the respective graph. In other\nwords, these diagrams are summarizing the evolution (according to the learned filtration values) of\nthe connected components and cycles in each graph.\n3 - Embed persistence diagrams. The diagrams for dimension 0of the original graph Gare passed\nto a permutation equivariant (set-to-set) DeepSet network (a separate network for each graph in the\nlifted set is used), which returns a vector for each node. These vectors are stored in a matrix E(0)\nG. The\ndiagrams for dimension 0for all clique graphs are passed to a permutation invariant DeepSet network\n(a separate network for each clique graph is used), which returns a single vector E(0)\nK(\u00b7)for each\nclique graph K(\u00b7)(G). The diagrams of dimension 1for all graphs are passed through a permutation\ninvariant (set-to-vector) DeepSet network (a separate network for each graph in the lifted set is used)\nto obtain a unique embedding, which returns a single vector for each graph: E(1)\nG,E(1)\nK(3), . . . ,E(1)\nK(r).\n3.4 Information Combination\nWe summarize the information from the persistent homology of each clique graph into a unique\nvector for each dimension. In more detail, we concatenate the vectors E(0)\nK(\u00b7)andE(1)\nK(\u00b7), and pass them\nthrough a two-layer MLP to obtain a vector EK(j)for each dimension j\u22653. Finally, we combine\nall the information from the persistent homology computations with the embeddings obtained in the\nmessage passing step. This is done by adding the embeddings together:\nH(Lm)+E(0)\nG+SCATTER (E(1)\nG) + SCATTER (EK(3)) +\u00b7\u00b7\u00b7+SCATTER (EK(r)) (5)\nwhere the function SCATTER indicates that the embeddings of each clique are added to all the node\nembeddings of the nodes that form that clique. More formally, given a 3-vertex clique k= (u, v, z )\nwith embedding ek, the operation H+SCATTER (ek)is doing the following\nHv=Hv+ek;Hu=Hu+ek;Hz=Hz+ek (6)\nand leaving the rows of Hrelated to other nodes unaltered. Additional round(s) of message passing\nonGcan then be performed before passing the embeddings to the final classifier. In the case of a\ngraph-level task, a standard graph pooling method (e.g., averaging) is used.\nFinally, we mention that CliquePH does not affect the permutation equivariance of GNNs, and is\nfully differentiable (hence allowing end-to-end training). More details can be found in Appendix A.\nWe futher provide a comparison between CliquePH and TOGL in Appendix B.\n5CliquePH: Higher-Order Information for Graph Neural Networks\n3.5 Limitations\nThere are three main limitations to our method. Firstly, for very large graphs, it may become difficult\ncomputationally to use values of rlarger than 3. While our experiments show that r= 3is already\nenough to provide benefits, this can prevent from exploiting the full potential of CliquePH. Secondly,\nour method avoids the computational complexity of the exact higher order persistent homology\nprocedure, but it is not equivalent to it, and hence does not have the same theoretical guarantees\nin terms of expressiveness. Finally, in tasks in which higher-order structures are not informative,\nCliquePH may not be useful, or may even promote overfitting.\n4 Theoretical Motivation\nThe expressivity of GNNs is measured by considering their ability of distinguishing non-isomorphic\ngraphs. This is done by relating GNNs to the Weisfeiler-Lehman algorithm (or WL algorithm) [ 19].\nWe refer to Morris et al. [20] for a complete introduction to WL, its variants, and its use in machine\nlearning. The WL algorithm cannot completely solve the graph isomorphism problem (in fact, there\nis no known polynomial time solution), but it can still distinguish a large number of graphs [ 21].\nModifications to the WL algorithm have been made to increase its distinguishing power. In particular,\nsome \u201chigher-order\u201d variants have been proposed. These variants form a hierarchy of algorithms:\n1-WL, 2-WL, ..., k-WL, each more expressive than the previous, i.e., i-WL can distinguish more\ngraphs than (i\u22121)-WL. It has been shown that standard message-passing GNNs are strictly less\npowerful than the 1-WL algorithm [ 6]. Two recent results, which we provide below, present a strong\nmotivation for using persistent homology to enhance the expressivity of GNNs.\nTheorem 4.1 (Persistent Homology is at least as expressive as the 1-WL - Thm. 2 in Horn et al. [9]).\nPersistent homology is at least as expressive as the 1-WL algorithm, i.e. if the 1-WL label sequences\nfor two graphs GandG\u2032diverge, there exists an injective filtration fsuch that the corresponding\n0-dimensional persistence diagrams D(0)\nGandD(0)\nG\u2032are not equal.\nTheorem 4.2 (k-dimensional Persistent Homology is as expressive as k-WL - Thm. 3 in Ballester and\nRieck [22]).Given k-WL colorings of two graphs GandG\u2032that are different, there exists a filtration\nofGandG\u2032such that the corresponding persistence diagrams in dimension k\u22121or dimension k\nare different.\nTheorem 4.1 entails that first-order persistent homology information can already make any GNN\nstrictly more expressive than the 1-WL. Theorem 4.2 is a weaker version of this result; it shows that\nhigher-order persistent homology can in fact match the expressiveness of higher-order variants of the\nWL algorithm\u2014without requiring an enumeration of all k-tuples.\nAs CliquePH makes use of first-order persistent homology information, it is straightforward to see\nfrom Theorem 4.1 that a GNN empowered with CliquePH is strictly more powerful than the 1-WL\nalgorithm. While we cannot use Theorem 4.2 to theoretically prove that a version of CliquePH with\ncliques up to order kleads to the expressiveness of k-WL (as CliquePH does not perform exact\nhigher-order persistent homology), we provide below empirical evidence of the practical increase in\nexpressiveness provided by CliquePH.\nEmpirical Validation of Expressiveness. We consider 6 datasets of strongly regular graphs [ 22]\n(i.e., graphs in which all nodes have the same degree), which are known to be hard to distinguish with\nthe 1-WL test. We then randomly initialize a GCN [ 23], a GCN with TOGL [ 9], and a GCN with\nCliquePH (with a lifting up to 3-vertex clique graphs), and use them to produce graph embeddings.\nAll models are set to have the same number of layers (set to 4), and the internal same dimensionalities\n(set to 128). We then count the number of unique representations (a network that can distinguish all\ngraphs will provide a different representation for each graph). We show results averaged of 10 random\nseeds in Table 3. CliquePH increases the capability of distinguishing regular graphs, improving\nalso over TOGL, which has an expressivity strictly higher than the 1-WL, empirically showing that\npersistent homology information from clique graphs provides important performance benefits.\n5 Experiments\nTo demonstrate the benefits provided by our method in practical scenarios, we follow prior liter-\nature [ 9] and apply our method to three GNNs among the most used by practitioners: GCN [ 23],\n6CliquePH: Higher-Order Information for Graph Neural Networks\nTable 2: We report the test accuracy (following standard practice, we report ROC-AUC on OGBG-\nmolhiv) obtained on standard benchmark datasets when only structural information is considered\n(i.e., no node attributes are used). We compare standard popular GNNs ( GCN ,GIN,GAT ) alone,\nand with the addition of TOGL and our proposed method, CliquePH. The highest performing model\nof each dataset is highlighted with a grey background and the highest performing variant for each\narchitecture in bold . The last column on the right shows the average performance improvement\nprovided by TOGL and CliquePH with respect to the base GNN.\nGraph classification\nMETHOD DD ENZYMES MNIST PROTEINS IMDB-B REDDIT-5K OGBG-molhiv Avg.\u2191\nGCN-4 68.0\u00b13.6 22.0\u00b13.3 76.2\u00b10.5 68.8\u00b12.8 65.1\u00b15.1 54.1\u00b10.8 66.4\u00b11.8 \u2212\nGCN-3-TOGL-1 75.1\u00b12.1 30.3\u00b16.584.8\u00b10.473.8\u00b14.3 72.5\u00b13.0 54.3\u00b11.4 69.4\u00b11.8 5.7\nGCN-3-CliquePH-1 76.1\u00b11.334.3\u00b14.783.6\u00b10.7 72.6\u00b11.574.7\u00b11.257.7\u00b10.5 69.6\u00b12.0 6.9\nGIN-4 75.6\u00b12.8 21.3\u00b16.5 83.4\u00b10.974.6\u00b13.171.1\u00b13.7 50.2\u00b12.4 68.7\u00b10.9 \u2212\nGIN-3-TOGL-1 76.2\u00b12.4 23.7\u00b16.9 84.4\u00b11.1 73.9\u00b14.974.4\u00b11.6 52.7\u00b10.9 65.1\u00b16.2 0.8\nGIN-3-CliquePH-1 77.6\u00b11.724.7\u00b13.188.9\u00b10.672.9\u00b10.9 72.4\u00b12.855.0\u00b11.4 69.6\u00b11.8 2.3\nGAT-4 63.3\u00b13.7 21.7\u00b12.9 63.2\u00b110.4 67.5\u00b12.6 63.6\u00b13.9 48.1\u00b11.1 51.8\u00b15.6 \u2212\nGAT-3-TOGL-1 75.7\u00b12.1 23.5\u00b16.1 77.2\u00b110.5 72.4\u00b14.6 71.2\u00b12.4 52.7\u00b11.4 68.6\u00b11.7 8.8\nGAT-3-CliquePH-1 75.2\u00b11.927.5\u00b14.083.8\u00b10.972.6\u00b10.871.5\u00b12.653.7\u00b11.0 69.9\u00b11.7 10.7\nTable 3: Number of strongly regular graphs distinguishable by untrained models. All models have\nthe same number of layers and dimensionality. CliquePH can enhance the ability of networks to\ndistinguish \u201chard\u201d non-isomorphic graphs.\nDataset Cubic12 Cubic14 Cubic16 Quartic10 Quartic11 Quartic12\n# Graphs 85 509 4060 59 265 1544\nGCN 78\u00b10.0458\u00b10.0 3604\u00b10.0 56\u00b10.0255\u00b10.0 1427\u00b10.0\nGCN-TOGL 85\u00b10.0507.2\u00b11.44049.4\u00b13.759\u00b10.0264\u00b10.01540.1\u00b12.1\nGCN-CliquePH 85\u00b10.0509\u00b10.04057.1\u00b10.759\u00b10.0265\u00b10.01542.3\u00b12.21\nGIN [ 6], and GAT [ 24]. In more detail, we report results for each GNN by itself, for the GNN\nwith the addition of TOGL [ 9], and for the GNN with the addition of our method CliquePH. The\ncomparison with TOGL is particularly useful, as TOGL is a topological method that can be used on\ntop of any GNN (like CliquePH), and it makes use of information coming from persistent homology\nup to dimension one. This then allows us to analyze the benefit provided by the persistent homology\ninformation obtained from the higher-order clique graphs that is present in our method. Finally, we\nprovide an ablation study to observe the effects of the position of the CliquePH layer in the model\narchitecture.\nExperimental Setup and Datasets. In order to ensure a fair performance comparison, we take\nthe optimal architectures and hyperparameters for the GNNs and for TOGL from prior work [ 9].\nFor CliquePH we use the same architecture for the base GNN of TOGL, and we tune only the\nlearning rate and the maximum dimension of the lifted clique graphs (between 3, 4, and 5) using\na random search approach (using values in the interval: (10\u22126,10\u22122)) by comparing the loss on\nthe validation set. Furthermore, the reported results are obtained by averaging over ten independent\nruns with varying random seeds. The code to replicate our experiments will be publicly released\nupon acceptance. In Tables 2 and 4, the notation \u201cGCN-4\u201d indicates a GCN architecture with 4\nmessage passing layers, while \u201cGCN-3-CliquePH-1\u201d indicates a GCN architecture with 3 message\npassing layers, and 1 CliquePH layer. We use the analogous notation for the other GNNs, and for\nnetworks with TOGL. All experimental results were conducted on a single NVIDIA A100 GPU, and\n20 allocated AMD EPYC 7742 64-Core CPUs, with the Adam optimizer, 4 workers, and a batch size\nof 32 for small datasets and 128 for large datasets. We use standard graph classification benchmark\ndatasets: DD [ 25], ENZYMES [ 26], MNIST [ 27], PROTEINS [ 28], IMDB-B [ 29], Reddit-5k [ 30],\nand OGBG-Molhiv [ 31]. We perform two types of experiments, the first, which we refer to as\nstructure-based , does not consider the information coming from node features; the second instead\nfollows the standard procedure, and makes use of the available node attributes. To ensure the same\ntraining, evaluation, and data setup, we use the code provided by the authors of TOGL [ 9], and we\nreport results for TOGL and base models from their paper.\nPerformance on Structure-based Experiments. Following prior work [ 9], to perform structure-\nbased experiments, we assign random node labels and input features. This approach allows us to\n7CliquePH: Higher-Order Information for Graph Neural Networks\nTable 4: We report the test accuracy (following standard practice, we report ROC-AUC on OGBG-\nmolhiv) obtained on standard graph classification benchmark datasets (considering node features).\nWe compare standard popular GNNs ( GCN ,GIN,GAT ) alone, and with the addition of TOGL and\nour proposed method, CliquePH. The highest performing model of each dataset is highlighted with\ngrey background and the highest performing variant for each architecture in bold . The last column\non the right shows the average performance improvement provided by TOGL and CliquePH with\nrespect to the base GNN.\nGraph classification\nMETHOD DD ENZYMES MNIST PROTEINS-FULL IMDB-B REDDIT-5K OGBG-molhiv Avg.\u2191\nGCN-4 72.8\u00b14.158.3\u00b16.190.0\u00b10.3 76.1\u00b12.4 68.6\u00b14.9 53.7\u00b11.7 71.9\u00b11.1 \u2212\nGCN-3-TOGL-1 73.2\u00b14.7 53.0\u00b19.2 95.5\u00b10.2 76.0\u00b13.9 72.8\u00b12.3 54.5\u00b11.2 72.6\u00b12.0 0.9\nGCN-3-CliquePH-1 75.0\u00b11.8 55.5\u00b14.295.9\u00b10.2 82.0\u00b10.5 71.1\u00b13.255.6\u00b11.2 75.2\u00b12.3 2.7\nGIN-4 70.8\u00b13.8 50.0\u00b112.3 96.1\u00b10.3 72.3\u00b13.3 72.8\u00b12.5 53.3\u00b11.6 69.8\u00b11.1 \u2212\nGIN-3-TOGL-1 75.2\u00b14.2 43.8\u00b17.9 96.1\u00b10.1 73.6\u00b14.8 74.2\u00b14.2 53.7\u00b11.1 67.3\u00b14.0 \u22120.17\nGIN-3-CliquePH-1 76.1\u00b12.654.8\u00b18.496.3\u00b10.1 81.4\u00b11.1 73.7\u00b12.155.1\u00b11.7 72.9\u00b11.6 3.6\nGAT-4 71.1\u00b13.1 26.8\u00b14.1 94.1\u00b10.3 71.3\u00b15.4 73.2\u00b14.1 51.4\u00b11.4 74.0\u00b12.1 \u2212\nGAT-3-TOGL-1 73.7\u00b12.9 51.5\u00b17.3 95.9\u00b10.3 75.2\u00b13.9 70.8\u00b18.0 52.5\u00b10.9 74.7\u00b11.8 4.6\nGAT-3-CliquePH-1 75.4\u00b13.157.5\u00b19.296.8\u00b10.2 80.8\u00b11.4 70.7\u00b13.053.8\u00b11.5 75.7\u00b11.1 7.0\nassess the practical effectiveness of our method in comparison to classical message-passing GNNs\nfor extracting underlying structural and topological information. Table 2 shows the results for the\nstructure-based graph classification task. Firstly, we observe that CliquePH and TOGL significantly\nimprove the performance of standard GNNs. CliquePh in particular leads to the highest average\nimprovement across datasets (last column on the right), with an improvement of 6.9%with respect to\nthe base GCN model, 2.3%with respect to the base GIN, and 10.7%with respect to the base GAT.\nLooking at individual datasets, we then notice that a CliquePH model is the best performing model\non 6 out of 7 datasets. Furthermore, it can be seen that in most cases, the results for CliquePH present\na lower variance across runs, showing an increased stability with respect to TOGL.\nPerformance on Benchmark Datasets. We now investigate the effect of CliquePH layer on\nstandard benchmark datasets making use of node attributes. We use the same datasets used for the\nstructure-based experiments, except for MNIST, which does not have node attributes. Results are\nshown in Table 4. We notice that CliquePH can significantly improve the performance of GNNs\neven in the presence of rich feature information, while this is not the case for TOGL. In fact, the\naverage performance improvement across datasets for CliquePH is always positive (while for TOGL\nit is negative for the GIN model) and in 90% of the cases, the addition of CliquePH improves the\nperformance of the baseline GNN, with improvements of up to 31%. As above, we notice that\nin the majority of cases, the results for CliquePH have a lower variance than the ones for TOGL,\nhighlighting the increased stability of our method.\nAblation Study. We investigate the effects of the position of the CliquePH layer inside a GNN\narchitecture, and the maximum dimension rof the lifted clique graphs. We provide below an overview\nof the results and we refer to Appendix C for the full results. We start by considering a 4-layer GNN,\nwith 3 message passing layers and 1 CliquePH layer, and we change the position of the latter. We\nconsider the structure-based scenario to exclude the influence of node features. Results are shown in\nFigure 2 (Left) for the DD dataset (other datasets are in the Appendix). We notice that the effects\nare highly dataset dependent: for DD using CliquePH as the first layer is more effective, while, e.g.,\nfor Enzymes the best performance is when the layer is 3rd, similar to PROTEINS. We then consider\na GNN with a number of layers that vary from 1 to 5 and with CliquePH as the last layer. Results\nare shown in Figure 2 (Center). Again we notice that the performance is highly dependent on the\ndataset, as for DD there is little change in performance for different numbers of layers, while, e.g.,\nfor ENZYMES it is important to have 2 to 4 layers prior to CliquePH. On IMDB-B we notice how\nthe addition of CliquePH significantly increases performance even for a 1-layer architecture. Finally,\nwe notice how in allcases the addition of CliquePH always leads to performance improvements.\nFinally, we apply CliquePH with a maximum dimension of the lifted clique graphs varying from 3 to\n5 on the DD dataset. Results are shown in Figure 2 (Right). We observe that tuning this parameter is\nimportant to avoid overfitting. These results suggest that tuning the position of CliquePH, and the\nvalue of r, is important to obtain the best results, and that CliquePH always leads to performance\nimprovements when no node features are available.\n8CliquePH: Higher-Order Information for Graph Neural Networks\n1 2 3 472747678\nPosition of CliquePH layerTest ACC (in %)Performance for DD-STRUCT\nGCN-CliquePH\n1 2 3 4 560657075\nNumber of LayersTest ACC (in %)Performance for DD-STRUCT\nGCN-CliquePH\nGCN\n3 4 57374757677\nMaximum dimension of the lifted clique graphsTest ACC (in %)Performance for DD-STRUCT\nGCN-CliquePH\nFigure 2: Ablation Results for different architectures. (Left) Test accuracy for the structure-based\nexperiments while changing the position of CliquePH layer. (Center) Test accuracy for the structure-\nbased experiments while increasing the total number of layers in the model. Error bars show standard\ndeviation over 10 runs. (Right) Test accuracy for the structure-based experiments while changing the\nmaximum dimension of the lifted clique graphs of CliquePH.\n6 Related Work\nGraph neural networks and topological data analysis have become very popular and prolific research\nfields. In this section, we review works at the intersection of graph machine learning and topology, and\nwe refer the interested reader to comprehensive surveys for the latest advancements in graph neural\nnetworks [ 1,2] and topological data analysis [ 32\u201334]. In the context of graph neural networks\naugmented with topological information , Bouritsas et al. [7]show that adding information about\nthe number of higher-order structures that a node belongs to can improve the theoretical expressivity\nand practical performance of a model. Similarly, Immonen et al. [35] use a topological descriptor\nbased on filtrations as additional node features for GNNs. Zhao et al. [36] use persistent homology\nto weight messages in GNNs in the message passing procedure. He et al. [37] use sheaf theory to\nproduce node position encodings that improve the performance of graph neural networks on several\ntasks. Song et al. [38] propose a feature augmentation method by obtaining topology embedding\nof nodes through Node2vec. This introduces topological structure information into an end-to-end\nmodel to improve node representation learning. In addition, several works discuss message-passing\non topological spaces . Bodnar et al. [39] introduce a model that performs message passing over\nsimplicial complexes. Bodnar et al. [40] successively extended this model to cellular complexes.\nLater work [ 41] has also introduced models that operate on cellular sheaves. While these works are\nrelated to CliquePH, there are some important differences. Firstly, these methods require a specific\nmessage-passing model, while our method CliquePH can be applied in conjunction with any existing\nGNN. Secondly, CliquePH makes use of Persistent Homology, which provides access to information\n(e.g., the exact number of cliques and cycles in the original graph and in each lifted graph) that cannot\nbe guaranteed to be accessible to methods that perform message passing on topological structures.\nFinally, several works also focus on the explicit connections between topological data analysis and\ngraph learning . O\u2019Bray et al. [42] use graph filtrations to compute efficient graph representations\nthat can outperform those obtained from GNNs. Southern et al. [43] leverage topological tools to\nrobustly evaluate generative graph models. Coupette et al. [44] introduces the concept of curvature\nfor hypergraphs and show that they can be used to effectively perform clustering. Carriere et al.\n[45] use extended persistent diagrams to produce graph embeddings. Srambical and Rieck [46] use\ngraph filtrations to classify dynamic graphs. Ye et al. [47] introduces the use of extended persistent\nhomology, a variant of persistent homology, into graph-related tasks.\n7 Conclusions\nGNNs are powerful models operating on graph structures, but they lack the ability of extracting\ninformation about higher-order structures. In this paper, we introduce a novel learnable topological\nlayer that provides GNNs with information from persistent homology on higher-order structures\npresent in a graph. Our experimental results show that our method leads to performance improvements\nof up to 31% on standard benchmarks for the tasks of graph classification.\n9CliquePH: Higher-Order Information for Graph Neural Networks\nReferences\n[1]Lilapati Waikhom and Ripon Patgiri. A survey of graph neural networks in various learning\nparadigms: methods, applications, and challenges. Artificial Intelligence Review , 56(7):6295\u2013\n6364, 2023. 1, 9\n[2]Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\ncomprehensive survey on graph neural networks. IEEE transactions on neural networks and\nlearning systems , 32(1):4\u201324, 2020. 1, 9\n[3]Re"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Summary of \"LT3SD: Latent Trees for 3D Scene Diffusion\"\n\n#### 1. Abstract\nLT3SD is a novel latent diffusion model designed for large-scale 3D scene generation. It leverages a latent tree-based representation to encode both low-frequency geometry and high-frequency detail in a hierarchical manner. This enables high-quality and scalable 3D scene generation using a patch-by-patch and coarse-to-fine generation process. The model outperforms existing diffusion-based 3D scene generation methods, achieving significant improvements in FID scores and demonstrating its efficacy through extensive experiments.\n\n#### 2. Introduction\nThe paper addresses the growing demand for high-quality 3D digital content creation, which is especially prevalent in fields like film, video games, and mixed reality. Traditional methods involve high costs and skilled labor, which motivates the development of automated generative models. While recent advancements in diffusion models have shown promise, they are often limited to generating individual 3D objects and face challenges when applied to more complex and expansive 3D scenes due to the unstructured and variable nature of such environments. Previous attempts have often been limited to low-resolution or spatially constrained scenes. LT3SD introduces an innovative hierarchical approach using latent trees to handle the increased complexity and extends diffusion models to support seamless infinite scene generation.\n\n#### 3. Related Work\n- **Neural 3D Scene Representations:** Previous works have explored various neural representations such as implicit functions, feature volumes, and triplanes to encode 3D shapes compactly. These methods often struggle with large-scale scenes due to the highly unstructured nature of 3D environments.\n- **3D Diffusion Models for Shape Generation:** Existing models have shown success in generating 3D shapes but face challenges scaling to complex scenes. These methods typically assume shapes reside in canonical spaces, making it difficult to extend to variable-sized 3D scenes.\n- **3D Scene Generation:** Recent methods have used 2D image generative models and diffusion methods for bounded 3D scenes. However, these methods are limited in terms of generated scene size, geometry detail, and practical applicability for large-scale unbounded 3D scene generation.\n\n#### 4. Method\n##### Latent Tree Representation:\n- **Construction:**\n  1. **Transformation into a Latent Tree:** The method involves progressively decomposing a 3D scene into a hierarchy wherein each level encodes the scene's geometry (as truncated unsigned distance field, TUDF) and its details (latent feature volume).\n  2. **Encoding Process:** Using 3D CNN-based encoders, each higher-resolution TUDF patch is decomposed into lower-resolution TUDF and latent feature patches. This is followed by reconstruction using decoders, ensuring accurate hierarchical representation.\n  3. **Training Loss:** The encoders and decoders are trained to minimize the reconstruction error (\u21132 loss) between the decomposed and original TUDF patches.\n\n##### Latent Scene Diffusion:\n- **Diffusion Model:** The method adopts a denoising diffusion probabilistic model (DDPM) in a patch-wise manner, learning to denoise and predict latent features conditioned on local geometry patches. The approach builds on 3D UNet backbones using random cropping and Gaussian noise addition in training.\n- **Training Objective:** The diffusion model aims to minimize the prediction error of Gaussian noise added to the latent patches during the forward diffusion process, ensuring high-fidelity latent patch reconstruction.\n\n##### Large-Scale Scene Generation:\n- **Inference Process:** Large-scale generation involves reversing the latent tree decomposition process. Starting from a coarse scene structure, the method employs inpainting-based patch-wise synthesis followed by multi-level hierarchical refinement. This ensures smooth transitions between patches and seamless scene integration.\n- **Efficiency:** The method leverages parallel denoising steps with denoising fusion schemes to expedite the generation process while maintaining high quality and coherence.\n\n#### 5. Experimental Setup\n- **Datasets:** The 3D-FRONT dataset, a comprehensive collection of diverse indoor scenes, is used for training. Preprocessing steps include the computation of UDFs and TUDF voxel grids with specific thresholds.\n- **Hardware and Software:** Implementation is done in Pytorch, running on NVIDIA RTX A6000 GPUs.\n- **Evaluation Metrics:** Key metrics include the Fr\u00e9chet Inception Distance (FID) based on rendered images, as well as point cloud similarity metrics like Coverage (COV), Minimum Matching Distance (MMD), and 1-Nearest Neighbor Accuracy (1-NNA).\n- **Baselines:** Comparisons are made against leading 3D diffusion models like PVD, NFD, and BlockFusion.\n\n#### 6. Results and Analysis\n- **Qualitative Analysis:** The generated scenes by LT3SD exhibit superior structural detail and surface quality compared to baselines. LT3SD effectively synthesizes large and complex scenes with plausible object arrangements and fine surface details.\n- **Quantitative Analysis:** LT3SD significantly improves key metrics such as FID, COV, and MMD, demonstrating a robust performance in both object detail fidelity and structural diversity.\n- **Ablation Studies:** Evaluations show that the multi-level latent tree structure combined with a hierarchical latent representation substantially enhances reconstruction accuracy and training efficiency.\n\n#### 7. Discussion\n- **Implications:** LT3SD sets new benchmarks for 3D scene generation by effectively combining hierarchical scene decomposition with latent diffusion, thus enabling practical applications in automated content creation.\n- **Limitations:** While LT3SD achieves substantial improvements, challenges remain in further optimizing generation speed and handling specific anomalous geometries. Understanding these limitations is crucial for future advancements.\n  \n#### 8. Conclusion\nLT3SD is a significant advancement in the domain of 3D scene generation, offering a novel method with latent trees for scalable and high-fidelity scene synthesis. Future research directions include enhancing efficiency, exploring new neural architectures, and adapting the model for various types of 3D environments beyond indoor scenes.\n\nBy following these comprehensive steps, researchers can replicate and build upon the LT3SD framework, driving forward the capabilities of automated 3D content creation."}, {"role": "assistant", "content": "LT3SD: Latent Trees for 3D Scene Diffusion\nQuan Meng Lei Li Matthias Nie\u00dfner Angela Dai\nTechnical University of Munich\nquan-meng.github.io/projects/lt3sd\nFigure 1. We introduce LT3SD, a novel latent 3D scene diffusion approach enabling high-fidelity generation of infinite 3D environments.\nWe train LT3SD on a latent tree-based 3D scene representation, encoding both lower-frequency geometry and higher-frequency detail, and\nsynthesize infinite scenes in a patch-by-patch and coarse-to-fine fashion.\nAbstract\nWe present LT3SD, a novel latent diffusion model for\nlarge-scale 3D scene generation. Recent advances in dif-\nfusion models have shown impressive results in 3D object\ngeneration, but are limited in spatial extent and quality\nwhen extended to 3D scenes. To generate complex and di-\nverse 3D scene structures, we introduce a latent tree rep-\nresentation to effectively encode both lower-frequency ge-\nometry and higher-frequency detail in a coarse-to-fine hi-\nerarchy. We can then learn a generative diffusion process\nin this latent 3D scene space, modeling the latent compo-\nnents of a scene at each resolution level. To synthesize\nlarge-scale scenes with varying sizes, we train our diffusion\nmodel on scene patches and synthesize arbitrary-sized out-\nput 3D scenes through shared diffusion generation across\nmultiple scene patches. Through extensive experiments, we\ndemonstrate the efficacy and benefits of LT3SD for large-\nscale, high-quality unconditional 3D scene generation and\nfor probabilistic completion for partial scene observations.1. Introduction\nIn recent years, there has been increasing demand for 3D\ndigital content, both for content creation purposes such as\nfilms, video games, and mixed reality, as well as for recon-\nstruction to enable machines to perceive real environments\nfrom visual inputs. For instance, video games have driven\nefforts to produce realistic virtual 3D environments, requir-\ning significant costs for highly-trained 3D artists to create\na vast array of 3D scene assets. In particular, AAA video\ngame budgets have risen dramatically in the last five years,\nwith games planned to be released this year and next having\ndevelopment budgets of $200 million or more [51].\nRecent advances in diffusion models [21, 33, 40, 42, 44,\n48] have achieved remarkable success in generating com-\nplex, high resolution, realistic images, and videos. In the\n3D domain, however, generative modeling remains a signif-\nicant challenge due to the data and representation complex-\nity and limited training data availability. In addition, signal\nin 3D content is highly unevenly distributed, with many re-arXiv:2409.08215v1  [cs.CV]  12 Sep 2024gions being predominantly free space, whereas most scene\ndetail is located only around certain surface areas. Exist-\ning 3D diffusion models [10, 14, 18, 30, 35, 38, 46, 56]\noverwhelmingly focus on object-level generation. Follow-\ning latent diffusion (LDMs) [44] for 2D images, current\nstate-of-the-art methods focus on training diffusion mod-\nels on learned implicit representations of 3D shapes. How-\never, these methods assume that shapes lie in a canonical\nand bounded space that is amenable to fixed and compact\nrepresentation encodings, such as global latent codes [14],\nhypernetworks [18], and triplanes [20, 46]. While these rep-\nresentations yield impressive results for class-specific dis-\ntributions, such as chairs or cars, they face challenges when\napplied to 3D scene environments characterized by highly\nunstructured geometries, diverse object arrangements, and\nvarying spatial extents. Although early attempts at scene-\nlevel synthesis exist, they predominantly focus on single\nrooms or limited spatial extents [23, 26, 43] with relatively\nlow-resolution [28, 29, 32] 3D scenes.\nTo this end, we propose LT3SD, a new probabilistic\nmodel for generating high-quality, large-scale, arbitrary-\nsized 3D scenes. Key to our approach is a novel latent\ntree representation (Fig. 2-right) that progressively decom-\nposes a 3D scene into a tree hierarchy from fine to coarse\nresolutions. Our latent tree representation efficiently fac-\ntorizes 3D scenes into geometry (lower-frequency) and la-\ntent feature (higher-frequency) volumes, enabling a more\ncompact, effective representation for latent scene diffusion.\nWe then synthesize 3D scenes in a coarse-to-fine, patch-by-\npatch fashion by reversing the latent tree decomposition.\nWe introduce a latent tree-based diffusion model to gen-\nerate the latent feature volume conditioned on the corre-\nsponding geometry volume at the same tree level. This con-\nditional learning of scene details helps to reduce the com-\nplexity of modeling 3D scene distributions. As 3D scenes\ncan vary significantly in size, we design our diffusion model\nto learn at the level of scene patches. This strategy helps to\nshift the learning focus from complex unaligned 3D scenes\nto local structures with higher shared similarity.\nAs a result, our approach can generate large-scale, even\ninfinite 3D scenes by coarse-to-fine construction of the la-\ntent trees in a patch-wise fashion. Starting from the coarsest\nlevel, our method operates patch-wise to generate the over-\nall structure of a 3D scene, which is critical for geometric\ncoherency. Then, in a coarse-to-fine manner, our method\nconditionally generates the latent feature volume, which\nrepresents fine details for each upper level\u2014the final 3D\nscene results from the inverse process of latent tree decom-\nposition. Experiments show that our method significantly\noutperforms existing baselines, improving FID scores by\n70% on this challenging 3D scene generation task.\nIn summary, we present the following contributions:\n\u2022 We introduce a novel latent 3D scene diffusion approach,\nLevel \ud835\udc56 Level \ud835\udc56\u22121\nLatent Tree (Ours) Single Latent / Latent Pyramid\nLevel \ud835\udc56\u22121\nLevel \ud835\udc56\nTUDF \nVolumeLatent \nVolume\nTUDF Volume\n TUDF Volume\nLatent \nVolumeLatent \nVolumeTUDF \nVolumeLatent \nVolume\nTUDF Volume\nDecoder Encoder Decoder EncoderDecoder Encoder\nDecoder EncoderFigure 2. Latent representations for 3D scenes. In contrast to\nencoding a 3D scene into a single latent feature grid or a multi-\nlevel latent pyramid on the left, our latent tree representation on\nthe right is a learned hierarchical decomposition with a series of\ngeometry (lower-frequency) and latent feature (higher-frequency)\nencodings at each resolution level.\nleveraging a latent tree representation to compactly en-\ncode complex 3D scene geometry. This enables high-\nfidelity 3D scene generation in a coarse-to-fine fashion\nalong the latent tree resolution levels.\n\u2022 We characterize 3D scene generation in a patch-wise\nfashion, enabling efficient training and seamless infinite\n3D scene generation through shared diffusion generation\nacross multiple scene patches.\n2. Related Work\nNeural 3D Scene Representations. Various neural repre-\nsentations have been developed to characterize 3D shapes\nor scenes, with recent works extensively exploring implicit\nfunctions [11, 36, 37, 39], feature volumes [24, 31, 41],\ntriplanes [7, 46], and tensor decompositions [9]. These\nrepresentations are proposed as alternatives to traditional\nrepresentations, such as voxels [15\u201317, 47, 53] or point\nclouds [1, 35, 55], to encode geometry more compactly\nand while maintaining high resolutions. Earlier approaches\nmodel an implicit function as an MLP, though such a global\nrepresentation can result in difficulty retaining both local\nand global structures, with limited editability [36, 37, 39].\nTri-planes and tensor decompositions more effectively bal-\nance memory cost and performance, making them popu-\nlar choices for object-level 3D shape generation [20, 46];\nhowever, scene outpainting is not straightforward as the\nn-planes are highly correlated, requiring intricate synchro-\nnization processing for extrapolation [54]. In this work, we\nuse a feature grid representation for capability to handle ar-\nbitrary topologies and its effective feature spatial locality to\nenable patch-based compositions of large-scale scenes.DiffusionEncoder\nDecoder\nDiffusionDecoder\nNoise LevelPatch -based Latent Tree Training: Level\nPatch -based Generation: Level\nNoiseLevel\nDecoderEncoderLatent Scene Diffusion\nTUDFLatent Tree\nTUDF\n TUDFTUDF\nTUDF\nLatent\nLatent\nTUDF\nTUDF\nTUDF\n Latent\nFigure 3. Overview of LT3SD. We formulate 3D scene generation as a patch-based latent diffusion process. Left: To characterize complex\nscene geometry, we encode 3D scenes in a novel latent tree representation, where each scene resolution level i\u2208[1, N\u22121]is decomposed\ninto a TUDF grid Ls\niand a latent feature grid Hs\ni.Top Right : During latent tree training, the encoder Eencodes a patch Li+1from\nthe scene grid Ls\ni+1at resolution level i+ 1 to a coarser TUDF patch Liand a latent feature patch Hiat level i. The decoder Dthen\nreconstructs the scene patch Li+1based on the factorized grids LiandHi.Bottom Right : During generation, the diffusion model G\nlearns to generate a latent feature patch Hiconditioned on a TUDF patch Liwithin the same level i. Our method enables arbitrary-sized\n3D scene generation at inference time by synthesizing scenes in a coarse-to-fine hierarchy and a patch-by-patch fashion.\n3D Diffusion Models for Shape Generation. Diffusion\nmodels have shown unprecedented capability in generating\n2D images and videos [5, 44]. Existing 3D diffusion work\nhas focused on synthesizing 3D shapes [6, 10, 13, 14, 18,\n20, 30, 46, 52, 56], showing quite promising results. Fol-\nlowing LDMs [44], most 3D diffusion models follow a two-\nstage approach: first learn a latent space of 3D shapes, and\nthen train diffusion in the latent space. However, extend-\ning the same success to complex 3D scenes remains a chal-\nlenge. Such 3D scene modeling faces higher data dimen-\nsionality, but with orders of magnitude less training data.\nExisting object generation methods assume that shapes lie\nin a canonical space and do not consider the arbitrary-sized\nnature of 3D scenes, making extending to 3D scenes diffi-\ncult. In contrast, our approach enables efficient infinite 3D\nscene generation via training on latent tree scene patches.\n3D Scene Generation. Inspired by the success of 2D im-\nage generation, various recent methods have proposed re-\nconstructing 3D scenes using 2D generative models [8, 12].\nThese approaches leverage 2D image generative models to\nlearn to generate consistent novel views; however, design-\ning the camera trajectory and generating a complete 3D\nscene is challenging due to occlusions. Scene graphs-based\n3D generation methods [50, 57] offer an abstract represen-\ntation for 3D scene generation. Several recent works have\nintroduced diffusion models for 3D scenes [2, 25, 26, 28,\n29, 32, 43, 54]. DiffRoom [25], NeuralField-LDM [26],\nCC3D [2], and XCube [43] focus on bounded generation of\n3D scenes, while SSG [28], [32], and SemCity [29] focus on\noutdoor 3D semantic scenes with limited geometry details.\nText-to-3D-based methods [23] typically require hours ofoptimization. Very recently, BlockFusion [54] proposed\nto generate larger-scale 3D scenes through latent triplane\nextrapolation, conditioned on 2D layout inputs. However,\n3D scene generation is still in its infancy, and it remains\na challenge to efficiently generate unbounded 3D scenes\nwith high-quality, coherent structures, high-frequency de-\ntails, and plausible layouts. In this work, we propose an ef-\nficient unconditional 3D scene generation pipeline that en-\nables seamless infinite 3D scene generation at high quality.\n3. Method\n3.1. Overview\nOur goal is to develop a probabilistic model for generating\n3D scenes encompassing complex geometric structures, di-\nverse object arrangements, and varying spatial extents. We\npropose LT3SD, a novel coarse-to-fine, patch-by-patch 3D\nscene generation approach by leveraging diffusion models\nin the space of a learned latent tree representation for 3D\nscenes. Our method has two main stages: latent tree encod-\ning and patch-based latent diffusion, as illustrated in Fig. 3.\nIn the first stage (Sec. 3.2), we progressively decom-\npose a 3D scene into a hierarchical latent tree from fine\nto coarse levels. Each tree level is a factorization of the\nscene into a geometry volume and a latent feature volume at\nthe corresponding resolution level (Fig. 3-left). The geom-\netry volume, represented as a truncated unsigned distance\nfield (TUDF), captures lower-frequency scene information,\nwhile the latent feature volume compactly encodes higher-\nfrequency details. As shown in Fig. 3-top right, we learn\nthis factorization with a 3D CNN-based encoder E. The in-verse process is built upon a decoder Dthat combines the\ntwo decomposed volumes to accurately recover the geome-\ntry volume at a higher resolution level.\nIn the second stage (Sec. 3.3), we train a 3D UNet-based\ndiffusion network on the latent tree encodings. At each\nlevel, we randomly crop fixed-size patches from the scene\ngeometry volumes and predict their corresponding latent\nfeature patches (Fig. 3-bottom right). At inference time,\nwe can effectively synthesize large-scale 3D scenes by pro-\ngressively generating latent trees in a patch-by-patch fash-\nion from coarse-to-fine levels.\n3.2. Latent Tree Representation for 3D Scenes\nTo enable high-quality 3D scene generation, we propose to\ntransform a 3D scene into a latent tree representation. Each\nlevel of the hierarchy comprises factorized geometry and\nlatent feature encodings of the scene at the corresponding\nresolution. We represent the geometry encoding as trun-\ncated unsigned distance field (TUDF) grids, characterizing\nlower-frequency scene information, while higher-frequency\nscene details are encoded into a latent feature grid.\nOur latent tree representation, as depicted in Fig. 2-right,\noffers the following advantages: (1) It allows for generat-\ning complex scene geometry in a coarse-to-fine fashion, in\ncontrast to a single latent (Fig. 2-left), which does not well-\nrepresent shared local structures; (2) Compared to a multi-\nlevel latent pyramid (Fig. 2-left), our latent tree offers better\nencoding of 3D scenes with higher reconstruction quality.\nConstruction. To construct the latent tree representations,\nwe first compute UDFs for 3D scenes. UDFs can easily en-\ncode arbitrary 3D scene geometry with various topologies.\nTo focus on surface geometry regions and reduce data distri-\nbution complexity, we clip distances exceeding a specified\nthreshold \u03c4to obtain TUDF voxel grids.\nConcretely, we decompose the TUDF grid of a 3D scene\ninto a latent tree with Nlevels, where each scene resolu-\ntion level i\u2208[1, N\u22121]consists of a TUDF grid Ls\niand a\nlatent feature grid Hs\ni. We denote the root of the tree ( i.e.,\nthe highest resolution) as Ls\nN. As illustrated in Fig. 3-top\nright, we learn this decomposition process with fixed-size\npatches Li+1randomly cropped from the scene grid Ls\ni+1\nfor computational efficiency. All scene levels can be trained\nin parallel through patch-based training.\nAs shown in Fig. 3-top right, at resolution level i+1, we\nuse the encoder Ei+1to factorize the TUDF patch Li+1\u2208\nR1\u00d7Di+1\u00d7Hi+1\u00d7Wi+1into a lower-resolution TUDF patch Li\nand a latent feature patch Hiat the next coarser level i:\nEi+1(Li+1)\u21d2[Li, Hi], (1)\nwhere Li\u2208R1\u00d7Di\u00d7Hi\u00d7Wirepresents the coarser patch ge-\nometry, and Hi\u2208RC\u00d7Di\u00d7Hi\u00d7Wiwith a latent feature di-\nmension Cencodes higher-frequency details. In practice,we compute Liby downsampling Li+1with average pool-\ning and predict Hiwith a 3D CNN.\nWe then train a decoder network Di+1to reconstruct the\nTUDF patch Li+1by combining the factorized grids Liand\nHias follows:\nDi+1([Li, Hi])\u21d2Li+1. (2)\nThe decoder Di+1is also implemented as a 3D CNN.\nLearning. To train the encoders and decoders, our loss\nmeasures the \u21132error between the reconstructed TUDFs and\nthe ground truth TUDFs across all the resolution levels:\nLlatent=X\u0010\nLi+1\u2212 D i+1\u0000\nEi+1(Li+1)\u0001\u00112\n.(3)\nGiven the trained encoders, we can progressively decom-\npose a high-resolution, arbitrary-sized 3D scene into a com-\npact latent tree {Ls\n1, Hs\n1,\u00b7\u00b7\u00b7, Ls\nN\u22121, Hs\nN\u22121}.\n3.3. Patch-Based Latent Scene Diffusion\nWe learn a denoising diffusion probabilistic model Gi[21]\nfor each resolution level iof the latent tree to generate high-\nfidelity 3D scenes. We leverage the explicit factorization in\nour latent tree (Sec. 3.2) and train Gito generate the latent\nfeature grid Hs\ni, conditioned on geometry grid Ls\ni. Since\n3D scenes typically consist of local structures with shared\nsimilarities, our latent diffusion models are trained on scene\npatches randomly cropped from scene grids.\nWe denote the diffusion model as Gi(zt, t, c), denoising a\nnoisy latent patch ztat each time step tbased on condition\nc. At each training step, a patch of Gaussian noise \u03f5\u223c\nN(0,1)of the same size and a time step t\u2208[1, T]are\nrandomly sampled and applied to a latent patch zto obtain\na noisy latent patch zt. At level i >1, we train the diffusion\nmodel Gisuch that given a coarse geometry patch Lias\ncondition, it predicts its corresponding latent feature patch\nHi,i.e.,z=Hiandc=Li. At level i= 1, the coarsest\nlevel of the latent tree, the diffusion model generates both\ngrids LiandHiunconditionally, i.e.,z= [Li, Hi]andc=\n\u2205. The diffusion model Gilearns to denoise ztusing the\nfollowing training loss:\nLdiff=Ez,c,\u03f5,t\u0002\n\u2225\u03f5\u2212 Gi(zt, t, c)\u22252\n2\u0003\n. (4)\nOur latent diffusion models {Gi}build upon 3D UNet\nbackbones. The patch-wise training strategy also serves as\ndata augmentation, helping our model avoid overfitting and\nlearn shared local structures across scenes. Our conditional\ndiffusion modeling can capture both lower-frequency struc-\ntures as well as higher-frequency detail, due to our decom-\nposition of 3D scenes into complementary geometry and la-\ntent feature grids at each resolution level. Our latent dif-\nfusion model can then synthesize arbitrary-sized 3D scene\noutputs by generating scenes in a patch-by-patch fashion.3.4. Large-Scale Scene Generation\nAt inference time, the generation of large-scale 3D scenes\nis enabled by our hierarchical latent tree representation\n(Sec. 3.2) and the latent diffusion models trained on scene\npatches (Sec. 3.3). The generation process reverses the la-\ntent tree decomposition and constructs the hierarchy from\ncoarse to fine levels. At each resolution level, the scene is\nsynthesized in a patch-wise manner, allowing for arbitrary-\nsized 3D outputs.\nPatch-by-Patch Scene Generation. To construct a latent\ntree from scratch, we first use the learned diffusion model\nG1to create a coarse scene structure at the lowest resolution\nlevel i= 1. That is, starting from random Gaussian noise,\nwe apply the diffusion model G1to unconditionally generate\nboth the geometry Ls\n1and latent feature Hs\n1grids of the\nscene. We find that using a simple inpainting-based patch-\ngeneration algorithm [26, 44], without requiring additional\nnetworks, works well to produce 3D scenes without seams.\nPatches of Ls\n1andHs\n1are synthesized autoregressively\non the ground ( i.e.,xy) plane. We adopt a breadth-first\npatch ordering scheme. From each known patch z0, we\nfirst generate its adjacent patches in the xdirection and then\ntheydirection; this is then repeated in a \u201cdilated\u201d scene\ngeneration process. To ensure a smooth transition between\npatches, we require that each patch to be generated by the\ndiffusion model G1has partial overlap with existing patches\n[34]. We use the same partial overlap size for patches at all\nresolution levels. Specifically, we follow LDMs [44] and\nadopt the autoregressive Stable Inpainting scheme:\nzt\u22121=m\u2299zknown\nt\u22121+ (1\u2212m)\u2299zunknown\nt\u22121, (5)\nwhere zknown\nt\u22121is sampled from the existing patch z0, while\nzunknown\nt\u22121 is sampled from the previous denoising iteration\nzt. These two are then combined using the inpainting mask\nmto form the new sample zt\u22121, keeping the known region\nunchanged.\nCoarse-to-Fine Refinement. We then refine the generated\ncoarse scene at higher resolution levels. At each level i >1,\nthe scene grid Ls\niis first reconstructed from the geometry\ngridLs\ni\u22121and the latent feature grid Hs\ni\u22121at the lower-\nresolution level i\u22121using the trained decoder Di, as de-\nfined in Eq. (2). Next, the diffusion model Gigenerates the\nlatent feature grid Hs\niconditioned on the geometry grid Ls\ni\nat patch-level with overlap.\nHowever, generating patches sequentially like Stable In-\npainting [44] at higher-resolution levels or infinite scenes\nwith significantly more patches can be extremely time-\nconsuming [54]. To speed up inference, we adapt the de-\nnoising fusion scheme from MultiDiffusion [3], which takes\neach denoising step on all patches simultaneously. First, we\ndivide the generated coarse 3D scene into npatches with\nthe same overlap size as previous levels. Then, the modelGiperforms a denoising step in parallel across all patches\nfrom the noisy scene grid zs\ntwith geometry patches from\nLs\nias the condition. Finally, an aggregation step Ablends\nthe patches by averaging the predictions from overlapping\nregions, ensuring smooth transitions across patches and pro-\nducing seamless 3D scenes. Concretely, each denoising step\ntat scene resolution level iis formulated as:\nzj\nt\u22121=Gi\u0010\nFj(zs\nt), t, F j(Ls\ni)\u0011\n, (6)\nzs\nt\u22121=A\u0012n\nzj\nt\u22121on\nj=1\u0013\n, (7)\nwhere \u2200j\u2208[1, n],Fjis an operation that crops the j-th\npatches from the scene grids zs\ntandLs\ni, respectively. The\nlatent feature volume Hs\niof the scene is obtained after com-\npleting all denoising time steps, i.e.,Hs\ni=zs\n0.\nThe decoders Diand diffusion models Giare then ap-\nplied alternately, as illustrated in Fig. 3-bottom right, until\nthe scene geometry Ls\nNat the highest level Nis synthesized\nas the final output.\n4. Experiments\nExperimental Setup. We train LT3SD on house-level\nscene data from the 3D-FRONT dataset [19], a large-scale\nindoor scene dataset with diverse structures and layouts.\nEach house contains various room types ( e.g., bedrooms,\nliving rooms, kitchens) with different layouts. After filter-\ning out rooms with incorrect furniture scales or no furni-\nture, we retained 6,479 houses. We used an 80%/5%/15%\ntrain/validation/test split for both stages of LT3SD and the\nbaselines. Due to open surfaces in scene meshes, consistent\nsigned distance computation is challenging, so we compute\nUDFs with a voxel size of 0.022m and a truncation value\nof\u03c4= 0.1m. Instead of preprocessing scenes into fixed\npatches, we continuously sample chunks from 3D scenes\nduring training to reduce overfitting, applying random flips\nand rotations for data augmentation.\nImplementation Details. We implemented our method in\nPytorch and ran all experiments on NVIDIA RTX A6000.\nTo first construct latent trees for 3D scenes, we train the\nencoders and decoders with Adam optimizer [27], using a\nbatch size of 4and a learning rate of 1e\u22124, which takes from\n5hours to one day for different resolution levels until con-\nvergence. We use N= 3 levels in our latent tree. For all\nlevels, the latent feature volumes have 4channels. We use\nthe Adam optimizer to train the diffusion models, with a\nbatch size of 8and a learning rate of 1e\u22124, which takes ap-\nproximately 6days until convergence on 2GPUs. Note that\nduring the second stage, we infer the latent of each TUDF\npatch on the fly without storing them on the disk in practice.\nBaselines. We compare our method against 3D object diffu-\nsion models PVD [58] and NFD [46], as well as the recentPVD NFD BlockFusion Ours\nFigure 4. Qualitative comparison. We compare unconditional 3D scene generation with diverse 3D diffusion methods PVD [58],\nNFD [46], and BlockFusion [54]. All methods were trained on houses from the 3D-FRONT dataset [19]. Our latent tree-based 3D\nscene diffusion approach synthesizes cleaner surfaces with more geometric details and captures diverse furniture objects.\n3D scene diffusion approach BlockFusion [54]. PVD is a\npoint-voxel-based diffusion model, while NFD and Block-\nFusion are triplane-based diffusion models. All baselines\nwere trained on 3D-FRONT. We uniformly sampled 8,192\npoints on the 3D scene surface for PVD. We used the same\nTUDF voxel grids for NFD as in our approach. The trained\nmodel of BlockFusion is provided by its authors, and we\nfollowed the authors\u2019 instructions for comparisons.\nEvaluation Metrics. We follow NFD [45] to evaluate gen-\neration ability and use an adapted Fr \u00b4echet Inception Dis-\ntance (FID) metric based on rendered images of generated\nmeshes to measure perceptual distribution similarity. Fol-\nlowing prior work [18, 43, 58], we also assess coverage\nand diversity using the Coverage (COV), Minimum Match-\ning Distance (MMD), and 1-Nearest Neighbor Accuracy (1-\nNNA) metrics. COV measures the fraction of test set shapesmatched to at least one in the generated set, indicating di-\nversity representation. MMD reflects the average distance\nfrom each test shape to its nearest neighbor in the gener-\nated set, measuring quality and fidelity. The 1-NNA metric\nevaluates distribution similarity, with a value closer to 50%\nindicating statistical indistinguishability between the gener-\nated and test distributions.\nFor FID, we render images of generated and ground-\ntruth 3D meshes by placing cameras at the scene center\nand rotating them horizontally to capture 20 images per\nscene. To compute COV , MMD, and 1-NNA, we mea-\nsure point cloud similarity using Chamfer Distance (CD)\nand Earth Mover\u2019s Distance (EMD). Points are uniformly\nsampled from the generated meshes of NFD, BlockFusion,\nour method, and GT, while the same number of points are\nrandomly sampled from PVD\u2019s generated point clouds.Generated Patch\n Nearest Neighbor Retrieval\nFigure 5. Generation novelty analysis. Our generated scene patches (left), compared with their nearest-neighbor retrieved training patches\nby Chamfer distance. Our approach can synthesize novel patches with different geometric structures than their training set neighbors.\nPart / Complete GT Multi -Hypothesis Generation\nFigure 6. Probabilistic Scene Completion. Given an initial part of a complete ground-truth 3D scene from the test split, LT3SD can\nsynthesize various plausible hypotheses of a complete scene by sampling various diffusion outputs.\n4.1. 3D Scene Generation\nWe evaluate our method on 3D scene generation, both quan-\ntitatively and qualitatively in comparison with state-of-the-\nart 3D diffusion alternatives. We generate 3D scenes of a\nfixed size for each method for evaluation.\nWe show qualitative comparisons in Fig. 4. Our results\ndemonstrate superior surface quality and object detail: PVD\ngenerates plausible global structures but produces incom-\nplete geometric information in point clouds; NFD [45] gen-\nerates plausible scene structures and scales well for com-\nplex 3D scenes, but struggles to produce complex arrange-\nments of furniture and smooth surfaces; BlockFusion [54]\nachieves smooth local surfaces and structural diversity, but\nstruggles in generating meaningful higher-level structures\n(e.g., Fig. 4-1st row, where walls are misaligned despite im-\nproved surface quality). In contrast, our approach gener-\nates scenes with both plausible room structures and layouts\nwhile maintaining the highest quality in finer-scale details.Table 1. Quantitative comparison. We compare unconditional\n3D scene generation with state-of-the-art 3D diffusion methods\nPVD [58], NFD [46], and BlockFusion [54] trained on 3D-\nFRONT data [19]. Our method significantly outperforms baselines\nin all metrics.\nMethodCOV (%) \u2191MMD ( \u00d71e2)\u21931-NNA (%) \u2193FID\u2193CD EMD CD EMD CD EMD\nPVD [58] 43.82 32.52 3.69 29.36 70.83 82.24 237.85\nNFD [46] 44.65 45.48 3.65 27.98 62.86 63.62 266.27\nBlockFusion [54] 24.32 22.85 5.10 31.82 89.01 90.85 45.55\nOurs (16 - 128) 28.61 29.98 4.93 30.17 74.51 69.39 59.23\nOurs (16 - 32 - 128) 53.10 54.66 3.51 27.30 53.22 52.56 13.39\nTab. 1 shows a quantitative comparison of our LT3SD\nagainst baselines. Our approach significantly improves both\npoint-based and perceptual-based metrics, demonstrating its\neffectiveness in 3D scene generation.\nSince the point-cloud-based PVD [58] generates only\nsparse point clouds, we convert PVD points to triangle\nmeshes with ball-pivoting [4] for FID computation. TheTable 2. Ablation over latent representation . Our latent\ntree-based representation achieves higher reconstruction accuracy\nwhile using less training time and latent storage by factorizing 3D\nscenes into complementary components.\nRepresentation # Params Training Storage Recon. Error ( \u21132)\nLatent CM 13.3 M \u00d71.00 \u00d71.00 4.91e-4\nLatent Tree (Ours) 13.3 M \u00d70.87 \u00d70.80 3.20e-4\ntriplane-based NFD [45] produces good scene structures,\nthough lacks perceptual quality in local surface smooth-\nness. As shown in Fig. 4, BlockFusion [54] produces recog-\nnizable furniture and smooth surfaces, but with quite frag-\nmented room structures ( e.g., in Fig. 4, the walls in the 1st\nand 3rd rows are fragmented and unlikely in structure), re-\nsulting in a good FID score but a relatively lower point\ncloud metrics that rely on more strongly reflecting global\nstructures in the ground-truth distribution.\nIn contrast, our method follows a hierarchical approach\non our latent tree encoding, starting with a good coarse\nscene structure (reflected in point-cloud metrics) and pro-\ngressively adding high-fidelity local detail, as indicated by\nthe FID score. Additionally, our method is also much more\nefficient while achieving superior quality: the large-scale\nscene generation in Fig. 1 of size 45.1m\u00d790.3m\u00d72.8m\ncontaining \u2248170rooms takes 2 hours on a single GPU,\nwhile BlockFusion [54] takes 3 hours for the generation of\n7 rooms.\n4.2. Ablation Study\nLatent Tree Structure. We validate the latent tree struc-\nture of our method in Tab. 1 for 3D scene generation. In\nthis ablation, we compare with the one-level latent tree of\nthe same spatial dimension as the coarsest resolution in our\nlatent tree. As shown in Tab. 1-bottom, the multi-level\nlatent tree-based diffusion model outperforms the single-\nlevel one. Rather than using a single level of the tree\nand generating a 3D scene in a single step, our LT3SD\nadopts a coarse-to-fine strategy for encoding; the diffusion\nmodel only generates the higher-frequency component of\n3D scenes in each step and, finally, both lower-frequency\nand higher-frequency components are gracefully combined\ntogether and decoded to a high-quality 3D scene. This ab-\nlation demonstrates that the hierarchical tree structure is es-\nsential for effective generative modeling with diffusion for\ncomplex 3D scenes.\nHierarchical Latent Representation. We compare our la-\ntent tree with latent Cascaded Models (CMs) (Fig. 2-left),\ncommonly used in 2D image synthesis [22] and 3D gener-\nation [32, 43]. Unlike CMs, which independently model\nredundant information at each level, our latent tree de-\ncomposes 3D scenes into coarse geometry and fine latent\ncomponents, capturing residual high-frequency details. As\nshown in Tab. 2, with the same autoencoder and parame-ter count, our model trains more efficiently (requiring less\ntime per level) and achieves higher reconstruction accu-\nracy (lower l2error) on test data. Additionally, its more\ncompact latent representation reduces storage requirements\nwhen converting TUDF grids to latent grids. Training time\nand storage statistics are reported as ratios w.r.t. latent CM;\nplease see the supplemental for more details.\n4.3. Novelty of Generated Scene Patches\nAn important consideration in unconditional generation is\nthe ability of trained models to produce novel outputs, in-\ncluding new shapes and scene layouts that differ from the\ntraining data. We retrieve the nearest training patch to each\ngenerated scene patch based on Chamfer Distance to eval-\nuate this. We perform retrieval from the entire training\nset, applying all permutations of the training augmentations\n(flip and rotation). In Fig. 5, we show scene patches gen-\nerated by our LT3SD, along with the top-4 nearest neigh-\nbor training patches. We observe that our generated patches\nproduce new structures different from the training patches.\nOur method generates strong differences in local object\ncharacteristics (top row) as well as differences in layout ar-\nrangements (bottom row). We attribute this partly to the\nrandom, continuous patch-wise training strategy employed\non the fly during the two training stages and the augmenta-\ntions.\n4.4. Probabilistic Scene Completion\nWe further demonstrate the generative capability of LT3SD\nfor scene completion from partial observations. Fig. 6\nshows that from small portions of a given scene, our\ndiffusion-based approach enables sampling a diverse set of\npossible completed scenes that explain the partial geometry\ninput. Our approach can preserve the initial scene geome-\ntry while generating completed scenes of various sizes. Al-\nthough LT3SD is trained without semantic annotations, our\ncompletion results respect the input object semantic struc-\ntures ( e.g., given a sofa, generating various living rooms).\n5. Conclusion\nWe have presented LT3SD, a novel approach for high-\nquality 3D scene generation through latent scene diffusion.\nWe propose decomposing high-resolution 3D scene voxel\ngrids to compact latent trees for efficient diffusion training\nin this latent space. This strategy enables seamless infi-\nnite 3D scene generation in a patch-wise manner. Exper-\nimental validation demonstrates the capacity of LT3SD for\nlarge-scale 3D scene generation, significantly outperform-\ning state-of-the-art alternatives for unconditional 3D scene\ngeneration. We hope our method provides an avenue for fu-\nture exploration towards the challenging task of 3D scene\ngeneration and automated 3D content creation.Acknowledgements. This work was supported by the ERC\nStarting Grant SpatialSem (101076253). Matthias Niess-\nner was supported by the ERC Starting Grant Scan2CAD\n(804724).\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3d point clouds. In International conference on\nmachine learning , pages 40\u201349. PMLR, 2018. 2\n[2] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou,\nXingguang Yan, Gordon Wetzstein, Leonidas Guibas, and\nAndrea Tagliasacchi. Cc3d: Layout-conditioned generation\nof compositional 3d scenes. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , pages\n7171\u20137181, 2023. 3\n[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultidiffusion: Fusing diffusion paths for controlled image\ngeneration. arXiv preprint arXiv:2302.08113 , 2023. 5\n[4] F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva, and\nG. Taubin. The ball-pivoting algorithm for surface recon-\nstruction. IEEE Transactions on Visualization and Computer\nGraphics , 5(4):349\u2013359, 1999. 7\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n22563\u201322575, 2023. 3\n[6] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun\nHao, Serge Belongie, Noah Snavely, and Bharath Hariha-\nran. Learning gradient fields for shape generation. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV) , 2020. 3\n[7] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In CVPR , 2022. 2\n[8] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-\nder W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait-\ntala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.\nGeNVS: Generative novel view synthesis with 3D-aware dif-\nfusion models. In arXiv , 2023. 3\n[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision (ECCV) , 2022. 2\n[10] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:\nA unified approach to 3d generation and reconstruction. In\nICCV , 2023. 2, 3\n[11] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. Proceedings of IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2019.\n2[12] Z. Chen, G. Wang, and Z. Liu. Scenedreamer: Unbounded\n3d scene generation from 2d image collections. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence , 45\n(12):15562\u201315576, 2023. 3\n[13] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-\nder G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal\n3d shape completion, reconstruction, and generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 4456\u20134465, 2023. 3\n[14] Gene Chou, Yuval Bahat, and Felix Heide. Diffusion-sdf:\nConditional generative modeling of signed distance func-\ntions. In The IEEE International Conference on Computer\nVision (ICCV) , 2023. 2, 3\n[15] Angela Dai, Charles Ruizhongtai Qi, and Matthias Nie\u00dfner.\nShape completion using 3d-encoder-predictor cnns and\nshape synthesis. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 5868\u20135877,\n2017. 2\n[16] Angela Dai, Christian Diller, and Matthias Nie\u00dfner. Sg-nn:\nSparse generative neural networks for self-supervised scene\ncompletion of rgb-d scans. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 849\u2013858, 2020.\n[17] Angela Dai, Yawar Siddiqui, Justus Thies, Julien Valentin,\nand Matthias Nie\u00dfner. Spsg: Self-supervised photomet-\nric scene generation from rgb-d scans. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 1747\u20131756, 2021. 2\n[18] Ziya Erkoc \u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner, and\nAngela Dai. Hyperdiffusion: Generating implicit neural\nfields with weight-space diffusion, 2023. 2, 3, 6\n[19] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming\nWang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Bin-\nqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts\nand semantics. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , pages 10933\u201310942,\n2021. 5, 6, 7, 14\n[20] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O \u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371 , 2023. 2, 3\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems , 33:6840\u20136851, 2020. 1, 4\n[22] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. arXiv preprint\narXiv:2106.15282 , 2021. 8\n[23] Lukas H \u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson,\nand Matthias Nie\u00dfner. Text2room: Extracting textured 3d\nmeshes from 2d text-to-image models. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 7909\u20137920, 2023. 2, 3\n[24] Chiyu Jiang, Avneesh Sud,"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "Certainly! Below is a comprehensive summary of the machine learning paper titled \"Adaptive Language-Guided Abstraction from Contrastive Explanations\" by Andi Peng et al., as requested.\n\n---\n\n### 1. Abstract:\nThe paper presents a method named ALGAE (Adaptive Language-Guided Abstraction from Contrastive Explanations), which aims to improve robot learning by iteratively identifying human-meaningful features using language models and assigning weights to these features using inverse reinforcement learning (IRL). This approach allows for the creation of generalizable reward functions from a small number of demonstrations. The empirical results show that ALGAE can identify missing features and efficiently acquire rich representations of user behavior.\n\n### 2. Introduction:\n- **Context of Research:**\n  - The paper addresses the challenge of robot learning, particularly in inferring reward functions from human demonstrations.\n  - Traditional methods either rely on end-to-end feature and reward learning or manual specification of features, which can be brittle and error-prone.\n  \n- **Research Questions:**\n  - How to build robots that incorporate strong human priors when learning from demonstrations?\n  - How to ensure that the learned reward functions are not sensitive to spurious state features?\n\n- **Literature Review:**\n  - The review highlights previous methods, including end-to-end approaches like deep IRL and program synthesis, and their limitations.\n  - Discusses how humans use strong priors to generalize from few demonstrations and the need to instill such priors into robot learning.\n\n### 3. Related Work:\n- **Inverse Reinforcement Learning:**\n  - IRL methods learn objective functions from observed actions, but suffer from identifiability issues and are prone to overfitting in high-dimensional spaces.\n- **Learning Features from Users:**\n  - Typically dependent on user-specified or hand-crafted features, which are laborious and may still lack expressiveness for complex tasks.\n- **Learning from Language:**\n  - Recent works leverage language models to capture task-relevant features via language-guided abstraction but are limited to imitation learning frameworks.\n\n### 4. Method:\n- **Overview:**\n  - ALGAE alternates between two stages: feature specification using language models and reward validation using inverse reinforcement learning.\n\n- **Mathematical Formulations:**\n  - The problem is modeled as a Markov Decision Process (MDP) \u27e8S, A, T, R\u27e9 with a state space S, action space A, transition distribution T, and reward function R.\n  - The reward function \\( R_\\theta(s) = \\theta^\\top \\phi(s) \\) where \\( \\phi(s) \\) is a feature vector and \\( \\theta \\) are the reward weights.\n\n- **Algorithmic Steps:**\n  1. **Feature Specification:**\n     - Uses an LM to identify missing features by prompting with contrastive pairs of user and robot trajectories.\n     - The LM generates natural language descriptions of task-relevant features, which are then converted into feature definition code.\n  2. **Reward Validation:**\n     - Uses the updated feature set to re-learn the reward weights via MaxEnt IRL and validate against user demonstrations.\n\n- **Iterative Optimization:**\n  - Alternates between expanding the feature set and validating rewards until the learned reward function sufficiently explains user behavior.\n\n### 5. Experimental Setup:\n- **Datasets and Environment:**\n  - Simulated environments include 2D maze navigation, 7DoF JACO tabletop manipulation, and real-world mobile manipulation with a Spot robot.\n- **Hardware and Software:**\n  - Simulated environments implemented in PyBullet.\n  - Real-world experiments conducted using a Spot robot.\n  - GPT-4 used for language models, image segmentation, and other software tools.\n- **Evaluation Metrics:**\n  - Ground truth reward normalization, L2 distance in feature space for validation.\n- **Baselines:**\n  - Full-State learning from the entire state space.\n  - LM-Feature and LM-Reward: prompting LMs for features and rewards.\n  - Random policy for benchmarking.\n\n### 6. Results and Analysis:\n- **Empirical Findings:**\n  - ALGAE achieves near-perfect feature recovery and high normalized rewards in both 2D navigation and JACO environments.\n  - In real-world scenarios, ALGAE significantly outperforms LM-based baselines due to avoiding over-parameterization.\n\n- **Graphs and Tables:**\n  - Tables show normalized rewards across different test scenarios.\n  - Graphs illustrate iterative improvements in multi-feature recovery experiments.\n- **Ablation Studies:**\n  - Analysis revealed that contrastive explanations enabled LMs to efficiently identify salient features.\n- **Unexpected Results:**\n  - The benefit of iterative refinement was prominent in cluttered real-world scenarios, where LM-only methods over-parameterized features.\n\n### 7. Discussion:\n- **Interpretation of Results:**\n  - ALGAE's advantage stems from iterative human-aligned feature construction, leveraging language models for commonsense priors.\n- **Implications:**\n  - Demonstrates the feasibility of efficiently learning reward functions with few demonstrations.\n- **Limitations:**\n  - Dependency on LM quality and alignment with human priors.\n  - Current approach focuses on linear reward functions; extension to non-linear rewards could be explored.\n\n### 8. Conclusion:\n- **Summary of Contributions:**\n  - Introduced ALGAE, a novel framework combining language models and IRL for adaptive feature abstraction.\n  - Demonstrated superior performance in both simulated and real-world scenarios with effective generalization to unseen environments.\n- **Future Research:**\n  - Investigate human-in-the-loop scenarios.\n  - Extend ALGAE to handle non-linear rewards and more complex tasks.\n\n---\n\nThis detailed summary should guide a researcher in replicating the study and potentially building upon the novel methodology proposed in the paper."}, {"role": "assistant", "content": "Adaptive Language-Guided Abstraction\nfrom Contrastive Explanations\nAndi Peng\nMITBelinda Z. Li\nMITIlia Sucholutsky\nPrincetonNishanth Kumar\nMIT\nJulie A. Shah\nMITJacob Andreas\nMITAndreea Bobu\nThe AI Institute\nAbstract: Many approaches to robot learning begin by inferring a reward func-\ntion from a set of human demonstrations. To learn a good reward, it is necessary to\ndetermine which features of the environment are relevant before determining how\nthese features should be used to compute reward. End-to-end methods for joint\nfeature and reward learning (e.g., using deep networks or program synthesis tech-\nniques) often yield brittle reward functions that are sensitive to spurious state fea-\ntures. By contrast, humans can often generalizably learn from a small number of\ndemonstrations by incorporating strong priors about what features of a demonstra-\ntion are likely meaningful for a task of interest. How do we build robots that lever-\nage this kind of background knowledge when learning from new demonstrations?\nThis paper describes a method named ALGAE (Adaptive Language-Guided Ab-\nstraction from [Contrastive] Explanations) which alternates between using lan-\nguage models to iteratively identify human-meaningful features needed to explain\ndemonstrated behavior, then standard inverse reinforcement learning techniques to\nassign weights to these features. Experiments across a variety of both simulated\nand real-world robot environments show that ALGAE learns generalizable reward\nfunctions defined on interpretable features using only small numbers of demon-\nstrations. Importantly, ALGAE can recognize when features are missing, then\nextract and define those features without any human input \u2013 making it possible to\nquickly and efficiently acquire rich representations of user behavior.\nKeywords: reward learning, language-guided abstraction, reward features\n1 Introduction\nWhen training robots to perform complex tasks \u2013 like watering plants in cluttered household envi-\nronments (Fig. 1) \u2013 it is often useful to begin by specifying a reward function from which optimal\nrobot behavior can be derived. Assigning rewards to trajectories typically requires extracting impor-\ntant state or trajectory-level features (e.g. distance to goal orend effector orientation ), then using\nthese features to compute scalar rewards. While it is sometimes possible for humans to directly\nspecify reward functions in code, this process is challenging and prone to error, even for experts\n[1]. A slightly more general class of approaches uses manual specification of feature functions (ei-\nther in code or with targeted supervision [2, 3]) followed by reward learning (e.g. with inverse\nreinforcement learning, or IRL [4]). But it can be challenging for users to identify and describe\nall features relevant to a task of interest (e.g. the distance between the watering can and an opti-\nmal pouring height relative to the pot ), so reward learning with human-designed feature sets runs\nthe risk of feature under-specification \u2013 situations in which important task-relevant information is\nunavailable to the reward function. An alternative family of end-to-end approaches, such as deep\nIRL [5, 6, 7] attempt to implicitly extract these features from user demonstrations, but can require\n8th Conference on Robot Learning (CoRL 2024), Munich, Germany.arXiv:2409.08212v1  [cs.RO]  12 Sep 2024Figure 1: Adaptive Language-Guided Abstraction from Contrastive Explanations (ALGAE)\nalternates between two main stages: in feature specification , ALGAE expands the current feature\nset by identifying under-specified features of the current reward; then in reward validation , ALGAE\nlearns an updated reward function defined on top of the new feature set and validates it can explain\nthe user demonstrations. ALGAE results in more generalizable learned reward functions vs. base-\nlines without manual feature specification, and can iteratively improve its own reward estimate given\nmultiple under-specified features.\nvery large numbers of demonstrations to ensure that learned features are robust, generalizable, and\ninsensitive to spurious features in the user demonstrations (e.g., distance to apple , etc. in Fig. 1).\nReliable, sample-efficient reward learning thus requires both identifying a discrete set of features\nthat are salient to the task and learning how they parameterize a continuous reward function [8].\nFortunately, good features that explain human decision-making do not come from a blank slate.\nWhen learning (and planning), humans draw upon a wealth of prior experience to identify which\nfeatures are meaningful and generalizable for acting in the world [9, 10]. Can we instill these priors\ninto reward learning procedures without requiring human supervision at an impractical scale?\nIn this work, we propose to leverage the expressive human priors embedded in natural language text\ncorpora. We describe an iterative framework for autonomously alternating between feature speci-\nfication and reward learning. Our approach, called Adaptive Language- Guided Abstraction from\ncontrastive Explanations, ( ALGAE ), leverages language models (LMs) in combination with user\ndemonstrations to learn human-aligned reward functions informed by semantically-relevant features.\nALGAE distills the reward learning problem into two components: 1) the feature specification prob-\nlem of identifying missing reward features that are salient to the user\u2019s reward, and 2) the reward\nvalidation problem of finding and verifying a reward function that best explains demonstrated user\nbehavior. By alternating between these two components, ALGAE iteratively builds increasingly\nrich representations of human decision-making while ensuring that learned rewards are explained\nby intuitive, semantically-meaningful features.\nWe empirically validate ALGAE\u2019s benefits across simulated and real-world robot tasks. ALGAE\nrecovers missing reward features and produces trajectories that better align with users\u2019 ground truth\nreward compared to several baselines. Importantly, ALGAE can iteratively refine its own represen-\ntation by sequentially identifying missing features and validating them against the demonstrations,\nthereby mitigating the over-parameterization problem exhibited by end-to-end approaches.\n2 Related Work\nOur work builds on several fundamental ideas in reinforcement learning and reward learning.\nInverse reinforcement learning. Inverse reinforcement learning (IRL) methods propose to learn the\nunknown objective function from observed actions in the environment, e.g., human demonstrations\n[4, 5, 11, 12]. Such methods suffer from identifiability issues [12, 13]. That is, given a constrained\n2number of demonstrations, multiple objective functions can explain the same observed behavior.\nThis is due to expressive function approximators overfitting to a few demonstrations, a problem that\nis exacerbated in high-dimensional and messy state spaces such as in robot learning.\nLearning features from users. A large body of work has studied how to elicit features (e.g. for\nuse in RL) from human users. In this paradigm, learning good objectives is dependent on a set\nof carefully, hand-specified features that capture aspects of the environment or task that the user\nmay care about [2, 14, 4, 1]. If selected well, this feature set introduces a well-formed inductive\nbias for facilitating more generalizable learning from few demonstrations. Such a feature set helps\nreduce the dimensionality of what is typically higher-dimensional states and trajectories, affording\na better-shaped space for IRL to operate. But defining a good feature set is both challenging and\nlaborious for experts and novices alike, and the chosen feature(s) may not be expressive enough to\nfully capture complex behavior in more ineffable robotic tasks [15]. Some studies explore having\nhumans offer corrections to the agent using spatial labels [16], teleoperation with joysticks [17, 18],\nphysical interaction [15], and natural language [19], but these approaches are difficult to scale as\nthey involve expensive online human interventions.\nLearning features from language. At a high level, most of the previous work can be described\nas designing methods for users to specify their priors about the environment and task to the robot.\nRecent work on language-guided abstraction ([20, 21]) instead aims to learn task-relevant features\nfrom language by leveraging the semantic priors embedded in language models [22] to best inform\nwhich environment elements are important to include in the state and which elements are not. These\napproaches are limited by the imitation learning framework\u2014that is, there is no true way to learn an\nobjective that explains the demonstrated behavior. As a result, these approaches reduce the number\nof human demonstrations required to learn preference-tailored generalizable policies, but may still\nrequire expensive, online interventions to correct for feature under-specification.\nIn this work, we extend the feature learning and language-guided frameworks to identify under-\nspecified features using language-guided contrastive explanations between the demonstration and\nsuboptimal trajectory, then iteratively learn reward functions using these features.\n3 Problem Definition\nWe consider the problem of learning reward functions that capture the (unknown) preferences held\nby a human given a small number of user demonstrations and language.\nPreliminaries. We model our problem as a Markov Decision Process (MDP) [23] \u27e8S,A,T,R\u27e9,\nwhere S \u2208Rdis the state space, Athe action space, T:S \u00d7 A \u00d7 S \u2192 [0,1]the transition\nprobability distribution, and R:S \u00d7A \u2192 Rthe reward function. A solution to the MDP is a policy\n\u03c0:S \u2192 A that specifies what actions the robot should take in different states. Following prior work\n[3], we assume that the robot operates under a parameterized linear reward function defined on the\nstate,R\u03b8(s) =\u03b8\u22a4\u03d5(s), where \u03d5:S \u2192Rpis a feature vector and \u03b8\u2208Rprepresents the reward\nweights on the features. The robot executes a task by following a state trajectory \u03c4={s0, ..., sT}\ngiven by the policy. To pick the best trajectory for the task, the robot optimizes the cumulative\nreward R\u03b8(\u03c4) =P\nst\u2208\u03c4\u03b8\u22a4\u03d5(st) =\u03b8\u22a4\u03d5(\u03c4), where \u03d5(\u03c4) =P\nst\u2208\u03c4\u03d5(st)is the total feature count\nalong the trajectory, and solves:\n\u03c4R\u03b8= arg max\n\u03c4R\u03b8(\u03c4) = arg max\n\u03c4\u03b8\u22a4\u03d5(\u03c4). (1)\nMaximum Entropy Inverse Reinforcement Learning (MaxEnt IRL). In practice, the reward\nfunction R\u03b8is typically unknown to the robot or very challenging to manually specify. Thus, in\nIRL, the robot\u2019s goal is to learnR\u03b8given human feedback like user demonstrations. Concretely,\ngiven human demonstrated trajectories D={\u03c4i}i=1...N, the robot interprets them as evidence for\nthe human\u2019s preferred behavior and attempts to recover the reward parameters \u03b8that explain the hu-\nman\u2019s desired objective. We adopt the maximum-entropy framework for modeling human decision-\nmaking [12, 5], and model the human as a noisily-optimal agent that tends to choose demonstrations\n3in proportion to the exponentiated reward:\np(\u03c4|\u03b8) =eR\u03b8(\u03c4)\nR\n\u00af\u03c4eR\u03b8(\u00af\u03c4)d\u00af\u03c4\u221dexp(R\u03b8(\u03c4)) = exp( \u03b8\u22a4\u03d5(\u03c4)). (2)\nUnder this framework, the human is likely to act optimally and will generate suboptimal trajectories\nwith a probability that decreases exponentially as the trajectories become lower in reward [12]. The\nunknown reward parameters \u03b8can then be optimized via gradient descent on the objective:\n\u03b8\u2217= arg max\n\u03b8L(\u03b8) = arg max\n\u03b8X\n\u03c4\u2208Dlogp(\u03c4|\u03b8). (3)\nTo optimize this objective, we approximate the intractable integral in Eq. (2) using importance sam-\npling, as in prior work [5, 3]. The robot then acts according to Eq. (1).\nThe features \u03d5we choose to represent the reward R\u03b8dramatically impact the reward functions that\ncan be learned [2]. In the limited-data regime, this is true even when R\u03b8belongs to an expressive\nfunction class (e.g., a neural network). Motivated by recovering a reward function R\u03b8that incorpo-\nrates human-like priors, we are interested in explicitly constructing human-meaningful features \u03d5.\nIn this sense, we consider a feature set to be under-specified if it does not explicitly represent all the\nsalient features relevant to the task, and consider a reward function to be under-specified if it uses an\nunder-specified feature set as its basis.\n4 Adaptive Language-Guided Abstraction from Contrastive Explanations\nAt a high level, ALGAE aims to identify the most likely set of preference weights \u03b8\u2217and preference\nfeaturizations \u03d5\u2217given the human demonstrations under p(\u03d5, \u03b8|\u03c41, ..., \u03c4 N). The idealized joint\noptimization given demonstration \u03c4thus takes the following form:\n\u03b8\u2217, \u03d5\u2217= arg max\n\u03b8,\u03d5p(\u03d5, \u03b8|\u03c4). (4)\nWe maximize this objective tractably by iteratively optimizing \u03b8and\u03d5. Given an initialization of\nthe feature vector \u03d50, ALGAE decomposes the learning problem into two subtasks: 1) performing\nfeature specification by expanding each feature set \u03d5kinto\u03d5k+1by querying for missing features in\nlanguage, then 2) performing reward validation by finding an optimal reward weight \u03b8kfor each \u03d5k\nand comparing its predictions to ground-truth trajectories.\n4.1 Identifying missing features\nInfeature specification , we use an LM to identify missing features salient to the user\u2019s reward given a\ndemonstration \u03c4and the robot\u2019s current best estimate of the (under-specified) reward function R\u03b8k.\nOnce the missing feature has been identified, we query the LM for code to compute that missing\nfeature and append the resulting feature function \u03d5ito the set of known features \u03d5kto obtain \u03d5k+1.\nOperationally, by fixing \u03b8=\u03b8k, Eq. (4) becomes:\n\u03d5k+1= arg max\n\u03d5p(\u03d5, \u03b8k|\u03c4) = arg max\n\u03d5p(\u03d5|\u03b8k, \u03c4)p(\u03b8k|\u03c4) = arg max\n\u03d5p(\u03d5|\u03b8k, \u03c4),(5)\nwhere p(\u03d5|\u03b8k, \u03c4)may be estimated using an LM as described below.\nQuerying for a missing feature. The key insight of our approach is that language models have\nstrong common-sense priors over salient task features (e.g., ideal watering height above plant )\nwhich we can leverage to approximate the probability p(\u03d5|\u03b8k, \u03c4)in Eq. (5). Concretely, we\napproximate the optimization Eq. (5) by first computing best estimated trajectory for the current re-\nward \u03c4R\u03b8kusing Eq. (1) and pair it with the user demonstrated trajectory \u03c4. We then prompt the LM\nwith the contrastive pair of trajectories (full state information concatenated over all timesteps) along\nwith a high level description of the task ( e.g. the user wishes to water the plant ) and environment\ncontext (a description of the state of the environment) and query the LM for the likeliest feature \u03d5i\nthat explains this difference1. Intuitively, \u03c4R\u03b8kacts as evidence for \u03b8kthat is easier for the LM to\n1If there are multiple missing features, the algorithm will identify them in future iterations.\n4Algorithm 1: ALGAE\nInput: Demonstrations D={\u03c4i}i=1...N\nInit: Features \u03d50= [distance to goal ], confidence threshold \u03f5\nwhile true do\n\u03b8k= arg max\u03b8p(D |\u03b8, \u03d5k)// Optimize reward as in Eq. (3).\n\u03c4R\u03b8k= arg max\u03c4R\u03b8k(\u03c4)// Optimize trajectory according to this reward as in Eq. (1).\n\u03c4H\u223c D // Sample one human demonstration to contrast it against.\nif\u2225\u03d5\u2217(\u03c4R\u03b8k)\u2212\u03d5\u2217(\u03c4H)\u2225< \u03f5// If optimized trajectory explains the demo then\nbreak\n\u03d5i=LM(\u03c4R\u03b8k, \u03c4H)// Query LM for missing feature that explains the contrast.\n\u03d5k+1\u2190\u03d5k\u222a\u03d5i\ninterpret than reward weights, resulting in a sample from the LM\u2019s prior on p(\u03d5i|\u03c4R\u03b8k, \u03c4). Thus,\nour approximate solution \u03d5k+1concatenates the LM sampled \u03d5iwith the previous vector \u03d5k.\nQuerying for feature code. Now that we have identified a missing feature \u03d5i, we query the LM to\ndirectly generate the feature definition code that can be used to compute a feature value from raw\nstate data over the given demonstrations. The LM is given raw trajectories \u03c4and a natural language\nfeature description of \u03d5i, generated from the previous stage, and is asked to generate its code \u03d5i. We\nappend \u03d5iand its respective function code to the existing list of features \u03d5k+1\u2190\u03d5k\u222a\u03d5i.\n4.2 Learning good reward functions\nIn the second stage, reward validation , we train and validate a reward function using the updated set\nof features \u03d5k+1as the basis function for the new reward, learning new parameters \u03b8k+1. Because\nthere may be additional missing features we have yet to identify, we now leverage existing user\ndemonstrations to validate whether reward learning is complete. This iterative update is motivated\nby the desire to not over-parameterize the reward by specifying non-salient features.\nLearning a new reward. Given our updated feature set \u03d5k+1, we fix \u03d5=\u03d5k+1in Eq. (4) to obtain:\narg max\n\u03b8p(\u03d5k+1, \u03b8|\u03c4) = arg max\n\u03b8p(\u03c4|\u03d5k+1, \u03b8)p(\u03b8|\u03d5k+1)p(\u03d5k+1)\np(\u03c4)= arg max\n\u03b8p(\u03c4|\u03d5k+1, \u03b8)(6)\nassuming uniform p(\u03b8|\u03d5k+1). This equation directly corresponds to the MaxEnt IRL optimization\nprocedure in Section 3 using \u03d5k+1as the feature vector. We now learn updated reward weights \u03b8k+1\nfrom the user demonstrations via Eq. (3), then optimize for an updated trajectory \u03c4R\u03b8k+1via Eq. (1).\nValidating the new reward. To determine whether we have recovered all salient features, we\nleverage existing user demonstrations for validation. In a real-world deployment scenario, there\nare many reasonable ways to implement a comparison between trajectories, with the ideal being a\nhuman user giving feedback as to whether the robot is executing the desired behavior. However, in\nour simulated experiments, we approximate this judgement by measuring L2 distance between these\ntrajectories in the ground truth feature space \u03d5\u2217. If this is within a threshold \u03f5, the learned reward\nfunction is sufficient for explaining the desired behavior and the algorithm terminates; otherwise,\nwe iterate through the feature specification stage again. Pseudocode for the full pipeline can be seen\nin Algorithm 1.\n5 Simulated Experiments\nWe evaluate ALGAE in both simulated and real-world robot environments. To highlight the flexibil-\nity of ALGAE across diverse feature types and high-dimensional control, our domains include 2D\nmaze navigation tasks and a 7DoF JACO tabletop manipulation task, along with real-world mobile\nmanipulation tasks with a Spot in Section 6. We use GPT-4o [24] to identify salient missing features\nin all experiments.\n52D NavigationJACOThrowAwayRag\nA\nB\nCGetWateringCanWaterPlant\nFigure 2: We evaluate on both simulated and real-world domains with a variety of missing features.\nA: 2D maze navigation, where the robot must navigate to a goal while interacting with other objects.\nB: 7DoF JACO manipulation, where the JACO arm must manipulate a held coffee mug while re-\nspecting features like end effector orientation .C: Spot mobile manipulation tasks, where the robot\nmust complete tasks like WaterPlant while respecting the height of pot .\n5.1 Experiment Setup\nDomain 1: 2D Navigation. Inspired by the AI Safety Gridworlds [25], where under-specified fea-\ntures can implicitly lead to reward hacking behavior [26], we evaluate ALGAE on the following 2D\nmaze navigation environments: GridRobot, Lavaland, and Island (Fig. 2). Trajectories are sequences\nof 9 states beginning with the start and ending at the goal. The 18-dimensional input consists of the\nxycoordinate positions of the robot in the grid at each timestep. The action space is the four coor-\ndinate directions. Each scenario is a 5-by-5 2D grid consisting of an agent, a goal the agent must\nnavigate to, and possible objects the agent must interact with. In GridRobot, the agent must avoid\nthe (1) Stove ; in Lavaland, the agent must avoid (2) Lava ; in Island, the agent needs to drink (move\nto) (3) Water . To ensure there is the possibility of over-specifying features, we also randomly place\nnon-relevant objects that do not impact the true reward in the environment (e.g., Ball).\nDomain 2: 7DoF Tabletop Manipulation. To assess a higher-dimensional state and action space,\nwe also evaluate a 7DoF JACO robot arm tasked with manipulating a coffee cup on a tabletop in a\nPyBullet simulator (see Fig. 2.) Each scenario is initialized with a starting and goal pose location, a\nlaptop placed on the tabletop, and a human by the table. The robot must manipulate the coffee cup\nit is carrying to a specific goal location while respecting user preferences for motion. Trajectories\nare length 21, and each state has 97 dimensions: the xyzpositions of all robot joints and angles, and\nxyzobject positions along with rotation matrices. We introduce four scenarios based on the relevant\nhuman preference: (1) Laptop :xy-plane distance of the EE to the laptop, to ensure the cup does not\npass over the laptop, (2) Table :z-distance of the EE to the table, to keep the mug of height close to\nthe table, (3) Orientation : EE orientation relative to upright, to ensure the cup does not spill, and (4)\nHuman :xy-distance of the EE to the human, to ensure the cup does not collide with the human.\nTest Scenarios. To study whether our learned reward functions generalize across different scenar-\nios related to spurious environment features, we construct two possible distribution shifts for each\ndemonstrated scenario: (1) different trajectory goals ( Goal ) (e.g., the robot must move to a different\ngoal location) and (2) different feature object locations ( Object ) (e.g., the laptop changes location\nin the scene). These are intended to test whether the learned reward captures true salient environ-\nment features rather than spurious feature correlations. We report the normalized reward of the\noptimized trajectory (against the reward of the ground truth best and worst trajectories) on a set of\nfive randomly sampled test scenarios across three seeds.\nComparisons. A straightforward way to learn rewards from demonstrations is to learn directly from\nthe state space. We refer to this baseline as (1) Full-State . We would expect such a baseline to fail to\ngeneralize to novel environments since the learned reward may be prone to overfitting spurious state\ncorrelations in the demonstrations, rather than true salient features in the environment. However,\nif access to a language prior was given without contrastive demonstrations (such that there is no\ndemonstrated trajectory of the under-specified feature), we could query for all hypothesized features\nat once as (2) LM-Feature . This baseline is intended to isolate the impact of the language model\nproposing features given only the task description and environment context, and can be seen as\na prompting-only baseline. For an additional prompting-only baseline, we also evaluate (3) LM-\n6GridRobot Lavaland Island JACO\nStove Lava Water Laptop Table Orientation Human\nGoal ALGAE 1.00 (0.00) 1.00 (0.00) 0.99 (0.01) 0.90 (0.06) 0.68 (0.12) 0.73 (0.08) 0.66 (0.12)\nFull-State 0.49 (0.06) 0.60 (0.06) 0.52 (0.06) 0.40 (0.05) 0.37 (0.07) 0.35 (0.03) 0.39 (0.01)\nLM-Feat 0.85 (0.02) 0.86 (0.05) 0.86 (0.08) 0.53 (0.05) 0.51 (0.16) 0.58 (0.10) 0.54 (0.04)\nLM-Rew 0.81 (0.04) 0.83 (0.10) 0.72 (0.09) 0.63 (0.08) 0.48 (0.17) 0.58 (0.07) 0.67 (0.01)\nRandom 0.58 (0.07) 0.62 (0.04) 0.57 (0.03) 0.37 (0.03) 0.39 (0.08) 0.36 (0.03) 0.38 (0.08)\nObj. ALGAE 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.93 (0.06) 0.65 (0.12) 0.67 (0.03) 0.53 (0.03)\nFull-State 0.62 (0.01) 0.55 (0.06) 0.44 (0.05) 0.38 (0.07) 0.31 (0.03) 0.31 (0.06) 0.44 (0.07)\nLM-Feat 0.77 (0.04) 0.81 (0.05) 0.90 (0.05) 0.54 (0.08) 0.45 (0.14) 0.59 (0.07) 0.50 (0.01)\nLM-Rew 0.76 (0.08) 0.81 (0.10) 0.79 (0.07) 0.61 (0.06) 0.56 (0.15) 0.58 (0.09) 0.53 (0.02)\nRandom 0.54 (0.02) 0.61 (0.06) 0.53 (0.02) 0.38 (0.02) 0.34 (0.09) 0.36 (0.02) 0.38 (0.08)\nTable 1: Normalized ground truth reward of the optimized trajectories produced by different methods\nacross simulated test environments and feature scenarios. Goal represents test scenarios where the\ngoal locations have changed from training and Object represents test scenarios where the relevant\nfeature has changed location from training. We report standard error across three seeds.\nReward , a baseline where the LM also proposes the reward weights in addition to the features.\nInspired by work that proposes to construct rewards directly from language [27, 28, 29], this baseline\nis intended to test the zero-shot reward specification capability of LMs. Finally, to benchmark these\nvalues, we report a (4) Random reward, which randomly samples a trajectory for each test scenario.\n5.2 Results and Analysis\nSingle feature recovery. We first assess the ability of ALGAE to recover a single missing fea-\nture. Table 1 shows results across test environments and their feature scenarios. As shown, ALGAE\noutperforms comparisons across all features. In simpler 2D environments such as GridRobot and\nLavaland, ALGAE achieves close to perfect feature recovery and generalization on unseen test sce-\nnarios. While the two prompting-only baselines (LM-Feature and LM-Reward) perform well above\nRandom, upon inspection of the LM rewards, they tend to produce many irrelevant features which\nresult in over-parameterized reward functions. In the more complex JACO scenarios, ALGAE con-\nsistently outperforms baselines. Its performance is close to the prompting-only baselines on the\nOrientation task, which is unsurprising as there is no explicit penalty for over-parameterizing the\nreward (i.e., the robot can keep the cup upright while optimizing for additional features).\nIterative feature recovery. We next study the case where more than one feature is under-specified in\na scenario, and ALGAE must recover them iteratively. In these experiments, the training scenario is\ninstantiated with two missing features ( Water andDinosaur forIsland ) and ( Laptop andOrientation\nforJACO ). As seen in Fig. 3, ALGAE iteratively improves reward given multiple missing features.\n6 Real-World Experiments\nTo assess the feasibility of our approach on robotic hardware operating in cluttered human environ-\nments, we now evaluate ALGAE\u2019s performance on mobile manipulation tasks with a Spot robot.\nDomain 3: Spot Mobile Manipulation. Spot is a legged robot equipped with a manipulator (arm\nwith gripper) and six RGB-D cameras (one in gripper, two in front, one on each side, one in back).\nEach scenario is initialized with a starting and goal robot pose location, along with the following\nobjects: watering can ,plant pot ,apple ,towel , and textbook . Trajectories are length 5 and each\nstate consists of 10 dimensions: the xyz positions of the robot and its hand, as well as rotation of\nthe gripper. We introduce the following three scenarios and their relevant reward preferences: (1)\nGetWateringCan (xy-distance of the robot to the watering can), to ensure the robot is positioned to\nexecute a successful grip, (2) WaterPlant (z-distance of the robot hand to the demonstrated watering\nheight above pot), to ensure the hand is adjusting to the pot size, and (3) ThrowAwayRag (xy-distance\nof the hand to the textbook), to ensure the robot does not drip water from the carried rag.\n7GetWateringCanWaterPlantThrowAwayRagScenario0.00.20.40.60.81.0RewardSpot\nA\nB\n123Iteration0.00.10.20.30.40.50.60.70.80.91.0RewardIsland\nALGAE (Ours)LM FeatureLM RewardRandom123Iteration0.00.10.20.30.40.50.60.70.80.91.0RewardJACO\nALGAE (Ours)LM FeatureLM RewardRandomFigure 3: A: Normalized reward across multiple iterations in simulated domains. ALGAE (pink)\nimproves across each iteration, continuously finding under-specified features and updating its reward\nestimate. In contrast, prompting-only baselines such as LM-Feature (gray) and LM-Reward (black)\ndo not iteratively improve themselves after instantiation. B: Normalized reward on Spot domains.\nReal-world clutter introduced in the scene leads to over-parameterized LM-only rewards, increasing\nthe gap between ALGAE and baselines. Error bars depict standard error across three seeds.\nData collection. For each scenario, a human teleoperates the robot from the starting to goal pose\nwhile satisfying the hidden preference (e.g. raise the robot hand to a particular height above the pot\nbefore performing a watering motion), and generates three demonstrations per scenario. We also\ncollect all detected object identities and positions via image segmentation from the robot cameras\n[30, 31] and captioning [32] as environment context for the demonstrations.\nTest scenarios and comparisons. We evaluate on the following test scenarios: for GetWateringCan ,\nwe vary the location of the watering can before executing a grip, for WaterPlant , we vary the height\nof the plant pot before executing a pour, and for ThrowAwayRag , we vary the location of the textbook\non the table the robot hand must avoid while carrying the wet rag. To ensure the robot does not\nexecute a drastically wrong action given an incorrectly learned reward, we constrain the sample\nspace of the robot pose at test time around safe zones. We report the normalized reward of the\noptimized trajectory on a set of three test scenarios across three seeds. We report comparisons with\nthe previous best performing method ( LM-Reward ) along with a Random benchmark.\nResults and Analysis. ALGAE outperforms baselines on real-world scenarios (Fig. 3). This gap\nactually increases relative to simulated environments due to the real-world clutter introduced in the\nscene. Upon visual inspection of the LM-Reward features, we indeed see unnecessary features (e.g.\nrelative position of the robot to allobjects in the scene, not just the salient ones) included. This\nunderscores the usefulness of ALGAE\u2019s interactive, iterative approach to feature construction.\n7 Discussion and Limitations\nALGAE is an iterative reward learning framework that alternates between specifying missing fea-\ntures with a LM and weighting them into a reward via IRL. One limitation is that we used the\nground truth feature vector to judge task completion on the contrasting pair of trajectories; future\nwork might evaluate a true human-in-the-loop scenario, and study whether LM-informed priors have\nadditional benefits beyond the performance improvements evaluated here. Moreover, we focus on\nreward functions that are linear in the feature set; in the future, we can imagine extending our method\nfor nonlinear rewards. Finally, ALGAE is only effective for cases where the LM prior agrees with\nthe desired behavior \u2013 which may not always be true in practice.\n8Acknowledgements\nWe thank the MIT Learning and Intelligent Systems Group for providing robotic hardware resources\nused in this project. We thank members of the Language and Intelligence and Interactive Robotics\nGroups for helpful feedback and discussions. Andi Peng is supported by the NSF Graduate Re-\nsearch Fellowship and Open Philanthropy. Belinda Li is supported by National Defense Science\nand Engineering Graduate Fellowships. Ilia Sucholutsky is supported by a NSERC Fellowship\n(567554-2022). This work was partially supported by the National Science Foundation under grant\nIIS-2212310.\nReferences\n[1] D. Hadfield-Menell, S. Milli, P. Abbeel, S. J. Russell, and A. Dragan. Inverse reward design.\nAdvances in neural information processing systems , 30, 2017.\n[2] A. Bobu, M. Wiggert, C. Tomlin, and A. D. Dragan. Feature expansive reward learning:\nRethinking human input. In Proceedings of the 2021 ACM/IEEE International Conference on\nHuman-Robot Interaction , pages 216\u2013224, 2021.\n[3] A. Bobu, M. Wiggert, C. Tomlin, and A. D. Dragan. Inducing structure in reward learning by\nlearning features. The International Journal of Robotics Research , 41(5):497\u2013518, 2022.\n[4] A. Y . Ng and S. Russell. Algorithms for inverse reinforcement learning. In Proceedings of the\nInternational Conference on Machine Learning , 2000.\n[5] C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via\npolicy optimization. In International conference on machine learning , pages 49\u201358. PMLR,\n2016.\n[6] C. Finn, P. Christiano, P. Abbeel, and S. Levine. A connection between generative adver-\nsarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint\narXiv:1611.03852 , 2016.\n[7] J. Ho and S. Ermon. Generative adversarial imitation learning. Advances in neural information\nprocessing systems , 29, 2016.\n[8] S. Levine, Z. Popovic, and V . Koltun. Feature construction for inverse reinforcement learning.\nAdvances in neural information processing systems , 23, 2010.\n[9] J. E. Fan, R. D. Hawkins, M. Wu, and N. D. Goodman. Pragmatic inference and visual ab-\nstraction enable contextual flexibility during visual communication. Computational Brain &\nBehavior , 3:86\u2013101, 2020.\n[10] M. K. Ho, D. Abel, T. L. Griffiths, and M. L. Littman. The value of abstraction. Current\nOpinion in Behavioral Sciences , 2019.\n[11] P. Abbeel and A. Y . Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In\nProceedings of the International Conference on Machine learning , 2004.\n[12] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al. Maximum entropy inverse reinforce-\nment learning. In Aaai , volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008.\n[13] T. Sumers, R. Hawkins, M. K. Ho, T. Griffiths, and D. Hadfield-Menell. How to talk so ai will\nlearn: Instructions, descriptions, and autonomy. Advances in Neural Information Processing\nSystems , 35:34762\u201334775, 2022.\n[14] A. Bobu, A. Peng, P. Agrawal, J. Shah, and A. D. Dragan. Aligning robot and human repre-\nsentations. arXiv preprint arXiv:2302.01928 , 2023.\n9[15] A. Bajcsy, D. P. Losey, M. K. O\u2019Malley, and A. D. Dragan. Learning from physical human\ncorrections, one feature at a time. In Proceedings of the 2018 ACM/IEEE International Con-\nference on Human-Robot Interaction , pages 141\u2013149, 2018.\n[16] S. A. Bowyer, B. L. Davies, and F. R. y Baena. Active constraints/virtual fixtures: A survey.\nIEEE Transactions on Robotics , 30(1):138\u2013157, 2013.\n[17] D. Rakita, B. Mutlu, and M. Gleicher. An autonomous dynamic camera method for effective\nremote teleoperation. In Proceedings of the 2018 ACM/IEEE International Conference on\nHuman-Robot Interaction , pages 325\u2013333, 2018.\n[18] J. Spencer, S. Choudhury, M. Barnes, M. Schmittle, M. Chiang, P. Ramadge, and S. Srinivasa.\nLearning from interventions: Human-robot interaction as both explicit and implicit feedback.\nIn16th Robotics: Science and Systems, RSS 2020 . MIT Press Journals, 2020.\n[19] P. Sharma, B. Sundaralingam, V . Blukis, C. Paxton, T. Hermans, A. Torralba, J. An-\ndreas, and D. Fox. Correcting robot plans with natural language feedback. arXiv preprint\narXiv:2204.05186 , 2022.\n[20] A. Peng, I. Sucholutsky, B. Li, T. Sumers, T. Griffiths, J. Andreas, and J. Shah. Learning with\nlanguage-guided state abstractions. In International Conference on Learning Representations ,\n2024.\n[21] A. Peng, A. Bobu, B. Z. Li, T. R. Sumers, I. Sucholutsky, N. Kumar, T. L. Griffiths, and\nJ. A. Shah. Preference-conditioned language-guided abstraction. In Proceedings of the 2024\nACM/IEEE International Conference on Human-Robot Interaction , pages 572\u2013581, 2024.\n[22] B. Z. Li, W. Chen, P. Sharma, and J. Andreas. Lampp: language models as probabilistic priors\nfor perception and action. arXiv preprint arXiv:2302.02801 , 2023.\n[23] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming . John\nWiley & Sons, 2014.\n[24] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,\nJ. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint\narXiv:2303.08774 , 2023.\n[25] J. Leike, M. Martic, V . Krakovna, P. A. Ortega, T. Everitt, A. Lefrancq, L. Orseau, and S. Legg.\nAi safety gridworlds. arXiv preprint arXiv:1711.09883 , 2017.\n[26] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man \u00b4e. Concrete prob-\nlems in ai safety. arXiv preprint arXiv:1606.06565 , 2016.\n[27] J. Rocamonde, V . Montesinos, E. Nava, E. Perez, and D. Lindner. Vision-language models are\nzero-shot reward models for reinforcement learning. arXiv preprint arXiv:2310.12921 , 2023.\n[28] Y . J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y . Zhu, L. Fan, and\nA. Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv\npreprint arXiv:2310.12931 , 2023.\n[29] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh. Reward design with language models. arXiv\npreprint arXiv:2303.00001 , 2023.\n[30] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead,\nA. C. Berg, W.-Y . Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023.\n[31] N. Kumar, T. Silver, W. McClinton, L. Zhao, S. Proulx, T. Lozano-P \u00b4erez, L. P. Kaelbling, and\nJ. Barry. Practice makes perfect: Planning to learn skill parameter policies. arXiv preprint\narXiv:2402.15025 , 2024.\n10[32] X. Zhou, R. Girdhar, A. Joulin, P. Kr \u00a8ahenb \u00a8uhl, and I. Misra. Detecting twenty-thousand classes\nusing image-level supervision. In European Conference on Computer Vision , pages 350\u2013368,\n2022.\n11A Full Prompts\nWe provide the prompts used for both the feature query in all domains.\n2D Navigation.\nAn agent is in a 2D gridworld environment of size 5x5. (0,0) is the top left\ncorner and (4,4) is the bottom right corner (note this is not a typical coordinate\ngrid).\nThe environment config has the following attributes: X (int): width of the\nenvironment Y (int): height of the environment starts (List[Tuple[int]]):\nstarting coordinate of the agent goals pos (List[Tuple[int]]): coordinate position\nof the goal goals color (str): color of the goal goals type (str): object\ntype of the goal objects {1-3}pos (List[Tuple[int]]): position(s) of other\nobjects in the environment. This will be empty if there is no object in\nthat slot. objects {1-3}color (str): color of the corresponding object at the\nposition objects {1-3}. This will be empty if there is no object in that slot.\nobjects {1-3}type (str): object type of the corresponding object at the position\nobjects {1-3}. This will be empty if there is no object in that slot.\nThis is an environment config (disregard the extra attributes that are not\nspecified above): {env}\nThe following is a demonstration the agent took in this first environment (each\ntrajectory contains x,y coordinates for the agent, 8 timesteps total). Keep track\nof where the agent\u2019s positions relative to the other objects in the environment.\nFor example, [0,0 0,1 0,2 0,3] means the agent started at [0,0] and moved to\n[0,3]. If there is an object at [0,3], the agent moved towards that object.\ndemonstration = {demo}\nThe following is an (incorrect) demonstration that the agent took in this\nenvironment. The trajectory is the same format as the first demonstration.\nincorrect trajectory = {best traj}\nDescribe the demonstration and trajectory in the context of the environment they\nare in, then use this to give the specific feature in that environment that\nexplains what the demonstration did better. Reason step by step and think about\nwhy the agent might be moving in the way that it is. If there are multiple of\nthe same object, please give the average distance to all objects of that type.\nPlease specify a feature (only one feature e.g. if there are multiple possible\nexplanations choose the most likely object) that we can write code to compute\nfrom the position of the agent to other objects in the"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "Certainly! Below is a comprehensive summary of the academic paper titled \"Graph Laplacian-Based Bayesian Multi-Fidelity Modeling\" that captures all the essential details needed to recreate the study from scratch.\n\n---\n\n### Abstract:\nThis paper presents a novel probabilistic method for generating multi-fidelity data while accounting for errors in both low- and high-fidelity data. A graph Laplacian constructed from low-fidelity data is used to define a multivariate Gaussian prior density for the true data points' coordinates. Few high-fidelity data points then construct a conjugate likelihood term. Bayes' rule is applied to derive the posterior density, also a multivariate Gaussian. The maximum a posteriori (MAP) estimate serves as the optimal multi-fidelity estimate, determined through linear systems of equations. Two efficient methods, spectral truncation and low-rank approximation, are introduced to solve these equations. The approach is validated using problems in solid and fluid mechanics, showing significant improvements using a small fraction of high-fidelity data.\n\n### Introduction:\n- **Context:** Modern computational engineering tasks like optimization, uncertainty quantification, sensitivity analysis, and optimal design require extensive simulations. These rely on models varying in fidelity and cost.\n- **Models:** High-fidelity models (accurate but costly) contrast with low-fidelity models (less accurate but computationally inexpensive).\n- **Problem:** Relying solely on high-fidelity models is prohibitive, while low-fidelity models may lack accuracy. Multi-fidelity methods aim to balance accuracy and cost.\n- **Objective:** To develop a method combining low-fidelity and selective high-fidelity data to improve the overall data accuracy cost-effectively.\n- **Literature Review:** The paper reviews multi-fidelity methods, uncertainty quantification approaches, co-kriging methods, radial basis functions, and neural network-based methods. It highlights their strengths and limitations.\n\n### Related Work:\n- **Multi-fidelity Methods:** These utilize correlations between different fidelities to construct more accurate models.\n- **Uncertainty Techniques:** Techniques inspired by multilevel Monte Carlo and others are used to compute statistics leveraging multiple models.\n- **Co-kriging and Surrogate Models:** Co-kriging involves representing low-fidelity and discrepancies as Gaussian processes. Surrogates often use radial basis functions.\n- **Neural Networks:** Deep learning fits low-fidelity data and refines it using high-fidelity data via transfer learning or physics-informed networks.\n- **Order-Reduction Methods:** These align low- and high-fidelity Proper Orthogonal Decomposition (POD) manifolds for reduced-dimension approximations.\n\n### Method:\n- **Low-fidelity Data and Graph Laplacian:**\n  - **Graph Construction:** Constructs a weighted graph from low-fidelity data using a Gaussian kernel.\n  - **Graph Laplacian:** Normalized graph Laplacian \\( L = D^{-1/2}(D - W)D^{-1/2} \\), where \\(W\\) is the adjacency matrix and \\(D\\) is the degree matrix.\n\n- **Bayesian Inference Setup:**\n  - **Prior Density:** Low-fidelity data defines a Gaussian prior centered at the data points with covariance \\( (L + \\tau I)^{-\\beta} \\).\n  - **Likelihood:** High-fidelity data introduces a likelihood assuming an additive Gaussian error model.\n  - **Posterior Density:** Combines the prior and likelihood, leading to a multivariate Gaussian posterior density.\n\n- **MAP Estimate Calculation:**\n  - **Formulation:** The MAP estimate is the mode of the posterior distribution, derived through solving a linear system.\n  - **Algorithms:** Efficient solvers based on spectral truncation and low-rank Nystr\u00f6m approximation handle the linear systems.\n\n### Experimental Setup:\n- **Datasets:** Problems from linear elasticity, Darcy flow, and Navier-Stokes equations.\n- **Hardware/Software:** Detailed specification not provided, but computational complexity is analyzed.\n- **Evaluation Metrics:** Not explicitly mentioned, but accuracy improvements over low-fidelity data are emphasized.\n- **Baselines:** Comparison with standard low-fidelity and existing multi-fidelity methods.\n\n### Results and Analysis:\n- **Empirical Findings:**\n  - **Accuracy Improvements:** Multi-fidelity estimates significantly enhance the accuracy using a small number of high-fidelity points.\n  - **Benchmarks:** Results demonstrated on various one- and two-dimensional spatial fields.\n  - **Graphical Representations:** Results are visualized, showing improvements over baseline low-fidelity data.\n- **Ablation Studies:** Analyses to study the impact of various components and hyper-parameters.\n\n### Discussion:\n- **Result Interpretation:** The multi-fidelity approach is validated to improve accuracy and computational efficiency.\n- **Implications:** This method's probabilistic nature provides both point estimates and uncertainty measures, useful for downstream applications and adaptive learning.\n- **Limitations & Bias:** The paper discusses method limitations such as computational complexity in large datasets and potential biases in the prior or likelihood choices.\n\n### Conclusion:\n- **Key Contributions:** Development of a computationally efficient multi-fidelity method using graph Laplacian and Bayesian inference. \n- **Future Directions:** Suggestions include exploring different graph constructions, extending to other application domains, and integrating with more complex neural network architectures.\n\n### Key Methodological Details:\n1. **Graph Construction:** \n   - Data normalization strategies.\n   - Weight computation via Gaussian kernels.\n   - Graph Laplacian formulation and eigen-decomposition.\n2. **Prior and Likelihood Construction:**\n   - Prior density using graph structure.\n   - Likelihood term based on high-fidelity error model.\n3. **Posterior Estimation:**\n   - Bayesian update combining prior and likelihood.\n   - Solution of linear systems to find MAP estimate.\n4. **Hyper-parameter Tuning:**\n   - Selection of \\( \\tau \\), \\( \\beta \\), and \\( \\omega \\).\n   - Empirical validation without requiring additional high-fidelity data.\n5. **Numerical Techniques:**\n   - Spectral truncation for efficient computation.\n   - Low-rank Nystr\u00f6m approximation for large-scale problems.\n\n### Comprehensive Findings:\n- **Illustrative Examples:** Solid mechanics and fluid dynamics problems.\n- **Quantitative Improvements:** Significant accuracy gains demonstrated in empirical tests.\n- **Computational Savings:** Methods providing substantive computational efficiency against exact high-fidelity models.\n\n### Replication Instructions:\n1. **Data Preparation:** Assemble low- and high-fidelity datasets for your domain.\n2. **Graph Laplacian Computation:** Construct graph and compute its Laplacian.\n3. **Prior & Likelihood Setup:** Define probabilistic models based on the graph structure.\n4. **Posterior Calculation:** Implement methods to estimate the posterior.\n5. **Hyper-parameter Optimization:** Calibrate model parameters according to given strategies.\n6. **Validation & Testing:** Comparative analysis with established methods.\n\n---\n\nThis summary encapsulates all the necessary details and context to replicate the study comprehensively. It covers conceptual frameworks, technical methodologies, empirical evaluations, and critical analyses\u2014all central to rebuilding or building upon this work."}, {"role": "assistant", "content": "GRAPH LAPLACIAN -BASED BAYESIAN MULTI -FIDELITY\nMODELING\nOrazio Pinti\nDepartment of Aerospace and Mechanical Engineering\nUniversity of Southern California\nLos Angeles, CA, USA\npinti@usc.eduJeremy M. Budd\nSchool of Mathematics\nUniversity of Birmingham\nBirmingham, UK\nj.m.budd@bham.ac.uk\nFranca Hoffmann\nComputing and Mathematical Sciences\nCalifornia Institute of Technology\nPasadena, CA, USA\nfranca.hoffmann@caltech.eduAssad A. Oberai\nDepartment of Aerospace and Mechanical Engineering\nUniversity of Southern California\nLos Angeles, CA, USA\naoberai@usc.edu\nABSTRACT\nWe present a novel probabilistic approach for generating multi-fidelity data while accounting for\nerrors inherent in both low- and high-fidelity data. In this approach a graph Laplacian constructed\nfrom the low-fidelity data is used to define a multivariate Gaussian prior density for the coordinates\nof the true data points. In addition, few high-fidelity data points are used to construct a conjugate\nlikelihood term. Thereafter, Bayes rule is applied to derive an explicit expression for the posterior\ndensity which is also multivariate Gaussian. The maximum a posteriori (MAP) estimate of this\ndensity is selected to be the optimal multi-fidelity estimate. It is shown that the MAP estimate and\nthe covariance of the posterior density can be determined through the solution of linear systems of\nequations. Thereafter, two methods, one based on spectral truncation and another based on a low-rank\napproximation, are developed to solve these equations efficiently. The multi-fidelity approach is\ntested on a variety of problems in solid and fluid mechanics with data that represents vectors of\nquantities of interest and discretized spatial fields in one and two dimensions. The results demonstrate\nthat by utilizing a small fraction of high-fidelity data, the multi-fidelity approach can significantly\nimprove the accuracy of a large collection of low-fidelity data points.\nKeywords Multi-fidelity modeling \u00b7Bayesian inference \u00b7Uncertainty quantification \u00b7Semi-supervised learning \u00b7\nGraph-based learning.\n1 Introduction\nIn numerous computational engineering tasks such as optimization, uncertainty quantification, sensitivity analysis, and\noptimal design, it is often necessary to conduct extensive simulations of a physical system. These simulations rely on\nmodels that approximate the true input-output relations of the system. Inputs typically include parameters or prescribed\ndata like initial conditions, boundary conditions and forcing functions, while outputs are the resulting quantities or\nfields of interest.\nEvaluating a model involves its numerical implementation and execution, a process that varies in fidelity and cost.\nFidelity refers to the accuracy of the model prediction relative to the true system behavior, whereas cost quantifies the\nrequired computational resources. Generally, higher fidelity is associated with greater cost. In practical scenarios, it\nis common to have access to multiple models of a system, or to be able to tune some hyper-parameter, like the mesh\nsize, or the time-integration step, of a given model to increase its accuracy (and cost). For instance, in computationalarXiv:2409.08211v1  [cs.LG]  12 Sep 2024Graph Laplacian-based Bayesian Multi-fidelity Modeling\nfluid dynamics, choices range from high-cost, accurate direct numerical simulations (DNS) to less expensive Reynolds-\naveraged Navier-Stokes (RANS) models. Similarly, when using the finite element method, the mesh size can be used to\ncontrol the accuracy and computational expense of the resulting model.\nA model is said to be high-fidelity if it can capture the true behavior of the system within a level of accuracy that is\nequal to or greater than the one required for the given task. If a model is not high-fidelity, it is said to be low-fidelity.\nLower-fidelity models are designed to trade some of the predictive accuracy in favor of a more competitive evaluation\ncost. This can be attained in several ways. The most common methods include: using simplified physics or making\nstronger modeling assumptions, linearizing the dynamics of the system, employing projection-based or data-fitting\nsurrogate models, or using coarser numerical discretizations. It is worth noting that a high- vs low-fidelity model pair\ncan also be represented by experiments and numerical simulations, respectively.\nCompleting a task that requires a large number of simulations solely relying on a high-fidelity model can often be\nprohibitive or impractical, while employing only lower-fidelity models might not lead to results that are accurate enough.\nThis is where multi-fidelity methods are useful. The goal of a multi-fidelity method is to retain as much of the accuracy\nof high-fidelity model as possible, while incurring a fraction of the cost by leveraging lower-fidelity models. In a typical\nmulti-fidelity framework, low-fidelity data is generated to explore the input space, and obtain an approximation of the\nresponse of the system. Then, a limited number of high-fidelity data points are computed or measured, and techniques\nthat learn the response from the low-fidelity data, and improve it by using the high-fidelity data are applied.\nMulti-fidelity methods have been widely used in optimization [ 1,2], uncertainty quantification [ 3], uncertainty\npropagation [ 4,5], and statistical inference [ 6] (see Fern\u00e1ndez-Godino et al. [ 7] and Peherstofer et al. [ 5] for two\ncomprehensive reviews).\nIn uncertainty quantification, the parameter inputs are typically modeled as random variables drawn from a prescribed\ndistribution, and the main objective is to compute the statistics of the output. When multiple models for the output\nare available, one can leverage the correlation between different approximations. The correlation coefficients act\nas auxiliary random variables, whose statistics are easier to compute. Inspired by multilevel Monte Carlo methods\n[8,9], various techniques have been developed to construct an unbiased estimator for the mean and variance of the\nhighest-fidelity model output, leveraging lower-fidelity approximations [ 4,5]. For instance, Geraci et al. [ 10] proposed\na multi-fidelity multi-level method, considering multiple models with different levels of accuracy at the same time.\nSimilarly, Peherstorfer et al. [ 5] suggested an optimal strategy for distributing computational resources across different\nmodels to minimize the variance of a multi-fidelity estimator.\nIn co-kriging methods [ 11,12,13,14] the multi-fidelity response is expressed as a weighted sum of two Gaussian\nprocesses, one modeling the low-fidelity data, and the other representing the discrepancy between the low- and high-\nfidelity data. The parameters of the mean and correlation functions of these processes are determined by maximizing\nthe log-likelihood of the available data. Co-kriging has also been extensively investigated in the context of multi-fidelity\noptimization [ 15,16,17,18]. Other methods make use of radial basis functions (RBFs) to model the low-fidelity\nresponse. Specifically, the low-fidelity surrogate is written as an expansion in terms of a set of RBFs, and the coefficients\nare determined by interpolating the available low-fidelity data. The multi-fidelity approximation is then obtained in\ndifferent ways. These include determining a scaling factor and a discrepancy function, which can be modeled using a\nkriging surrogate [ 14], or another expansion in terms of RBFs [ 19,20]. In some cases the multi-fidelity surrogate is\nconstructed by mapping the low-fidelity response directly to the high-fidelity response [21].\nMore recently, deep neural networks have been used to fit low-fidelity data and learn the complex map between\nthe input and output vectors in the low-fidelity model. Then, the relatively small amount of high-fidelity data is\nused in combination with techniques such as transfer learning [ 22,23], embedding the knowledge of a physical law\nthrough physics-informed loss functions [ 24,25,26,27], or, in the case of multiple levels of fidelity, concatenating\nmultiple neural networks together to yield a multi-fidelity model [ 28]. An approach that involves training a physics-\nconstrained generative model, conditioned on low-fidelity snapshots to produce solutions that are higher-fidelity and\nhigher-resolution, has also been proposed [ 29]. Similarly, a bi-fidelity formulation of variational auto-encoders, trained\non low- and high-fidelity data has been developed to generate bi-fidelity approximations of quantities of interests and\nestimate their uncertainty [30].\nAnother class of methods, suitable when the response of the system consists of a high-dimensional vector, first performs\norder-reduction using low-fidelity data, and then injects accuracy using high-fidelity data in a reduced-dimensional\nlatent space. This is accomplished by computing the low- and high-fidelity Proper Orthogonal Decomposition (POD)\nmanifolds, aligning them with each other, and replacing the low-fidelity POD modes with their high-fidelity counterparts\n[31]. Similar multi-fidelity surrogate models have been developed by first solving a subset selection problem to construct\n2Graph Laplacian-based Bayesian Multi-fidelity Modeling\na surrogate model of the low-fidelity response in terms of a few important snapshots, then generating the high-fidelity\ncounterparts of the important snapshots, and finally using these in the multi-fidelity model [32, 33, 34].\nIn this work we present a novel multi-fidelity approach based on the spectral properties of the graph Laplacian. The\nstarting point of this method is the generation of a large number of low-fidelity data. Thereafter, each data point is\ntreated as a node of a weighted graph, where the weights on the edges are determined by the distance between the\nnodes. A normalized graph Laplacian and its eigendecomposition are then evaluated, and the nodes are embedded in the\neigenfunction space. The data are clustered and the points closest to the clusters centroids are identified. Thereafter, the\nhigh-fidelity counterparts of these points are evaluated. The problem of computing a multi-fidelity approximation using\nthe low- and high-fidelity data is posed as a Bayesian inference problem. Within this framework, the prior probability\ndistribution of the multi-fidelity data is defined to be a normal distribution centered at the low-fidelity points, with\na covariance from the inverse of the graph Laplacian. The likelihood term is derived by assuming a simple additive\nGaussian model for the error in the high-fidelity data. Given these choices, it is shown that the multi-fidelity data is also\ndrawn from a multi-variate Gaussian distribution whose mean can be determined by solving a linear system of equations.\nEfficient algorithms for solving this linear system as well as determining the covariance matrix for the multi-fidelity\ndata are described. The new multi-fidelity method is then applied to several problems drawn from linear elasticity,\nDarcy flow, and Navier-Stokes equations. It is used to improve the accuracy of vector quantities of interest, and spatial\nfields defined in one and two spatial dimensions. In each case it is observed that a small number of high-fidelity data\nleads to a multi-fidelity dataset that is significantly more accurate than its low-fidelity data counterpart.\nThe approach described in this study has strong connections with semi-supervised classification algorithms on graphs\n[35,36,37,38,39], and relies on theoretical results on consistency of graph-based methods in the limit of infinite data\n[40,41]. The results presented in this study may be considered as a probabilistic extension of the ideas developed for\nthe SpecMF method [ 42]. They also improve on those ideas in several ways. First, in addition to providing a point\nestimate for the multi-fidelity data, they also provide a measure of uncertainty associated with it. This measure may be\nused for downstream tasks like adaptive or active learning, or as demonstrated in this manuscript, it may be used to\ndetermine an important hyperparamater of the method, thereby obviating the need of validation data. Second, through\nthe use of conjugate priors and likelihood terms we ensure that the point estimate is obtained though the solution of a\nlinear system of equations, as opposed to a nonlinear system that is solved in the SpecMF method. Finally, the proposed\napproach is applied to problems where each data point is a discrete representation of spatial fields defined on one- and\ntwo-dimensional grids, with dimensions as high as O(104).\nThe layout of the remainder of this manuscript is as follows. In Section 2, we define the multi-fidelity problem and\ndescribe the proposed methodology. In Section 3, we detail efficient numerical techniques to approximate the graph\nLaplacian and evaluate the posterior distribution of the multi-fidelity estimates. In Section 4, we provide a theoretical\nresult on convergent regularization. In Section 5, we present a comprehensive set of numerical experiments that quantify\nthe performance of the method, and, finally, in Section 6 we conclude with final remarks. Appendix A discusses\ngeneralizations of the employed graph Laplacian framework for different choices of normalizations.\n2 Problem formulation\nWe are interested in solving the problem of determining a quantity of interest u\u2208RDas a function of input parameters\n\u00b5\u2208RPsampled from a distribution P\u00b5(\u00b5). We assume that we do not have direct access to the underlying true values\nu, but for each input parameter value \u00b5(i)we have access to two models to estimate it. One of these is the high-fidelity\nmodel whose estimate, denoted by \u02c6u(i), is accurate but computationally expensive. The other is the low-fidelity model,\nwhose estimate, denoted by \u00afu(i), is less accurate but computationally inexpensive.\nOur goal is to combine a large set of low-fidelity data points \u00afD={\u00afu(i)}N\ni=1, with a smaller set of select high-fidelity\npoints, \u02c6D={\u02c6u(i)}M\ni=1, with M\u226aN, to generate a multi-fidelity estimate for all points in \u00afD. Following the SpecMF\nmethod [ 42], our approach exploits the spectral properties of the graph Laplacian constructed from the low-fidelity data\npoints to identify the key input parameter values at which to acquire the high-fidelity data, and to combine the low-\nand high-fidelity data to construct a probabilistic multi-fidelity estimate. This approach leverages the graph Laplacian\ncomputed using the low-fidelity data to define a prior distribution for the multi-fidelity estimates. Thereafter, it utilizes\nthe few select high-fidelity data points to construct a likelihood term. The prior and the likelihood terms are chosen to\nbe Gaussian conjugate pairs, which yields a Gaussian distribution for the posterior density as well. In what follows, the\naforementioned steps are discussed in detail.\n3Graph Laplacian-based Bayesian Multi-fidelity Modeling\n2.1 Low-fidelity data and graph Laplacian\nAs a starting point, N\u226b1instances of parameters \u00b5(i)are sampled from the density P\u00b5(\u00b5)and for each parameter\nvalue\u00b5(i)the low-fidelity model is deployed to generate the low-fidelity data \u00afu(i)\u2208RD. These points are used to\nassemble the low-fidelity dataset \u00afD.\nDepending on the application, different techniques may be used to normalize the low-fidelity data if needed. A first\napproach is to normalize every component of \u00afu(i)to have zero-mean and unit standard deviation. That is,\n\u00afu(i)\nk\u2190\u00afu(i)\nk\u2212E(\u00afuk)p\nVAR (\u00afuk), i \u2208 {1, . . . , N }, k\u2208 {1, . . . , D }, (2.1)\nwhereE(\u00b7)andVAR (\u00b7)denote the mean and variance, respectively, computed over the low-fidelity dataset. This is\nmore suitable for cases where \u00afu(i)is a set of physical quantities of interest with different scales.\nIf\u00afu(i)represents a discretized field, a more appropriate alternative is to normalize each instance as\n\u00afu(i)\u2190\u00afu(i)\n\u2225\u00afu(i)\u2225, i \u2208 {1, . . . , N }, (2.2)\nwhere \u2225 \u00b7 \u2225 is a suitable norm. We note that once a normalization procedure for the ithdata point is determined, it is\napplied to both the low- and high-fidelity data (Section 2.3).\nThereafter, a complete weighted graph Gwith nodes Vand edges Eis constructed from the low-fidelity data. Specifically,\nthe graph has Nnodes i\u2208 V:={1,2, . . . , N }, equipped with attributes given by the corresponding low-fidelity data\npoints, \u00afu(i). The weight Wijassociated with each edge Eij\u2208 E, which measures the similarity between the nodes i\nandj, is computed via a monotonically decreasing function of the Euclidean distance between \u00afu(i)and\u00afu(j). In this\nwork, we use a Gaussian kernel, and the resulting weights are given by\nWij:=(\nexp\u0010\n\u2212\u2225\u00afu(i)\u2212\u00afu(j)\u22252\n2\n\u21132\nij\u0011\n,ifi\u0338=j,\n0, ifi=j.\nThis leads to a fully connected graph, but other choices leading to sparser graphs are also common. Choosing the\nweights Wijplays an important role in the behavior of any graph Laplacian-based algorithm, an aspect we are not\nfocusing on in this work. The scale \u2113ijcan be seen as an additional hyper-parameter, with a common choice being to\nset it to be constant for the entire graph, or it may be tuned. Here we use a self-tuning approach, setting \u2113ij:=p\n\u2113i\u2113j\nand tuning \u2113iby examining the statistics of the neighborhood of i, following Zelnik-Manor and Perona [ 43]. We remark\nthat all our analysis also applies for any other choice of weights satisfying Wij=Wji,Wij\u22650, and Wii= 0.\nThe weights Wijform the components of the adjacency matrix W\u2208RN\u00d7N, which is used to compute the (diagonal)\ndegree matrix D,\nDii:=NX\nj=1Wij,\nand a family of graph Laplacians,\nL:=D\u2212p(D\u2212W)D\u2212q. (2.3)\nDifferent choices of pandqresult in different normalizations of the graph Laplacian [ 44,45,46]. For simplicity,\nin the body of this work we will assume that p=q, i.e.,Lis symmetrically normalized; however, our framework\nextends to the p\u0338=qcase (for details see Appendix A). The graph Laplacian embeds many important properties of the\nstructure and topology of the graph. In particular, the eigenvectors of Lcorresponding to small eigenvalues provide an\nembedding of the graph that promotes the clustering of vertices that are strongly connected [42, 47].\n2.2 Construction of prior density\nWe assume that the low-fidelity data \u00afDrepresents an approximation of the true data, and denote the difference between\nthe (unknown) true data u(i)and the observed low-fidelity data \u00afu(i)by\n\u03d5(i):=u(i)\u2212\u00afu(i), i \u2208 {1, . . . , N }.\n4Graph Laplacian-based Bayesian Multi-fidelity Modeling\nThis relation is shown graphically in Figure 1. In the equation above \u03d5(i)may be interpreted as the displacement vector\nfrom a given low-fidelity data point to its true counterpart. Our goal is to write a prior probability distribution for the\ndisplacements \u03d5(i)\u2208RD. In order to accomplish this we define the displacement matrix\n\u03a6:=\uf8eb\n\uf8ec\uf8ed\u03d5(1)\n...\n\u03d5(N)\uf8f6\n\uf8f7\uf8f8= (\u03d51\u00b7\u00b7\u00b7\u03d5D)\u2208RN\u00d7D,\nwhere a superscript on \u03d5indicates a row vector, while a subscript indicates a column vector. We note that while \u03d5(i)\nrepresents the displacement vector for the ithdata point, the vector \u03d5irepresents the displacement field for the ith\ncomponent of the data.\nFor any vector v\u2208RN, the quantity1\n2vTLvis referred to the as the Dirichlet energy associated with the graph\nLaplacian L. We construct a prior for \u03a6by stipulating that displacement fields that yield large values of the Dirichlet\nenergy are less likely. In particular, we set\np(\u03a6)\u221dexp \n\u2212DX\nm=1\u03c9\n2\u03d5T\nm(L+\u03c4IN)\u03b2\u03d5m!\n= exp\u0010\n\u2212\u03c9\n2\u27e8\u03a6,(L+\u03c4IN)\u03b2\u03a6\u27e9F\u0011\n,(2.4)\nwhere IN\u2208RN\u00d7Nis the identity matrix, \u27e8A,B\u27e9F:= tr(ATB) =P\nijAijBijis the Frobenius inner product, and\n\u03c9, \u03c4, \u03b2 > 0are hyper-parameters. More precisely, each displacement component \u03d5mis independently and identically\nGaussian distributed with zero mean and with covariance1\n\u03c9(L+\u03c4IN)\u2212\u03b2. The parameter \u03c4 >0is chosen suitably\nsmall in practice, and has been introduced to allow for this interpretation of the prior since the graph Laplacian Litself\nis not invertible.1An alternative to this approach is to only consider components \u03d5mthat are orthogonal to the kernel\nofL, thus avoiding the introduction of the additional parameter \u03c4as done in [ 48]. The parameters \u03c9(regularization\nstrength) and \u03b2(regularization exponent) are both used to control the regularity of the MAP estimator, and will be\ndescribed in detail in Subsections 2.2.1 and 2.6.\n2.2.1 Interpretation of the prior density\nSince we have assumed that p=q, the graph Laplacian Lis a symmetric positive semi-definite matrix, and so permits\nan eigenvalue decomposition of the form L=\u03a8\u039b\u03a8T, where \u039bis the diagonal matrix of eigenvalues, \u03a8is a matrix\nwhose columns are the eigenvectors, and \u03a8T\u03a8=\u03a8\u03a8T=IN.\nThe spectral decomposition of the graph Laplacian has the useful property that when the underlying data is clearly\nseparated into clusters, the eigenfunctions corresponding to the smallest eigenvalues tend to vary smoothly over the\nclusters on a coarser spatial scale, when compared with eigenfunctions corresponding to larger eigenvalues. We utilize\nthis observation to better understand the prior distribution in (2.4). In particular, we express \u03a6as a linear combination\nof the eigenvectors via the coefficients A= [Aij]\u2208RN\u00d7D, that is \u03a6=\u03a8A. Thereafter, we substitute this in (2.4) to\nobtain\np(A)\u221dexp\u0010\n\u2212\u03c9\n2tr\u0010\nAT\u03a8T\u03a8(\u039b+\u03c4IN)\u03b2\u03a8T\u03a8A\u0011\u0011\n= exp\u0010\n\u2212\u03c9\n2tr\u0010\nAT(\u039b+\u03c4IN)\u03b2A\u0011\u0011\n= exp\uf8eb\n\uf8ed\u2212\u03c9\u03c4\u03b2\n2DX\nj=1NX\ni=1A2\nij\u0012\n1 +\u03bbi\n\u03c4\u0013\u03b2\uf8f6\n\uf8f8(2.5)\nThe final expression in the equation above makes it clear that for every component of the displacement field (denoted by\nthe index jin the sum above), the prior promotes coefficients of eigenfunctions corresponding to smaller eigenvalues.\nWhen the data is arranged in clusters, these eigenfunctions have little variation over the clusters, which in turn means\nthat displacement fields that are constant over clusters are considered more likely by the prior density. In other words,\nwhile moving the low-fidelity data points to their multi-fidelity location, the prior deems that it is more likely that points\nwithin a cluster will move together. When the data is not arranged in clusters, or for data points within each cluster, the\n1\u03d5m=Dq1Nis an eigenvector of Lwith eigenvalue zero.\n5Graph Laplacian-based Bayesian Multi-fidelity Modeling\neigenfunctions corresponding to smaller eigenvalues are those that tend to vary more smoothly. This in turn implies\nthat displacements fields that vary smoothly are considered more likely. That is, the prior ensures that low-fidelity data\npoints that are close to each other will have multi-fidelity approximations that continue to be close. In summary, the\nprior ensures that multi-fidelity approximations that are consistent with the structure of the low-fidelity data are more\nlikely.\nThe analysis above also provides an intuitive interpretation of the hyper-parameters. The parameter \u03c4scales with the\neigenvalues, so that if set to be equal to the smallest non-zero eigenvalue, the problem is equivalent to considering a\nscaled spectrum with smallest non-zero eigenvalue equal to 1. The regularization exponent \u03b2controls the extent to which\nhigher-order eigenvalues are penalized; larger values of \u03b2make the contribution from the eigenfunctions corresponding\nto large eigenvalues to the displacement vector less likely. Finally, \u03c9controls the strength of the contribution of the prior\nto the posterior distribution, relative to the contribution of the likelihood term. By re-parameterizing the regularization\nstrength \u03c9=\u03ba\u03c4\u2212\u03b2,\np(A)\u221dexp\uf8eb\n\uf8ed\u2212\u03ba\n2NX\ni=1NX\nj=1A2\nij\u0012\n1 +\u03bbi\n\u03c4\u0013\u03b2\uf8f6\n\uf8f8\nwe note that the strength of the prior depends only on \u03baand is independent of \u03c4and\u03b2. This expression is convenient\nwhen comparing priors for different values of hyper-parameters.\n2.3 Selection of high-fidelity data\nIn this section, we describe the policy used to select M\u226aNhigh-fidelity data points to acquire. This is accomplished\nby utilizing the graph constructed from the low-fidelity data to determine Mclusters and their corresponding centroids.\nThereafter, the data points closest to each of these centroids are determined, and the corresponding high-fidelity data\nis acquired. The logic in selecting these points is driven by the observation that points that belong to a given cluster\nwill tend to move together from their low-fidelity coordinates to their high-fidelity coordinates. Thus the displacement\nvector for the centroid is likely to be representative of the displacement vector for most of the points in the cluster. The\nsteps in this process are:\n1. Compute the low-lying eigenfunctions of the graph Laplacian, \u03c8(m), for each m\u2208 {1, . . . , M }.\n2.Compute the coordinates of every low-fidelity data point \u00afu(i)in the eigenfunction space. That is, compute\n\u03be(i):= [\u03c8(1)\ni, . . . , \u03c8(M)\ni]for each i\u2208 {1, . . . , N }.\n3. Use k-means clustering on the points {\u03be(i)}N\ni=1to find Mclusters.\n4. For each cluster, determine the centroid and the low-fidelity data point closest to it.\n5.Re-index the low-fidelity data set \u00afDand the corresponding input parameters so that the points identified above\ncorrespond to the first Mpoints.\n6.Compute the high-fidelity data at the parameter values corresponding to these points, and assemble the data set\n\u02c6D:=\b\u02c6u(i)\tM\ni=1. Note that the elements of \u02c6Dare the high-fidelity counterparts of the first Melements of \u00afD.\n7. Scale the high-fidelity data by the same procedure used to normalize the low-fidelity data.\n2.4 Construction of the likelihood\nWe now wish to update the prior density for the displacement matrix based on the newly acquired high-fidelity\nmeasurements. This step is performed via a likelihood term within a Bayesian update. The construction of the likelihood\nterm is described next.\nFrom the high-fidelity data \u02c6D={\u02c6u(i)}M\ni=1, we construct the matrix of low-to-high-fidelity displacements \u02c6\u03d5(i):=\n\u02c6u(i)\u2212\u00afu(i),i\u2208 {1, . . . , M }(see Figure 1),\n\u02c6\u03a6:=\uf8eb\n\uf8ec\uf8ed\u02c6\u03d5(1)\n...\n\u02c6\u03d5(M)\uf8f6\n\uf8f7\uf8f8=\u0000\u02c6\u03d51\u00b7\u00b7\u00b7 \u02c6\u03d5D\u0001\n\u2208RM\u00d7D.\n6Graph Laplacian-based Bayesian Multi-fidelity Modeling\nFigure 1: Schematic representation of the low-fidelity approximation \u00afu(i), connected to the high-fidelity approximation\n\u02c6u(i)and the true data point u(i)through their respective displacement vectors, \u02c6\u03d5(i)and\u03d5(i), fori\u2208 {1, . . . , M }.\nWe assume that the error in the high-fidelity data is additive and can be modeled with a multivariate normal distribution\nwith zero mean and variance \u03c32ID, i.e.,\n\u02c6u(i)=u(i)+\u03b5(i), i \u2208 {1, . . . , M },\nwith\u03b5(i)\u223c N D(0D, \u03c32ID)i.i.d.\nRemark. One could also consider correlations between the components of the error in the high-fidelity data by\nconsidering instead \u03b5(i)\u223c N D(0D,CD)for some non-diagonal covariance matrix CD. We will not consider this case\nin this paper.\nSubtracting \u00afu(i)from both sides of the equation above we arrive at,\n\u02c6\u03d5(i)=\u03d5(i)+\u03b5(i), i \u2208 {1, . . . , M }.\nThis implies that the likelihood of observing the measurements \u02c6\u03a6given the displacement matrix \u03a6is\np(\u02c6\u03a6|\u03a6)\u221dexp \n\u2212\u2225\u02c6\u03a6\u2212PM\u03a6\u22252\nF\n2\u03c32!\n, (2.6)\nwhere PM\u2208RM\u00d7Nis the matrix that extracts the first Mrows from \u03a6, and is given by,\nPM:= (IM0M\u00d7(N\u2212M))\u2208RM\u00d7N.\n2.5 Definition of the posterior density\nThe posterior distribution of \u03a6is proportional to the product of the prior and the likelihood, that is\np(\u03a6|\u02c6\u03a6)\u221dp(\u02c6\u03a6|\u03a6)p(\u03a6)\n\u221dexp \n\u2212\u2225\u02c6\u03a6\u2212PM\u03a6\u22252\nF\n2\u03c32\u2212\u03c9\n2\u27e8\u03a6,(L+\u03c4IN)\u03b2\u03a6\u27e9F!\n=DY\nm=1exp \n\u2212\u2225\u02c6\u03d5m\u2212PM\u03d5m\u22252\n2\n2\u03c32\u2212\u03c9\n2\u03d5T\nm(L+\u03c4IN)\u03b2\u03d5m!\n,(2.7)\nwhere we have made use of (2.4) and (2.6). Since both the prior and the likelihood are normal distributions, we can\nwrite the expression above as\np(\u03a6|\u02c6\u03a6)\u221dexp\u0012\n\u22121\n2\u27e8\u03a6\u2212\u03a6\u2217,C\u22121(\u03a6\u2212\u03a6\u2217)\u27e9F\u0013\n=DY\nm=1exp\u0012\n\u22121\n2(\u03d5m\u2212\u03d5\u2217\nm)T\u00b7C\u22121(\u03d5m\u2212\u03d5\u2217\nm)\u0013\n,(2.8)\n7Graph Laplacian-based Bayesian Multi-fidelity Modeling\nwhere\nC=\u00121\n\u03c32PT\nMPM+\u03c9(L+\u03c4IN)\u03b2\u0013\u22121\n, (2.9)\n\u03a6\u2217=1\n\u03c32CPT\nM\u02c6\u03a6. (2.10)\nHence, \u03d5\u2217\nm=1\n\u03c32CPT\nM\u02c6\u03d5m. That is, the posterior distribution is also normal, and the mean for the mthcomponent\nof the low-fidelity displacement vector is given by \u03d5\u2217\nmand the covariance is given by C. Further, since for a normal\ndistribution the mean and the mode are the same, this means that \u03d5\u2217\nmis also the maximum a posteriori (MAP) estimate\nfor the mthcomponent of the low-fidelity displacement vector. This implies that the estimated posterior distribution for\nthemthcomponent of the true data is also multivariate normal. The mode of this distribution is given by \u00afum+\u03d5\u2217\nm. We\ndefine this quantity to be our multi-fidelity estimate. We note that the estimated covariance for all components of the\ntrue data is the same and is given by the matrix C.\nIt is instructive to write (2.10) as a solution to a linear system of equations. This yields,\u00121\n\u03c32PT\nMPM+\u03c9(L+\u03c4IN)\u03b2\u0013\n\u03a6\u2217=1\n\u03c32PT\nM\u02c6\u03a6. (2.11)\n2.6 Selection of the hyper-parameters\nThe method described requires the evaluation of the three hyper-parameters \u03c4,\u03c9and\u03b2. The parameter \u03c4 >0was\nartificially introduced into the problem to guarantee invertibility of L+\u03c4IN, and therefore allows the interpretation of\nour set-up as a Bayesian inverse problem with (2.9) providing the expression of the covariance matrix of the posterior\ndistribution. As described in Section 2.2.1, \u03c4is set equal to the smallest non-zero eigenvalue of the graph Laplacian.\nNext, we comment on the role of the regularization exponent \u03b2. When maximizing the posterior distribution p(\u03a6|\u02c6\u03a6)\nover\u03a6, higher values of \u03b2enforce that the resulting \u03d5mhas smooth variation within clusters also for higher derivatives;\nin short, it enforces control of derivatives of order \u03b2and may be selected \u03b2\u22651[40]. Given this, we select \u03b2= 2and\nnote that this choice is adequate in all our numerical experiments.\nFinally, \u03c9determines the strength of the prior distribution with respect to the likelihood; we consider it a regularization\nstrength since the prior distribution plays the role of a regularizer when maximizing the posterior distribution. The\nparameter \u03c9can be determined by requiring the average standard deviation of the multi-fidelity estimates to be greater\nthan the standard deviation of the high-fidelity data, \u03c3. That is, select \u03c9such that\n1\nN\u2212MNX\ni=Mp\nCii=r\u03c3,\nfor some r >1. This guarantees that the confidence of the multi-fidelity model is not greater than the one of the\nhigh-fidelity model. In our numerical experiments we have observed that that a value of r= 3leads to good results\nfor all cases. Remarkably, the selection of the hyper-parameters using the approach described above does not require\nknowledge of any additional high-fidelity data. All the high-fidelity data is used to estimate the mode of the true data\npoints.\n3 Evaluating the posterior distribution\nIn this section we describe two computational methods for determining \u03a6\u2217andC, and thereby completely characterizing\nthe posterior distribution. The first method relies on computing the spectral decomposition of the graph Laplacian,\nwhereas the second relies on constructing a low-rank approximation of the inverse of the covariance matrix.\n3.1 Expansion in a truncated eigenfunction basis\nThis method utilizes the facts that:\n1.The eigenfunctions of the graph Laplacian form a complete basis that can be used to represent the displacement\ncomponents.\n8Graph Laplacian-based Bayesian Multi-fidelity Modeling\n2.Once this basis is used, the prior penalizes the contributions from eigenfunctions corresponding to large\neigenvalues.\nTherefore, a reasonable approximation is to represent the displacement components using a truncated basis set\ncomprising of eigenfunctions with small eigenvalues.\nWe compute the low-lying spectrum of the graph Laplacian L. That is, we compute the eigenpairs (\u03bbk,\u03c8k)for each\nk\u2208 {1, . . . , K }, where \u03c8k\u2208RNandK\u226aNindicates a cutoff. This can be done as follows: First, let\na:= 2 max\ni\u2208{1,...,N}D1\u2212p\u2212q\nii .\nThen it is a standard result that the eigenvalues of Llie in [0, a][49, Lemma 2.5(f)].2Next, compute the Kleading\neigenvalues \u03bb\u2032\nkand eigenvectors \u03c8\u2032\nkof the symmetric positive semi-definite matrix aIN\u2212L. Finally, the low-lying\neigenvectors of Lare\u03c8k=\u03c8\u2032\nk, with corresponding eigenvalues \u03bbk=a\u2212\u03bb\u2032\nk.\nWe store the eigenfunctions and eigenvalues in the matrices:\n\u03a8K= (\u03c81\u00b7\u00b7\u00b7\u03c8K)\u2208RN\u00d7K,\n\u039bK= diag ( \u03bb1, . . . , \u03bb K)\u2208RK\u00d7K,\nwhere \u03a8T\nK\u03a8K=IK.\nThereafter, we express \u03a6as a truncated expansion of the eigenfunctions via the truncated coefficients matrix AK=\n[Aij]\u2208RK\u00d7D,\n\u03a6=\u03a8KAK.\nHence, each component of the displacement \u03d5m\u2208RN,m\u2208 {1, . . . , D }, may be written as\n\u03d5m=KX\nk=1Akm\u03c8k.\nUsing this expansion in the posterior distribution for the displacement (2.7), we arrive at an expression for the distribution\nfor the coefficients AK,\np(AK|\u02c6\u03a6)\u221dexp \n\u2212\u2225\u02c6\u03a6\u2212PM\u03a8KAK\u22252\nF\n2\u03c32\u2212\u03c9\n2tr\u0010\nAT\nK\u03a8T\nK(L+\u03c4IN)\u03b2\u03a8KAK\u0011!\n= exp \n\u2212\u2225\u02c6\u03a6\u2212PM\u03a8KAK\u22252\nF\n2\u03c32\u2212\u03c9\n2tr\u0010\nAT\nK(\u039bK+\u03c4IK)\u03b2AK\u0011!\n, (3.1)\nwhere we have used that, since \u03a8T\nK(L+\u03c4IN)\u03a8K=\u039bK+\u03c4IK, it follows that \u03a8T\nK(L+\u03c4IN)\u03b2\u03a8K=\n(\u039bK+\u03c4IK)\u03b2.3\nRecognizing that the distribution for AKis the product of two normal distributions and is therefore itself a normal\ndistribution, we may write the distribution (3.1) as,\np(AK|\u02c6\u03a6)\u221dexp\u00121\n2\u27e8AK\u2212A\u2217\nK,C\u22121\nAK(AK\u2212A\u2217\nK)\u27e9F\u0013\n=DY\nm=1exp\u0012\n\u22121\n2(am\u2212a\u2217\nm)T\u00b7C\u22121\nAK(am\u2212a\u2217\nm)\u0013\nwhere amis the mthcolumn of AK. Further for the posterior distribution, the mean, A\u2217\nK\u2208RK\u00d7D, and the covariance,\nCAK\u2208RK\u00d7K, are given by\nC\u22121\nAK=1\n\u03c32(PM\u03a8K)TPM\u03a8K+\u03c9(\u039bK+\u03c4IK)\u03b2, (3.2)\nA\u2217\nK=1\n\u03c32CAK(PM\u03a8K)T\u02c6\u03a6. (3.3)\n2To apply this Lemma, we use that Lis similar to D\u2212(p+q)(D\u2212W).\n3Recall that L+\u03c4IN=\u03a8\u039b\u03a8T+\u03c4IN=\u03a8(\u039b+\u03c4IN)\u03a8T. It directly follows that (L+\u03c4IN)\u03b2=\u03a8(\u039b+\u03c4IN)\u03b2\u03a8T. Then\nsince\u03a8T\nK\u03a8=\u0000IK0K\u00d7(N\u2212K)\u0001and\u03a8T\u03a8K=\u0000IK0(N\u2212K)\u00d7K\u0001Tby the orthonormality of the eigenvectors, the result\nfollows.\n9Graph Laplacian-based Bayesian Multi-fidelity Modeling\n3.1.1 Computational complexities\nThe quantities to be computed and stored for this method, and the computational complexity of doing so, are summarized\nin Table 1. For the complexity of computing the Kleading eigenvalues and eigenvectors of aIN\u2212L(which is the\nbottleneck for computing the truncated eigendecomposition), we assume that randomized block Krylov methods [ 50]\nare used.\nQUANTITY COMPLEXITY IN TIME COMPLEXITY IN SPACE\n\u03a8K,\u039bKO(N2KlogN)[50] O(N2(logN)2)[50]\nPM\u03a8KO(MK) O(MK)\nC\u22121\nAKO(MK2) O(K2)\nCAK O(K3) O(K2)\nA\u2217\nK O(DMK +DK2) O(DK)\nTable 1: Computational complexity of the truncation method.\n3.2 Low rank Nystr\u00f6m approximation\nRecall that to compute \u03a6\u2217we need to solve (2.11). In this section we describe a way to compute this in a way that is\nvery efficient in N, in the setting where O(N2)time and space complexity are both intractable. We can do this in the\nspecial case of p=q= 1/2by computing a low rank approximation of (L+\u03c4IN)\u03b2via the Nystr\u00f6m extension.\n3.2.1 Nystr\u00f6m-QR approximation\nNotice that, since p=q= 1/2,L:=IN\u2212D\u22121/2WD\u22121/2. Choosing X\u2286 {1, ..., N}at random (with the condition\nthatXcontain some or all of the high-fidelity points) with |X|=:K, we can approximate Wvia the Nystr\u00f6m\nextension [51, 52, 53]:\nW\u2248W(:, X)W(X, X )\u2020W(X,:), (3.4)\nwhere we have used the M ATLAB notation W(:, X) := [ Wij]N\ni=1,j\u2208XandW(X, X ) := [ Wij]i,j\u2208X, andW(X, X )\u2020\ndenotes the pseudoinverse of W(X, X ). Observe that this approximation of Whas at most rank K. In this subsection\nwe will use this approximation to derive a low-rank approximation for (L+\u03c4IN)\u03b2, and thereby efficiently approximate\nsolutions to (2.11) and matrix-vector products with C.\nRemark. The Nystr\u00f6m extension is most well-behaved applied to symmetric positive semidefinite matrices, as if Sis\nsuch a matrix then the condition number of S(X, X )is bounded above by that of S. However, this is not satisfied by W,\nwhich is constructed to have zeroes on the diagonal (i.e., the graph has no self-loops) and hence has trace zero, and is\ntherefore an indefinite matrix. In the indefinite case, positive and negative eigenvalues can combine to leave W(X, X )\nwith much smaller (in magnitude) non-zero eigenvalues than Wand correspondingly a much higher condition number.\nThis case has seen recent attention in Nakatsukasa and Park [ 54], which recommends taking Khigher than one\u2019s\ndesired rank r, and then truncating the spectrum of W(X, X )to the rlargest (in magnitude) eigenvalues before\ncomputing the pseudoinverse, to control the condition number. We will ignore this refinement in the below, but it is only\na minor modification to include.\nGiven (3.4), we can therefore compute\nD\u2248\u02c6D:= diag( W(:, X)W(X, X )\u2020W(X,:)1N)\nwith1N\u2208RNbeing a column vector of ones, and thus\nL\u2248IN\u2212\u02c6D\u22121\n2W(:, X)W(X, X )\u2020W(X,:)\u02c6D\u22121\n2.\nNext, we can extract an approximate low rank eigendecomposition of Lfrom this approximation by following a method\nof Bebendorf and Kunis [ 55], first recommended in the graph-based learning context by Alfke et al. [56]. We compute\nthe thin QR factorization\nQR=\u02c6D\u22121\n2W(:, X),\n10Graph Laplacian-based Bayesian Multi-fidelity Modeling\nwhere Q\u2208RN\u00d7Khas orthonormal columns and R\u2208RK\u00d7Kis upper triangular. Then we compute the eigendecom-\nposition\nRW (X, X )\u2020RT=\u0393\u03a3\u0393T,\nwhere \u0393\u2208RK\u00d7Kis orthogonal and \u03a3\u2208RK\u00d7Kis diagonal. Finally, by computing \u02dcU:=Q\u0393\u2208RN\u00d7K(which\ntherefore has orthonormal columns) we have that\nL\u2248IN\u2212\u02dcU\u03a3\u02dcUT,\nwhere \u02dcUT\u02dcU=IK, where IKis the K\u00d7Kidentity matrix. We can therefore approximate\n(L+\u03c4IN)\u03b2\u2248((1 + \u03c4)IN\u2212\u02dcU\u03a3\u02dcUT)\u03b2\n=\u0010\n(1 +\u03c4)(IN\u2212\u02dcU\u02dcUT) +\u02dcU((1 + \u03c4)IK\u2212\u03a3)\u02dcUT\u0011\u03b2\n= (1 + \u03c4)\u03b2(IN\u2212\u02dcU\u02dcUT)\u03b2+\u0010\n\u02dcU((1 + \u03c4)IK\u2212\u03a3)\u02dcUT\u0011\u03b2\n(\u2217)\n= (1 + \u03c4)\u03b2(IN\u2212\u02dcU\u02dcUT) +\u02dcU((1 + \u03c4)IK\u2212\u03a3)\u03b2\u02dcUT(\u2217\u2217)\n= (1 + \u03c4)\u03b2IN+\u02dcU\u0000\n((1 + \u03c4)IK\u2212\u03a3)\u03b2\u2212(1 +\u03c4)\u03b2IK\u0001\u02dcUT.\nHere line (\u2217)follows from the fact that if AB=BA=0andA,Bare diagonalizable, then (A+B)\u03b2=A\u03b2+B\u03b2,\nwhich is here satisfied by A= (1 + \u03c4)(IN\u2212\u02dcU\u02dcUT)andB=\u02dcU((1 + \u03c4)IK\u2212\u03a3)\u02dcUT, since \u02dcUT\u02dcU=IKandA\nandBare symmetric matrices. Line (\u2217\u2217)follows from the fact that \u02dcUT\u02dcU=IKand therefore that IN\u2212\ufffd"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**1. Abstract:**\n\nThe paper introduces \"Deep Schema Grounding (DSG),\" a framework aimed at improving the visual abstraction understanding of vision-language models (VLMs). DSG leverages \"schemas,\" structured representations of abstract concepts, to enhance the reasoning and prediction capabilities of these models. The paper systematically evaluates DSG using a newly created Visual Abstractions Dataset (VAD), showcasing significant improvements in abstract visual reasoning performance.\n\n**2. Introduction:**\n\n- **Context of Research:**\n  The paper addresses the challenge of interpreting abstract visual concepts, which human vision does adeptly but remains challenging for current VLMs. Abstract concepts are not strictly defined by concrete visual features but by high-level rules and patterns that can be applied across various contexts.\n\n- **Research Questions/Hypotheses:**\n  The central question is: \"What makes a maze look like a maze?\" The research hypothesizes that schemas can enhance VLMs by structuring their interpretation process for abstract concepts.\n\n- **Literature Review:**\n  Previous work includes various visual-language benchmarks focusing on concrete objects and commonsense reasoning. However, reasoning about abstract concepts, which involves grounding low-level entities into complex patterns, remains underexplored. This work extends these benchmarks to address this gap.\n\n**3. Related Work:**\n\n- **Relevant Prior Work:**\n  The paper reviews visual-language grounding, hierarchical reasoning, and the theory-theory of concepts. Past benchmarks include works by Chen et al. (2015), Krishna et al. (2017), and Zellers et al. (2019) focusing on content within images but lacking emphasis on abstract concepts.\n\n- **Advances by This Paper:**\n  DSG introduces a hierarchical schema grounding approach with LLMs and VLMs, advancing the interpretative capabilities for abstract visual reasoning. It evaluates DSG using a unique dataset aimed at testing these higher-order abstractions.\n\n**4. Method:**\n\n- **Proposed Approach:**\n  The DSG framework involves three main steps:\n  1. Extracting schemas of abstract concepts using large language models (LLMs).\n  2. Hierarchically grounding these schemas onto images using VLMs.\n  3. Augmenting visual question-answering tasks with the resolved schemas.\n\n- **Mathematical Formulations and Algorithms:**\n  Schemas are represented as directed acyclic graphs (DAGs) that decompose an abstract concept into more primitive-level symbols. DSG uses programmatic representations to facilitate hierarchically structured grounding.\n\n- **Architectural Details:**\n  The framework utilizes pre-trained LLMs like GPT-3/4 for generating schemas and open-vocabulary models such as LLaVA for vision-language alignment. The grounding involves iteratively refining symbolic interpretations based on hierarchical dependencies outlined in schemas.\n\n- **Novel Techniques/Adaptations:**\n  DSG's novelty lies in its combination of hierarchical schema extraction and grounding into a coherent visual reasoning system. It further uses resolved schema contexts as holistic inputs to VLMs for improved performance.\n\n**5. Experimental Setup:**\n\n- **Datasets Used:**\n  The Visual Abstractions Dataset (VAD) was introduced, comprising diverse images representing abstract concepts and annotated with multiple-choice questions and answers. The dataset includes 540 instances across 12 abstract concepts spanning strategic, scientific, social, and domestic categories.\n\n- **Specifications of Hardware and Software:**\n  Experiments were run using high-performance GPUs (details unspecified) and a combination of pre-trained models available through accessible APIs. OpenAI's GPT-4V and integrated LLMs were utilized.\n\n- **Evaluation Metrics:**\n  Performance was measured using exact-match accuracy for multiple-choice questions. Additional metrics included graded accuracy and BERTScores for free-form answers.\n\n- **Baselines and Comparison Methods:**\n  Baseline comparisons included ViperGPT, VisProg, LLaVA, InstructBLIP, and GPT-4V. These models were tested with and without DSG augmentation.\n\n**6. Results and Analysis:**\n\n- **Empirical Findings:**\n  DSG significantly improved the performance of VLMs, including a 5.4% absolute improvement and an 8.3% relative improvement on GPT-4V. The most notable gains were in counting questions, with an 11.0% relative improvement.\n\n- **Graphs, Tables, and Figures:**\n  Detailed comparisons and accuracy metrics were provided in tables, showing DSG's performance across different question types and concept categories. Additional figures illustrated schema examples and grounding processes.\n\n- **Analysis of Results:**\n  DSG outperformed baselines consistently, particularly in complex reasoning tasks like counting and open-ended questions. The ablation studies highlighted the importance of hierarchical schema representation and grounding.\n\n- **Discussion of Anomalous Results:**\n  While DSG excelled overall, certain VLMs struggled with specific schemas requiring detailed spatial understanding, indicating areas for further improvement.\n\n**7. Discussion:**\n\n- **Interpretation of Results:**\n  Results suggest that DSG effectively enhances VLMs' abstract reasoning capabilities by structuring their understanding through schemas. LLMs' extensive pre-training on human language data aids in accurate schema extraction, enhancing visual-textual alignment.\n\n- **Implications of Findings:**\n  DSG's structured thinking process models human-like abstractions, making it a potent tool for future AI systems needing abstract concept comprehension.\n\n- **Limitations and Potential Biases:**\n  DSG's dependency on pre-trained schemas may propagate biases inherent in the training data. Additionally, challenges in spatial reasoning suggest areas for improvement, particularly in comprehending complex layouts.\n\n**8. Conclusion:**\n\n- **Key Contributions:**\n  The introduction of DSG for abstract visual reasoning, a new Visual Abstractions Dataset for benchmarking, and empirical validation showcasing significant performance improvements.\n\n- **Future Research Directions:**\n  Suggestions include enhancing spatial reasoning in schemas, exploring more complex abstract concepts, and refining schema extraction and grounding techniques to mitigate biases and improve generalizability.\n\nThis detailed summary captures the intricate methodologies, comprehensive evaluations, and significant contributions presented in the paper, laying a solid foundation for replicating or building upon this work."}, {"role": "assistant", "content": "WHAT MAKES A MAZE LOOK LIKE A MAZE?\nJoy Hsu\nStanford University\njoycj@stanford.eduJiayuan Mao\nMIT\njiayuanm@mit.eduJoshua B. Tenenbaum\nMIT\njbt@mit.edu\nNoah D. Goodman\nStanford University\nngoodman@stanford.eduJiajun Wu\nStanford University\njiajunwu@cs.stanford.edu\nABSTRACT\nA unique aspect of human visual understanding is the ability to flexibly interpret\nabstract concepts: acquiring lifted rules explaining what they symbolize, grounding\nthem across familiar and unfamiliar contexts, and making predictions or reasoning\nabout them. While off-the-shelf vision-language models excel at making literal\ninterpretations of images (e.g., recognizing object categories such as tree branches),\nthey still struggle to make sense of such visual abstractions (e.g., how an arrange-\nment of tree branches may form the walls of a maze). To address this challenge,\nwe introduce Deep Schema Grounding (DSG), a framework that leverages explicit\nstructured representations of visual abstractions for grounding and reasoning. At\nthe core of DSG are schemas \u2014dependency graph descriptions of abstract concepts\nthat decompose them into more primitive-level symbols. DSG uses large language\nmodels to extract schemas, then hierarchically grounds concrete to abstract com-\nponents of the schema onto images with vision-language models. The grounded\nschema is used to augment visual abstraction understanding. We systematically\nevaluate DSG and different methods in reasoning on our new Visual Abstractions\nDataset, which consists of diverse, real-world images of abstract concepts and\ncorresponding question-answer pairs labeled by humans. We show that DSG sig-\nnificantly improves the abstract visual reasoning performance of vision-language\nmodels, and is a step toward human-aligned understanding of visual abstractions.\n1 I NTRODUCTION\nHumans possess the remarkable ability to flexibly acquire and apply abstract concepts when inter-\npreting the concrete world around us. Consider the concept \u201cmaze\u201d: our mental model can interpret\nmazes constructed with conventional materials (e.g., drawn lines) or unconventional ones (e.g., icing),\nand reason about mazes across a wide range of configurations and environments (e.g., in a cardboard\nbox or on a knitted square). Our goal is to build systems that can make such flexible and broad\ngeneralizations as humans do. This necessitates a reconsideration of a fundamental question: what\nmakes a maze look like a maze? A maze is not defined by concrete visual features such as the specific\nmaterial of walls or its perpendicular intersections, but by lifted rules over symbols\u2014a plausible\nmodel for a maze includes its layout, the materials composing the walls, and the designated entry and\nexit. Crucially, these model components can be realized by infinitely many real-world variations. For\ninstance, walls constructed from candy canes, hay, or popsicle sticks, despite their diverse materials,\nare all identifiable as parts of a maze.\nHowever, current vision-language models (VLMs) often struggle to reason about visual abstractions\nat a human level, frequently defaulting to literal interpretations of images, such as a collection of\nobject categories. These interpretations may be technically correct descriptions of the scene, but may\nnot align with the abstract concept underlying the image. For example, when given the intended\nconcept of \u201cmaze\u201d and an image that realizes the concept with unconventional objects, VLMs such as\nGPT-4V (OpenAI, 2023) and LLaV A (Liu et al., 2024) fail to correctly recognize objects in the scene\nas components of the visual abstraction. It is unclear whether these off-the-shelf models leverage\nknowledge of abstract concepts to make sense of images as humans do.\n1arXiv:2409.08202v1  [cs.CV]  12 Sep 2024VLMAbstract Concept: Maze\u2026LLM\nVLM answer: No \u2718DSG answer: Yes \u2713The layoutis grid-like, the wallsare ice, and the entry and exit are the gates.  +  QDeep Schema Grounding\nV:VSchema:Q: \u201cImagine that the image represents a maze. Are there clear entrances or exits in the maze? Answer with one of the following answer choices: yes, no.\u201dwallslayoutentry-exitmaze\nVLM answer: Stick \u2718\nV:Q: \u201cImagine that the image represents a maze. What is the player in this maze? Return none if there is no player. Answer with one of the following answer choices: grass, moss, stick, none.\u201dSchemaV\nVLMDSG answer: None \u2713The layoutis triangular, the wallsare branches, and the entry and exit are at the corners.  +  QVSchemaVFigure 1: There exist abstract concepts, such as \u201cmaze\u201d, which are defined by lifted symbols\nand patterns, instead of concrete visual features. We propose Deep Schema Grounding (DSG), a\nframework for visual reasoning over such abstract concepts, which uses schemas to structure models\u2019\ninterpretation of images. DSG hierarchically grounds conceptual schemas on images and uses them\nto provide holistic context to VLMs, improving performance across diverse downstream queries.\nTo address the challenge of understanding abstract visual concepts, we propose Deep Schema\nGrounding (DSG), a framework for models to interpret visual abstractions. At the core of DSG are\nschemas \u2014a dependency graph description of abstract concepts (see Figure 1). Schemas characterize\ncommon patterns that humans use to interpret and reason about the visual world (Schank & Abelson,\n1975). A schema for \u201chelping\u201d allows us to understand relations between characters in a finger puppet\nscene, while a schema for \u201ctic-tac-toe\u201d allows us to play the game even when the grid is composed of\nhula hoops instead of drawn lines. A schema for \u201cmaze\u201d is what makes a maze look like a maze.\nDSG explicitly uses schemas generated by and grounded by large pretrained models to reason\nabout visual abstractions. Concretely, we model schemas as programs encoding directed acyclic\ngraphs (DAGs), which decompose an abstract concept into a set of more concrete visual concepts\nas subcomponents, as illustrated in Figure 1. The full framework is composed of three steps. First,\nwe extract schema definitions of abstract concepts from a large language model (LLM). Next, DSG\nhierarchically queries a vision-language model (VLM), first grounding concrete symbols in the DAG\n(i.e. symbols that do not depend on the interpretation of other symbols), then using those symbols\nas conditions to ground more abstract symbols. Finally, we use the resolved schema, including the\ngrounding of all its components, as an additional input into a vision-language model to provide\nholistic context about the image, such that we can improve visual reasoning across diverse downstream\n2queries about the abstract concept. Our method is a general framework for abstract concepts that does\nnot depend on specific models; the LLMs and VLMs used are interchangeable.\nTo investigate the capabilities of models in understanding visual abstractions, we introduce the Visual\nAbstractions Dataset (V AD). V AD is a visual question-answering dataset that consists of diverse,\nreal-world images representing abstract concepts. The abstract concepts span 4different categories:\nstrategic concepts that are characterized by rules and patterns (e.g., \u201ctic-tac-toe\u201d), scientific concepts\nof phenomena that cannot be visualized in their canonical forms (e.g., \u201ccell\u201d), social concepts that\nare defined by theory-of-mind relations (e.g., \u201cdeceiving\u201d), and domestic concepts of household\nobjectives that cannot be directly defined by specific arrangements of objects (e.g, \u201ctable setting for\ntwo\u201d). Each image is an instantiation of an abstract concept, and is paired with questions that probe\nunderstanding of the visual abstraction; for example, \u201cImagine that the image represents a maze. Are\nthere clear entrances or exits in the maze?\u201d The Visual Abstractions Dataset comprises 540of such\nexamples, with answers labeled by 5human annotators from Prolific. We found that off-the-shelf\nVLMs and integrated LLMs with APIs have significant room for improvement on this benchmark.\nWe evaluate Deep Schema Grounding on the Visual Abstractions Dataset, and show that DSG\nconsistently improves performance of vision-language models across question types, abstract concept\ncategories, and base models. Notably, DSG improves GPT-4V by 5.4percent point overall ( \u21918.3%\nrelative improvement), and, in particular, demonstrates a 6.7percent point improvement ( \u219111.0%\nrelative improvement) in questions that involve counting. While the challenge of visual abstraction\nunderstanding is still far from being fully solved, our results show that DSG is a promising solution\nthat effectively schemas to structure the thinking process of visual reasoning systems.\nIn summary, our contributions are the following:\n\u2022We propose Deep Schema Grounding (DSG), a hierarchical, decomposition-based framework\nthat explicitly extracts and grounds schemas of concepts with large pretrained models.\n\u2022We introduce the Visual Abstractions Dataset (V AD), a diverse, real-world visual question-\nanswering benchmark that evaluates VLMs\u2019 understanding of visual abstractions.\n\u2022We demonstrate DSG\u2019s improvement over prior state-of-the-art works across a variety of\nabstract concept categories, question types, response types, and metrics.\n2 R ELATED WORKS\nVision-language grounding and reasoning benchmarks. Visual-language benchmarks have\ntraditionally tackled reasoning about content in images, such as objects, relations, and actions (Chen\net al., 2015; Yu et al., 2015; Antol et al., 2015; Wu et al., 2016; Zhu et al., 2016; Krishna et al.,\n2017; Wang et al., 2017). Recently, an increasing number of works have focused on evaluating\nvisual-language tasks that require commonsense reasoning, usually by asking \u201cwhat if\u201d or \u201cwhy\u201d\nquestions (Pirsiavash et al., 2014; Wagner et al., 2018; Zellers et al., 2019; Lu et al., 2022), or by\ngenerating holistic scene descriptions (Song et al., 2024). In contrast to these related works, our paper\nemphasizes the visual grounding and reasoning problem of abstract concepts, such as uncommon\nvisualizations of scientific terms or unconventional constructions of strategic games. Such abstract\nconcepts should be grounded on the relationship between lower-level concepts such as entities and\npatterns, therefore positing novel challenges to vision-language models.\nConcept representations in minds and machines. Our schema-based concept representations\nare inspired by the theory-theory of concepts in cognitive science and artificial intelligence (Schank\n& Abelson, 1975; Morton, 1980; Carey, 1985; Gopnik, 1988; Gopnik & Meltzoff, 1997; Carey,\n2000). In short, these works treat concepts as organized within and around theories. Therefore,\nacquiring the concept involves learning its theory, and reasoning with a concept involves causal-\nexplanatory reasoning of the theories. Such views have been largely leveraged in generating symbolic\nrepresentations for concepts such as word meanings (Speer et al., 2017) and object shapes (Biederman,\n1987). However, most works have focused on using only symbolic features to describe scenes and\nhave been primarily applied to simple object-level concrete concepts. In contrast, we propose using\nour schema representation as a guide for grounding scene-level, abstract concepts in natural images.\n3PromptExtracting Schemas of Abstract ConceptsLLM\u201cMaze\u201d\n{layout: rectangular,walls: coffee beans,entry-exit: coffee cups}Q: \u201cImagine that the image represents a maze, what is the layout?\u201dQ: \u201cImagine that the image represents a maze, what are the walls?\u201dQ: \u2018Imagine that the image represents a maze, and the layoutis {VLM layout prediction}, what is the entry and exit?\u201dVLM\nVLMHierarchically Grounding Schemas on Images\nVisual Question Answering Augmented with Resolved SchemasSchema:\n\u201cImagine that the image represents a maze, the layoutis rectangular, the wallsare coffee beans, and the entry and exit are coffee cups.\u201dQ: \u201cWhat is the player in this maze?\u201dA: \u201cNone\u201d\nSchema:wallslayoutentry-exitmazegen(concept=maze) =gen(layout| concept=maze)gen(walls| concept=maze)gen(entry-exit| concept=maze, layout)Figure 2: Deep Schema Grounding consists of three main stages: (1) extracting schemas of abstract\nconcepts with large language models, (2) hierarchically grounding schemas on images with vision-\nlanguage models, and (3) conducting visual question-answering augmented with resolved schemas.\nHierarchical grounding of visual concepts. Our approach is related to recent works on chain-\nof-thought reasoning in vision-language models (Lu et al., 2022; Chen et al., 2023), hierarchical\nimage classification (Koh et al., 2020; Menon & V ondrick, 2022), hierarchical relation detection (Li\net al., 2024), and visual reasoning via programs (Hu et al., 2017; Mao et al., 2019; Hsu et al., 2024).\nIn contrast to these algorithms, which are primarily designed for object and relational reasoning,\nwe focus on the grounding of scene-level concepts. Our framework also shares a similar spirit to\nprior approaches (Menon & V ondrick, 2022; Li et al., 2024) that leverage vision-language models\nto generate text descriptions of class features for classification tasks. However, our paper proposes\nthe generation of hierarchical schemas with LLMs to ground visual abstractions for reasoning.\nAdditionally, our hierarchical schema representation is related to hierarchical program representations\nused in text-only reasoning (Gao et al., 2023; Zelikman et al., 2023; Zhang et al., 2023); in contrast,\nwe leverage program representations for understanding concepts in natural images. Notably, DSG\nperforms schema inference with a VLM; compared to fully symbolic inference processes, our\nframework is capable of leveraging all visual information in the image for interpreting abstractions.\n3 D EEPSCHEMA GROUNDING\nWe study the task of reasoning about visual abstractions by concretely focusing on its visual question-\nanswering (VQA) form, although our idea naturally generalizes to other visual interpretation and\nreasoning tasks. A VQA instance is a tuple of \u27e8v, q, a\u27e9, where vis an image, qis a natural language\nquestion, and ais the answer to the question. Here, acan take the form of multiple choices or\n4Table Setting for Two\nCell\nDeceivingTic-tac-toecellmembranenucleusorganelledeceivingdeceivervictimdeceptive-objecttic-tac-toesymbolsstrategyboard\ntable-setting-for-twotwo-chairstable-settingtableFigure 3: Examples of schemas for concepts and the visual features that they may be grounded to.\nfree-form responses in natural language. We assume that each image vhas an underlying abstract\nconcept associated with it (e.g., \u201cmaze\u201d), which is not associated with concrete visual features, but\na set of lifted rules that define the concept. We also assume that qis a question about the abstract\nconcept, and the concept is revealed to all models in question q.\nThe visual abstraction understanding task posits a series of challenges to off-the-shelf VLMs. These\nquestions are centered around concepts with infinite possible instantiations in the real world. As a\nresult, current VLMs struggle with this problem due to the potential novelty of the scene and the visual\nabstraction combinations encountered. However, a crucial insight regarding these abstract concepts is\nthe consistency in composition patterns of lower-level concepts, which aggregate to construct the\nhigh-level concept; these underlying patterns remain the same across different variations.\nBased on these insights, we propose Deep Schema Grounding (DSG), a framework that leverages\nschemas to interpret and reason about visual abstractions. DSG can be built on top of most pretrained\nvision-language models (VLMs) to perform the visual question-answering task. Illustrated in Figure 2,\nthe DSG framework consists of three main steps: (1) extracting a schema of the concept, (2)\nhierarchically grounding the schema to the visual input, and (3) leveraging the resolved schema as\ninput to the base VLM. We describe the schema definition and each stage in detail below.\n3.1 V ISUAL ABSTRACTION SCHEMA\nA visual abstraction schema is a concise program that defines a directed acyclic graphical (DAG)\nrepresentation of a particular concept. As illustrated in Figure 2, each node in the schema corresponds\nto a subcomponent concept of the higher-level abstract concept. For example, the formation of a\nmaze can be decomposed into three components: the layout, the construction of the walls, and the\npositioning of the entry and exit of the maze. The dependencies among individual components yield a\nDAG configuration\u2014in this case, the placement of the entry and the exit of the maze depends on the\nlayout of the maze. From an inference perspective, the dependency graph describes the order in which\nthe model should interpret each component, and how the interpretation of a particular component\nshould be conditioned on the interpretation of one or more previously interpreted components.\nThe most important feature of a visual abstraction schema is that the definition of such concepts\nisuniversal , in the sense that the program describes a universal characterization of mazes made of\nvarious components, rather than being restricted to specific visual instantiations (e.g., mazes formed\nby drawn lines). Schemas enable machines to better generalize to novel scenes.\n3.2 E XTRACTING SCHEMAS OF ABSTRACT CONCEPTS\nDSG utilizes large language models (LLMs) as a repository of knowledge from which schemas can be\nacquired. Given that LLMs are trained on extensive corpora of human language data, we hypothesize\nthat they contain human-aligned schemas for a wide range of abstract concepts. Compared to prior\n5works that use LLMs to translate natural language utterances into programs, DSG uses LLMs to\nextract explicitly structured knowledge of abstract concepts based on the concept\u2019s name.\nIn particular, our goal is to extract a universal schema for abstract concepts that is not instance-specific,\nbut is applicable across diverse visual stimuli. Notably, our prompt to the LLMs is generic and\nsimple, with only one example of a schema for the abstract concept \u201cacademia\u201d (which consists of\nkey components: faculty, students, etc.), and a short instruction of the schema generation task. We\nfind that LLMs demonstrate a remarkable ability to extrapolate across a wide array of concepts, and\nproduce faithful decompositions of the visual abstractions. We detail the prompt and LLM-generated\nschemas for each concept in the Appendix.\n3.3 H IERARCHICALLY GROUNDING SCHEMAS ON IMAGES\nInstead of directly answering questions given the image and the question inputs, we first hierarchically\nground individual components in the schema to visual entities in the image. In the maze example in\nFigure 2, this corresponds to inferring the materials of the walls, the layout of the maze, etc.\nQ: \u201cImagine that the image represents a maze, what is the layout?\u201dA: Rectangular.gen(layout| concept=maze)Q: \u201cImagine that the image represents a maze, what are the walls?\u201dA: Coffee beans.Q: \u201cImagine that the image represents a maze, and the layout is rectangular, what is the entry and exit?\u201dA: Coffee cups.gen(walls| concept=maze)gen(entry-exit| concept=maze, layout)\nFigure 4: The schema grounding process.Formally, given a schema definition of the con-\ncept, DSG hierarchically grounds each component\nin the concept DAG. The outcome of this process\nis represented as short text descriptions, such as\n\u201cwall: coffee beans \u201d. We implement this procedure\nby leveraging pretrained VLMs to predict the most\nlikely grounding of a component given the image\nand a text query (see Figure 4). Since our grounding\nprocedure is hierarchical, for components that are\nconditioned on other concepts (e.g., the interpreta-\ntion of entry-exit is conditioned on the layout ), the\nVLM also takes into account the grounding results\nof these prerequisite components. This ensures that\neach step of the grounding process is informed by\nthe previously established context, facilitating more\ncoherent and accurate grounding.\nGrounding components following the hierarchical schema structure is crucial for abstract concepts\nwith many possible instantiations. For example, for the \u201cmaze\u201d concept, DSG first resolves compo-\nnents that are more coarse and concrete (e.g., layout) , then uses those components as conditions to\nresolve more difficult grounding problems of fine-grained and abstract components (e.g., entry-exit ).\nFigure 3 shows examples of schemas and how they may be grounded onto images. More examples\nand failure modes can be found in the Appendix.\n3.4 V ISUAL QUESTION -ANSWERING AUGMENTED WITH GROUNDED SCHEMAS\nFinally, DSG leverages the grounding of individual components in the image to answer questions. For\nexample, given the concept \u201cmaze\u201d and the grounded mapping of individual components: {layout:\nrectangular, walls: coffee beans, entry-exit: coffee cups }, we augment the final question-\nanswering step by providing the VLM with the component mapping, as well as the full conversation\nhistory of the grounding process. In particular, the groundings of the components are included in a\ntext prompt such as: \u201cImagine that the image represents a maze, and the layout is rectangular, and\nthe walls are coffee beans, and the entry and exit are coffee cups.\u201d Compared with baselines that\nonly have access to the abstract concept itself (e.g., \u201cImagine that the image represents a maze.\u201d), our\nconcept grounding system shows significant performance improvement with the resolved schema as\nholistic context for the image.\nNotably, our DSG framework is inference-only : it leverages the strong generalization capabilities\nof LLMs (for proposing schemas) and VLMs (for grounding schemas and reasoning), to better\nunderstand visual abstractions. Programmatic schemas are used as the intermediate representation to\nbridge the explicit commonsense knowledge in LLMs and the visual reasoning capability of VLMs.\n6Domestic ConceptsTable setting for twoGuest bedrooms\nScientific ConceptsSolar systemAtom\nSocial ConceptsHelpingNegotiatingStrategic ConceptsTic-tac-toeMaze\nDoor decoration\nCell\nDeceiving\nTreasure map\nFigure 5: The Visual Abstractions Dataset comprises diverse, real-world images that represent 12\ndifferent abstract concepts across 4categories.\n4 V ISUAL ABSTRACTIONS DATASET\nIn order to evaluate models on visual abstraction reasoning, we propose a new dataset, the Visual\nAbstractions Dataset (V AD), illustrated with examples in Figure 5. It consists of 180images and 3\nquestions per image, with a total of 540test examples. V AD is comprised of 12different abstract\nconcepts spanning 4categories, where each concept is associated with 15examples of real-world\nimages. The questions bridge multiple question types: binary-choice, counting, and open-ended\nquestions. We detail the set of abstract concepts and the image and text components of V AD below.\nAbstract concepts. We choose 12different visual abstractions as concepts. All of them can be\ngrounded on different objects and configurations of objects in images. We focus on concepts that\nare not tied to distinct visual features (e.g., unlike canonical colors and shapes) but instead require\nhigher-level, relational, and lifted patterns. These visual abstractions cover a wide range of visual\nscenarios and possible linguistic queries, and they can be grouped into four broader categories.\nThe first category consists of strategic concepts , which are games characterized by rules and patterns,\nincluding \u201ctic-tac-toe\u201d, \u201cmaze\u201d, and \u201ctreasure map\u201d. The exact object instantiations and visual\nfeatures may vary, but humans can easily identify the state of the game when given the concept. The\nsecond category consists of scientific concepts with no possible visualization in the physical world,\nincluding \u201csolar system\u201d, \u201catom\u201d, and \u201ccell\u201d. While we cannot directly view these concepts, we can\ncreate analogies of them based on everyday objects. The third category consists of social concepts ,\nwhich are intentional actions that involve the deduction of roles and relations, including \u201chelping\u201d,\n\u201cdeceiving\u201d, and \u201cnegotiating\u201d. Examples of these theory-of-mind concepts in the real world can\n7Table 1: DSG outperforms prior works across all questions and different question types in the Visual\nAbstractions Dataset. Results are averaged over 5runs.\nAll Counting Binary Open\nViperGPT (Sur \u00b4\u0131s et al., 2023) 0.260 0 .191 0 .426 0 .141\nVisProg (Gupta & Kembhavi, 2023) 0.304 0 .249 0 .385 0 .255\nLLaV A (Liu et al., 2024) 0.415 0 .344 0 .552 0 .321\nInstructBLIP (Dai et al., 2024) 0.437 0 .322 0 .538 0 .392\nGPT-4V (OpenAI, 2023) 0.653 0 .611 0 .659 0 .663\nDSG (ours) 0.707 0 .678 0 .707 0 .719\nHuman 0.846 0 .816 0 .833 0 .868\nTable 2: DSG improves the performance of GPT-4V across categories of abstract concepts.\nStrategic Scientific Social Domestic\nGPT-4V (OpenAI, 2023) 0.644\u00b10.033 0 .572\u00b10.098 0 .679\u00b10.098 0 .717\u00b10.053\nDSG (ours) 0.721\u00b10.057 0.631\u00b10.134 0.724\u00b10.071 0.753\u00b10.058\nbe seen in many videos and movies; for a single image frame, we must make deductions to assign\npossible roles to entities in the scene. The fourth category consists of domestic concepts , inspired\nby everyday household tasks (Li et al., 2023). These concepts are goals in household environments\nthat cannot be easily defined by specific arrangements of objects, including \u201ctable setting for two\u201d,\n\u201cguest bedrooms\u201d, and \u201cdoor decoration\u201d. While humans easily recognize when such objectives are\ncompleted, we do not have a simple definition for what exact configuration should be generated.\nImages and question-answers. We curate 15images for each abstract concept from the Internet.\nOur criteria for the images are to be real, natural, and captured in the physical world. Our final set of\nimages is diverse in terms of subjects, environments, and domains. See Figure 5 for examples.\nWe create 3types of natural language questions for each abstract concept in V AD. These questions\nask about different aspects of the images and take various forms, including binary-choice questions,\ncounting questions, and general open-ended questions. In total, there are 6types of counting questions,\n14types of binary-choice questions, and 16types of open-ended questions. An example of a binary-\nchoice question is \u201cImagine that the image represents table setting for two. What is the configuration\nof the seats? Describe the answer as across or adjacent.\u201d; an example of a counting question is\n\u201cImagine that the image represents a cell. Excluding the nucleus, how many types of organelles\nare there?\u201d; an example of an open-ended question is \u201cImagine that the image represents a treasure\nmap. Based on the image, find the starting location; what is the main color of the map region that is\nphysically closest to the starting location?\u201d The overall dataset comprises 540of such questions.\nAnswers to all image-question pairs are labeled by 5human annotators from Prolific; we take their\nmodal response as the answer, to be used for exact-match text accuracy. While the Visual Abstractions\nDataset focuses on multiple-choice questions and answers, we also provide a free-response variant of\nV AD. We present examples for each type of question along with corresponding images and answers\nin the Appendix, and release our dataset here.\n5 R ESULTS\nWe validate Deep Schema Grounding in comparison to baseline VLMs and integrated LLMs with\nAPIs on the Visual Abstractions Dataset. We first compare our work with ViperGPT (Sur \u00b4\u0131s et al.,\n2023), VisProg (Gupta & Kembhavi, 2023), LLaV A (Liu et al., 2024), InstructBLIP (Dai et al.,\n2024), and GPT-4V (OpenAI, 2023). Then, we ablate different components of the DSG framework.\nFinally, we show that DSG is model-agnostic and can benefit both closed-source and open-source\nmodels. Baseline models are given the abstract concept itself in the prompt along with the question\n(e.g., \u201cImagine that the image represents a maze.\u201d), but without DSG\u2019s resolved schema of the\nconcept. All results are averaged over 5runs. Here, we focus on a systematic evaluation of multiple-\nchoice answers, where accuracy is calculated as exact-match accuracy. In the Appendix, we provide\nadditional comparisons with a graded accuracy metric based on the degree of alignment to human\njudgments, and report results on free-form answers evaluated by BERTScores (Zhang et al., 2020).\n8Table 3: Summarization of DSG ablations; the full DSG framework shows strongest performance.\nschema grounding hierarchy context All Counting Binary Open\n+ schema \u2713 \u2717 \u2717 \u2717 0.647 0 .613 0 .647 0 .659\n+ grounding \u2713 \u2713 \u2717 \u2717 0.671 0 .653 0 .674 0 .676\n+ hierarchy \u2713 \u2713 \u2713 \u2717 0.683 0 .651 0 .690 0 .688\nDSG (ours) \u2713 \u2713 \u2713 \u2713 0.707 0 .678 0 .707 0 .719\nVLMs benefit from decomposing abstract concepts into primitive-level components. Table 1\nsummarizes the multiple-choice accuracy of DSG with GPT-4V as the base VLM in comparison\nto that of prior works across 5runs. DSG significantly outperforms VLMs and integrated LLMs\nwith APIs across all questions and all types of questions. In particular, our method demonstrates a\nboost in accuracy of 5.4percent point overall ( \u21918.3%relative improvement) compared to GPT-4V .\nAcross different question types, DSG shows the largest performance improvement of 6.7percent\npoint (\u219111.0%relative improvement) for counting-based questions, such as \u201cHow many entities are\nhelping?\u201d We hypothesize that the significant improvement in accuracy for these questions is due to\nmore accurate labels for objects with DSG, as the schema grounding step will concretely associate\nconcepts such as \u201cthe entity who is helping\u201d with particular parties in the image (e.g., pandas). We\nsee that these questions usually require a more sophisticated reasoning process to answer correctly.\nIn comparison, for binary-choice questions, there may exist more superficial correlations that off-\nthe-shelf VLMs can rely on. Importantly, we also report a random human annotator\u2019s accuracy of\nadhering to the modal human response, to highlight the difficulty of problems in V AD.\nIn Table 2, we more closely analyze the performance improvement of DSG on GPT-4V across concept\ncategories. We report the average performance and the standard deviation for 5runs. We see that\nDSG outperforms GPT-4V across the four categories. In particular, our method demonstrates a strong\nboost in accuracy of 7.7percent point ( \u219112.0%relative improvement) for strategic concepts, and 5.9\npercent point ( \u219110.3%relative improvement) for scientific concepts. In the Appendix, we include\nqualitative examples of cases where DSG succeeds and fails in correctly grounding schemas. Overall,\nwe see that DSG\u2019s decomposition-based framework improves performance of VLMs.\nExplicit hierarchical grounding of schemas is better than implicit and sequential grounding.\nWe present ablation studies of different modules in DSG in Table 3. In particular, we evaluate\n(1) a variant of DSG that is given only the LLM-generated schema (+ schema), without explicit\ngrounding of each symbol, (2) a variant where DSG performs decomposed grounding sequentially\ninstead of hierarchically (+ grounding), and (3) a hierarchical variant (+ hierarchical) where in the\nfinal question-answering step, we provide the model with only the grounding result of each schema\ncomponent, instead of a full history of its grounding process.\nWith the first variant, we test whether GPT-4V can use the schema appropriately without a structured\nthinking process. We explore the second variant to test whether decomposing the visual abstraction\ninto a set of key elements is sufficient for improving the understanding of abstract concepts, and\nevaluate the role of hierarchical dependencies in lower-level concepts for grounding. Finally, we\npresent results on the third variant to test whether the full grounding trace of DSG is helpful during\nthe final question-answering step. The results show that all these design choices help: a schema only\ndoes not lend itself to improved performance, and hierarchical grounding from concrete to abstract\nsymbols, as well as the inclusion of the full context, improves performance in many concepts.\nDSG improves other (open-source) VLMs too. Tables 4 and 5 summarize the results of DSG with\nopen-source LLaV A as the base VLM. DSG yields consistent improvement overall ( \u21917.0%relative\nimprovement). However, in contrast to GPT-4V , both LLaV A and DSG yield poor results on counting\nquestions, as the base VLM struggles at enumeration regardless of its abstraction understanding.\nTable 4: DSG improves the performance of the open-source LLaV A model.\nAll Counting Binary Open\nLLaV A (Liu et al., 2024) 0.415 0.344 0.552 0 .321\nDSG (ours) 0.444 0 .344 0 .590 0 .354\n9Table 5: Comparison of LLaV A and DSG across categories; DSG outperforms the base VLM.\nStrategic Scientific Social Domestic\nLLaV A (Liu et al., 2024) 0.407\u00b10.038 0 .333\u00b10.101 0 .378\u00b10.126 0 .541\u00b10.091\nDSG (ours) 0.430\u00b10.046 0.378\u00b10.119 0.393\u00b10.154 0.578\u00b10.096\n6 D ISCUSSION\nOur framework demonstrates that by explicitly grounding conceptual schemas through large pretrained\nvision and language models, we can achieve better visual abstraction understanding in diverse real\nworld instantiations. We find that LLMs generally possess well-structured knowledge of concepts,\noutputting comprehensive schemas that are accurately decomposed into more concrete symbols and\ntheir dependencies. The extracted schemas effective capture the concise essence of a concept, such as\nthe answer to our initial question of what makes a maze look like a maze . However, as DSG relies on\npre-trained LLMs to generate schemas and does not restrict the schemas to a set of specified symbols,\nit is possible that the schemas may contain harmful biases based on the concept given, or may include\nsymbols that are difficult for VLMs to interpret.\nIn particular, we observe that current VLMs struggle to ground challenging schema components\ninvolving spatial constraints. DSG does not explicitly improve this capability or specify how to\nparameterize spatial configurations. While DSG can generate schema components that contain spatial\nrelationships, and try to ground them by describing them in language, it remains limited by the\nVLMs\u2019 abilities to conduct such spatial understanding. For example, let us consider a case where the\nextracted schema for the concept maze includes a symbol representing \u201can unobstructed path from\nentry to exit\u201d. That such a path exists in the maze\u2014typically only one and often one that is difficult to\nfind\u2014is a defining feature of mazes, yet is challenging for VLMs to correctly identify. VLMs\u2019 lack of\nspatial understanding weakens DSG ability to interpret complex and spatially grounded components.\nScaling DSG to better understand concepts that require precise layouts of schema components is\ndifficult but crucial for future work.\nA key aspect of DSG is that our framework does not limit the expressiveness of schemas to a specific\nset of symbols or the grounding process to a specific set of answers. The only prior we inject into\nDSG is the hierarchical process of resolving the schema\u2014the interface between the LLM and VLMs\nis unconstrained language. In theory, DSG can propose and ground any concept and any individual\ncomponent, allowing the LLM and VLM to determine what defines a visual abstraction without any\nrestrictions. But such flexibility may also render the schema ambiguous and not universal enough to\nimprove downstream tasks. However, we see that the empirical evidence in our paper supports DSG,\nand we believe that DSG serves as a proof of concept for the kind of inductive biases that should\nbe incorporated into visual reasoning systems\u2014a structured thinking process guided by flexible but\nexplicit knowledge can augment visual abstraction interpretation.\nWhile we have made progress on understanding what makes a maze look like a maze, in proposing\nDSG as a framework for leveraging explicit schemas backed by powerful LLMs and VLMs, we are\nfar from solving all reasoning across abstract concepts as humans do. There may be ways to improve\ngrounding of individual components, as well as more efficient priors that can be incorporated into the\nsystem. We leave such exploration to future work.\n7 C ONCLUSION\nWe propose Deep Schema Grounding as a promising approach to understanding visual abstractions.\nDSG leverages schemas of concepts to decompose abstract concepts into subcomponents and model\ntheir dependencies. Our framework extracts explicitly structured schemas from large language\nmodels and hierarchically grounds them to images with vision-language models. On the Visual\nAbstractions Dataset, a visual question-answering benchmark composed of real-world images with\ndiverse underlying abstract concepts, DSG demonstrates significant improvements compared to base\nVLMs. DSG is a step toward interpreting visual abstractions as humans do; more remains to be done\nin the challenge of visual abstraction reasoning.\n10ACKNOWLEDGMENTS\nWe thank Michael Li and Weiyu Liu for providing valuable feedback. This work is in part supported\nby the Stanford Institute for Human-Centered Artificial Intelligence (HAI), ONR N00014-23-1-2355,\nONR YIP N00014-24-1-2117, NSF RI #2211258, Air Force Office of Scientific Research (AFOSR)\nYIP FA9550-23-1-0127, the Center for Brain, Minds, and Machines (CBMM), the MIT Quest for\nIntelligence, MIT\u2013IBM Watson AI Lab, and Analog Devices. JH is also supported by the Knight\nHennessy Scholarship and the NSF Graduate Research Fellowship.\nREFERENCES\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. VQA: Visual Question Answering. In Proceedings of the IEEE international\nconference on computer vision , pp. 2425\u20132433, 2015.\nIrving Biederman. Recognition-by-Components: A Theory of Human Image Understanding. Psycho-\nlogical review , 94(2):115, 1987.\nSusan Carey. Conceptual Change in Childhood . MIT Press, 1985.\nSusan Carey. The Origin of Concepts. Journal of Cognition and Development , 1(1):37\u201341, 2000.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll \u00b4ar,\nand C Lawrence Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server.\narXiv:1504.00325 , 2015.\nYangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Measuring and\nImproving Chain-of-thought Reasoning in Vision-Language Models. arXiv:2309.04461 , 2023.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale N Fung, and Steven Hoi. InstructBLIP: Towards General-purpose Vision-\nlanguage Models with Instruction Tuning. Advances in Neural Information Processing Systems ,\n36, 2024.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. PAL: Program-aided Language Models. In International Conference on Machine\nLearning , pp. 10764\u201310799. PMLR, 2023.\nAlison Gopnik. Conceptual and Semantic Development as Theory Change: The Case of Object\nPermanence. Mind & Language , 3(3):197\u2013216, 1988.\nAlison Gopnik and Andrew N Meltzoff. Words, Thoughts, and Theories . MIT Press, 1997.\nTanmay Gupta and Aniruddha Kembhavi. Visual Programming: Compositional Visual Reasoning\nwithout Training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 14953\u201314962, 2023.\nJoy Hsu, Jiayuan Mao, Josh Tenenbaum, and Jiajun Wu. What\u2019s Left? Concept Grounding with\nLogic-Enhanced Foundation Models. Advances in Neural Information Processing Systems , 36,\n2024.\nRonghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to\nReason: End-to-End Module Networks for Visual Question Answering. In Proceedings of the\nIEEE International Conference on Computer Vision (ICCV) , 2017.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and\nPercy Liang. Concept Bottleneck Models. In International conference on machine learning , pp.\n5338\u20135348. PMLR, 2020.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual Genome: Connecting Language\nand Vision using Crowdsourced Dense Image Annotations. International journal of computer\nvision , 123:32\u201373, 2017.\n11Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart \u00b4\u0131n-\nMart \u00b4\u0131n, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A\nBenchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation. In CoRL ,\npp. 80\u201393. PMLR, 2023.\nLin Li, Jun Xiao, Guikun Chen, Jian Shao, Yueting Zhuang, and Long Chen. Zero-shot Visual\nRelation Detection via Composite Visual Cues from Large Language Models. Advances in Neural\nInformation Processing Systems , 36, 2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. NeurIPS ,\n36, 2024.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to Explain: Multimodal Reasoning via Thought Chains for\nScience Question Answering. Advances in Neural Information Processing Systems , 35:2507\u20132521,\n2022.\nJiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The Neuro-\nSymbolic Concept Learner: Interpreting Scenes, Words, and Sentences from Natural Supervision.\nInICLR , 2019.\nSachit Menon and Carl V ondrick. Visual Classification via Description from Large Language Models.\narXiv:2210.07183 , 2022.\nAdam Morton. Frames of Mind: Constraints on the Common-sense Conception of the Mental . Oxford\nUniversity Press, 1980.\nOpenAI. ChatGPT Can Now See, Hear, and Speak. https://openai.com/blog/\nchatgpt-can-now-see-hear-and-speak , 2023.\nHamed Pirsiavash, Carl V ondrick, and Antonio Torralba. Inferring the Why in Images.\narXiv:1406.5472 , 2, 2014.\nRoger C Schank and Robert P Abelson. Scripts, Plans, and Knowledge. In IJCAI , volume 75, pp.\n151\u2013157, 1975.\nXiujie Song, Mengyue Wu, Kenny Q Zhu, Chunhao Zhang, and Yanyi Chen. A Cognitive Eval-\nuation Benchmark of Image Reasoning and Description for Large Vision Language Models.\narXiv:2402.18409 , 2024.\nRobyn Speer, Joshua Chin, and Catherine Havasi. ConceptNet 5.5: An Open Multilingual Graph of\nGeneral Knowledge. In Proceedings of the AAAI conference on artificial intelligence , volume 31,\n2017.\nD\u00b4\u0131dac Sur \u00b4\u0131s, Sachit Menon, and Carl V ondrick. ViperGPT: Visual Inference via Python Execution for\nReasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.\n11888\u201311898, 2023.\nMisha Wagner, Hector Basevi, Rakshith Shetty, Wenbin Li, Mateusz Malinowski, Mario Fritz,\nand Ales Leonardis. Answering Visual What-if Questions: From Actions to Predicted Scene\nDescriptions. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops ,\npp. 0\u20130, 2018.\nPeng Wang,"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**1. Abstract:**\nThe paper evaluates machine learning (ML) methods for two-sample testing with right-censored observations. Several ML-based methods with different architectures are developed and implemented as two-sample tests, forming an ensemble that combines predictions from classical tests. The study presents training results, statistical power comparison to classical tests, analysis of test statistic distributions under the null hypothesis, and feature importance evaluations, all conducted on a synthetic dataset using Monte Carlo simulations. Materials for replication are available on GitHub and Hugging Face.\n\n**2. Introduction:**\n- **Context:** Two-sample problem testing on censored data is crucial in multiple domains such as reliability, medicine, economics, and more. Classical methods, while efficient in certain scenarios, often rely on parametric or non-parametric assumptions which may not be universally optimal. There is a significant need for improved methods capable of handling the nuances of right-censored data effectively.\n- **Research Questions:** The paper addresses the following objectives:\n  1. Develop ML-based methods for two-sample testing under right-censored data.\n  2. Evaluate the effectiveness of these methods compared to classical ones.\n  3. Outline the practical applicability of the proposed methods.\n  4. Make the materials and results publicly available.\n- **Literature Review:** Existing methods largely rely on specific sensitive statistics based on assumptions, leading to diverse tests suited to different scenarios. This work integrates ML to combine different test properties to enhance sensitivity and adaptability to censored data scenarios.\n\n**3. Related Work:**\n- **Review of Prior Work:**\n  - Different generalizations of classical tests (e.g., the Cram\u00e9r\u2013von-Mises and Kolmogorov-Smirnov) have been explored for censored data.\n  - Weighted function-based tests (e.g., weighted log-rank tests) offer enhanced properties.\n  - Specific competing hypothesis models have been proposed by Bagdonavi\u010dius and Nikulin among others.\n  - Attempts to use auxiliary functions to develop more sensitive tests have shown potential.\n- **Advancement:** This paper proposes a novel approach where different classical two-sample tests serve as features combined through ML methods. This ensemble (stacking) approach leverages the strengths of various tests to improve overall performance on censored data.\n\n**4. Method:**\n- **Proposed Approach:**\n  - **Problem Formulation:** Two samples (\u03be1 and \u03be2) are considered, each with survival distributions S1(t) and S2(t). Tests are conducted on right-censored observations tij, comparing null (H0: S1(t) = S2(t)) and alternative hypotheses (HA: S1(t) \u2260 S2(t)).\n  - **Classical Two-Sample Tests:** Implemented tests include log-rank, Gehan\u2019s generalized Wilcoxon, Peto and Peto\u2019s Wilcoxon, weighted tests, and Bagdonavi\u010dius-Nikulin tests.\n  - **ML-based Ensemble Method:** The two-sample problem is transformed into a binary classification task where each classical test serves as an independent feature. ML models used include:\n    - Logistic Regression\n    - Random Forest\n    - Gradient Boosting Machine\n    - XGBoost\n    - CatBoost\n    - LightAutoML\n  - The models compute p-values from classical tests and include sample sizes and censoring rates as additional features.\n\n**5. Experimental Setup:**\n- **Datasets:**\n  - Synthetic data generated using the Smirnov transform and Monte Carlo simulation.\n  - 27 alternative hypotheses applied across varying sample sizes (20 to 1000) and censoring rates (0% to 50%).\n  - 37,650 replications per hypothesis, with the dataset split for training and testing.\n- **Hardware and Software:**\n  - Implementation details are not explicitly provided but are available through provided code.\n  - Standard libraries such as sklearn for ML methods.\n- **Evaluation Metrics:**\n  - Accuracy, Precision, Recall, ROC AUC, Average Precision, Specificity, and Proportion without False Negatives.\n- **Baselines:**\n  - Compared against classical two-sample tests and state-of-the-art methods in the field.\n\n**6. Results and Analysis:**\n- **Training Results:**\n  - Metrics for each ML method on test subsamples (Accuracy, Precision, Recall, etc.) were provided, indicating that LightAutoML performed best overall.\n- **Feature Importance:**\n  - Significant features included Bagdonavi\u010dius-Nikulin and MIN3 tests. Sample size features were more significant than censoring rate features.\n- **Test Power:**\n  - The proposed methods generally showed higher test power than classical ones, especially under varying censoring rates.\n- **Test Statistic Distribution:**\n  - Detailed exploration of test statistic distributions under H0 confirmed robustness of methods in practice.\n\n**7. Discussion:**\n- **Interpretation of Results:**\n  - The ensemble methods outperformed classical tests in terms of test power and robustness across various scenarios.\n  - The inclusion of multiple tests as features helped achieve higher sensitivity to differences between survival distributions.\n- **Implications:**\n  - Improved methods for hypothesis testing on censored data could significantly impact fields requiring reliability analysis, medical research, and more.\n- **Limitations and Biases:**\n  - Potential overfitting on synthetic data, requiring further validation on real-world datasets.\n  - Some features may become less significant depending on the nature of the underlying data distributions.\n\n**8. Conclusion:**\n- **Key Contributions:**\n  - Development and evaluation of ML-based methods for two-sample testing under censored data.\n  - Demonstration of superior performance compared to classical methods.\n  - Provision of detailed results and open-access materials for further research.\n- **Future Research:**\n  - Application of these methods to diverse, real-world datasets.\n  - Exploration of additional features or models to further enhance performance.\n  - Investigation into the generalizability of findings across different domains and data characteristics.\n\nThis comprehensive summary captures the essence of the paper and provides detailed insights required to replicate the study or build upon this work."}, {"role": "assistant", "content": "ARTICLE\nMachine Learning for Two-Sample Testing\nunder Right-Censored Data: A Simulation Study\nPetr Philonenkoaand Sergey Postovalovb\naSber AI Lab, Moscow, Russia\nbNovosibirsk State Technical University, Novosibirsk, Russia\nARTICLE HISTORY\nCompiled September 13, 2024\nABSTRACT\nThe focus of this study is to evaluate the effectiveness of Machine Learning (ML)\nmethods for two-sample testing with right-censored observations. To achieve this, we\ndevelop several ML-based methods with varying architectures and implement them\nas two-sample tests. Each method is an ensemble (stacking) that combines predic-\ntions from classical two-sample tests. This paper presents the results of training the\nproposed ML methods, examines their statistical power compared to classical two-\nsample tests, analyzes the distribution of test statistics for the proposed methods\nwhen the null hypothesis is true, and evaluates the significance of the features in-\ncorporated into the proposed methods. All results from numerical experiments were\nobtained from a synthetic dataset generated using the Smirnov transform (Inverse\nTransform Sampling) and replicated multiple times through Monte Carlo simula-\ntion. To test the two-sample problem with right-censored observations, one can use\nthe proposed two-sample methods. All necessary materials (source code, example\nscripts, dataset, and samples) are available on GitHub and Hugging Face.\nKEYWORDS\nHypothesis Testing; Two-Sample Problem; Lifetime Data; Machine Learning;\nMonte-Carlo Simulation; GitHub.\n1. Introduction\nTwo-sample problem testing (and hypothesis testing in general) on censored data is an\nintegral part of solving problems in reliability, physics, biology, medicine, economics,\nbioinformatics, sociology, engineering, and many other fields [1]. It is well known that\nthere is no universal method preferred in all usage scenarios for this class of problems.\nGiven the wide applicability, there is a constant need for improvement in the methods\nused. Machine learning methods, which are currently being applied in various domains,\ncan help address this need [2].\nIn the construction of classical two-sample tests, reliance is primarily placed on\ncertain parametric or non-parametric assumptions [3]. This allows the method to ef-\nfectively distinguish between certain competing hypotheses, but unfortunately, it may\nstruggle to differentiate others. Researchers have made numerous attempts to com-\nbine the properties of different tests [4\u20136]. However, such attempts are complicated by\nthe peculiarities of the subject area, which involves censored data. In the literature,\nCONTACT Petr Philonenko. E-mail: petr-filonenko@mail.ruarXiv:2409.08201v1  [cs.LG]  12 Sep 2024there are also modern works that contrapose ML methods against classical two-sample\ntests [7] or for the uncensored case [8\u201310]. However, these comparisons often involve a\nsmall number of methods. In our work, we propose considering a wide range of diverse\ntwo-sample tests. Each two-sample tests is treated as an feature, and through the use\nof ML-methods, we create a stacking (ensemble) that combines these features.\nTherefore, the main objectives of this research are as follows:\n(1) Develop methods for two-sample problem under right-censored data using ML-\nmethods;\n(2) Evaluate the effectiveness of the proposed methods compared to existing classical\nmethods;\n(3) Describe the practical applicability of the proposed methods.\n(4) Publish the repositories and results of the proposed methods for public use.\n2. Related Works\nAs mentioned earlier, existing two-sample tests often rely on constructing a specific\nsensitive statistic based on certain parametric or non-parametric assumptions. This\napproach enhances sensitivity to certain types of alternatives.\nOne group of such tests includes various generalizations of classical tests such as\nthe Cram\u00b4 er\u2013von-Mises and Kolmogorov-Smirnov for censored data, as shown in the\nworks of Gill [11], Fleming et al. [12], Koziol [13], Schumacher [14], and Fleming [15].\nAnother group of tests is based on the use of weight functions, such as the weighted\nlog-rank test. The use of different weight functions generates a multitude of different\ntwo-sample tests with new properties compared to the original ones. For example,\nGehan [16], Peto and Peto [17], Tarone and Ware [18], Prentice [19], Kalbfleisch and\nPrentice [20], Fleming and Harrington [21, 22].\nThe third group of tests is based on the assumption of a specific form of competing\nhypotheses. This approach was used in the works of Bagdonavi\u02c7 cius and Nikulin. They\nassumed that competing hypotheses could correspond to models with single (SCE-\nmodel [23]) or multiple (MCE-model [24]) intersections, and the GPH-model [25].\nAdditionally, due to their simplicity and efficiency, the method of constructing meth-\nods using auxiliary functions has gained popularity. These methods involve computa-\ntion the value of the selection function in advance, and then using this value to make\ndecisions about the value of the test statistic. For example, Martinez and Naranjo [5], as\nwell as Philonenko and Postovalov [26], propose using the selection function to choose\nbetween the log-rank and Wilcoxon tests. In another work [27], authors combine the\nBagdonavi\u02c7 cius-Nikulin and weighted Kaplan-Meier tests by the selection function.\nEstablished the potential effectiveness of this approach, the question arises of con-\nstructing a selection function that is more sensitive to the characteristics of the un-\nderlying tests. ML methods can be helpful in this regard, as they can be considered\nas a selection function on a set of two-sample tests, using each test as a feature in the\nmodel.\nThus, in Section 3, we present the problem formulation and describe existing sta-\ntistical methods. In Section 4, we describe the proposed methods for the two-sample\nproblem using machine learning. In Section 5, we provide the results of numerical\nexperiments, including the training results of the proposed methods, power study of\nthe proposed methods, study of the test statistic distribution, and feature importance\nresults.\n23. Materials & Methods\n3.1. Problem Statement\nSuppose that we have two samples of continues variables \u03be1and\u03be2respectively, X1=\n{t11, t12,\u00b7\u00b7\u00b7, t1n1}andX2={t21, t22,\u00b7\u00b7\u00b7, t2n2}of two survival distributions S1(t)\nandS2(t). The observation tij=min(Tij, Cij), where TijandCijare the failure\nand censoring times for the jth object of the ith group. TijandCijare i.i.d. with a\ncumulative distribution function (CDF) Fi(t) and FC\ni(t) respectively. Survival curve\nmeans the probability of survival in the time interval (0 , t), i.e. Si(t) =P{\u03bei> t}=\n1\u2212Fi(t).\nThen the null hypothesis isH0:S1(t) =S2(t) against an alternative hypoth-\nesisHA:S1(t)\u0338=S2(t).\nFurther, we suppose that the elements of samples are ordered: t11<\u00b7\u00b7\u00b7< t1n1and\nt21<\u00b7\u00b7\u00b7< t2n2. Denote the censoring indicators cijascij=(\n0, tijis failure,\n1, tijis censored,\nandthe censoring rate riasri=Pni\nj=1cij/ni.\n3.2. Two-Sample Tests\n3.2.1. Log-rank test\nThelog-rank test is also called as Mantel\u2013Haenszel test orMantel\u2013Cox test [3, 28,\n29]. The test statistic is\nSLG=Pn\ni=1(1\u2212ci)\u2212Pi\nj=11\nn\u2212j+1q\n(Pn\ni=1(1\u2212ci)n\u2212i\nn\u2212i+1)n1n2\nn(n\u22121).\nThe null hypothesis H0is rejected with the significance level \u03b1, if|SLG|> z1\u2212\u03b1/2,\nwhere z1\u2212\u03b1/2is the (1 \u2212\u03b1/2)-quantile of the standard normal distribution.\n3.2.2. Generalizations of Wilcoxon test\nGehan\u2019s generalized Wilcoxon test. This test is also called as Gehan-Breslow-\nWilcoxon test . The test statistic [3, 16] is\nSG=Pn\ni=1(1\u2212\u03bdi)hiq\nn1n2\nn(n\u22121)Pn\ni\u22121(1\u2212\u03bdi)h2\ni,\nwhere hi=Pn\nj=1\u03bdjhijandhij=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3+1, ti> tj&cj= 0 & \u03bdi= 0 & \u03bdj= 1,\n\u22121, ti< tj&cj= 0 & \u03bdi= 0 & \u03bdj= 1,\n0,otherwise.\nThe null hypothesis H0is rejected with the significance level \u03b1, if|SG|> z 1\u2212\u03b1/2,\nwhere z1\u2212\u03b1/2is the (1 \u2212\u03b1/2)-quantile of the standard normal distribution.\nPeto and Peto\u2019s generalized Wilcoxon test. The test statistic [3, 17] is\nSP=n(n\u22121)Pn\ni=1ui(1\u2212\u03bdi)\nn1n2Pn\ni=1u2\ni,\n3where ui=(\u02c6SKM(ti) +\u02c6SKM(tki), c= 0,\n\u02c6SKM(tki)\u22121, c= 1,ki=max{j|j\u2208 {0, ..., i\u22121}, cj= 0},\nc0= 0 and \u02c6SKM(t) is the Kaplan-Meier estimator [30] of S(t) by the pooled sample T.\nThe null hypothesis H0is rejected with the significance level \u03b1, if|SP|> z 1\u2212\u03b1/2,\nwhere z1\u2212\u03b1/2is the (1 \u2212\u03b1/2)-quantile of the standard normal distribution.\n3.2.3. Weighted tests\nWeighted log-rank test. Based on the findings of [31], we describe the statistics in\na unified format, covered all previously presented tests and incorporating additional\nones. The test statistic can be computed as SWLG =U2/\u03c3where\nU=n1X\nj=1(1\u2212c1j)K(t1j)Y2(t1j)\nY(t1j)+n2X\nj=1(1\u2212c2j)K(t2j)Y1(t2j)\nY(t2j),\n\u03c3=2X\ni=1niX\nj=1(1\u2212cij)K2(tij)Y1(tij)Y2(tij)\nY2(tij),\nY(t) =Y1(t) +Y2(t), Yi(t) =niX\nj=1Yij(t), Yij= 1tij\u2265t, i= 1,2.\nUsing a certain parameterizations of K(t), it is possible to vary a behaviour of the\ntest statistic. We present some possible examples of parameterization:\n(1)log-rank (Mantel\u2013Haenszel) statistic :K(t) = 1 /\u221an1+n2;\n(2)Gehan-Breslow-Wilcoxon statistic :K(t) =Y(t)/p\n(n1+n2)3;\n(3)Peto-Peto-Prentice statistic :K(t) =\u02c6S(t)/\u221an1+n2;\n(4)Tarone-Ware statistic :K(t) =p\nY(t)/(n1+n2);\n(5)Prentice statistic :K(t) =\u02c6S(t)\u221an1+n2\u00b7Y(t)\nY(t)+1;\nwhere \u02c6S(t) is the Kaplan-Meier estimator on the pooled sample.\nThe test statistic SWLG is asymptotically distributed as chi-square distribution with\none degree of freedom. The test statistic has a right-critical area.\nWeighted Kaplan-Meier test. The weighted Kaplan\u2013Meier test statistic mea-\nsures the average difference between the survival functions.\nLet\u02c6S1(t) and \u02c6S2(t) are the Kaplan\u2013Meier estimates [30] of the S1(t) and S2(t)\nconsequently, and \u02c6S(t) is the Kaplan\u2013Meier estimate of the pooled sample. For every\nsample Xi, we construct additional sample Xichanging the indicator cijto the oppo-\nsitecij= 1\u2212cij. Thus, \u02c6SC\n1(t) and \u02c6SC\n2(t) are the Kaplan\u2013Meier estimates for additional\nsamples X1andX2.\nThe test statistic [32] can be computed as SWKM =U/\u221a\n\u03c32where\nU=rn1n2\nn1+n2ZTC\n0\u02c6w(t){\u02c6S2(t)\u2212\u02c6S1(t)}dt,\u02c6w(t) =(n1+n2)\u02c6S1C(t\u2212)\u02c6S2C(t\u2212)\nn1\u02c6S1C(t\u2212) +n2\u02c6S2C(t\u2212)\n4andTC= max t\u2208R\u0010\nmin{\u02c6S1(t),\u02c6S2(t),\u02c6S1C(t),\u02c6S2C(t)}>0\u0011\n.\nThe weight function w(t) downweights the contributions of the S2(t)\u2212S1(t) inU\nover later time periods if censoring is heavy. The statistic Uis asymptotically normal\ndistributed with a variance \u03c32:\n\u03c32=\u2212ZTC\n0\u0012ZTC\nt\u02c6w(\u03c4)\u02c6S(\u03c4)d\u03c4\u00132n1\u02c6SC\n1(t\u2212) +n2\u02c6SC\n2(t\u2212)\n(n1+n2)\u0010\n\u02c6SC\n1(t\u2212)\u02c6SC\n2(t\u2212)\u0011d(\u02c6S(t)\u2212\u02c6S(t\u2212))\n\u02c6S(t)\u02c6S(t\u2212).\nThe test statistic SWKM is asymptotically distributed as the standard normal dis-\ntribution N(0,1) with two-side critical area.\n3.2.4. Bagdonavi\u02c7 cius-Nikulin tests\nTest based on the Simple Cross-Effect (SCE) model. The test statistic [23] is\nSBN-SCE = (U1, U2)T\u03a3\u22121(U1, U2) where\nU1=n1X\nj=1(1\u2212c1j)K1(t1j)\nY(t1j)Y2(t1j)\u2212n2X\nj=1(1\u2212c2j)K1(t2j)\nY(t2j)Y1(t2j),\nU2=n1X\nj=1(1\u2212c1j)K2(t1j)\nY(t1j)Y2(t1j)\u2212n2X\nj=1(1\u2212c2j)K2(t2j)\nY(t2j)Y1(t2j).\nThe elements of the matrix \u03a3 can be computed as\n\u03c3ij=2X\nr=1nrX\ns=1(1\u2212crs)Ki(trs)Y1(trs)Y2(trs)\nY2(trs).\nThe following functions are K1(t) and K2(t) may be used\nK1(t) =exp(\u2212\u039b(t))/\u221an, K 2(t) = (exp(\u2212\u039b(t))\u22121)/\u221an,\nY(t) =Y1(t) +Y2(t), Yi(t) =niX\nj=1Yij(t), Yij(t) = 1 tij\u2265t, i= 1,2,\n\u039b(t) =2X\ni=1X\nj:cij\u22120,tij\u2264t1\nY(tij).\nThe test statistic SBN-SCE is asymptotically distributed as chi-square distribution\nwith two degrees of freedom. The test has right-side critical area.\n5Test based on the Multiple Cross-Effect (MCE) model. The test statistic [24]\nisSBN-MCE = (U1, U2, U3)T\u03a3\u22121(U1, U2, U3),where\nU1=X\nj:c1j=0Y2(t1j)\nY(t1j)\u2212X\nj:c2j=0Y1(t2j)\nY(t2j), U2=\u2212X\nj:c1j=0Y2(t1j)\nY(t1j)\u039b(t1j)\u2212X\nj:c2j=0Y1(t2j)\nY(t2j)\u039b(t2j),\nU3=\u2212X\nj:c1j=0Y2(t1j)\nY(t1j)\u039b2(t1j)\u2212X\nj:c2j=0Y1(t2j)\nY(t2j)\u039b2(t2j).\nThe elements of the matrix \u03a3 can be computed as\n\u03c311=2X\ni=1X\nj:cij=0Y1(tij)Y2(tij)\nY2(tij), \u03c312=\u03c321=2X\ni=1X\nj:cij=0Y1(tij)Y2(tij)\nY2(tij)\u039b(tij),\n\u03c313=\u03c331=\u03c322=2X\ni=1X\nj:cij=0Y1(tij)Y2(tij)\nY2(tij)\u039b2(tij),\n\u03c323=\u03c332=2X\ni=1X\nj:cij=0Y1(tij)Y2(tij)\nY2(tij)\u039b3(tij), \u03c333=2X\ni=1X\nj:cij=0Y1(tij)Y2(tij)\nY2(tij)\u039b4(tij).\nThe test statistic SBN-MCE is asymptotically distributed as chi-square distribution\nwith three degrees of freedom. The test has right-side critical area.\nTest based on the Generalized Proportional Hazards (GPH) model. The\ntest statistic [25] is SBN-GPH = (U1, U2)T\u03a3\u22121(U1, U2) where\nU1=X\nj:c1j=0Y2(t1j)\nY(t1j)\u2212X\nj:c2j=0Y1(t2j)\nY(t2j),\nU2=\u2212X\nj:c1j=0Y2(t1j)\nY(t1j)ln(1 + \u039b( t1j)) +X\nj:c2j=0Y1(t2j)\nY(t2j)ln(1 + \u039b( t2j)).\nThe elements of the matrix \u03a3 can be computed as\n\u03c311=2X\ni=1X\nj:cij=0Y1(tij)Y2(tij)/Y2(tij),\n\u03c322=2X\ni=1X\nj:cij=0Y1(tij)Y2(tij) ln2(1 + \u039b( tij))/Y2(tij),\n6\u03c312=\u03c321=2X\ni=1X\nj:cij=0Y1(tij)Y2(tij) ln(1 + \u039b( tij))/Y2(tij).\nThe test statistic SBN-GPH is asymptotically distributed as chi-square distribution\nwith two degrees of freedom. The test has right-side critical area.\n3.2.5. Two-Stage tests\nQ-test. The test statistic [4] is SQ=SLGifQ < 0 else SPwhere Q={\u02c6S2(q0.6)\u2212\n\u02c6S1(q0.6)}\u2212{ \u02c6S2(q0.2)\u2212\u02c6S1(q0.2)},qp=\u02c6S1\u22121(p), and \u02c6Si(t) is the Kaplan-Meier estimator.\nThe test statistic SQhas a double-side critical area and the H0is rejected under a\nlarge value of the |SQ|.\nMaximum value test. The test statistic [26] is SMAX = max( |SLG|,|SG|).\nThe test statistic SMAX has right-side critical area. The distribution of the test\nstatistic SMAX under the H0is represented in [33].\nMIN3 test. The test statistic [27] is SMIN 3= min( pvWKM , pvBN-MCE , pvBN-GPH )\nwhere pvWKM = 2\u00b7min{FN(0,1)(SWKM ),1\u2212FN(0,1)(SWKM )}, pv BN-MCE = 1\u2212\nF\u03c72(3)(SBN-MCE ),pvBN-GPH = 1\u2212F\u03c72(2)(SBN-GPH ).\nThe test statistic SMIN 3has left-side critical area. The distribution of the test\nstatistic SMIN 3under the H0is represented in [27].\n3.3. Alternative Hypotheses\nWe study the performance of two-sample tests using alternative hypotheses where each\nalternative is a pair of competing hypotheses S1(t) and S2(t). Studies [5, 6] examine the\nimpact of alternatives with differences at early, middle, and late time on the test power.\nStudies of Bagdonavi\u02c7 cius and Nikulin [23, 24] use another way based on alternatives\nwith different numbers of intersections of competing hypotheses. We combine these\nexperiments together and provide experiments that have alternatives with 0, 1, and 2\nintersections at early, middle, and late time.\nTaking into account that the findings obtained for one alternative are not repre-\nsentative in the general case. Therefore, we form groups of similar alternatives that\ncontain various closest competing hypotheses. The alternative groups are represented\nin Figure 1. Additionally, in order to reduce the impact of a random choice for alter-\nnative, we vary families of F(t) and FC(t) for each alternative. The following families\nof distributions are used in the construction of alternative hypotheses:\n(1)fExp(x;\u00b5, \u03bb) =\u03bbexp(\u2212\u03bb(x\u2212\u00b5)), x\u2265\u00b5;\n(2)fWe(x;\u00b5, \u03bb, \u03bd ) =\u03bd(x\u2212\u00b5)\u03bd\u22121exp(\u2212((x\u2212\u00b5)/\u03bb)\u03bd)/\u03bb\u03bd, x\u2265\u00b5;\n(3)fG(x;\u00b5, \u03bb, \u03bd ) = (( x\u2212\u00b5)/\u03bb)\u03bd\u22121exp(\u2212(x\u2212\u00b5)/\u03bb)\u03bb\u0393(\u03bd), x\u2265\u00b5;\n(4)fLgN(x;\u00b5, \u03bb) = exp( \u2212(lnx\u2212\u00b5)2/2\u03bb2)/x\u221a\n2\u03c0\u03bb2, x\u22650.\nNote that we fix the size of inequality between competing hypotheses S1(t) and S2(t)\nbyL1= 0.1 where L1is the l1-norm. It is necessary to ensure the closest experimental\nconditions between alternatives. The choice of this value is due to the following reasons:\n(1) It provides similar conditions between alternatives to ensure numerical experi-\nments.\n(2) It provides the test power 1 \u2212\u03b2to be in the range \u03b1 < 1\u2212\u03b2 < 1 for most\nconsidered sample sizes where \u03b1and\u03b2are errors of the I and II types.\n7Figure 1. Groups of alternative hypotheses\nCensoring rate rcan be analytically computed as r=R+\u221e\n\u2212\u221e\u0000\n1\u2212FC(t;\u03b8)\u0001\nf(t;\u03b7)dt\nwhere 0 \u2264r\u22641,f(t;\u03b7) =dF(t;\u03b7)/dtand\u03b7,\u03b8are some parameters of the F(t;\u03b7) and\nFC(t;\u03b8). If we fix \u03b7andr, then we can find the parameters \u03b8of the FC(t;\u03b8) by solving\nthe corresponding equation above. This is exactly how alternatives were constructed\nin this study. As a result, we present the parameters of alternatives in Table 1.\n4. Proposed ML-based Methods for Two Sample Problem\nWe approach the two-sample problem as a binary classification task (0 if the H0is\ntrue, 1 if the H0is rejected). Each two-sample test can be considered as an indepen-\ndent feature. Therefore, to solve our problem, we combine the properties of various\nstatistical methods using a stacking architecture. For the binary classification task, we\nconsider various machine learning methods:\n(1)Logistic Regression (denote as LogReg ) implemented in sklearn;\n(2)Random Forest (denote as RF) implemented in sklearn;\n(3)Gradient Boosting Machine (denote as GBM ) implemented in sklearn;\n(4)XGBoost ;\n(5)CatBoost [34] proposed by Yandex;\n(6)LightAutoML [35] (denote as LAMA ) proposed by Sber AI Lab.\nOur method takes two right-censored samples X1andX2as input. For these sam-\nples, we compute the p-value for the two-sample tests described earlier, since small\np-value rejects the null hypothesis H0. We also include information about the sample\nsizes and the censoring rate in order to be more sensitive for the alternative. As output,\nour method returns a value in the interval [0 ,1], which serves as the test statistic for a\nnew two-sample test. For this method, we can estimate the test power, investigate the\ntest statistic distribution under the null hypothesis G(S|H0), and conduct any other\nexperiments. We detail the process of the described method in Figure 2.\nThe test statistic of each proposed method has a right-side critical area (according to\n8Table 1. Groups of alternative hypotheses\nGroup Description Hi S1(t) S2(t)\nI0 intersections\nwith difference\nin the early timeH01 Exp(0,1) Exp(0.1,1)\nH02 We(0,1.1,2.4) LgN(0,0.370)\nH03 LgN(0.01,0.913) Exp(0,0.742)\nII0 intersections\nwith difference\nin the middle timeH04 G(0,1.060,1.160) Exp(0,0.863)\nH05 Exp(0,1.3) We(0,0.9,1.1)\nH06 Exp(0,1) We(0.09,1.1,1.07)\nIII0 intersections\nwith difference\nin the late timeH07 Exp(0,1.3) G(0,0.806,1.064)\nH08 We(0.5,1,1.2) Exp(0.567,1)\nH09We(0.118,1.1,1.735) LgN(0.01,0.6)\nIV1 intersection\nin the\nearly timeH11 Exp(0,1) Exp(0.05,1.159)\nH12 G(0,1.273,1.475) G(0.159,1.300,1.273)\nH13 We(0.02,1,1.1) Exp(0,0.909)\nV1 intersection\nin the\nmiddle timeH14 We(0,0.980,0.905) G(0,0.972,0.974)\nH15 Exp(0,1) We(0.071,0.906,1.059)\nH16 G(0.01,1,1.15) Exp(0,0.833)\nVI1 intersection\nin the\nlate timeH17 We(0,0.968,1.214) Exp(0,1.107)\nH18 G(0,1.1,1.040) G(0,0.9,1.302)\nH19 We(0.5,1.1,1.1) Exp(0.471,1)\nVII2 intersections\nin the early\nand middle timeH21 LgN(0,0.948) We(0.173,1.325,0.911)\nH22 Exp(0.5,1.047) LgN(0.141,0.596)\nH23 We(0.5,1,1.2) Exp(0.530,1)\nVIII2 intersections\nin the early\nand late timeH24 LgN(0,0.916) G(0.01,1.213,1.192)\nH25 LgN(0,0.817) Exp(0.185,0.818)\nH26We(0.01,1.697,1.846) LgN(0.293,0.569)\nIX2 intersections\nin the middle\nand late timeH27 We(0,1.355,1.018) LgN(0.000,0.867)\nH28 G(0,1.134,1.231) LgN(0,0.876)\nH29 Exp(0,0.744) LgN(0,0.866)\nFigure 2. The implementation flow chart of the proposed methods\n9the problem statement based on the binary classification task) and the null hypothesis\nH0is rejected when the test statistic is large.\nFor the proposed methods, we limit the number of features at the feature engineering\nstage, since the goal is to add information to the methods about the properties of the\ntwo-sample tests on various sample sizes and censoring rates instead of overfitting on\nthe selected alternatives.\nIn training ML models, we select the best models by maximizing the Accuracy met-\nric. Since we have a balanced dataset (50% with the H0is true and 50% with the H0is\nrejected) of synthetic data, maximizing Accuracy allows us to minimize the probabili-\nties of Type I and Type II errors. The synthetic data were simulated using the Monte\nCarlo replications. Also, we report other metrics for reference: the Precision, Recall,\nROC AUC, Average Precision, Specificity and Proportion of predictions without FN\n(false negative): ( TP+FP+TN)/(TP+FP+FN+TN).\nYou also can solve the two-sample problem by the proposed methods which are\navailable in GitHub.\n5. Numerical Experiments\n5.1. Dataset\nTo conduct numeric experiments for the proposed methods, we have simulated dataset\nincluding statistics of the two-sample tests described earlier. Each observation in the\ndataset contains the set of computed two-sample test statistics under following pa-\nrameters:\n\u202227 alternative hypotheses ( H01\u2212H09,H11\u2212H19, and H21\u2212H29);\n\u20222 competing hypotheses S1(t) and S2(t) for each alternative:\n(1) if X1\u223c\u03be1compares to X2\u223c\u03be1, then H0is true. It is indicated as \u2019H0\u2019;\n(2) if X1\u223c\u03be1compares to X2\u223c\u03be2, then H0is rejected. It is indicated as \u2019H1\u2019;\n\u202210 sample sizes for each competing hypothesis (20, 30, 50, 75, 100, 150, 200, 300,\n500, and 1000 observations);\n\u20226 censoring rates for each competing hypothesis (0%, 10%, 20%, 30%, 40%, 50%);\n\u202237,650 Monte Carlo replications for each competing hypothesis.\nFor each observation in the dataset, the target variable indicates \u2019H0\u2019 if the null\nhypothesis is true, or \u2019H1\u2019 if the null hypothesis is rejected. In total, the dataset\nincludes 27 x 2 x 10 x 6 x 37,650 = 121,986,000 observations. We split the dataset into\ntwo samples:\n(1) 7,650 Monte Carlo replications with all combinations of parameters ( \u223c24.7\nmillion observations) to train the proposed methods ( Train sample );\n(2) rest of 30,000 Monte Carlo replications ( Simulation sample ) for statistical\ninvestigations (test power, test statistic distributions and etc.).\nThis full dataset, split into samples, source code (C++) for simulation, and a brief\nsummary are available in the Hugging Face repository (DOI: 10.57967/hf/2978).\n5.2. Proposed Methods Training\nIn this numeric experiment, we train the proposed methods, then test them using sev-\neral metrics and assess the feature importance. To reach this, we stratify the Train\n10Table 2. Training results of the proposed methods (with 95%-th confidence intervals)\nMetric LogReg RF GBM\nAccuracy 0.633 \u00b10.0006 0.6459 \u00b10.0007 0.6465 \u00b10.0008\nPrecision 0.6305 \u00b10.0007 0.7012 \u00b10.0011 0.6955 \u00b10.0010\nRecall 0.6422 \u00b10.0005 0.5086 \u00b10.0008 0.5211 \u00b10.0010\nROC AUC 0.686 \u00b10.0009 0.7155 \u00b10.0005 0.7153 \u00b10.0006\nAvg. Precision 0.6945 \u00b10.0011 0.743 \u00b10.0006 0.7423 \u00b10.0007\nSpecificity 0.6237 \u00b10.0010 0.7833 \u00b10.0008 0.7719 \u00b10.0007\nProp. w/o FN 0.8211 \u00b10.0002 0.7543 \u00b10.0004 0.7605 \u00b10.0005\nMetric XGBoost CatBoost LAMA\nAccuracy 0.672 \u00b10.0010 0.674 \u00b10.0007 0.6771 \u00b10.0008\nPrecision 0.7197 \u00b10.0013 0.7181 \u00b10.0012 0.7281 \u00b10.0012\nRecall 0.5635 \u00b10.0008 0.5728 \u00b10.0006 0.5653 \u00b10.0007\nROC AUC 0.7478 \u00b10.0009 0.7501 \u00b10.0008 0.7539 \u00b10.0008\nAvg. Precision 0.7772 \u00b10.0008 0.7789 \u00b10.0008 0.7828 \u00b10.0008\nSpecificity 0.7805 \u00b10.0012 0.7751 \u00b10.0012 0.7888 \u00b10.0012\nProp. w/o FN 0.7818 \u00b10.0004 0.7864 \u00b10.0003 0.7827 \u00b10.0003\nsample into: train subsample (55%) with \u223c13.6 million observations, validate sub-\nsample (30%) with \u223c7.4 million observations, and test subsample (15%) with \u223c3.7\nmillion observations.\nThe ratio of class balance is equal to 50% in each sample. To find optimal hyper-\nparameters, we apply the Optuna framework with 10 trials. The resulting metrics\ncomputed on the test sample are presented in Table 2.\nThe sequence of the most preferred methods based on the primary metric, Accuracy,\nis as follows: LAMA >CatBoost >XGBoost >GBM or RF >LogReg. This indicates\nthat, on the test subsample , LAMA significantly reduces Type I and Type II errors\ncompared to the other methods.\nNext, let us analyze the Proportion without False Negatives (Type II error). In\nthis scenario, the preferred methods rank as follows: LogReg >CatBoost >LAMA >\nXGBoost >GBM >RF. This suggests that LogReg minimizes Type II errors more\neffectively than the other proposed methods on the test subsample .\nAdditionally, we discuss the ranking ability of the proposed methods as measured\nby Average Precision. This metric assesses the methods\u2019 effectiveness in distinguishing\ndifferences in ordered values, particularly at the extremes of distributions. Given that\nour proposed methods focus on the right-side critical area, they demonstrate an ability\nto distinguish between competing hypotheses. The methods are ranked for this metric\nas follows: LAMA >CatBoost >XGBoost >RF or GBM >LogReg.\nWe also evaluate the importance of the features included in the final versions of each\nproposed method. For this, we use two methods: Permutation Importance (available\nfor all methods) and Feature Importance (available for some methods, as an additional\nassessment). The results are shown in Tables 3 and 4.\nAs a result, we present several key findings for the proposed methods, including: a) as\nexpected in [27], one of most significant features are the two-sample Bagdonavi\u02c7 cius-\nNikulin and MIN3 tests; b) features based on the sample size are more significant than\nfeatures based on the censoring rate; c) features of the censoring rate are at a level\nclose to the insignificance.\n11Table 3. Permutation Importance (features sorted by average rank among all methods)\nfeature LogReg RF GBM XGBoost CatBoost LAMA\nBN-SCE 21.24 0.95 1.64 18.92 26.81 12.03\nBN-GPH 4.42 0.18 0.30 24.20 23.53 11.49\nTarone-Ware 1.01 0.15 0.05 27.66 26.75 12.72\nMIN3 1.02 3.33 2.55 16.16 12.56 7.84\nBN-MCE 15.59 2.15 3.59 6.58 7.64 4.41\nWKM 8.22 0.06 0.02 21.37 16.55 10.70\nGehan 21.36 0.05 0.03 13.30 17.83 8.33\nMAX 2.58 0.06 0.02 12.85 16.93 9.58\nlog-rank 0.08 0.13 0.15 12.81 13.20 7.00\nPeto 1.59 0.04 0.00 17.40 7.21 8.70\nPeto-Prentice 0.21 0.08 0.13 7.70 9.47 4.31\nPrentice 1.29 0.10 0.07 2.38 3.76 3.12\nmin(n1, n2)\u2265500 0.42 0.99 1.09 1.95 1.66 0.77\nQ-test 0.53 0.03 0.04 5.83 4.28 4.91\n0< min (n1, n2)<50 0.08 0.48 0.44 0.29 0.46 0.20\n100\u2264min(n1, n2)<500 0.42 0.08 0.05 0.52 0.39 0.09\n50\u2264min(n1, n2)<100 0.08 0.10 0.08 0.06 0.12 0.05\nn1r1+n2r2\nn1+n2>35% 0.68 0.00 0.00 0.27 0.35 0.24\nn1r1+n2r2\nn1+n2= 0% 0.04 0.00 0.00 0.19 0.25 0.21\n15%<n1r1+n2r2\nn1+n2\u226435% 0.40 0.00 0.00 0.05 0.03 0.03\n0%<n1r1+n2r2\nn1+n2\u226415% 0.11 0.00 0.00 0.03 0.05 0.05\nTable 4. Feature Importance (features sorted by average rank among all methods)\nfeature CatBoost XGBoost RF GBM\nMIN3 20.08 40.16 24.37 17.81\nmin(n1, n2)\u2265500 12.37 18.18 7.99 9.23\nBN-MCE 7.33 4.43 20.44 29.32\nBN-SCE 10.48 3.46 13.90 12.20\nBN-GPH 8.88 2.07 10.01 15.50\n0< min (n1, n2)<50 1.30 4.59 2.53 3.71\nTarone-Ware 8.33 1.57 1.46 0.87\nlog-rank 7.44 1.84 0.99 0.94\nGehan 2.68 1.04 3.17 1.27\n100\u2264min(n1, n2)<500 1.86 9.16 1.14 0.85\nQ-test 4.51 1.35 0.70 4.35\nWKM 3.56 1.41 2.62 0.60\nPeto 1.47 0.82 2.59 1.03\nMAX 2.00 1.02 5.28 0.40\n50\u2264min(n1, n2)<100 0.35 2.81 0.96 0.78\nPrentice 2.39 0.84 1.02 0.52\nPeto-Prentice 2.94 0.62 0.83 0.64\nn1r1+n2r2\nn1+n2>35% 1.05 1.55 0.00 0.00\nn1r1+n2r2\nn1+n2= 0% 0.74 1.60 0.00 0.00\n0%<n1r1+n2r2\nn1+n2\u226415% 0.15 0.69 0.00 0.00\n15%<n1r1+n2r2\nn1+n2\u226435% 0.10 0.78 0.00 0.00\n125.3. Study of the Test Power\nIn this experiment, we determine the statistical power of the proposed methods. To\ndo this, we compute estimates of the power of all the methods considered using the\nalternative hypotheses described earlier. As the result obtained, rather than reporting\nabsolute power values, we compute the average rank among the set of methods con-\nsidered, since the average rank more clearly reflects the preference of one method in\ncomparison with rest methods. Detailed absolute values of the test power are repre-\nsented in Supplementary materials.\nFor values of the average rank, we also compute the values of the Wald [36] and\nSavage [37] criteria, which are used for decision-making under risk and uncertainty\nof environment. In this case, the \u201denvironment\u201d is an alternative hypothesis, and the\nutility function is the rank of the method on the alternative hypothesis. The lower the\nrank of the method, the higher it is in the ordered list, which means that this method\nis better than others. The Wald criterion focuses on maximizing utility in worst-case\nscenarios, whereas the Savage criterion aims to minimize \u2019regret\u2019 associated with the\nutility loss compared to the best possible outcome. A similar methodology has already\napplied previously in [38].\nTo reach this, we provide the investigation using the Simulation sample for each\nalternative group in the range HI-HIXunder censoring rates 0% and 0%-50%. We\npresent results in Figure 3. The obtained results are ordered by the average rank\nvalues (AVG column).\nThe results indicate that the proposed methods are at the top of the ordered lists\n(under censoring rate 0%-50%) with the exception of LogReg. This suggests that the\nproposed methods, on average, have a more preferable test power than the classic\ntwo-sample tests in terms of average rank. Comparing the values of the Wald crite-\nrion (assuming that the environment is in the worst state), we note that the proposed\nmethods are also more preferable (under censoring rate 0%-50%). Nevertheless, the\nclassic two-sample MIN3 test was among the TOP-4 preferred methods, since the con-\nstruction of this test statistic is based on the same assumption. Comparing the values\nof the Savage criterion (\u2019regret\u2019 minimization), we note that the proposed methods\nalso demonstrate superior performance (under censoring rate 0%-50%). Thus, we have\nshown that the proposed methods have high potential, since they have higher test\npower in the numerical experiments conducted.\n5.4. Study of the Test Statistic Distribution under the H0\nAnother investigation that is necessary for practical application is the study of the test\nstatistic distribution. This ensures the accurate determination of the critical value\nin hypothesis testing. As is known, the actual (pre-limit) distribution of statistics\ncan differ from the limit distribution and depend on many factors, such as sample\nsize, censoring rate, data distribution, and many others. Therefore, it is important to\nevaluate its behavior in a numerical experiment in order to make correct statistical\nconclusions in practice.\nThe design of our numeric experiment"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**Summary of \"AudioBERT: Audio Knowledge Augmented Language Model\"**\n\n**1. Abstract:**\n   - The paper discusses the limitation of pretraining language models exclusively on text datasets, leading to an inadequacy in handling auditory knowledge. To address this issue, the authors introduce a benchmark dataset called AuditoryBench, comprised of animal sound recognition and sound pitch comparison tasks. They also propose AudioBERT, a retrieval-based method to integrate auditory knowledge into BERT effectively, showing a significant performance improvement on AuditoryBench.\n\n**2. Introduction:**\n   - **Context:** Recent advancements in pre-trained language models have revolutionized language-related tasks but these models fall short in handling non-textual information, primarily visual and auditory knowledge.\n   - **Research Questions:** The primary question is whether language models lack auditory knowledge, similar to their known shortcomings in visual tasks.\n   - **Hypothesis:** Language models pre-trained on text-only data have inadequate commonsense auditory knowledge.\n   - **Literature Review:** Discovering the deficit in visual commonsense in language models, various methods have been devised to integrate visual data, but similar studies for auditory information are sparse. While some previous works have incorporated audio snippets into language models, their impact on commonsense auditory knowledge hasn't been thoroughly explored.\n   - **Positioning work:** The authors aim to fill this gap by creating AuditoryBench for evaluating auditory knowledge and proposing AudioBERT to augment BERT with auditory information.\n\n**3. Related Work:**\n   - **Pretrained Language Models:** Overview of BERT and other transformer-based models like RoBERTa and LLaMA, which show limitations in non-textual domains.\n   - **Multimodal Models:** Examples include visual-augmented models like Vokenization and Imagination-Augmented Models, which have inspired auditory augmentation exploration.\n   - **Audio-Language Models:** Describes models like CLAP for audiotext contrastive learning but highlights the absence of commonsense auditory knowledge integration.\n\n**4. Method:**\n   - **AuditoryBench Dataset:**\n     - **Data Source:** Based on LAION-Audio-630K, filtered and categorized into over 600,000 audio-text pairs.\n     - **Tasks:**\n       - Animal Sound Recognition: Tasks the model to predict an animal based on onomatopoeia, generated using an LLM-based pipeline.\n       - Sound Pitch Comparison: Assesses the model's ability to compare pitches between two audio sources.\n   - **AudioBERT Framework:**\n     - **Auditory Knowledge Span Detector:** A transformer encoder trained to detect spans in text relevant to auditory information.\n     - **CLAP for Retrieval:** Utilizes the CLAP model to contrastively learn audio-text pairs and retrieve relevant audio samples.\n     - **Model Integration:**\n       - **Detection:** Determines spans requiring auditory knowledge.\n       - **Retrieval:** Queries audio relevant to detected spans using CLAP\u2019s text encoder.\n       - **Embedding:** Injects audio embeddings into BERT, activating Low-Rank Adaptation (LoRA) weights for relevant task adaptation.\n       - **Training:** Combination of masked language modeling loss with LoRA specific finetuning.\n\n**5. Experimental Setup:**\n   - **Datasets:**\n     - Animal Sound Recognition: Split into train (4,211), dev (593), test (1,211), and additional wiki test set (197).\n     - Sound Pitch Comparison: Train (8,312), dev (1,178), test (2,387), and wiki test (3,625) sets.\n   - **Hardware:** Custom configuration for experiments was not detailed but generally assumed to be GPUs given the nature of processing.\n   - **Evaluation Metrics:** Use accuracy and F1-score for evaluation.\n   - **Baselines:** Compared against BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, Gemma2-2B, and LLaMA3.1-8B.\n\n**6. Results and Analysis:**\n   - **Performance Metrics:**\n     - **BERT's Deficiency:** BERT models perform poorly in auditory tasks, demonstrating lack of auditory knowledge.\n     - **AudioBERT Enhancement:** Shows substantial improvements (over 40%) in prediction accuracy on AuditoryBench.\n     - **Table Analysis:** \n       - Displays accuracy improvements from low teens to mid-thirties in animal sound recognition tasks.\n       - Similar trend observed in sound pitch comparision, with enhanced performance surpassing 70%.\n   - **Graphical and Tabular Data:** Multiple figures and tables showing performance metrics across different model configurations were not visualized here, but appear extensively in the paper.\n\n**7. Discussion:**\n   - **Results Interpretation:** The significantly improved performance of AudioBERT validates the hypothesis of auditory knowledge deficiency in text-only pretrained language models.\n   - **Implications:** Emphasizes the significance of integrating non-textual data in enhancing model\u2019s generalization capability to multimodal commonsense reasoning.\n   - **Limitations:** Recognizes constraints in the scale of dataset and the selection of audio categories, suggesting broader inclusions may offer richer contextual understanding.\n   - **Potential Biases:** Identifies potential biases due to reliance on specific types of audio data or annotation inconsistencies.\n\n**8. Conclusion:**\n   - **Key Contributions:**\n     - Introduction of AuditoryBench, the pioneering dataset for assessing auditory knowledge in language models.\n     - Demonstrated the inadequacy of existing models in handling auditory information.\n     - Propose and validate AudioBERT as an effective retrieval-augmented model.\n   - **Future Directions:** Encourage research into diverse modalities integration, further refinement of datasets like AuditoryBench, and adaptation mechanisms in language models.\n\nBy following the provided instructions and methodological details, a researcher can replicate the study or extend upon it to investigate the integration of more advanced auditory knowledge and potentially other cross-modal datasets."}, {"role": "assistant", "content": "AudioBERT: Audio Knowledge Augmented\nLanguage Model\nHyunjong Ok1,2\u2217\u2020Suho Yoo3\u2217Jaeho Lee1\u2020\n1POSTECH2HJ AILAB3Inha University\n{hyunjong.ok, uso7d0 }@gmail.com, jaeho.lee@postech.ac.kr\nAbstract \u2014Recent studies have identified that language models,\npretrained on text-only datasets, often lack elementary visual\nknowledge, e.g., colors of everyday objects. Motivated by this\nobservation, we ask whether a similar shortcoming exists in terms\nof the auditory knowledge. To answer this question, we construct\na new dataset called AuditoryBench, which consists of two novel\ntasks for evaluating auditory knowledge. Based on our analysis\nusing the benchmark, we find that language models also suffer\nfrom a severe lack of auditory knowledge. To address this limi-\ntation, we propose AudioBERT, a novel method to augment the\nauditory knowledge of BERT through a retrieval-based approach.\nFirst, we detect auditory knowledge spans in prompts to query\nour retrieval model efficiently. Then, we inject audio knowledge\ninto BERT and switch on low-rank adaptation for effective\nadaptation when audio knowledge is required. Our experiments\ndemonstrate that AudioBERT is quite effective, achieving supe-\nrior performance on the AuditoryBench. The dataset and code\nare available at https://github.com/HJ-Ok/AudioBERT .\nIndex Terms \u2014Auditory Knowledge, Language Model, Re-\ntrieval Augmented Prediction\nI. I NTRODUCTION\nThe advance of pretrained language models has spurred\nsignificant improvements across various language-related tasks\n[1], [2], and has been extended to processing of multimodal in-\nformation [3], [4]. However, the limitation of popular language\nmodels is that they are pretrained only on textual data, which\npotentially leads to a lack of knowledge related to information\nfrom other domains. Indeed, in visual domains, it has been\nrepetitively discovered that common language models do not\nhave sufficient visual commonsense knowledge, such as the\ncolor of common objects, leading to a poor performance on\nvisual tasks [5]\u2013[8]. In response to this limitation, recent works\nhave developed algorithms to augment language models with\nvisual knowledge [9]\u2013[12].\nDoes the same limitation hold for auditory tasks? This\nquestion, unfortunately, has not been clearly addressed yet;\nalthough a recent work studies the effectiveness of audio\nsnippet embeddings in the language model representation\nspace [13], it is not known whether the language models have\nrich commonsense knowledge regarding the auditory signals,\ne.g., which animal make a specific sound (Fig. 1).\nTo answer this question, we present AuditoryBench , the\nfirst benchmark dataset (to our knowledge) for evaluating\nthe language models\u2019 auditory knowledge. In particular, we\n\u2020corresponding author\n\u2217equal contribution\nInput: Meow is the sound a [MASK] makes.\ndog (0.420)\ncat (0.193)\nbird (0.084)\nwolf (0.032)\nbaby (0.023)\nFig. 1. An illustration of the logit probabilities of the BERT when asked to\ncomplete an example sentence from the AuditoryBench dataset.\npropose two auditory knowledge tasks: (1) animal sound\nrecognition and (2) sound pitch comparison. The animal sound\nrecognition task asks the language model to predict which\nanimal is likely to make a sound that corresponds to specific\nechoing words ( i.e., onomatopoeia), such as \u201cmeow.\u201d The\nsound pitch comparison task asks the language model to\npredict which sound source ( e.g., musical instruments, objects,\nor environments) is likelier to produce sound with a higher\npitch. To construct this benchmark, we propose an LLM-based\ndata-processing pipeline for the sake of the scalability of the\nbenchmark dataset [14], [15].\nUsing AuditoryBench, we discover that language models\nseverely lack auditory commonsense knowledge. In particular,\nwe test three different language models\u2014BERT [1], Gemma\n[16], and LLaMA [17]\u2014and find that all models achieve low\npredictive accuracy in both benchmark tasks (Table III).\nTo address this shortcoming, we propose AudioBERT , a\nsimple yet effective retrieval-based framework for injecting\nauditory knowledge into language models. Our approach\ninvolves detecting text spans where auditory knowledge is\nnecessary. Whenever needed, relevant audio is retrieved by\nquerying the detected text span to CLAP [18], a model\nthat measures the text-audio similarity. Then, the embedding\nof the retrieved audio sample is injected into the language\nmodel. Upon identification of auditory spans by the detector,\nthe language model activates Low-Rank Adaptation (LoRA)\n[19] weights, which is finetuned with AudiotoryBench, which\nmaintains its pretrained knowledge makes the model perform\nwell in other tasks by deactivating LoRA weights. Our experi-arXiv:2409.08199v1  [cs.CL]  12 Sep 2024;A cat meowing. Recorded with a Zoom H5.\nanimal/cat\nanimal/insect/cricket\nmusic/drum/snare\n...Tag\nTextDescription\nCategorize\n(a) Categorized dataset (c) Generate s ound pitch comparison  dataset(b) Generate animal sound recognition datasetTag\nTextDescription\nCategoryYou will be given information about specific\naudio.\u00a0 Your task is to make a prompt that\ncan measure the auditory knowledge about\nanimals .\u00a0Please make sure you read and\nunderstand these instructions carefully.\u00a0\n(...)\u00a0\nTag : {{Tag}}\nText : {{Text}}\nDescription : {{Description}}\nCategory : {{Category}}\u00a0Prompt: \u00a0meow is the sound a [MASK] makes.\n\u00a0Answer:  cat\n\u00a0Auditory span:  meow\nTag\nTextDescription\nCategoryYou will be given information about specific\naudio. Your task is to make a prompt to\ncompare the pitch of two suggested\nauditory objects . Please make sure you read\nand understand these instructions carefully.\u00a0\n(...)\u00a0\nInput1:\nTag : {{Tag}}\nText : {{Text}}\nDescription : {{Description}}\nCategory : {{Category}}\nInput2:\u00a0(...)Categorized\nDataset['cat', 'meow', 'animal', 'sound-effect']\n['Cat meow.', 'A cat meowing.']\n\u00a0Prompt: \u00a0The sound of a synthesizer typically has\u00a0\n\u00a0 a [MASK] pitch than an acoustic bass.\n\u00a0Answer: \u00a0higher\n\u00a0Auditory span: \u00a0sound of a synthesizer,\u00a0\n\u00a0 acoustic bass\nFig. 2. An illustration of the process generating the AuditoryBench.\nments demonstrate that the proposed method is quite effective,\nimproving the prediction accuracy on AuditoryBench by more\nthan 40% in the test set. As far as we know, AudioBERT is\nthe first algorithm to augment language models with auditory\ncommonsense knowledge.\nThe key contributions of our work are threefold:\n\u2022We introduce the first benchmark dataset for evaluating\nthe auditory knowledge of language models via an effi-\ncient LLM-based data generation pipeline.\n\u2022We discover that common language models pretrained on\ntext-only datasets suffer from a severe lack of auditory\ncommonsense knowledge.\n\u2022We propose a novel and effective framework to inject\nauditory knowledge into pretrained language models,\nwhich adaptively retrieves and utilizes relevant auditory\nknowledge, utilizing switch-on-off LoRA to preserve\npretrained knowledge.\nII. M ETHODS\nA. AuditoryBench\nTo assess the auditory commonsense knowledge, we con-\nstruct a dataset called AuditoryBench. AuditoryBench is based\non LAION-Audio-630K [20], a large-scale audio-text dataset\nwith over 600,000 audio-text pairs.\nThe first step to construct AuditoryBench is to categorize\neach audio sample of the dataset by processing the paired text\ninformation with an LLM (Fig. 2(a)); the text information con-\nsists of the audio metadata (description and tag) and the paired\ntext. Based on this information, we give a detailed instruction\nto categorize hierarchically ( e.g., \u201canimal/dog/pomeranian\u201d)\nwith few-shot examples.\nThe next step is to construct tasks based on the generated\ncategories and the text information. We design two tasks:\nAnimal sound recognition and sound pitch comparison.\nAnimal sound recognition: This task asks to predict an\nanimal from the given onomatopoeia that corresponds to an\nanimal\u2019 sound. Each datum consists of a prompt ( e.g., \u201cmeowTABLE I\nSTATISTICS FOR ANIMAL SOUNDS RECOGNITION AND SOUND PITCH\nCOMPARISON DATA .\nSplits # Captions # Words/Caption # Total tokens\nAnimal sound recognition\nTrain 4,211 9.27 39,024\nDev 593 9.39 5,567\nTest 1,211 9.30 11,268\nWiki 197 7.01 1,381\nTotal 6,212 9.21 57,240\nSound pitch comparison\nTrain 8,312 18.18 151,081\nDev 1,178 18.12 21,343\nTest 2,387 18.19 43,431\nWiki 3,625 12.41 44,987\nTotal 15,502 16.83 260,842\nis the sound a [MASK] makes\u201d), an answer ( e.g., \u201ccat\u201d), and\nan onomatopoeic span ( e.g., \u201cmeow\u201d). This triplet is generated\nby first selecting a datum that is categorized as \u201canimal,\u201d and\nprocessing its text information with an LLM; see Fig. 2(b) for\nan overall illustration and the prompt used. To enhance the\ndata quality, we have additionally employed a human anno-\ntator to filter our inappropriate animal answers(e.g., \u201czombie,\nvampire\u201d).\nUsing this approach, we have generated a task with 6,015\nsamples. The dataset is then split into training, development,\nand test sets with 70%/10%/20% of samples. We have also\ncollected animal sound data from Wikipedia,1allocating them\nas additional test data. This additional set lets us assess the\ngeneralizability and quality of the dataset.\nSound pitch comparison: This task asks to compare the\npitch of two different sound sources. Each datum consists\nof a prompt ( e.g., \u201cThe sound of a synthesizer typically\nhas a [MASK] pitch than an acoustic bass\u201d), an answer\n(e.g., \u201chigher\u201d), and two auditory spans ( e.g., \u201csound of a\n1https://en.wikipedia.org/wiki/List ofanimal soundsmeow soundmeow soundBERT                 LoRA\nme##ow is thesound sound a[MASK] makes .\nmeow is the sound a [MASK] makes.Auditory knowledge span detectormeow  is the sound a [MASK] makes.\nFreesound audio dataCLAP  text encoder CLAP  audio encoderAudio encoderAudio\nEmbeddings\ncat\n Audio retrieval\ud83d\udd25\n\u2744 \u2744 \u2744 \n\u2744 Auditory knowledge span detector\n(a) Audio knowledge recognition110000000\nme##owisthesound a[MASK]makes .\n(b) CLAP  retrieval (C) AudioBER T\nCLAP  text encoder CLAP  audio encoder\nmeow sound......A1\nA2\n...\nAN... T2 TNT1\n... A1*T2 A1*TN\n... A2*TNA2*T1\n... ... ...\n... AN*T2AN*T1 AN*TNA1*T1\nA2*T2\n... ......\ud83d\udd25Fig. 3. Overall process of our method\nsynthesizer,\u201d \u201cacoustic bass\u201d). The triplet is generated by first\nselecting two audio samples, categorized into various music,\nobjects, and environment categories (Fig. 2(c)). Then, using\nthe librosa library [21], we extract the average pitch for each\naudio sample. We then filter the data by selecting only the\npairs in which the pitch difference exceeds 10%, and an LLM\ncan accurately classify which one is higher.\nWe split the data into training, development, and test sets,\nwith fractions 70%/10%/20%. We have additionally collected\nthe pitch range of musical instruments from Wikipedia2, to\nutilize this as a test set. This dataset lets us assess our dataset\u2019s\ngeneralization and quality.\nFor both tasks, we use Qwen2-72B-Instruct-AWQ [22] as\nthe LLM. Detailed statistics of these tasks are given in Table I.\nB. AudioBERT\nNow, we introduce our retrieval-based framework, which\nis coined AudioBERT. AudioBERT employs two models as\nits building block: An auditory knowledge span detector,\nwhich identifies the spans related to auditory knowledge within\nthe given text, and CLAP [18], a model that is used for\nretrieval. In what follows, we first describe each component\n(in Sections II-B1 and II-B2, respectively) and then describe\nthe overall AudioBERT framework in Section II-B3.\n1) Auditory knowledge span detector: This model extracts\nthe audio-relevant span from the given text, which can work as\na good query. To obtain such model, we simply train a trans-\nformer encoder [23] to classify the auditory knowledge span\nfrom other tokens, using the cross-entropy loss (Fig. 3(a)).\n2) CLAP: Audio-text contrastive learning: To retrieve the\nrelevant audio from the detected auditory knowledge span, we\nutilize the audio-text model called CLAP [18]. CLAP is trained\n2https://en.wikipedia.org/wiki/Range (music)via audio-text contrastive learning: Given a batch of N audio-\ntext pairs, the cosine similarity between the representations\nof matched pairs (positive) is maximized, while the cosine\nsimilarity is minimized in unmatched (negative) pairs. The loss\nfunction is formalized as follows:\nLAudio=\u22121\nNNX\ni=1logexp( cos(Ai, Ti)/\u03c4)PN\nj=1exp( cos(Ai, Tj)/\u03c4)(1)\nLText=\u22121\nNNX\ni=1logexp( cos(Ai, Ti)/\u03c4)PN\nj=1exp( cos(Aj, Ti)/\u03c4)(2)\nL=1\n2(LAudio+LText) (3)\nAkandTkare the audio and text embeddings, cos (\u00b7,\u00b7)\ndenotes cosine similarity, and \u03c4is a temperature parameter.\n3) AudioBERT: AudioBERT injects auditory knowledge\ninto a language model using the auditory knowledge span\ndetector and CLAP. Our approach works in three steps: We\nretrieve the relevant audio from the database using the span\ndetector and the text encoder of CLAP, generate embedding\nusing the CLAP audio encoder, and add this embedding to the\nfirst token of the auditory knowledge span (Fig. 3(c)). During\nthe training, we employ masked language modeling loss and\nLoRA to finetune the language model\u2019s LoRA weights and the\naudio encoder while keeping the other parameters free. For in-\nference, the auditory knowledge span detector first determines\nwhether auditory knowledge is required. If not, inference\nproceeds without activating the LoRA weights, maintaining\nthe model\u2019s original pretrained weights. However, for tasks\nthat require auditory knowledge, the auditory knowledge span\ndetector recognizes this requirement, retrieves relevant audio,\nactivates the LoRA weights, and injects the audio embedding\ninto the model. This novel architecture enables AudioBERT\nto dynamically adapt to tasks requiring auditory knowledgeTABLE II\nEXPERIMENT RESULTS OF AUDIO KNOWLEDGE RECOGNITION USING F1- SCORE METRICS .\nTrained w/Animal sound Sound pitch comparison Combined\nDev. Test Wiki Test Dev. Test Wiki Test Dev. Test Wiki Test\nAnimal sound 92.75 \u00b10.37 92.76 \u00b10.19 100 \u00b10.00 34.87 \u00b18.48 34.61 \u00b18.17 60.37 \u00b112.61 43.67 \u00b17.01 43.72 \u00b16.77 60.96 \u00b112.42\nHeight of sound 52.63 \u00b16.73 52.19 \u00b16.40 79.08 \u00b113.75 94.76 \u00b10.13 95.04 \u00b10.09 91.12 \u00b12.12 88.64 \u00b10.47 95.04 \u00b10.43 89.89 \u00b12.25\nCombined dataset 92.00 \u00b10.39 92.67 \u00b10.25 100 \u00b10.00 94.88 \u00b10.24 95.12 \u00b10.24 92.47 \u00b11.76 94.55 \u00b10.19 94.84 \u00b10.21 92.06 \u00b11.74\nTABLE III\nEXPERIMENT RESULTS OF AUDIO BERT USING ACCURACY METRICS .\nMethodsAnimal sound Sound pitch comparison Combined\nDev. Test Wiki Test Dev. Test Wiki Test Dev. Test Wiki Test\nBERT-base [1] 15.51 13.46 3.05 59.42 60.41 48.06 44.72 44.61 45.72\nBERT-large [1] 16.53 15.85 5.58 59.59 58.90 54.90 45.17 44.41 52.36\nRoBERTa-base [24] 14.67 14.04 2.54 54.50 55.84 47.45 41.16 41.77 45.14\nRoBERTa-large [24] 16.36 14.70 7.61 55.26 56.64 48.80 42.23 42.52 46.68\nGemma2-2B [16] 14.33 15.11 6.60 59.25 60.45 47.86 44.21 45.19 45.73\nLLaMA3.1-8B [17] 23.10 21.80 16.24 61.46 62.55 47.72 48.62 48.83 46.10\nOurs (BERT-base) 38.28\u00b10.27 36.63 \u00b10.38 14.32 \u00b11.66 73.18 \u00b10.95 74.83 \u00b10.70 55.31 \u00b12.38 61.49 \u00b10.72 61.97 \u00b10.59 53.20 \u00b12.34\nOurs (BERT-large) 37.30 \u00b10.47 36.69\u00b10.27 15.03 \u00b11.17 75.11\u00b10.80 76.31\u00b10.60 56.27\u00b11.18 62.45\u00b10.69 62.97\u00b10.49 54.14\u00b11.18\nwhile maintaining its performance on general language under-\nstanding tasks.\nIII. E XPERIMENTS\nEvaluation metrics: We employ accuracy and F1-score\nfor our metrics. Higher scores indicate better performance.\nImplementation details: In our implementation, we em-\nploy a BERT-base model for the auditory knowledge span\ndetector. We trained with 5epochs with a batch size of 16, a\nlearning rate of 1\u00d710\u22125, and utilizing AdamW [25] optimizer.\nFor AudioBERT training, we experimented using BERT for\nthe language model and employed an AST [26] encoder for\nauditory knowledge embedding injecting. We trained with 20\nepochs with a batch size of 32, a learning rate of 3\u00d710\u22124,\nand utilizing AdamW optimizer. For LoRA, we set the rank\nand alpha to 64 and 128, respectively.\nWe report the average score from 5 runs with different\nrandom seeds for each setting.\nIV. R ESULTS\nA. Experiments results\n1) Auditory knowledge span detector: We report our audi-\ntory knowledge span detector performance in various trained\ndata in Table III. The results suggest that training on single\ndata may perform poorly, but the combined dataset works even\nwith performance gain in sound pitch comparison.\n2) AudioBERT: To evaluate the effectiveness of our ap-\nproach, we compared our model against the existing language\nmodels: BERT, RoBERTa, Gemma2-2B, and LlaMA3.1-8B.\nAs shown in Table III, language models show the absence\nof auditory knowledge, and AudioBERT exhibits competitive\nperformance, demonstrating its effectiveness in augmenting\nauditory knowledge.TABLE IV\nTHE RESULT OF HUMAN EVALUATION ON THE AUDITORY BENCH\nDATASET . W E RANDOMLY SAMPLED 100 DATA FROM EACH BENCHMARK\nAND ASKED THE PARTICIPANTS TO RANK THE OUTPUTS OF DIFFERENT\nSYSTEMS \u2019ACCURACY AND FLUENCY .\nBenchmark Acc. Fluency\nAnimal sound recognition 92.00 2.91\nSound pitch comparison 87.00 3.00\nB. Data quality assessment\nWe took the following steps to ensure the quality and\nrobustness of our data: When constructing the AuditoryBench,\nwe first filtered the data. For animal sound recognition, human\nannotators filtered out inappropriate labels. For the sound\npitch comparison task, we measured the pitch of each audio\nsample and maintained those correctly labeled. Additionally,\nwe augmented the test set with information from Wikipedia\nto measure generalization and robustness. Finally, we con-\nducted human evaluations on the test set for each benchmark.\nThree human annotators evaluated the criteria of accuracy\nand fluency, scoring out of 100 and 3, respectively. Accuracy\nreflects whether the labels were correctly applied, while flu-\nency measures grammar, spelling, punctuation, word choice,\nand sentence structure. As shown in Table IV, AuditoryBench\nexhibited high quality and robustness.\nV. C ONCLUSIONS\nWe introduce AuditoryBench, the first benchmark to assess\nthe auditory knowledge of language models and propose\nAudioBERT, a novel method for injecting auditory knowledge\ninto language models. We believe that our discovery of the lack\nof auditory knowledge in LMs can have implications for audio-\nlanguage multimodal research and contribute to the research\nof language models effectively adapting to diverse modalities.REFERENCES\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of\ndeep bidirectional transformers for language understanding,\u201d in NAACL ,\n2019.\n[2] A.c Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\n\u201cLanguage models are unsupervised multitask learners,\u201d OpenAI blog ,\nvol. 1, no. 8, pp. 9, 2019.\n[3] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee, \u201cVisual\ninstruction tuning,\u201d in NeurIPS , 2023.\n[4] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang,\n\u201cPengi: An audio language model for audio tasks,\u201d in NeurIPS , 2023.\n[5] C. Zhang, B. Van Durme, Z. Li, and E. Stengel-Eskin, \u201cVisual\ncommonsense in pretrained unimodal and multimodal models,\u201d in\nNAACL , 2022.\n[6] X. Liu, D. Yin, Y . Feng, and D. Zhao, \u201cThings not written in text:\nExploring spatial commonsense from visual signals,\u201d in ACL, 2022.\n[7] M. Alper, M. Fiman, and H. Averbuch-Elor, \u201cIs bert blind? exploring\nthe effect of vision-and-language pretraining on visual language under-\nstanding,\u201d in CVPR , 2023.\n[8] P. Rahmanzadehgervi, L. Bolton, M. R. Taesiri, and A. T. Nguyen,\n\u201cVision language models are blind,\u201d arXiv preprint arXiv:2407.06581 ,\n2024.\n[9] H. Tan and M. Bansal, \u201cV okenization: Improving language understand-\ning with contextualized, visual-grounded supervision,\u201d in EMNLP , 2020.\n[10] Y . Lu, W. Zhu, X. Wang, M.l Eckstein, and W. Y . Wang, \u201cImagination-\naugmented natural language understanding,\u201d in NAACL , 2022.\n[11] W. Wang, L. Dong, H. Cheng, H. Song, X. Liu, X. Yan, J. Gao, and\nF. Wei, \u201cVisually-augmented language modeling,\u201d in ICLR , 2023.\n[12] T. Tang, Y . Chen, Y . Du, J. Li, W. X. Zhao, and J.-R. Wen, \u201cLearning\nto imagine: Visually-augmented natural language generation,\u201d in ACL,\n2023.\n[13] J. Ngo and Y . Kim, \u201cWhat do language models hear? probing for\nauditory representations in language models,\u201d in ACL, 2024.\n[14] H. Kim, J. Hessel, L. Jiang, P. West, X. Lu, Y . Yu, P. Zhou, R. Bras,\nM. Alikhani, G. Kim, et al., \u201cSoda: Million-scale dialogue distillation\nwith social commonsense contextualization,\u201d in EMNLP , 2023.\n[15] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. Plumbley, Y . Zou,\nand W. Wang, \u201cWavcaps: A chatgpt-assisted weakly-labelled audio\ncaptioning dataset for audio-language multimodal research,\u201d IEEE/ACM\nTASLP , 2024.\n[16] G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju,\nL\u00b4e. Hussenot, T. Mesnard, B. Shahriari, A. Ram \u00b4e, et al., \u201cGemma 2:\nImproving open language models at a practical size,\u201d arXiv preprint\narXiv:2408.00118 , 2024.\n[17] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nA. Mathur, A. Schelten, A. Yang, A. Fan, et al., \u201cThe llama 3 herd of\nmodels,\u201d arXiv preprint arXiv:2407.21783 , 2024.\n[18] B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, \u201cClap learning\naudio concepts from natural language supervision,\u201d in ICASSP , 2023.\n[19] E. J Hu, y. shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and\nW. Chen, \u201cLoRA: Low-rank adaptation of large language models,\u201d in\nICLR , 2022.\n[20] Y . Wu, K. Chen, T. Zhang, Y . Hui, T. Berg-Kirkpatrick, and S. Dubnov,\n\u201cLarge-scale contrastive language-audio pretraining with feature fusion\nand keyword-to-caption augmentation,\u201d in ICASSP , 2023.\n[21] Brian McFee et al., \u201clibrosa/librosa: 0.10.2.post1,\u201d May 2024.\n[22] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li,\nD. Liu, F. Huang, et al., \u201cQwen2 technical report,\u201d arXiv preprint\narXiv:2407.10671 , 2024.\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez,\n\u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in NeurIPS ,\n2017.\n[24] Y . Liu, \u201cRoberta: A robustly optimized bert pretraining approach,\u201d arXiv\npreprint arXiv:1907.11692 , 2019.\n[25] I. Loshchilov. and . Hutter, \u201cDecoupled weight decay regularization,\u201d\ninICLR , 2019.\n[26] Y . Gong, Y . Chung, and J. Glass, \u201cAST: Audio Spectrogram Trans-\nformer,\u201d in Interspeech , 2021."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary\n\n#### Abstract\n\nThe paper, titled \"Fine-tuning Large Language Models for Entity Matching,\" explores the potential of fine-tuning generative large language models (LLMs) to enhance their performance in entity matching tasks. The authors examine fine-tuning along two dimensions: the representation of training examples, including various types of LLM-generated explanations, and the selection and generation of training examples using LLMs. The experiments indicate that fine-tuning significantly boosts the performance of smaller models, while the results for larger models are mixed. Fine-tuning also improves in-domain generalization but hampers cross-domain transfer. The inclusion of structured explanations positively impacts most LLMs while new example selection methods only benefit specific models. \n\n#### Introduction\n\nEntity matching (EM) is a task that identifies identical entity descriptions across different data sources, crucial for data integration pipelines. Traditionally, pre-trained language models (PLMs) such as BERT and RoBERTa have dominated this domain. Recent studies suggest that generative LLMs may perform better due to their high zero-shot performance and superior generalization to unseen entities. The existing research primarily focuses on prompt engineering and in-context learning, lacking thorough investigations into the utility of fine-tuning LLMs. This paper aims to fill that gap by exploring how fine-tuning can optimize LLMs for EM, influencing both matching performance and generalization to various datasets, by focusing on augmented example representation and distinct training set manipulation strategies.\n\n#### Related Work\n\nThe paper provides a detailed review of the current methodologies in entity matching, including traditional PLMs and their limitations. The authors review how prompt engineering and in-context learning with LLMs offer improvements but lack consistency and transferability across different domains. Prior studies that initiated early explorations into fine-tuning LLMs for EM are referenced, with this paper positioned as an advanced deep dive into optimizing fine-tuning techniques.\n\n#### Method\n\nThe approach is assessed along two dimensions:\n\n1. **Example Representation**:\n   - Training examples are augmented with textual and structured explanations generated by LLMs.\n   - Simple pairs with \u201cYes\u201d or \u201cNo\u201d labels are used as a baseline.\n   - Textual explanations consist of long-form manual interpretations and concise versions following prompt-based heuristics.\n   - Structured explanations explicitly mention attributes, their importance, and value similarities.\n\n2. **Example Selection and Generation**:\n   - Error-based and relevancy-based filtering techniques are applied to remove misleading examples from training sets.\n   - Artificial training examples are generated to expand the training pool.\n   - The effectiveness of each strategy is assessed individually and collectively.\n\n**Architectural Details**:\n   - Models used include GPT-4o, GPT-4o-mini, Meta-Llama-3.1-8B, and Meta-Llama-3.1-70B.\n   - The paper relies on OpenAI's batch API and Hugging Face\u2019s Transformers library for model prompting and fine-tuning.\n   - OpenAI models are tuned with a learning rate multiplier of 1.8 and batch size of 16. Open-source models use Low-Rank Adaptation (LoRA) with specific parameters for optimal weight adjustment and computational efficiency.\n\n#### Experimental Setup\n\n**Datasets**:\n   - Various benchmarks such as WDC Products, Abt-Buy, Walmart-Amazon, Amazon-Google, DBLP-Scholar, and DBLP-ACM.\n   - Dataset selection criteria include size, complexity, and domain variety.\n\n**Hardware**:\n   - Configurations include machines with AMD EPYC 7413 processors, 1024GB RAM, and Nvidia RTX6000 GPUs.\n\n**Evaluation Metrics**:\n   - Precision, recall, and F1-score are used for evaluating models, with detailed tables showing exact performance metrics.\n\n**Baselines and Comparison Methods**:\n   - Zero-shot performance of each model serves as the baseline.\n   - Fine-tuning of models using default hyperparameters compared against filtered and augmentation strategies.\n\n#### Results and Analysis\n\n**Empirical Findings**:\n   - Fine-tuning smaller models (e.g., Llama-8B) led to an average increase in F1 score by 17.31 points.\n   - Larger models show mixed results; GPT-4o fine-tuning significantly improved performance whereas Llama-70B showed intermittent improvement.\n   - Adding structured explanations generally improved the performance of three out of four fine-tuned models.\n   - Filtration methods enhanced performance in non-transfer scenarios but showed limited generalization.\n\n**Analytical Breakdown**:\n   - **Graphical and Tabular Analyses**: Detailed comparison tables highlight performance differentials among various approaches and LLMs.\n   - **Ablation Studies**: Conducted to isolate the impact of specific augmentations and example selection strategies.\n   - **Prompt Sensitivity**: Assessed by testing models with different prompts post-fine-tuning, revealing reduced sensitivity and consistent improvements.\n\n#### Discussion\n\n**Interpretation of Results**:\n   - Fine-tuning strategies, specifically example augmentation with structured explanations, significantly boost in-domain performance but often hinder cross-domain generalization.\n   - Model performance is closely tied to prompt consistency; fine-tuning with the same prompt used during inference yields the best results.\n   - Filtration approaches like relevancy-based filtering enhance dataset quality, which correlates with improved model accuracy.\n\n**Implications**:\n   - Findings suggest a nuanced approach to LLM fine-tuning for entity matching, balancing between dataset-specific optimization and cross-domain generalizability.\n   - Structured explanations should be prioritized for practical EM tasks involving nuanced entity distinctions.\n\n**Limitations**:\n   - High computational costs restrict the extensive fine-tuning of larger models.\n   - Cross-domain generalization remains an area needing further exploration to ensure robustness across diverse datasets.\n\n#### Conclusion\n\nThe paper concludes by summarizing the key contributions:\n   - LLMs for EM benefit significantly from fine-tuning, especially on smaller models.\n   - Structured explanations and example filtration enhance in-domain performance and generalization.\n   - Future work should explore hybrid strategies combining example filtration with diverse training sets to bolster cross-domain relevance and model robustness.\n\n**Future Research Direction**:\n   - Extend fine-tuning experiments to more diverse sets of LLMs.\n   - Investigate advanced filtering techniques that better balance training set quality and quantity.\n   - Broaden the scope of cross-domain generalization trials with a wider array of datasets.\n\nThis detailed summary provides a comprehensive blueprint for replicating and building upon the presented study, capturing all critical aspects required for a deep technical understanding."}, {"role": "assistant", "content": "Fine-tuning Large Language Models for Entity Matching\nAaron Steiner\nUniversity of Mannheim\nMannheim, Germany\naasteine@mail.uni-mannheim.deRalph Peeters\nUniversity of Mannheim\nMannheim, Germany\nralph.peeters@uni-mannheim.deChristian Bizer\nUniversity of Mannheim\nMannheim, Germany\nchristian.bizer@uni-mannheim.de\nABSTRACT\nGenerative large language models (LLMs) are a promising alterna-\ntive to pre-trained language models for entity matching due to their\nhigh zero-shot performance and their ability to generalize to unseen\nentities. Existing research on using LLMs for entity matching has\nfocused on prompt engineering and in-context learning. This paper\nexplores the potential of fine-tuning LLMs for entity matching. We\nanalyze fine-tuning along two dimensions: 1) The representation\nof training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2)\nthe selection and generation of training examples using LLMs. In\naddition to the matching performance on the source dataset, we\ninvestigate how fine-tuning affects the model\u2019s ability to general-\nize to other in-domain datasets as well as across topical domains.\nOur experiments show that fine-tuning significantly improves the\nperformance of the smaller models while the results for the larger\nmodels are mixed. Fine-tuning also improves the generalization to\nin-domain datasets while hurting cross-domain transfer. We show\nthat adding structured explanations to the training set has a positive\nimpact on the performance of three out of four LLMs, while the\nproposed example selection and generation methods only improve\nthe performance of Llama 3.1 8B while decreasing the performance\nof GPT-4o Mini.\nArtifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/wbsg-uni-mannheim/TailorMatch.\n1 INTRODUCTION\nEntity matching [ 1,2,5] is the task of identifying entity descriptions\nin different data sources that refer to the same real-world entity. En-\ntity matching is a central step in data integration pipelines [ 3]. Many\nstate-of-the-art entity matching methods build on pre-trained lan-\nguage models (PLMs) [ 20], such as BERT or RoBERTa [ 9,16,19,24].\nRecent work on using generative large language models (LLMs) [ 26],\nsuch as GPT, Llama, Gemini, or Mixtral, for entity matching has\nshown that LLMs have higher zero-shot performance compared\nto PLMs and generalize better to unseen entities [ 14,17,18]. Most\nresearch on using LLMs for entity matching focuses on prompt\nengineering and in-context learning [ 6,14,17]. Initial papers have\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XXstarted to investigate the potential of improving beyond in-context\nlearning by fine-tuning LLMs for entity matching [ 18,25]. This\npaper builds on this work and presents a deeper investigation of\nthe utility of fine-tuning LLMs for entity matching. Besides the\neffect of fine-tuning on the matching performance, we investigate\nhow fine-tuning influences the model\u2019s ability to generalize to\ndifferent in-domain datasets as well as across domains. Our inves-\ntigation focuses on two dimensions: First, the representation of\ntraining examples, where we investigate the effect of augmenting\neach training example with textual as well as structured explana-\ntions about why a pair of entity descriptions matches or not. The\nsecond dimension is the selection and generation of examples for\nthe training set that is used for fine-tuning. We experiment with\nusing LLMs to filter misleading examples from the training set as\nwell as to generate additional training examples. We further in-\nvestigate the error-based selection of training examples. Figure 1\ngives an overview of the fine-tuning and inference setup. The ef-\nfects of fine-tuning are compared between open-source (Llama-3.1)\nand proprietary (GPT-4o) models. While we test models of varying\nsizes, the focus of the investigation lies on smaller models, as these\nmodels are less resource-intensive to deploy.\nFigure 1: Fine-tuning and inference setup including explana-\ntion generation, example generation, and filtration.\nContributions: This paper makes the following contributions:\n(1)Standard Fine-Tuning: To establish baselines, we fine-\ntune open source and proprietary LLMs of different sizes\nusing standard fine-tuning techniques. The baselines show\nthat fine-tuning significantly improves the performance of\nthe smaller models while the results for the larger models\nare mixed.\n(2)In-domain and Cross-domain Generalization: We as-\nsess the effectiveness of fine-tuned models in both in-domainarXiv:2409.08185v1  [cs.CL]  12 Sep 2024Aaron Steiner, Ralph Peeters, and Christian Bizer\nand cross-domain settings. The experiments show that fine-\ntuned models often generalize to in-domain datasets, while\ntheir performance in cross-domain settings falls short of\nthe zero-shot baseline in the majority of cases.\n(3)Example Representation: We investigate the impact of\naugmenting the training set with different types of LLM-\ngenerated explanations. Adding structured explanations\nimproves the performance of three out of four fine-tuned\nmodels, while also improving in-domain generalization.\n(4)Example Selection and Generation: We introduce dif-\nferent approaches for using LLMs for filtering the training\nset and for the generation of new training examples. Both\ntechniques improve the performance after fine-tuning, with\nexample generation particularly enhancing in-domain gen-\neralization.\n(5)Prompt Sensitivity: We examine how fine-tuning affects\nthe model\u2019s sensitivity to variations of the prompt. Our ex-\nperiments show that fine-tuning generally reduces prompt\nsensitivity, with the introduction of structured explana-\ntions further stabilizing model performance across different\nprompts.\n2 EXPERIMENTAL SETUP\nThis section provides details on the models, benchmark datasets,\nfine-tuning setup, and the performance evaluation that we use for\nthe experiments. All artefacts are provided in the project repository,\nmeaning that all experiments can be replicated.\nLarge Language Models: We compare the following hosted\nproprietary models and locally-run open-source models:\n(1)gpt-4o-2024-08-06: Released in May 2024, this model is\nOpenAI\u2019s flagship LLM and currently used by ChatGPT.\n(2)gpt-4o-mini-2024-07-18: This hosted LLM, released in\nJuly 2024, has lower usage fees compared to GPT-4o. Al-\nthough the exact model size is not disclosed, the naming\nconvention indicates that it is smaller than GPT-4o.\n(3)Meta-Llama-3.1-8B-Instruct1:Released in July 2024, this\nopen-source model from Meta features the same 128k token\ncontext length as the GPT models.\n(4)Meta-Llama-3.1-70B-Instruct2:Also developed by Meta,\nthis model is medium-sized within the Llama series. Further\ndetails are available in the accompanying paper by Meta [ 4].\nWe use the OpenAI batch API3and Hugging Face\u2019s Transform-\ners4library to prompt both hosted and local models. In both cases,\nthe generation temperature is set at zero to minimize random-\nness. The open-source models run on local machines with varying\nhardware configurations, including an AMD EPYC 7413 processor,\n1024GB RAM, and up to eight Nvidia RTX6000 GPUs.\nDatasets: We evaluate in-domain and cross-domain generaliza-\ntion using benchmark datasets from two topical domains [ 8,19]:\nproducts and scholarly works. The datasets are selected based on\nthree criteria: First, we chose datasets with at least 150 matches\n1https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n2meta-llama/Meta-Llama-3.1-70B-Instruct\n3https://platform.openai.com/docs/guides/batch\n4https://huggingface.co/docs/transformers/en/indexin the test set (see Table 1) to ensure the stability of the perfor-\nmance measurement. Second, we prioritize datasets that contain a\nsignificant number of challenging corner case record pairs. These\ncorner cases are either matching or non-matching pairs that closely\nresemble the opposite class due to very similar or dissimilar surface\nforms [ 19]. Finally, to evaluate cross-domain generalization, we\nchose datasets from two topical domains. We use the following\ndatasets:\n\u2022WDC Products: This dataset includes product offers from\nvarious categories (e.g., electronics, clothing) originating\nfrom numerous online shops. We use the most challenging\nversion, with 80% corner cases (hard positives and nega-\ntives).\n\u2022Abt-Buy : This benchmark provides examples within simi-\nlar categories to those found in WDC Products.\n\u2022Walmart-Amazon: The Walmart-Amazon benchmark in-\ncludes products from both Walmart and Amazon, with cate-\ngories comparable to those in WDC Products and Abt-Buy.\n\u2022Amazon-Google: Unlike the other benchmarks, the Amazon-\nGoogle dataset focuses on software products, such as differ-\nent versions of the Windows operating system and various\nimage/video editing applications. This dataset introduces a\ndifferent product type for evaluating model performance.\n\u2022DBLP-Scholar: This benchmark requires matching biblio-\ngraphic entries between DBLP and Google Scholar, repre-\nsenting a domain distinct from the product datasets.\n\u2022DBLP-ACM: Similar to DBLP-Scholar, the DBLP-ACM\nbenchmark involves matching bibliographic entries.\nWe use the title attribute of the product datasets for fine-tuning,\nwhile we concatenate the author, title, venue, and year attributes of\nthe bibliographic datasets using a semicolon character as a delimiter.\nTable 1: Dataset statistics for the training, validation, and\ntest sets.\nDataset Training Set Validation Set Test Set\n# Pos # Neg # Pos # Neg # Pos # Neg\nWDC Products (small) 500 2,000 500 2,000 500 4,000\nWDC Products (medium) 1,500 4,500 500 3,000 500 4,000\nWDC Products (large) 8,471 11,364 500 4,000 500 4,000\nAbt-Buy (A-B) 822 6,837 206 1,710 206 1,710\nAmazon-Google (A-G) 933 8,234 234 2,059 234 2,059\nWalmart-Amazon (W-A) 769 7,424 193 1,856 193 1,856\nDBLP-Scholar (D-S) 4,277 18,688 1,070 4,672 1,070 4,672\nDBLP-ACM (D-A) 1,776 8,114 444 2,029 444 2,029\nFine-tuning Setup: In order to keep computational costs man-\nageable, we use the default hyperparameter setting recommended\nby the model providers and do not perform a hyperparameter search\nfor each experiment. The fine-tuning setup differs for hosted and\nopen-source models. We use the following default hyperparameter\nsettings for the hosted OpenAI models: learning rate multiplier 1.8,\nbatch size 16, and a constant random seed for reproducibility. We\nuse the following fine-tuning setup for the open-source models:\nWe employed Low-Rank Adaptation (LoRA) with an alpha of 16 to\nallow moderate adjustments to model weights, a dropout rate of 0.1\nto prevent overfitting, and a rank (r) of 64 to balance performance\nand computational efficiency. The learning rate was configured atFine-tuning Large Language Models for Entity Matching\n2e-4. To ensure reproducibility, a consistent random seed was used\nacross all libraries, and CUDA was set to deterministic mode. Both\nopen-source and hosted models are trained for 10 epochs, with\ncheckpoints generated after each epoch. Open-source models are\nvalidated at each checkpoint using custom evaluation callbacks. In\ncontrast, OpenAI models provided only two intermediate check-\npoints plus the final version, limiting the validation process.\nEvaluation: We use the standard evaluation metrics precision,\nrecall, and F1-score, following related work [ 1]. All subsequent\ntables show the F1 performance of a model in the given setting.\nThe precision and recall results of all experiments are provided\nin the repository accompanying this paper. Model responses in\nnatural language are evaluated using Narayan et al.\u2019s [14] method\nof parsing responses to contain \u201cyes\u201d or \u201cno\u201d.\n3 STANDARD FINE-TUNING\nIn order to establish baselines, this section reports the results of a\nseries of fine-tuning experiments using a simple, straight-forward\nrepresentation of the training examples. Figure 2 shows two training\nexamples from this fine-tuning set. Each example is represented\nby a prompt inquiring whether the entity descriptions refer to the\nsame entity, followed by the two descriptions and the completion\n\u201cYes\u201d or \u201cNo\u201d, depending on whether the descriptions are a match\nor a non-match.\nFigure 2: Representation of two training examples in the\nstandard fine-tuning setup.\nTable 2 reports the results of the standard fine-tuning experi-\nments. Each row reports the results for a specific LLM (column\nModel) which was fine-tuned using a specific training set (column\nTraining set). The following columns contain the results of evalu-\nating this fine-tuned model on different test sets. The \u201czero-shot\u201d\nrows report results for the specific models without any fine-tuning.\n3.1 Effectiveness\nThis section discusses the results of testing the LLMs with exam-\nples from the same dataset (a non-transfer setting). The fine-tuned\nmodels show a performance increase on most datasets compared\nto the baseline models.\nSmaller Models: Llama-8B shows an average F1 score gain of\n17.31 points over the zero-shot baseline (Table 2, upper section).\nFive out of six fine-tuned Llama 8B models significantly improve on\ntheir zero-shot baselines, with Abt-Buy achieving the highest gain\nof 30.77 points (Table 2, A-B column). The Amazon-Google model\nis an outlier, only exceeding the baseline by 0.84 points, underper-\nforming compared to other product domain models (Table 2, A-G\ncolumn). GPT-4o-mini shows an average F1 score increase of 11.72\npoints, a significant gain given the models high zero-shot perfor-\nmance (Table 2, , rows gpt-4o-m). Unlike Llama-8B, GPT-4o-minidemonstrates significant performance gains on the Amazon-Google\ndataset, achieving the highest absolute performance improvement\namong all 4o-mini models as presented in the A-G column of Table\n2.\nLarge Models: Given resource constraints, we fine-tune the\nlarger models only on the WDC small training set. For the Llama\n70B model fine-tuning results in a decrease in performance, with a\nreduction of 2.53 points in F1 score on the WDC test set, as shown in\nthe WDC column of Table 2. In contrast, GPT-4o, which starts with a\nsimilar zero-shot score as GPT-4o-mini, continues the trend of fine-\ntuning improving performance. It achieves a 5.43-point increase to\na F1 score of 87.07 on the WDC dataset, the highest result so far.\n3.2 Generalization\nWe evaluate two types of generalization: in-domain, where perfor-\nmance is assessed across datasets within the same topical domain,\nand cross-domain, where a model is trained using a training set\nfrom one domain and tested with a test set from a different topical\ndomain (e.g., from products to scholar).\nTransfer Gain: We quantify the capability of a fine-tuned model\nto generalize to target datasets by relating the performance gains of\nthe fine-tuned model on the target datasets to the performance gain\nthat is achieved by fine-tuning specialized models with training\nexamples from each target dataset. The resulting transfer gain is\ncalculated by dividing the average performance gain on target\ndatasets by the average gain achieved by models which were fine-\ntuned specifically on those datasets.\nTransfer Gain =Avg performance gain on target datasets\nAvg performance gain of specialized models\nFor example, the generalization gain of the WDC model is 72% (Ta-\nble 2, row Llama 8B/WDC, column Transfer Gain) and is calculated\nby dividing the average gain of the WDC model of 10.52 points on\nthe product datasets by the average gain of specialized models for\nthese datasets, which is 18.41 points F1.\nIn-domain Generalization: In the product domain, five out of\nsix Llama 8B models improve performance on both their own and\nother datasets. Even the underperforming Amazon-Google model\nshows a gain on the Abt-Buy dataset. However, it is the only model\nto experience a decline in performance in an in-domain general-\nization setting, as indicated by the WDC column in Table 2. On\naverage, Llama 8B models that were trained on a source dataset\nachieve 59% of the performance in an in-domain transfer scenario\ncompared to models that were fine-tuned directly for the target\ndatasets. GPT-4o-mini also exhibits some ability to transfer between\ndifferent product datasets, but to a lesser extent, achieving only a\n15% performance increase relative to dataset-specific models. This\nlower average is primarily influenced by the poor performance of\nthe Amazon-Google and Walmart-Amazon models on the WDC\ndataset, as shown in Table 2. Considering that the larger models are\nonly fine-tuned using the WDC dataset, in-domain generalization\ncan only be assessed within the product domain. The fine-tuned\nLlama 70B model demonstrates the ability to generalize to the\nAmazon-Google and Walmart-Amazon datasets, as evidenced by\nits performance exceeding the baseline by 3.92 and 4.94 F1 points,\nrespectively. However, it underperforms on the Abt-Buy dataset,Aaron Steiner, Ralph Peeters, and Christian Bizer\nTable 2: F1 scores after standard fine-tuning. The rows determine the model/training set combination, while the columns\ncontain results for different test sets. The best result of each model per test set is set bold, the second best result is underlined.\nModel Training setProduct Domain Transfer Scholar Domain Transfer\nA-B A-G W-A WDC Gain D-A D-S Gain\nLlama 8B Zero-shot 56.57 (0.00) 49.16 (0.00) 42.04 (0.00) 53.36 (0.00) - 85.52 (0.00) 67.69 (0.00) -\nLlama 8B A-B 87.34 (+30.77) 59.16 (+10.00) 60.39 (+18.35) 66.07 (+12.71) 102% 79.60 (-5.92) 42.89 (-24.80) -83%\nLlama 8B A-G 67.48 (+10.91) 50.00 (+0.84) 44.73 (+2.69) 39.53 (-13.83) -1% 76.28 (-9.24) 60.89 (-6.80) -43%\nLlama 8B W-A 86.24 (+29.67) 60.41 (+11.25) 65.65 (+23.61) 57.80 (+4.44) 96% 71.71 (-13.81) 51.19 (-16.50) -82%\nLlama 8B WDC 81.78 (+25.21) 52.29 (+3.13) 53.74 (+11.70) 69.19 (+15.83) 72% 74.52 (-11.00) 67.40 (-0.30) -30%\nLlama 8B D-A 58.02 (+1.45) 49.66 (+0.50) 40.82 (-1.22) 39.63 (-14.72) -20% 97.42 (+11.90) 79.56 (+11.87) 47%\nLlama 8B D-S 65.71 (+9.15) 46.22 (-2.95) 42.35 (+0.31) 52.00 (-1.35) 7% 96.70 (+11.18) 92.95 (+25.26) 94%\ngpt-4o-m Zero-shot 87.68 (0.00) 59.20 (0.00) 65.06 (0.00) 81.61 (0.00) - 94.16 (0.00) 87.96 (0.00) -\ngpt-4o-m A-B 94.09 (+6.41) 67.18 (+7.98) 68.81 (+3.75) 82.69 (+1.08) 35% 96.94 (+2.79) 88.85 (+0.89) 28%\ngpt-4o-m A-G 83.51 (-4.17) 80.25 (+21.05) 68.97 (+3.91) 73.99 (-7.62) -36% 96.28 (+2.12) 85.60 (-2.37) -2%\ngpt-4o-m W-A 92.08 (+4.40) 67.50 (+8.30) 78.85 (+13.79) 78.52 (-3.09) 33% 95.58 (+1.43) 86.97 (-0.99) 3%\ngpt-4o-m WDC 91.44 (+3.76) 64.11 (+4.91) 68.92 (+3.86) 84.38 (+2.77) 9% 85.35 (-8.81) 76.33 (-11.63) -155%\ngpt-4o-m D-A 88.94 (1.26) 67.32 (+8.12) 67.51 (2.45) 81.34 (0.27) 27% 99.10 (+4.94) 89.93 (+1.97) 24%\ngpt-4o-m D-S 89.76 (+2.08) 65.71 (+6.50) 68.46 (+3.39) 70.87 (-10.73) 3% 95.36 (+1.20) 96.22 (+8.26) 24%\nLlama 70B Zero-shot 79.12 (0.00) 51.44 (0.00) 55.62 (0.00) 75.19 (0.00) - 80.50 (0.00) 69.47 (0.00) -\nLlama 70B WDC 77.94 (-1.18) 55.36 (+3.92) 60.56 (+4.94) 72.66 (-2.53) - 69.90 (-10.60) 63.85 (-5.62) -\ngpt-4o Zero-shot 92.20 (0.00) 63.45 (0.00) 70.67 (0.00) 81.64 (0.00) - 87.18 (0.00) 74.59 (0.00) -\ngpt-4o WDC 91.99 (-0.21) 65.12 (+1.67) 68.55 (-2.12) 87.07 (+5.43) - 89.27 (+2.09) 80.74 (+6.15) -\nwith a decrease of 1.18 F1 points. Thus, fine-tuning improves per-\nformance on two out of three in-domain datasets.\nIn contrast, GPT-4o, while showcasing higher performance gains\non the WDC dataset, exhibits poorer in-domain generalization. The\nfine-tuned version only manages to outperform the baseline on one\nout of three datasets.\nThe Llama 8B models achieve 62% of the dedicated performance\nwhen generalizing within the scholar domain, while GPT-4o-mini\nattains 66%. These higher generalization scores are likely due to the\nhigher similarity among the scholar datasets, as both benchmarks\ninclude records from DBLP.\nCross-Domain Generalization: In a product-to-scholar cross-\ndomain generalization scenario, Llama 8B models experience an\naverage decrease of 11.05 F1 points, while GPT-4o-mini models\nshow a decrease of 2.07 F1 points compared to their zero-shot\nperformance. Specifically, all eight Llama 8B models and four out of\neight GPT-4o-mini models underperform relative to their zero-shot\nbaselines, as seen in the DBLP-ACM and DBLP-Scholar columns of\nTable 2.\nThis trend continues with the scholar versions of the models.\nLlama 8B models underperform their zero-shot counterparts in the\nproduct domain by 1.11 F1 points, whereas GPT-4o-mini models\ngain an average of 1.60 F1 points over zero-shot. Larger models\nexhibit similar patterns: Llama 70B fails to surpass zero-shot perfor-\nmance in cross-domain scenarios, while GPT-4o gains an average\nof 4.12 F1 points, as reflected in the scholar domain columns of\nTable 2.\nIn conclusion, standard fine-tuning significantly enhances in-\ndomain generalization for the smaller models, while the results for\nthe larger models as well as for cross-domain generalization are\nmixed.3.3 Prompt Sensitivity\nVariations of the wording of a prompt can have considerable perfor-\nmance implications on the model [ 10,15,17,27]. It is recommended\nto use the same prompt for fine-tuning that will later be used for\nquerying the model5. In this section, we investigate how the perfor-\nmance of the fine-tuned models changes when a different prompt\nis used for querying the model.\nPrompts: We assess the performance of the following three\nalternative prompts for querying models that were fine-tuned using\nthe prompt shown in Figure 2:\n\u2022simple-free: \u201cDo the two product descriptions match?\u201d\n\u2022complex-force: \u201cDo the two product descriptions refer to\nthe same real-world product? Answer with \u2018Yes\u2019 if they do\nand \u2018No\u2019 if they do not.\u201d\n\u2022simple-force: \u201cDo the two product descriptions match?\nAnswer with \u2018Yes\u2019 if they do and \u2018No\u2019 if they do not.\u201d\nWe measure the prompt sensitivity as the standard deviation\nof the performance across the prompt that is used for fine-tuning\nand the three alternative prompts. The detailed prompt sensitivity\nresults can be found in the accompanying repository. Post-fine-\ntuning Llama 8B\u2019s standard deviation drops from 15.76 to 1.87 in\na non-transfer setting. The sensitivity increases slightly for in-\ndomain transfer scenarios, with product models at 2.27 F1 points\nand scholar models at 2.60 F1 points. Fine-tuned Llama models show\nan average prompt sensitivity of 3.54 F1 points, down from 15.76\npre-fine-tuning. Furthermore, the prompt used for fine-tuning does\nnot necessarily perform best with the alternative prompts resulting\nin the highest F1 score in 29 out of 42 scenarios for Llama.\n5https://platform.openai.com/docs/guides/fine-tuningFine-tuning Large Language Models for Entity Matching\nGPT-4o-mini shows a similar trend, with a pre-fine-tuning prompt\nsensitivity of 2.72 F1 points. Post-fine-tuning, prompt sensitivity is\nreduced to 0.26 F1 points in non-transfer scenarios, 0.85 F1 points\nin the product domain, and 0.18 F1 points in the scholar domain.\nAcross all datasets, fine-tuned GPT-4o-mini models exhibites a\nprompt sensitivity of 1.31 F1 points. With the prompt used for\nfine-tuning reporting the highest result in 50% of cases.\n4DIMENSION 1: EXAMPLE REPRESENTATION\nThis section investigates how different training example represen-\ntations affect the performance and generalization capabilities of\nfine-tuned models. We compare three representations: 1) simple\npairs as used in Section 3, 2) pairs augmented by the LLM with\ntextual explanations why the entities match or do not match, and\n3) pairs augmented with structured explanations. The complete\nprompts for instructing the model to generate explanations as well\nas the augmented training sets are found in the project repository.\nTextual Explanations: We evaluate two types of textual expla-\nnations generated using GPT-4o-mini. The first approach presents\nthe model with the prompt shown in Figure 2, followed by the label.\nAfterwards, the model is asked to explain the label. The model\nis given no guidance on the format of the explanation. This re-\nsults in detailed explanations, having an average length of 293\ntokens, which describe why the entity descriptions match or do not\nmatch. As a second approach, we re-run the technique proposed by\nWadhwa et al. [ 21], which generates more concise explanations by\nincluding short explanations for other entity pairs as demonstra-\ntions in the prompt. The resulting explanations have an average\nlength of 90 tokens. Figure 3 shows a training example containing\nan explanation in the AI part which was generated using this ap-\nproach. The prompts used for generating these explanations, as\nwell as the longer, more open-ended explanations, are available in\nthe accompanying repository.\nFigure 3: Training example containing a textual explanation.\nThe explanation is generated using Wadhwa et al.\u2019s prompt.\nStructured Explanations: Our third approach is based on the\nassumption that LLMs can learn more from structured explanations\ncompared to textual ones. Thus, building on the work of Peeters\net al. [ 18], we instruct the model to generate explanations in a\nstructured format that explicitly mention the attributes that were\nused for the matching decision, their importance for the decision,\nas well as the similarity of the attribute values. Figure 4 shows an\nexample of a structured explanation that was generated by GPT-4o-\nmini. The complete prompt for the generation of the explanation is\nfound in the project repository and is discussed in [ 18]. The exampleshows that the LLM incorporates factors beyond string matching\nfor calculating similarities, such as recognizing the relationship\nbetween Microsoft and Skype for Business.\nFigure 4: Training example including a structured explana-\ntion as used during fine-tuning.\n4.1 Effectiveness\nWe compare the effectiveness of different explanation approaches\nin a non-transfer scenario using the WDC Products dataset. The\nresult of these experiments are found in the No Transfer column of\nTable 3.\nSmaller Models: Llama 8B outperforms standard fine-tuning\nacross all evaluated explanation approaches, as shown in Table 3,\ncolumn No Transfer. Long textual explanations provide the smallest\nincrease of 1.48 F1 points. The approach by Wadhwa et al. results in\na 4.01-point improvement. Structured explanations further improve\nF1 scores by 0.93 over Wadhwa\u2019s approach. GPT-4o-mini shows a\ndifferent trend, where only structured explanations outperform the\nstandard fine-tuning, increasing by 0.97 F1 points (Table 3, WDC\nProducts column). The other configurations underperform, with\nthree out of five approaches performing worse than zero-shot (Table\n3, Non-transfer column).\nLarger Model: Structured explanations are exclusively tested\non larger models due to their consistent performance improvement\nin smaller models. While Llama 70B does not benefit from standard\nfine-tuning, structured explanations improve its performance by\n1.50 F1 points over zero-shot (Table 3, WDC column). In contrast,\nGPT-4o does not follow this trend. While structured explanations\nlead to a 1.60-point increase over zero-shot, standard fine-tuning\nresults in a 5.50-point improvement, as shown in the GPT-4o rows\nin the WDC column of Table 3. Therefore, GPT-4o does not sig-\nnificantly benefit from structured explanations in a non-transfer\nsetting.\n4.2 Generalization\nWe fine-tune the models using the WDC dataset and afterwards\napply them to the test sets of the other datasets. We measure the\ngeneralization performance within the product domain and across\ndomains using the scholarly datasets.\nIn-Domain Generalization: Standard fine-tuning results in\nLlama 8B achieving 72% of the dedicated models\u2019 performance on\nproduct datasets (Table 3, In-Domain Transfer Gain column). TheAaron Steiner, Ralph Peeters, and Christian Bizer\nTable 3: Fine-tuning results for different types of explanations (F1 scores, deltas to standard fine-tuning on WDC).\nModel Train setNo Transfer In-Domain Transfer Transfer Cross-Domain Transfer Transfer\nWDC A-B A-G W-A Gain D-A D-S Gain\nLlama 8B Zero-shot 53.36 (-15.83) 56.57 (-25.21) 49.16 (-3.13) 42.04 (-11.70) - 85.52 (+11.00) 67.69 (+0.30) -\nLlama 8B WDC 69.19 (0.00) 81.78 (0.00) 52.29 (0.00) 53.74 (0.00) 72% 74.52 (0.00) 67.40 (0.00) -30%\nLlama 8B long textual 70.67 (+1.48) 83.33 (+1.55) 45.95 (-6.34) 46.53 (-7.21) 51% 51.11 (-23.41) 47.92 (-19.48) -146%\nLlama 8B Wadhwa et al. 73.20 (+4.01) 79.00 (-2.78) 50.30 (-1.99) 48.90 (-4.84) 55% 69.14 (+5.39) 63.35 (-4.05) -56%\nLlama 8B no imp.&sim. 73.58 (+4.39) 85.25 (+3.47) 52.56 (+0.27) 55.76 (+2.02) 83% 55.55 (-18.98) 51.14 (-16.25) -125%\nLlama 8B no importance 73.82 (+4.63) 84.82 (+3.04) 54.26 (+1.97) 60.00 (+6.26) 93% 86.06 (+11.63) 69.19 (+1.79) 5%\nLlama 8B structured 74.13 (+4.94) 86.89 (+5.11) 51.84 (-0.45) 59.32 (+5.58) 91% 79.88 (+5.36) 63.67 (-3.72) -26%\ngpt-4o-m Zero-shot 81.61 (-1.80) 87.68 (-2.77) 59.20 (-3.09) 65.06 (-2.39) - 94.16 (+8.81) 87.96 (+11.63) -\ngpt-4o-m WDC 83.41 (0.00) 90.45 (0.00) 62.29 (0.00) 67.45 (0.00) 13% 85.35 (0.00) 76.33 (0.00) -55%\ngpt-4o-m long textual 81.30 (-2.11) 88.94 (-1.51) 61.37 (-0.92) 64.23 (-3.22) 5% 89.75 (+4.40) 88.10 (+11.77) -11%\ngpt-4o-m Wadhwa et al. 80.81 (-2.60) 84.12 (-6.33) 59.03 (-3.26) 64.19 (-3.26) -14% 93.18 (+7.83) 87.77 (11.44) -3%\ngpt-4o-m no imp.&sim. 81.04 (-2.37) 90.95 (+0.50) 61.30 (-0.99) 66.40 (-1.05) 7% 92.80 (+7.45) 85.73 (+9.40) -10%\ngpt-4o-m no importance 83.17 (-0.24) 90.26 (-0.19) 60.71 (-1.58) 65.09 (-2.36) 4% 90.51 (+5.16) 84.82 (+8.49) -18%\ngpt-4o-m structured 84.38 (+0.97) 91.44 (+0.99) 64.11 (+1.82) 68.92 (+1.47) 23% 88.87 (+3.52) 79.45 (+3.12) -37%\nLlama 70B Zero-shot 75.20 (+2.50) 79.10 (+1.20) 51.40 (-4.00) 55.60 (-5.00) - 80.50 (+10.60) 69.50 (+5.60) -\nLlama 70B WDC 72.70 (0.00) 77.90 (0.00) 55.40 (0.00) 60.60 (0.00) - 69.90 (0.00) 63.90 (0.00) -\nLlama 70B structured 76.70 (+4.00) 84.80 (+6.90) 52.80 (-2.60) 65.80 (+5.20) - 70.10 (+0.20) 62.10 (-1.80) -\ngpt-4o Zero-shot 81.60 (-5.50) 92.20 (+0.20) 63.45 (-1.65) 70.67 (+2.17) - 87.18 (-2.09) 74.59 (-6.15) -\ngpt-4o WDC 87.10 (0.00) 92.00 (0.00) 65.10 (0.00) 68.50 (0.00) - 89.27 (0.00) 80.74 (0.00) -\ngpt-4o structured 83.20 (-3.90) 90.60 (-1.40) 62.80 (-2.30) 66.50 (-2.00) - 84.69 (-4.58) 74.90 (-5.84) -\nWadhwa et al. approach and longer textual explanations underper-\nform, achieving 55% and 51%, respectively. However, structured\nexplanations surpass the fine-tuned baseline, with the structured\nandno importance approaches achieving 93% and 91% of source\nmodel performance (Table 3, In-Domain Transfer Gain column).\nGPT-4o-mini continues the trend where four out of five explana-\ntion approaches underperform the fine-tuned baseline. However,\nstructured explanations outperform the baseline, achieving 23% of\nthe source models\u2019 performance compared to 13% by the fine-tuned\nbaseline (Table 3, In-Domain Transfer Gain column).\nLlama 70B benefits from structured explanations, outperforming\nboth the fine-tuned baseline and zero-shot in in-domain general-\nization. It shows the highest generalization performance on two\nout of three product datasets (Table 3, In-Domain Transfer Gain\ncolumn).\nAs shown in the last row of Table 3, structured explanations\ndo not improve the generalization performance of GPT-4o, as this\napproach underperforms both the fine-tuned baseline and zero-\nshot.\nCross-Domain Generalization: Llama 8B struggles to general-\nize effectively in cross-domain settings, with all approaches under-\nperforming compared to zero-shot performance. In contrast, GPT-\n4o-mini shows improvements across all explanation approaches,\nhowever non-rival zero-shot performance.\nIn conclusion, structured explanations significantly enhance in-\ndomain generalization, while cross-domain generalization remains\nlimited.5 DIMENSION 2: EXAMPLE SELECTION AND\nGENERATION\nThis section explores how the selection and generation of training\nexamples for fine-tuning impact in-domain performance and model\ngeneralization. We examine three strategies: filtering the existing\ntraining sets, generating new artificial examples based on existing\ndata, and adding additional training examples that are similar to\nexamples on which the model failed. We evaluate the techniques\nwithout augmenting the training examples with explanations. We\nprovide the complete prompts for the example selection and gener-\nation as well as filtered and augmented training sets in the project\nrepository.\n5.1 Training Set Filtration\nThe filtration approach is based on the assumption that training\ndata quality outweighs quantity and it is thus beneficial to remove\npotentially misleading examples from the training set.\nError-based Filtering: GPT-4o-mini is prompted to match pairs\nin the small WDC training split using the complex-force prompt (see\nSection 3.3). Training examples for which the model\u2019s labels differ\nfrom the ground truth labels are discarded. This approach may\ndiscard some correctly labelled examples. However, it is hypothe-\nsized that overall dataset quality improves. The resulting training\nset WDC-filtered has a size of 2006 examples, compared to 2500\nexamples of the original WDC-small training set (see Table 4).\nRelevancy-based Filtering: The second approach involves fil-\ntering for relevance, operating on the assumption that fewer, moreFine-tuning Large Language Models for Entity Matching\nTable 4: Impact of the filtration and generation methods on\nthe size the training set.\nDataset # Pos # Neg # Total\nWDC-small 500 2000 2500\nWDC-filtered 445 1561 2006\nWDC-filtered-rel 442 166 608\nSyn 4932 15208 20140\nSyn-filtered 3264 10560 13824\nSyn-filtered-rel 2182 6718 8900\nmeaningful examples lead to better outcomes. For example, compar-\ning a hard drive and a TV might pass initial filtration but offer lim-\nited value due to their low complexity. By removing such examples,\ncomputational demands are certainly reduced, while training re-\nsults may potentially be enhanced. This approach prompts GPT-4o\nto select only \u201cinteresting\u201d training examples, a term we intention-\nally leave undefined, allowing the model to interpret it. The model\nappears to define \u201cinteresting\u201d as pairs that share many attributes\nand are thus highly similar (corner cases). Pairs deemed irrelevant\nby the model are discarded. The resulting WDC-filtered-rel training\nset has a total size of 608 examples (see Table 4).\nEffectiveness: As shown in Table 5 column No Transfer, the\nLlama 8B model trained on the WDC-s-filtered andWDC-s-filtered-\nreldatasets outperforms the fine-tuned baseline by 4.73 and 3.18\nF1 points, respectively. Both filtered approaches surpass the Llama\n8B model trained on the WDC large training set, containing nearly\n20,000 examples (see Table 1). In contrast, GPT-4o-mini does not\nexhibit similar improvements, as both filtered datasets perform\nbelow the zero-shot model.\nGeneralization: While filtration improves Llama 8B\u2019s perfor-\nmance, it does not significantly impact in-domain generalization.\nThe fine-tuned baseline captures 72% of dedicated performance,\nwhile filtered approaches capture 75% and 76% with relevancy\nfiltration (see Table 5, In-Domain Transfer Gain column). Since\nGPT-4o-mini does not improve its performance with filtration in\na no-transfer scenario, the same trend is observed in in-domain\ngeneralization, where this approach underperforms the fine-tuned\nbaseline across all product datasets.\nFor cross-domain generalization, both Llama 8B and GPT-4o-\nmini continue to underperform relative to the zero-shot baselines.\n5.2 Example Generation\nWhile the earlier approaches focused on discarding irrelevant exam-\nples, this section investigates using GPT-4o to generate additional\ntraining examples for fine-tuning. We test three methods for gener-\nating examples"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract\nThis study presents a deep learning model designed to accurately predict detailed head impact information, crucial for studying traumatic brain injuries (TBI) and improving helmet design. Leveraging a dataset of 16,000 simulated helmeted head impacts from the Riddell helmet finite element model, it employs a Long Short-Term Memory (LSTM) network to process tri-axial linear accelerations and angular velocities. The model predicts impact location, direction, speed, and force profile with over 70% accuracy (R2 > 0.7). Validation with an on-field dataset of 79 head impacts shows the model achieving a 79.7% accuracy in identifying impact locations, significantly outperforming existing methods.\n\n### 2. Introduction\n**Context:** TBI is a prevalent neurological condition caused by head impacts that result in brain deformation. Accurate measurement of head impact characteristics (location, direction, speed, and force) is essential for TBI research, helmet design, and safety evaluations.\n\n**Research Questions:** How accurately can head kinematic data predict the detailed parameters of head impacts? Can deep learning models improve prediction accuracy over traditional methods?\n\n**Literature Review and Positioning:**\n- Previous methods based on inertial sensors and mechanical models often fail to predict impact forces and directions accurately.\n- Mechanical models have major limitations, especially in helmeted sports where the force is dispersed.\n- Recent studies (Kuo et al., Bartsch et al.) show limited accuracy in predicting impact locations from head kinematics.\n- This study aims to develop and validate a deep learning-based model to overcome these limitations.\n\n### 3. Related Work\n- **Sensor-Based Measurement Systems:** Various systems like Head Impact Telemetry System (HITS) and instrumented mouthguards have been used, but they lack the precision needed for detailed impact analysis.\n- **Mechanical Models:** Previous models estimated impact location by considering head kinematics but showed limited accuracy, especially with helmet-induced dispersions.\n- **Finite Element Analysis (FEA):** Used for simulating head impacts, but not linked with predictive models leveraging deep learning.\n  \n**Advancement Over State of the Art:** This paper combines FEA simulations with deep learning to predict multiple impact characteristics from head kinematics, significantly enhancing prediction accuracy.\n\n### 4. Method\n**Proposed Approach:**\n- **Deep Learning Model:** Utilized an LSTM network for its capability to handle time-series data and long-term dependencies.\n  \n**Mathematical Formulations and Algorithms:**\n- Input features: Tri-axial linear accelerations and angular velocities recorded in 145 ms timeframes.\n- Model architecture: Two one-dimensional LSTM layers followed by dropout layers and a dense layer.\n\n**Architectural Details:**\n- **Input:** 48 channels of head kinematics across four reference frames providing data for 145 ms.\n- **Output:** Impact parameters (\u03b1, \u03b2, Y, Z, speed) and force profiles (145-element vector).\n\n**Novel Techniques or Adaptations:**\n- Use of multi-reference frames to capture comprehensive features.\n- Transfer learning potential to adapt the model for different helmets.\n\n### 5. Experimental Setup\n**Datasets:**\n- **In-silico Dataset:** 16,000 simulated helmeted impacts from Riddell helmet FEA.\n- **On-field American Football Dataset:** 118 head impacts recorded and validated via instrumented mouthguards and video, with clear locations for 79 impacts.\n\n**Hardware and Software:**\n- Simulation run on FEA tools.\n- Model training and evaluation on high-performance computing clusters.\n\n**Evaluation Metrics:**\n- Mean Absolute Error (MAE)\n- Root Mean Squared Error (RMSE)\n- Coefficient of Determination (R2)\n\n**Baselines and Comparison Methods:**\n- Existing mechanical methods like HITS, models by Kuo et al., and Bartsch et al., which utilize linear and angular kinematics.\n\n### 6. Results and Analysis\n**Empirical Findings:**\n- **Impact Parameter Prediction:** R2 values for speed, \u03b1, \u03b2, and Y exceed 70%, indicating high accuracy. Z prediction showed the least accuracy (R2 = 0.159).\n- **Force Prediction:** Pointwise MAE and RMSE for force were less than 0.15 kN and 0.5 kN, respectively. Peak R2 values exceeded 85%.\n  \n**Graphs, Tables, Figures:**\n- Confusion matrices indicating model success in predicting correct impact locations.\n- Visual comparisons of force profiles showing model predictions versus FEA results.\n\n**Analysis:**\n- The deep learning model outperformed existing methods significantly in predicting impact locations, demonstrating robustness across various impact types.\n- Notably more accurate in predicting facemask impacts than existing methods.\n\n**Unexpected Results:**\n- The least accuracy was observed in predicting Z, attributed to the relative consistency in Z across impacts.\n\n### 7. Discussion\n**Interpretation of Results:**\n- The model's superiority in prediction accuracy suggests its significant potential for applications in helmet design and sports safety.\n- The extensive training dataset and LSTM's capability to capture temporal dependencies are crucial to its performance.\n\n**Implications:**\n- Enhanced helmet designs can be derived from accurate impact data, potentially reducing TBI risk.\n- Real-time application in sports can improve player safety by assessing impact severity instantly.\n\n**Limitations and Biases:**\n- Model trained on a specific helmet type, limiting generalizability. Transfer learning could mitigate this.\n- On-field dataset predominantly composed of facemask impacts, requiring more diverse impact datasets for thorough validation.\n\n### 8. Conclusion\n**Key Contributions:**\n- Developed a highly accurate LSTM model for predicting detailed head impact parameters.\n- Demonstrated superior performance to existing methods in both simulated and on-field datasets.\n\n**Future Research Directions:**\n- Validation across different helmet types and sports.\n- Integration of more diverse datasets to enhance model robustness.\n- Exploration of transfer learning to adapt the model for various real-world applications.\n\n### Code and Data Availability\nThe code and datasets used for developing the model are publicly available at [GitHub Repository](https://github.com/xzhan96-stf/impact_retriever.git).\n\n### Declaration of Interest\nThe authors declare no conflicts of interest.\n\n---\n\nThis detailed summary outlines the study's purpose, methods, and findings, providing the necessary insights for other researchers to replicate or build upon the work."}, {"role": "assistant", "content": "1\nIdentification of head impact locations, speeds, and\nforce based on head kinematics\nXianghao Zhan, Yuzhe Liu, Nicholas J. Cecchi, Jessica Towns, Ashlyn A. Callan, Olivier Gevaert, Michael M.\nZeineh, David B. Camarillo\nAbstract \u2014Objective: Head impact information including im-\npact directions, speeds and force are important to study traumatic\nbrain injury, design and evaluate protective gears. This study\npresents a deep learning model developed to accurately predict\nhead impact information, including location, speed, orientation,\nand force, based on head kinematics during helmeted impacts.\nMethods: Leveraging a dataset of 16,000 simulated helmeted\nhead impacts using the Riddell helmet finite element model, we\nimplemented a Long Short-Term Memory (LSTM) network to\nprocess the head kinematics: tri-axial linear accelerations and\nangular velocities. Results: The models accurately predict the\nimpact parameters describing impact location, direction, speed,\nand the impact force profile with R2exceeding 70% for all\ntasks. Further validation was conducted using an on-field dataset\nrecorded by instrumented mouthguards and videos, consisting\nof 79 head impacts in which the impact location can be clearly\nidentified. The deep learning model significantly outperformed\nexisting methods, achieving a 79.7% accuracy in identifying\nimpact locations, compared to lower accuracies with traditional\nmethods (the highest accuracy of existing methods is 49.4%).\nConclusion: The precision underscores the model\u2019s potential in\nenhancing helmet design and safety in sports by providing more\naccurate impact data. Future studies should test the models\nacross various helmets and sports on large in vivo datasets to\nvalidate the accuracy of the models, employing techniques like\ntransfer learning to broaden its effectiveness.\nIndex Terms \u2014traumatic brain injury, head impact, impact\ndirection, deep learning, recurrent neural network\nI. I NTRODUCTION\nHead impacts may lead to Traumatic Brain Injury (TBI),\nwhich is the neurological disease with the highest global\nincidence and prevalence [1]. Recent studies found evidence\nshowing that neurological pathologies were closely associated\nwith the brain deformation produced by head impacts [2]\u2013[6].\nTherefore, measuring head impact is essential in TBI research.\nMechanically, a head impact is characterized by the impact\nlocation on the head, the direction, speed of the impact mass,\nXianghao Zhan, Nicholas Cecchi, Jessica Towns, Ashlyn Callan, David Ca-\nmarillo are with Department of Bioengineering, Stanford University, Stanford,\nCA, 94305, USA; Yuzhe Liu is with School of Biological Science and Medical\nEngineering, BeiHang University, Beijing, 10019, China; Olivier Gevaert is\nwith Department of Biomedical Data Science, Stanford University, Stanford,\n94305, CA, USA; Michael Zeineh is with Department of Radiology, Stanford\nUniversity, Stanford, 94305, CA, USA. (Corresponding Author: Yuzhe Liu\ne-mail: yuzheliu@buaa.edu.cn)\nThis research was supported by the National Natural Science Founda-\ntion of China Young Scholar Program 1230021530, Young Elite Scientists\nSponsorship Program by CSTAM 2023-XSC-HW3, Fundamental Research\nFunds for the Central Universities YWF-23-Q-1029, the Pac-12 Conference\u2019s\nStudent-Athlete Health and Well-Being Initiative, the National Institutes of\nHealth (R24NS098518), Taube Stanford Children\u2019s Concussion Initiative and\nStanford Department of Bioengineering.and impact force. These head impact parameters are influential\nto the risk of TBI. Obviously, higher impact speed leads\nto more severe brain deformation and thus results in higher\nTBI risk. Then, the impact direction and location are also\nimportant because the complicated anisotropic brain structures\nmake the brain\u2019s vulnerability vary significantly when the head\nacceleration direction is changed [7].\nHelmets are designed according to the head impact infor-\nmation: if a helmet is used for a situation where the head\nimpact is of high impact energy, the energy absorbing system\nis stiffer and heavier, and vice versa. Then, if the risky head\nimpact in the situation frequently happens at specific locations,\nthose locations should be strengthened. For example, position-\nspecific features were found in concussive head impacts in\nAmerican Football, therefore position-specific helmets were\nsuggested to reduce the TBI risk [8]. The energy absorption\nat the forehead of the motorbike helmet was strengthened\nby inserting aluminum honeycomb fillers [9]. Furthermore,\nspecific impact locations were required in the helmet tests [10],\nand the scores evaluating the helmet protection were weighted\naccording to the frequency of concussive head impacts. Head\nimpact information is also used to analyze the head impact\nand identify the TBI risk factors. In American football, front,\nand side head impacts are more frequent, and the back impact\nis always more severe [11], especially for quarterbacks. The\nside and rear head impacts were found to be associated with\nobvious performance decrement in on-field American Football\ngames [12].\nAlthough the head impact information is important, it is\nhard to directly measure because the sensors have to cover all\nthe potential impact locations. A recent attempt is the Head\nImpact Remote Sensing (HIRS) system [13], which is a self-\npowered sensor array based on a multiangle triboelectric nano-\ngenerator (MA-TENG). HIRS was integrated into the snow\nhelmet to detect head impact and has exhibited a promising\nability to measure the impact force and location. However,\nHIRS applicability in on-field games needs to be further\nexamined considering the weight and expense, and there is\nno head impact dataset collected by HIRS so far. Another\nsolution is to reconstruct the head impacts with dummies in\nlabs according to the videos: 54 professional football head\nimpacts were reconstructed by Sanchez et al. [14] and the\nNational Football League (NFL) helmet testing protocols were\ndeveloped based on the head impact information collected by\nthis method. Furthermore, the helmets that were damaged in\nhead impacts were collected to reconstruct the head impact\naccident by comparing the cracks in the helmets [15]\u2013[17].arXiv:2409.08177v1  [eess.SP]  12 Sep 20242\nThese two methods do not need pre-deploy sensors, but it\nis limited by two factors: the accuracy of reconstructing\nhead impact according to helmet cracks and the expense of\ncollecting large amounts of damaged helmets.\nAn alternative method is to estimate the head impact\ninformation according to the head kinematics, which has\nbeen widely measured by wearable inertial sensors such as\ninstrumented mouthguards, helmet-based and skin-based sen-\nsors [18]\u2013[21]. Based on the head kinematics, mechanical\nmodels were developed to reconstruct the head impact and thus\nretrieve the impact location. In the Head Impact Telemetry\nSystem (HITS), the direction of linear acceleration at the\nCenter of Gravity (CoG) of the head was considered as the\nimpact direction, and part of the head in the opposite direction\nwas considered as the impact region [22]. Then, the direction\nof angular motions (acceleration, velocity, and location) was\nused to find an orthogonal cross vector, which is used to\ncorrect the impact direction [23]. However, this correction was\nfound not to be effective because only 37% of the impact was\nidentified as the right of the impact region [23], and only 28%\nof the impact was identified correctly by directly using linear\nacceleration [22]. Furthermore, another model is developed\nby neglecting the neck and assuming that the head is a free\nrigid body [24]. They used linear and angular accelerations to\ncalculate the total force and torque on this body, which was\nthen used to solve the impact direction, location, and force\nmagnitude. Then, a multi-degree of freedom (MDOF) system\nof head-neck systems was proposed to estimate the impact\nforce, and promising accuracy was found compared with the\nimpact test using Hybrid III headform and neck [25]. However,\nthe accuracies of these kinematics-based methods [24], [25]\nwere not analyzed. It should be noted that these kinematics-\nbased methods listed above [22]\u2013[25] did not include helmets,\nbut some [22]\u2013[24] were widely used in helmeted sports like\nAmerican Football. As helmets are designed to disperse the\nimpact force and mitigate the head acceleration, they may\ngenerate considerable error in estimating the head impact\ninformation. Another limitation of these kinematics-based\nmethods is that they only provide the impact locations, while\nthey can not predict the impact speed and impact force.\nIn this paper, we developed a new impact information\nretrieval model based on deep learning to predict all the head\nimpact information including impact location, direction, speed,\nand impact force according to the head kinematics in helmeted\nimpacts. A large dataset of helmeted head impacts (n=16,000)\nwas established to develop the model, and promising accuracy\nof head impact information was found. Furthermore, we used\nan on-field American football dataset of head kinematics with\nrecorded video (n=118) to examine the accuracy of the deep-\nlearning model in on-field games. The impact retrieval model\nsignificantly improves the accuracy in identifying the impact\nlocation compared with current methods [22]\u2013[24]. With this\npaper, we published the large head impact simulation datasets\n(n=16,000) for those willing to develop signal-based data-\ndriven models.\nFig. 1. The impact model setup. (A) Finite element model of helmet impact\ntest [26]. The parameters of headform adjustment and the positive directions.\nThe impact line indicates the trajectory of the impactor head; (B) The\nintersection point between the impact line and the circumscribed sphere.\nII. M ATERIALS AND METHODS\nA. Dataset of in-silico Head Impacts\nA large head impact dataset with 16,000 head impacts was\ngenerated to develop the impact retrieval model based on the\nnumerical model of the NFL standard helmet testing platform\nconsisting of a pneumatics impact, hybrid III headform and\nneck, and a sliding table to support the neck (Fig. 1). The FE\nmodel of Riddell helmets [26] is installed on the FE Hybrid\nIII headform. As the linear impactor fires, the impactor head\nwill be launched at a controlled initial speed along its axial\ndirection until it blows the headform. In both the physical\nmodel and the FE model, the farthest distance that the impactor\ncan go is fixed, and we removed this constraint to make\nsure the impactor can go far enough, which is closer to the\nactual situation of on-field head impacts. The accelerometer\nand gyroscope built in the FE headform model will record the\nlinear acceleration and the angular velocity of the headform,\nand the impact force on the helmet and the head will be\nrecorded.\nThe axis of the pneumatics impactor is fixed, and the\nheadform can move translationally and rotationally to have\ndifferent impact directions and locations. As shown in Fig.\n1A, the headform can be adjusted in five degrees of freedom\nto produce different head impacts: pitch angle, which shows\nthe headform rotation in the sagittal plane, is denoted as\n\u03b2; yaw angle, which shows the headform rotation in the\nhorizontal plane, is denoted as \u03b1; the headform translational\nmovement along the impactor axis, denoted as X; the headform\ntranslational movement in the horizontal plane and vertically\nto the impactor axis, denoted as Y; the headform translational\nmovement upwards and downwards, denoted as Z. The roll\nangle is not a degree of freedom because the whole test\nsetup is axis symmetrical to the impactor axis. Then, since\nthe impactor head can move freely in X direction, a group of\nparameters including \u03b1,\u03b2, Y , Z and impact speed determine\na head impact: \u03b1,\u03b2determine the impact direction, and \u03b1,\u03b2,\nY , Z together determine the impact line, which is the line the\nimpact header moves along. The intersection point between the\nimpact line and the helmet is the impact location. To simplify\nthe calculation of the intersection point, the helmet profile3\nwas represented by a sphere with a radius of 135 mm and\nthe center at the head CoG. In the spherical coordinate, the\nimpact location can be represented by \u03b8and\u03b7, as shown in\nFig. 1B.\nTo avoid the impactor hitting the bottom of the headform,\nwhich is impossible in the real world, we excluded \u03b2which is\nclose to 0\u00b0 or 180\u00b0. The basic dataset was established with \u03b1\nranging from 10\u00b0 to 180\u00b0 (8 values, \u03b1is positive here because\nof the symmetry), \u03b2ranging from -45\u00b0 to 70\u00b0 (8 values), Y\nranging from -120 mm to 120 mm (5 values) and Z ranging\nfrom -120 mm to 120 mm (5 values). Impact speed ranges\nfrom 3 m/s to 10 m/s with 5 values. These ranges include\nmost of the head impacts in the real world and 8,000 head\nimpacts were generated.\nEvery head impact was simulated for 150 ms to make sure\nthe impact was finished. Then, linear acceleration, angular\nvelocity, and force on the helmet and head/face were outputted\nwith an interval of 1 ms and then filtered by zero-phase digital\nfiltering with a cut-off frequency of 300 Hz. The headform\nkinematics were transferred from the anatomical reference\n(moving with the headform) to the global reference using\nquaternary. Because the whole impact setup is symmetrical in\nthe y direction, another 8,000 head impacts can be obtained\nby mirroring head impacts against the vertical plane passing\nthe impactor axis. For the mirrored cases, -1 was multiplied\nto y and \u03b1for an impact setup, the linear acceleration and\nforce in Y direction (X, Y , Z is the direction in the headform\nreference, X is posterior to anterior, Y is left to right, Z is top\nto bottom), and angular velocity in X and Z direction. As a\nresult, we generated 16,000 head impacts dataset.\nB. Dataset of on-field American Football Head Impacts and\nImpact Location Review\nA dataset of head kinematics was collected in the varsity\nStanford American Football team with instrumented mouth-\nguards [27], [28]. The instrumented mouthguards were built\nwith a set tri-axial accelerometer and gyroscope, which can\nmeasure linear acceleration and angular velocity of heads. The\nmouthguard can rigidly fit to the upper dentition, therefore\nsensors can accurately measure the head kinematics [29]. All\nhead impacts recorded by the instrumented mouthguard were\nconfirmed by videos, and in total 118 head impacts were\nincluded in the dataset [19].\nTo obtain the impact locations of these head impacts, the\nscreenshot of the head impact was saved and reviewed by\nfive reviewers who have experience in analyzing American\nFootball head impacts. According to the screenshot, five re-\nviewers determined if the impact location is \u201dfacemask\u201d, \u201dTop\nof helmet\u201d, \u201dBack of helmet\u201d, \u201dLeft of helmet\u201d or \u201dRight of\nhelmet\u201d independently. If more than three reviewers observed\na head impact at the same location, we considered this impact\nlocation of this head impact as clearly identified. Otherwise,\nthe head impact was excluded from the dataset. Then, the rest\nof the head impacts (n=79) were used as a reference dataset\nto test the ability of the impact retrieval model as well as\nprevious models [22]\u2013[24] to determine the location of head\nimpacts. The impact retrieval model as well as previous modelscalculate the spherical coordinate of the impact location \u03b7and\n\u03b8, which were different from the description of the impact\nlocation in video reviews. To transform \u03b7and\u03b8to the impact\nlocation description, we measured the FE model of the Riddell\nhelmet, and determined the head impact with \u03b7 < -34\u00b0 is at\nthe \u201dTop of helmet\u201d. Otherwise, if -45\u00b0 < \u03b8 < 45\u00b0, the impact\nis at \u201dFacemask\u201d; 45\u00b0 < \u03b8 < 135\u00b0, it\u2019s at \u201dRight of helmet\u201d;\n-135\u00b0< \u03b8 < -45\u00b0, it\u2019s at \u201dLeft of helmet\u201d; 135\u00b0 < \u03b8 < 180\u00b0 or\n-180\u00b0< \u03b8 < -135\u00b0, it\u2019s at \u201dBack of helmet\u201d. Then, the ratios\nof correct predictions were compared among different methods\nto show their ability to determine the impact locations.\nC. Impact Retrieval Deep Learning Model\nHead kinematics are results of the impact force, so it is\npossible to predict the impact information based on the head\nkinematics. We developed the force retrieval model to retrieve\nthe head impact information including the speed, orientation\nparameters ( \u03b1,\u03b2, Y , and Z), as well as the impact force curve\naccording to the head kinematics. Recurrent Neural Networks\n(RNNs), and particularly Long Short-Term Memory (LSTM)\nnetworks, have gained prominence in analyzing time-series\ndata and signal processing. In this study, we harnessed LSTM-\nbased RNNs to infer impact speed, orientation and force\nprofiles from head kinematic data, using a training dataset\nderived from Riddell helmet head impact simulations.\nThe input of the force retrieval model is head kinematics,\nwhich consists of signals spanning 145 milliseconds, capturing\nthe duration of the impact. We enhanced model accuracy by\nincorporating a rich set of features: tri-axial angular velocity\nand its magnitude (4 channels), tri-axial angular acceleration\nand its magnitude (4 channels), and tri-axial linear acceleration\nand its magnitude (4 channels). These were captured across\nfour reference frames: global, global spherical, local, and local\nspherical, culminating in 48 signal channels (12 channels of\nkinematics under four reference frames), each with a 145 ms\ntimeframe.\nThe output of the force retrieval model includes head impact\nparameters: \u03b1,\u03b2, Y , Z, and the impact speed, as well as the\nforce profiles exerted on the helmet and head/face (in kN). The\nmodel outputs a single value for each of the first five metrics\nand a 145-element vector for the force profiles.\nTo strike a balance between model complexity, the risk of\noverfitting, and computational efficiency, our LSTM archi-\ntecture comprised two one-dimensional LSTM layers, each\nsucceeded by a dropout layer, and a fully connected dense\nlayer preceding the output layer.\nThe LSTM\u2019s hyperparameters\u2014including the number of\nunits per LSTM cell, learning rate, number of epochs, dropout\nrate, and L2 regularization on the LSTM kernel\u2014were metic-\nulously optimized on a validation dataset to minimize the\nMean Absolute Error (MAE). For force profile predictions,\nboth LSTM layers were configured to return sequences, while\nfor the impact information (speed and orientation parameters)\nprediction, only the first LSTM layer returned sequences.\nTherefore, assuming the batch size of training data was N\nwhile the hidden units were D, for the impact information\nprediction model, the shapes of data flow were: Input: (N,4\n145, 48), LSTM layer 1 output: (N, 145, D), Dropout layer 1\noutput: (N, 145, D), LSTM layer 2 output: (N, D), Dropout\nlayer 2 output: (N, D), Final output: (N, 1). For the force\nprofile prediction, the shapes of the data flow were: Input: (N,\n145, 48), LSTM layer 1 output: (N, 145, D), Dropout layer\n1 output: (N, 145, D), LSTM layer 2 output: (N, 145, D),\nDropout layer 2 output: (N, 145, D), Final output: (N, 145,\n1). This approach was designed to ensure the LSTM models\nwere adequately expressive yet computationally manageable,\nallowing for accurate and efficient predictions of both impact\ncharacteristics and force profiles.\nTo rigorously assess the performance of our model and the\nrobustness of our test results, we distributed the comprehensive\nRiddell helmet head impacts dataset into three distinct subsets:\ntraining, validation, and testing, with an allocation of 80%,\n10%, and 10%, respectively. This partitioning was replicated\nacross 20 parallel experiments, each initialized with a unique\nrandom seed to test the robustness of the models. For each\nsubset within these partitions, we employed three key metrics\nto quantify the accuracy and predictive power of our model:\nMean Absolute Error (MAE), Root Mean Squared Error\n(RMSE), and the Coefficient of Determination ( R2). These\nmetrics were calculated for both validation and test datasets\nto ensure thorough evaluation. Specific to the task of force\nprofile prediction, we undertook a more granular analysis. We\ncalculated pointwise MAE and RMSE to assess overall fidelity\nat each timestep, and peak MAE, peak RMSE, and peak R2to\nspecifically scrutinize the model\u2019s accuracy at critical points\nof impact.\nThe outcomes of these 20 parallel experiments were syn-\nthesized to provide a comprehensive overview of the model\u2019s\nperformance. In the results section, we have presented these\nfindings in the form of summary statistics\u2014namely the mean\nand standard deviation for each metric\u2014to offer a clear and\nstatistical insight into the model\u2019s predictive reliability.\nIII. R ESULTS\nA. Predicting Head Impact Parameters in in-silico Head Im-\npacts Dataset\nTo evaluate the potential of estimating impact speed, ori-\nentation and force using head kinematic signals captured,\nwe developed LSTM models utilizing the Riddell Helmet\nfinite-element-simulation datasets (n=16,000). After training\non 80% of the data, the model demonstrated high accuracy\nwhen assessed against the 10% hold-out test data (n=1,600).\nDetails of the tuned hyperparameters can be found in Table S1.\nFig. 2 shows the performance of the force retrieval model in\nterms of head impact parameter prediction and promising R2\ncan be found in predicting the impact parameters except for\nZ. The LSTM models achieved coefficients of determination\nexceeding 70% for all tasks except for the prediction of Z.\nThe mean absolute error (MAE) values, computed from 20\nparallel experiments for estimating impact speed, \u03b1,\u03b2, and\nY , were 0.335 m/s, 25.467 degrees, 9.384 degrees, and 26.226\nmm, respectively.\nMAE\nRMSE0.335 (0.126) (m/s)\n0.600 (0.122) (m/s)25.467 (3.155) ( \u00b0)\n55.897 (5.874) ( \u00b0)9.384 (2.176) ( \u00b0)\n12.395 (2.440) ( \u00b0)26.226 (5.368) (mm)\n36.625 (5.395) (mm)77.821 (6.879) (mm)\n97.441 (6.501) (mm)Fig. 2. The R2of predicting impact speed and orientation based on head\nkinematics based on the LSTM model on the Riddell Helmet finite-element-\nsimulation datasets. The mean and standard deviation of mean absolute\nerror (MAE) and root mean squared error (RMSE) on the test set over\n20 experiments with random training/validation/test dataset partitions were\nreported.\nB. Predicting Head Impact Force in in-silico Head Impacts\nDataset\nBesides the prediction of impact parameters, we also de-\nveloped LSTM models to predict the impact force. Fig. 3\nshows the performance of the force retrieval model in terms\nof impact force profile prediction. We assessed the pointwise\nMAE, pointwise RMSE, peak MAE, peak RMSE, and peak R2\non the hold-out test set (n=1,600) in 20 parallel experiments\nwith random test set partitions. Both the pointwise MAE and\npeak MAE measured less than 0.15kN and 0.5kN, respectively,\nwhile the peak R2values exceeded 85%. To offer a visual rep-\nresentation of the prediction accuracy, we randomly selected\none test set among 20 parallel experiments and showcased\n12 examples in Fig. 4. These visual results and statistics\ncorroborate that the LSTM models typically provide precise\npredictions of force profiles, capturing both peak values and\noverall traces accurately.\nTo sum up, the LSTM models demonstrated promising accu-\nracy in estimating impact speed, orientation and force profiles\nusing the Riddell helmet datasets (n=16,000), particularly with\nan extensive training set (n=12,800).\nC. Predicting Head Impact Location in on-field American\nFootball Dataset\nTo assess the real-world applicability of impact parameter\nprediction models, specifically their accuracy, we employed\nimpact retrieval models (LSTM) to predict the head impact\ninformation and then to identify the impact locations of on-\nfield American football head impacts in which the impact\nlocations can be clearly observed. The impact locations iden-\ntified by the models were compared with the ones observed\nfrom the video, which were used as references to evaluate\nthe ability of models. Furthermore, the methods developed\nin previous studies [22]\u2013[24] were also compared with the\nreferences. The outcomes, illustrated in Fig. 5 via confusion\nmatrices, highlight the superior identifying accuracy of our\nLSTM-based impact retrieval model. It successfully identified\n63 out of 79 impacts, achieving an accuracy rate of 79.7%,5\nA B C\nD E\nFig. 3. The performance of predicting force profiles based on head kinematics based on the LSTM model on the Riddell Helmet finite-element-simulation\ndatasets. The distribution of the performance metrics on the test set over 20 experiments with random training/validation/test dataset partitions were reported:\npointwise mean absolute error (MAE) (A), peak MAE (B), pointwise root mean squared error (RMSE) (C), peak RMSE (D), peak R2(E).\nthe highest among the evaluated methods. This performance\nsurpasses that of the revised opposite linear velocity and\nacceleration methods (successfully identified 29 out of 79\nimpacts, achieving an accuracy of 36.7%) proposed by Kuo\net al. [23], and that of the matching torque and force method\n(successfully identified 39 out of 79 impacts, achieving an\naccuracy of 49.4%) proposed by Bartsch et al. [24] These\nresults show that our impact retrieval models trained on\nthe finite element simulated impacts can be more accurate\nin retrieving the impact direction when compared with the\nexisting methods.\nIV. D ISCUSSION\nThe goal of this study is to develop a method to accurately\nreconstruct the head impact information including the impact\nspeed, location, direction, and impact force, which are es-\nsential to the development of helmet and head impact data\nanalysis. Currently, massive head impact kinematics data in\nsports has been collected by wearable inertial sensors, and\nthe head impact information was estimated by methods based\non mechanical simplifications and assumptions. However, es-\ntimating is difficult because the head impact process is com-\nplicated. First, when the impact mass contacts the helmet, the\nimpact is mitigated by the energy-absorbing system, and the\nimpact force is transmitted to the head through the interaction\nbetween the helmet and the head. This interaction includes\ncompressional force that is normalized to the head surface\nand the frictional force that is tangential to the head surface.\nTherefore, the direction of the impact force may change when\nit is transmitted to the head. Second, the head rotates because\nof the impact force and the constraint of the neck, and theinstantaneous center of head rotation varies in sagittal exten-\nsion, while it was fixed in the coronal flexion. The previous\nmethods to estimate the head impact information simplified the\nimpact process significantly [23], which inhibited them from\npredicting the impact speed and force. The impact location and\ndirection can be predicted, but the accuracy is not satisfying.\nTo overcome the difficulty brought by the complexity of the\nhead impact process, we developed a head impact retrieval\nmodel to predict the head impact information. Recurrent\nneural networks are particularly useful in modeling time series\nand signal-based data. LSTM, as a type of recurrent neural\nnetworks, was adopted considering its ability to learn the long-\nterm temporal dependencies relationship: the head kinematics\nare dependent on the whole impact process instead of the\ninstantaneous impact force. Additionally, different from the\nconventional feed-forward neural networks, the LSTM doesn\u2019t\nrequire feature engineering to extract relevant information\nfrom the time traces based on expert knowledge [27], [28],\nand it learns the feature representation in a completely data-\ndriven manner. To train the model , we generated a dataset\nusing FE model [26] because it can provide a large amount of\nhead impacts and the numerical simulation can provide all the\nhead impact information. The prediction of impact speed is the\nmost accurate, potentially because the impact speed is closely\nrelated to the magnitude of the head acceleration. R2of the\npredictions for \u03b2and Y are higher than 0.8, while R2of the\nprediction for \u03b1is 0.737. These show the ability of this impact\nretrieval model to reconstruct the head impact. However, the\nimpact retrieval model could not accurately predict Z ( R2\nof the prediction for Z is only 0.159). This may be owing\nto the relative position between the impact location and the6\nA B C\nD E F\nG H I\nJ K L\nFig. 4. The examples of force profile prediction are based on the LSTM model on the Riddell Helmet test set. The reference results given by finite-element\nsimulation and LSTM prediction were shown. The examples for force on the helmet (A-F), for force on the head/face (G-L) are selected based on one\nrandomly selected partition of training/validation/test partition among 20 repeated experiments. The pointwise MAE values were labeled in the subplot titles.\ninstantaneous head rotation axis. For different Y , the impact\nlocation varies from left to right of the rotation center, and\nthe head kinematics are in the opposite direction. However,\nfor different Z, the impact location is always higher than the\nrotation center, so the head kinematics are closer, which makes\npredicting Z from the kinematics more difficult. Besides the\nhead impact parameters, the retrieval model can accurately\npredict the impact force (Fig. 3). The MAE of both force on\nthe helmet and force on the head/face are smaller than 0.2 kN,\nand the R2for the peak force is close to 0.9.\nThe on-field American football head impact dataset wasused to test if the impact retrieval model can be translated\nto the real head impact. Based on the video, each head impact\nwas labeled by one of five different locations, which were\nused as the reference. Then, the impact location predicted\nby the retrieval model as well as previous methods were\ncompared with the reference. 79.7% of head impact locations\nwere correctly identified, which is significantly higher than\nthe previous methods. The second best is the matching force\nand torque method proposed by Bartsch et al in 2014 with\nan accuracy of 49.4%. We also found the revised opposite\nlinear acceleration gave the most accurate prediction than7\nA B C\nD\nE\n F[22] [23]\n[23][23] [24]\nFig. 5. The confusion matrices in impact direction identification on the video-verified on-field American football dataset from different models. The reference\nfor 79 impacts was given by two independent video reviewers. Orange: wrong predictions, blue: correct predictions, emphasized with dashed red boxes. The\nconfusion matrices of the impact retrieval model developed in this study (A) show a clear improvement in identification accuracy when compared against the\nexisting methods (B-F).\nlinear velocity and position, which agrees with the comparison\nreported by Kuo et al. [23]. The ratio of correct identification\nis 37.6% for the revised linear acceleration model, which\nis close to the ratio reported in the paper 37.3%. It should\nbe noted that most of the head impact was facemask head\nimpact, and the accuracies of other methods were limited\nbecause they mispredicted some of the facemask head impacts\nas side impact or back impact. Since the impact direction\nwas assumed as the direction of the linear acceleration of the\nhead, a potential explanation for why the previous method\ncannot accurately predict the facemask head impact is that the\ninteraction between the helmet and the head and the energy\nabsorbing system of the helmet change the direction of the\nimpact force.\nThis study has several limitations: First, the difference in\nthe helmet has not been adopted. Both in-silicon and on-\nfield datasets were collected with American Football helmets.\nThe energy absorption varies across different helmets, which\naffects the relationship between the head kinematics and\nthe head impact information. This difference may produce\nconsiderable errors when the current retrieval model is used\nwith other helmets. This limitation can be solved by adopting\na transfer learning technique, which requires a smaller dataset\nto tune the current retrieval model; Second, most of the head\nimpacts in the on-field data head impact dataset are facemask\nimpacts. Although this agrees with the distribution of the\nimpact location in American Football games, the numbers of\nother head impacts were small, therefore the ability of the\nretrieval model to detect other head impact locations is notwell tested. This can be solved with more head impact datasets\nin the future.\nV. C ONCLUSION\nThis study introduces a novel deep learning approach to\naccurately predict head impact parameters, including loca-\ntion, speed, orientation, and force, based on helmeted head\nkinematics. By utilizing a Long Short-Term Memory (LSTM)\nnetwork trained on a comprehensive dataset of 16,000 sim-\nulated helmeted head impacts, we have demonstrated the\nmodel\u2019s ability to achieve R2values exceeding 70% across\nall predictive tasks. Furthermore, validation against an on-field\ndataset of 79 recorded head impacts showcased the model\u2019s\nsuperior performance, with a remarkable accuracy of 79.7%\nin identifying impact locations, significantly outperforming\ntraditional methods. The findings underscore the significant\npotential of our model in enhancing helmet design and safety\nin sports by providing more precise and reliable impact data.\nVI. C ODE AND DATA AVAILABILITY\nThe code to develop this model and the simula-\ntion datasets are posted at: https://github.com/xzhan96-\nstf/impact retriever.git.\nVII. D ECLARATION OF INTEREST\nThe authors declare no conflicts of interest.8\nREFERENCES\n[1] A. I. Maas, D. K. Menon, G. T. Manley, M. Abrams, C. \u02daAkerlund,\nN. Andelic, M. Aries, T. Bashford, M. J. Bell, Y . G. Bodien et al. ,\n\u201cTraumatic brain injury: progress and challenges in prevention, clinical\ncare, and research,\u201d The Lancet Neurology , vol. 21, no. 11, pp. 1004\u2013\n1060, 2022.\n[2] E. O\u2019Keeffe, E. Kelly, Y . Liu, C. Giordano, E. Wallace, M. Hynes,\nS. Tiernan, A. Meagher, C. Greene, S. Hughes et al. , \u201cDynamic blood\u2013\nbrain barrier regulation in mild traumatic brain injury,\u201d Journal of\nneurotrauma , vol. 37, no. 2, pp. 347\u2013356, 2020.\n[3] M. Hajiaghamemar, T. Wu, M. B. Panzer, and S. S. Margulies, \u201cEm-\nbedded axonal fiber tracts improve finite element model predictions of\ntraumatic brain injury,\u201d Biomechanics and modeling in mechanobiology ,\nvol. 19, pp. 1109\u20131130, 2020.\n[4] X. Zhan, Y . Li, Y . Liu, A. G. Domel, H. V . Alizadeh, S. J. Raymond,\nJ. Ruan, S. Barbat, S. Tiernan, O. Gevaert et al. , \u201cThe relationship\nbetween brain injury criteria and brain strain across different types of\nhead impacts can be different,\u201d Journal of the Royal Society Interface ,\nvol. 18, no. 179, p. 20210260, 2021.\n[5] X. Zhan, Y . Li, Y . Liu, A. G. Domel, H. V . Alizadeh, Z. Zhou, N. J.\nCecchi, S. J. Raymond, S. Tiernan, J. Ruan et al. , \u201cPredictive factors\nof kinematics in traumatic brain injury from head impacts based on\nstatistical interpretation,\u201d Annals of biomedical engineering , vol. 49, pp.\n2901\u20132913, 2021.\n[6] X. Zhan, A. Oeur, Y . Liu, M. M. Zeineh, G. A. Grant, S. S. Margulies,\nand D. B. Camarillo, \u201cTranslational models of mild traumatic brain in-\njury tissue biomechanics,\u201d Current Opinion in Biomedical Engineering ,\nvol. 24, p. 100422, 2022.\n[7] F. Hernandez, C. Giordano, M. Goubran, S. Parivash, G. Grant,\nM. Zeineh, and D. Camarillo, \u201cLateral impacts correlate with falx cerebri\ndisplacement and corpus callosum trauma in sports-related concussions,\u201d\nBiomechanics and modeling in mechanobiology , vol. 18, pp. 631\u2013649,\n2019.\n[8] D. J. Lessley, R. W. Kent, J. M. Cormier, C. P. Sherwood, J. R. Funk,\nJ. R. Crandall, B. S. Myers, and K. B. Arbogast, \u201cPosition-specific\ncircumstances of concussions in the nfl: Toward the development of\nposition-specific helmets,\u201d Annals of biomedical engineering , vol. 48,\npp. 2542\u20132554, 2020.\n[9] S. Li, Z. Xiao, Y . Zhang, and Q. Li, \u201cImpact analysis of a honeycomb-\nfilled motorcycle helmet based on coupled head-helmet modelling,\u201d\nInternational Journal of Mechanical Sciences , vol. 199, p. 106406, 2021.\n[10] National Football League, \u201cHelmet test protocol,\u201d https:\n//www.nfl.com/playerhealthandsafety/equipment-and-innovation/\nequipment-testing/helmet-test-protocol, 2024, accessed on 5/10/2024.\n[11] N. J. Cecchi, A. G. Domel, Y . Liu, E. Rice, R. Lu, X. Zhan, Z. Zhou,\nS. J. Raymond, S. Sami, H. Singh et al. , \u201cIdentifying factors associated\nwith head impact kinematics and brain strain in high school american\nfootball via instrumented mouthguards,\u201d Annals of biomedical engineer-\ning, vol. 49, pp. 2814\u20132826, 2021.\n[12] A. J. Bartsch, D. Hedin, J. Alberts, E. C. Benzel, J. Cruickshank, R. S.\nGray, K. Cameron, M. N. Houston, T. Rooks, G. McGinty et al. , \u201cHigh\nenergy side and rear american football head impacts cause obvious\nperformance decrement on video,\u201d Annals of biomedical engineering ,\nvol. 48, pp. 2667\u20132677, 2020.\n[13] L. Zu, J. Wen, S. Wang, M. Zhang, W. Sun, B. Chen, and Z. L. Wang,\n\u201cMultiangle, self-powered sensor array for monitoring head impacts,\u201d\nScience Advances , vol. 9, no. 20, p. eadg5152, 2023.\n[14] E. J. Sanchez, L. F. Gabler, A. B. Good, J. R. Funk, J. R. Crandall,\nand M. B. Panzer, \u201cA reanalysis of football impact reconstructions for\nhead kinematics and finite element modeling,\u201d Clinical biomechanics ,\nvol. 64, pp. 82\u201389, 2019.\n[15] A. R. Harlos and S. Rowson, \u201cLaboratory reconstructions of real-world\nbicycle helmet impacts,\u201d Annals of biomedical engineering , vol. 49, pp.\n2827\u20132835, 2021.\n[16] \u2014\u2014, \u201cThe range of bicycle helmet performance at real world impact\nlocations,\u201d Proceedings of the Institution of Mechanical Engineers, Part\nP: Journal of Sports Engineering and Technology , vol. 237, no. 4, pp.\n233\u2013239, 2023.\n[17] M. L. Bland, C. McNally, J. B. Cicchino, D. S. Zuby, B. C. Mueller,\nM. L. McCarthy, C. D. Newgard, P. E. Kulie, B. N. Arnold, and\nS. Rowson, \u201cLaboratory reconstructions of bicycle helmet damage: in-\nvestigation of cyclist head impacts using oblique impacts and computed\ntomography,\u201d Annals of biomedical engineering , vol. 48, pp. 2783\u20132795,\n2020.[18] D. B. Camarillo, P. B. Shull, J. Mattson, R. Shultz, and D. Garza, \u201cAn\ninstrumented mouthguard for measuring linear and angular head impact\nkinematics in american football,\u201d Annals of biomedical engineering ,\nvol. 41, pp. 1939\u20131949, 2013.\n[19] Y . Liu, A. G. Domel, N. J. Cecchi, E. Rice, A. A. Callan, S. J. Raymond,\nZ. Zhou, X. Zhan, Y . Li, M. M. Zeineh et al. , \u201cTime window of head\nimpact kinematics measurement for calculation of brain strain and strain\nrate in american football,\u201d Annals of biomedical engineering , vol. 49,\nno. 10, pp. 2791\u20132804, 2021.\n[20] X. Zhan, Y . Liu, N. J. Cecchi, A. A. Callan, E. Le Flao, O. Gevaert,\nM. M. Zeineh, G. A. Grant, and D. B. Camarillo, \u201cAi-based denoising\nof head impact kinematics measurements with convolutional neural\nnetwork for traumatic brain injury prediction,\u201d IEEE Transactions on\nBiomedical Engineering , 2024.\n[21] K. L. O\u2019Connor, S. Rowson, S. M. Duma, and S. P. Broglio, \u201cHead-\nimpact\u2013measurement devices: a systematic review,\u201d Journal of athletic\ntraining , vol. 52, no. 3, pp. 206\u2013227, 2017.\n[22] S. M. Duma, S. J. Manoogian, W. R. Bussone, P. G. Brolinson, M. W.\nGoforth, J. J. Donnenwerth, R. M. Greenwald, J. J. Chu, and J. J. Crisco,\n\u201cAnalysis of real-time head accelerations in collegiate football players,\u201d\nClinical Journal of Sport Medicine , vol. 15, no. 1, pp. 3\u20138, 2005.\n[23] C. Kuo, L. Wu, J. Loza, D. Senif, S. C. Anderson, and D. B. Camarillo,\n\u201cComparison of video-based and sensor-based head impact exposure,\u201d\nPloS one , vol. 13, no. 6, p. e0199238, 2018.\n[24] A. Bartsch, S. Samorezov, E. Benzel, V . Miele, and D. Brett, \u201cValidation\nof an \u201cintelligent mouthguard\u201d single event head impact dosimeter,\u201d SAE\nTechnical Paper, Tech. Rep., 2014.\n[25] J. Sohail, F. Teixeira-Dias, and S. Merriman, \u201cHead impacts in non-\nhelmeted sports: Measuring and locating the impact force,\u201d Proceedings\nof the Institution of Mechanical Engineers, Part P: Journal of Sports\nEngineering and Technology , p. 17543371221130894, 2022.\n[26] S. S. Ahmadisoleymani, J. Yang, A. Schmit, and J. Rasty, \u201cSimulation-\nbased study of impact energy absorption in american football helmet,\u201d in\nInternational Design Engineering Technical Conferences and Computers\nand Information in Engineering Conference , vol. 50183. American\nSociety of Mechanical Engineers, 2016, p. V006T09A002.\n[27] X. Zhan, Y . Liu, S. J. Raymond, H. V . Alizadeh, A. G. Domel,\nO. Gevaert, M. M. Zeineh, G. A. Grant, and D. B. Camarillo, \u201cRapid\nestimation of entire brain strain using deep learning models,\u201d IEEE\nTransactions on Biomedical Engineering , vol. 68, no. 11, pp. 3424\u2013\n3434, 2021.\n[28] X. Zhan, Y . Liu, N. J. Cecchi, O. Gevaert, M."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "Certainly! Here's a detailed summary of the machine learning paper \"Open Source Infrastructure for Automatic Cell Segmentation\" that includes all the relevant details for a comprehensive understanding:\n\n---\n\n**1. Abstract:**\nThe paper presents an open-source infrastructure for automated cell segmentation using the UNet model, integrated into the DeepChem package. The implementation aims to facilitate the tasks of cell counting, morphology analysis, and drug discovery by providing a robust, user-friendly tool that maintains high accuracy and performs well across various imaging conditions and cell types. Benchmarks on multiple datasets demonstrate the model\u2019s robustness and versatility.\n\n---\n\n**2. Introduction:**\n\n- **Context of the Research:**\n  Precise cell segmentation is vital in biological and medical research for tasks like cell counting, morphology analysis, and drug discovery. Manual segmentation is labor-intensive, subjective, and error-prone, highlighting the need for automated solutions.\n\n- **Research Questions:**\n  The work seeks to develop a robust, user-friendly tool for automated cell segmentation that simplifies the use of advanced segmentation techniques and integrates seamlessly into existing workflows.\n\n- **Literature Review:**\n  The paper discusses the UNet model\u2019s emergence as a powerful tool for image segmentation and its use in various tasks. The introduction of DeepChem, an open-source Python library, sets the stage for integrating image-based data handling capabilities and addressing its limitations in handling such data.\n\n---\n\n**3. Related Work:**\n\n- The review discusses various manual and automated methods for cell segmentation, emphasizing the limitations of manual methods.\n- The UNet model is highlighted for its effectiveness in image segmentation tasks due to its deep learning capabilities.\n- DeepChem\u2019s role in scientific machine learning and its applications are elaborated. The paper builds upon DeepChem by enhancing its support for image-based data.\n- The contribution of the work lies in simplifying the application of the UNet model for cell segmentation and improving DeepChem's infrastructure to handle image data effectively.\n\n---\n\n**4. Method:**\n\n- **Proposed Approach:**\n  The approach involves integrating the UNet model into DeepChem for automated cell segmentation, enhancing the package\u2019s image-handling functionality, and providing a user-friendly interface.\n\n- **Mathematical Formulations:**\n  The paper does not delve deeply into the mathematical formulations but focuses on the architectural details of the UNet model:\n  - **UNet Architecture:**\n    - Symmetric encoder-decoder convolutional neural network.\n    - Contracting path with convolutional layers and max-pooling for feature extraction.\n    - Expansive path with upsampling and concatenation of feature maps via skip connections to preserve spatial information.\n  \n- **Novel Adaptations:**\n  The implementation pads images to maintain consistent spatial dimensions across all network layers, preventing information loss at the image borders.\n\n---\n\n**5. Experimental Setup:**\n\n- **Datasets Used:**\n  - BBBC003v1 (Mouse embryos, Differential Interference Contrast)\n  - BBBC039v1 (Human osteosarcoma U2OS cells, Fluorescence)\n  - Datasets from the ISBI Cell Tracking Challenge incorporating various microscopy techniques and cell types.\n  \n- **Hardware and Software:**\n  - Experiments were run on Google Colab Pro with an NVIDIA A100 GPU.\n  \n- **Evaluation Metrics:**\n  - Precision, Recall, F1 Score, Intersection over Union (IoU), and Area under ROC (AuROC).\n\n- **Baselines and Comparisons:**\n  - Comparisons were made against state-of-the-art models where available, as well as baseline PyTorch implementations.\n\n---\n\n**6. Results and Analysis:**\n\n- **Empirical Findings:**\n  - The model showed robust performance with high Precision, Recall, F1 Score, and AuROC across various datasets.\n  - Table 3: Evaluation on BBBC003 (Precision: 0.7624, Recall: 0.8263, F1: 0.7930, AuROC: 0.9888, mIoU: 0.6571)\n  - Table 4: Evaluation on Cell Tracking Challenge datasets, e.g., DIC-C2DH-HeLa (Precision: 0.8977, Recall: 0.7841, F1: 0.8371, AuROC: 0.9327, mIoU: 0.7198)\n\n- **Graphs, Tables, Figures:**\n  - Detailed tables compare performance metrics across datasets.\n  - Figures illustrate segmentation pipeline steps and example outputs with true segmentation masks.\n\n- **Analysis:**\n  - The UNet model handled diverse techniques and cell samples effectively, although variations in performance indicate areas for further improvement.\n\n---\n\n**7. Discussion:**\n\n- **Interpretation of Results:**\n  - The integration of the UNet model into DeepChem effectively meets the need for an automated, user-friendly cell segmentation tool.\n  - High performance across multiple metrics underscores the model\u2019s robustness and practical utility in diverse research contexts.\n\n- **Implications:**\n  - The tool accelerates workflows and enhances reproducibility in cell segmentation tasks, benefiting large-scale studies and clinical diagnostics.\n  \n- **Limitations:**\n  - Current evaluation is limited to a few datasets; expanding this scope could improve robustness validation.\n  - Future work could include integrating additional machine learning models and continuous improvements to pre-processing and training pipelines.\n\n---\n\n**8. Conclusion:**\n\n- **Summary of Contributions:**\n  - Development of an open-source, user-friendly cell segmentation tool by integrating the UNet model into DeepChem.\n  - Enhancement of DeepChem\u2019s image-handling capabilities and provision of pre-configured pipelines for ease of use.\n  \n- **Future Research Directions:**\n  - Expanding dataset evaluations to include more diverse examples.\n  - Investigating the integration of other machine learning models to handle different biological data types.\n  - Ongoing improvements in pre-processing and training pipelines for better segmentation accuracy.\n\n---\n\nReferences:\n1. Various sources providing definitions, previous research, and benchmarks.\n2. Key details and performances of algorithms used in the study.\n3. Essential tools and libraries (DeepChem, PyTorch). \n\nThis summary ensures a comprehensive understanding, enabling researchers to replicate the study or build directly upon this work. The detailed approach from methodology to results provides a clear blueprint for similar implementations or extensions in the domain of automated cell segmentation.\n\n---"}, {"role": "assistant", "content": "Open Source Infrastructure for Automatic Cell Segmentation\nAaron Rock Menezes1,2,*, Bharath Ramsundar2,**,\n1Department of Biological Sciences, BITS Pilani, K.K. Birla Goa Campus\n2Deep Forest Sciences\n* aaron.r.menezes@gmail.com\n** bharath@deepforestsci.com\nAbstract\nAutomated cell segmentation is crucial for various biological and medical applications,\nfacilitating tasks like cell counting, morphology analysis, and drug discovery. However,\nmanual segmentation is time-consuming and prone to subjectivity, necessitating robust\nautomated methods. This paper presents open-source infrastructure, utilizing the UNet\nmodel, a deep-learning architecture noted for its effectiveness in image segmentation\ntasks. This implementation is integrated into the open-source DeepChem package,\nenhancing accessibility and usability for researchers and practitioners. The resulting\ntool offers a convenient and user-friendly interface, reducing the barrier to entry for cell\nsegmentation while maintaining high accuracy. Additionally, we benchmark this model\nagainst various datasets, demonstrating its robustness and versatility across different\nimaging conditions and cell types.\nIntroduction\nIn biological and medical research, achieving precise cell segmentation is essential for\ntasks like cell counting, morphology analysis, and drug discovery [1]. However, manual\nsegmentation methods are laborious, subjective, and prone to errors, especially when\nworking with low-contrast microscopy techniques like bright field microscopy or with\nlow-resolution imagery. These challenges prompt the need for robust automated\nsolutions for cell segmentation. In recent years, the UNet model has emerged as a\npromising tool for image segmentation tasks, leveraging deep learning techniques to\nlearn effective segmentations [2]. Implementing and training such a model can be\ndaunting for users who aren\u2019t familiar with programming. Our work simplifies this\nprocess with a modular implementation that shields users from technical complexities,\nwhilst enabling easy and convenient application of advanced image segmentation\ntechniques. Our aim is to improve ease-of-use of automated cell segmentation, in order\nto help advance research across various scientific domains. Through evaluations on\nopen-source microscopy datasets, we compare our UNet model on various open-source\nmicroscopy datasets and present a comparison with the current state-of-the-art results.\nDeepChem, [3], is an open-source Python library aimed at scientific machine\nlearning and deep learning, focusing on molecular and quantum datasets. DeepChem\u2019s\nstructure empowers the tackling of complex scientific challenges in areas such as drug\ndiscovery, bioinformatics, and physics. The organized framework of DeepChem has\nenabled it to be used for applications from molecular machine learning assessments\nusing the MoleculeNet benchmark suite [4] to protein-ligand interaction modeling [5],\nand generative modeling of molecules [6], among others. DeepChem currently has\nSeptember 13, 2024 1/8arXiv:2409.08163v1  [cs.LG]  12 Sep 2024limited support for image-based data. This work addresses this shortcoming by\nimproving DeepChem\u2019s image-handling functionality and leverages this improved\nfunctionality to build effective cell segmentation tools.\nIn particular, this work integrates the UNet model in addition to tutorials for\nautomated cell counting and segmentation to enable users to analyze image data, e.g.\nmicroscopy or biomedical imagery. In addition to the integration of the UNet model, we\nhave also improved support for image-based datasets by improving the implementation\nof the ImageLoader ,ImageDataset andImageTransformer classes in DeepChem to\nfacilitate the loading and pre-processing of image data.\nImplementation\nWe have built a full pipeline for analyzing microscopy images using DeepChem. This\npipeline consists of the following parts: loading microscopy datasets using DeepChem\u2019s\nImageLoader class, creating an ImageDataset and pre-processing the data, and then\nusing the UNet model to train a model on the dataset.\nMicroscopy Datasets\nThere are many open-source microscopy datasets such as LIVECell [7], the Broad\nBioimage Benchmark Collection [8], the ISBI Cell Tracking Challenge [9]. To evaluate\nthe generalizability of our model, we utilize a diverse dataset collection encompassing\nvarious microscopy techniques and biological subjects. This includes the publicly\navailable BBBC003v1 and BBBC039v1 datasets from the Broad Bioimage Benchmark\nCollection [8], featuring DIC microscopy of Mouse embryos and fluorescence microscopy\nof the U2OS cell line of human osteosarcoma cells. We further leverage the open source\ndatasets from the ISBI Cell Tracking Challenge [9], incorporating a wider range of\nmicroscopy methods (phase contrast, fluorescence, and differential interference contrast\nmicroscopy) and cell types (mouse stem cells, HeLa cells, pancreatic stem cells, etc.).\nThis selection allows us to test our model\u2019s performance on distinct cell morphologies\nand imaging conditions, fostering a comprehensive assessment of its robustness.\nTable 1. Summary of the datasets used in our benchmark experiments, with the types of cells imaged and\nmicroscopy techniques used to capture the images.\nDataset Cell Type Microscopy Technique\nBBBC003 Mouse embryos Differential Interference Contrast\nBBBC039 Huma osteosarcoma U2OS cells Fluorescence\nDIC-C2DH-HeLa HeLa cells Differential Interference Contrast\nFluo-C2DL-MSC Rat mesenchymal stem cells Fluorescence\nFluo-N2DH-GOWT1 GFP-GOWT1 Mouse stem cells Fluorescence\nFluo-N2DH-HeLa HeLa cells Fluorescence\nFluo-N2DH-SIM Human Leukemia HL60 cells Fluorescence\nPhC-C2DH-U373 Glioblastoma-astrocytoma U373 cells Phase Contrast\nPhC-C2DL-PSC Pancreatic stem cells Phase Contrast\nImage Loaders, Datasets, and Pre-Processing\nImage Loaders and Image Datasets are an integral part of the DeepChem package and\nallow scientists to easily access image datasets to visualize and process their data. The\nImageLoader class can be used to load data from folders and create ImageDatasets ,\nSeptember 13, 2024 2/8which can be used to access, analyze, and pre-process image datasets as well as for\ntraining and testing models. We improved the ImageDatasets to be able to work with\nimages as both inputs and labels, a functionality that was missing prior to this work.\nThis enables practitioners and researchers to conduct a wide array of computer vision\nbased experiments.\nDeepChem also has several Transformers which allow users to process data in\nvarious ways, such as using the NormalizationTransformer for normalization of data or\ntheImageTransformer for resizing of images.\nUNet Model\nThe UNet model architecture is structured as a symmetric encoder-decoder\nconvolutional neural network. At its core, it comprises a contracting path, where each\nlayer progressively downsamples the input image\u2019s spatial dimensions while increasing\nthe number of feature channels. This contracting path consists of convolution layers\nfollowed by max-pooling operations, enabling the extraction of high-level features.\nFig 1. Block diagram of the UNet model architecture. The numbers show the number\nof channels in the image, we can see that our input and output are both 3-channel\nimages.\nTo facilitate information flow between corresponding encoder and decoder layers,\nskip connections are employed, which concatenate feature maps from the contracting\npath with those from the expansive path, aiding in the preservation of spatial\ninformation and enabling precise segmentation [10,11]. Our UNet model, unlike the\noriginal implementation, pads all images to ensure spatial dimensions are consistent\nacross all layers of the network to prevent any loss of information at the image borders.\nOur implementation allows users to select the number of input and output channels in\naddition to the optimizer, loss function, and the learning rate they wish to use.\nIncorporating the UNet model into DeepChem involved utilizing PyTorch [12] as the\nbackend framework. We integrated the UNet model into DeepChem, enabling\nresearchers to access and utilize this tool for automated cell segmentation and other\nrelated tasks.\nSeptember 13, 2024 3/8Cell Segmentation Pipeline\nLoading and pre-processing of data, training of the model and evaluation and inference\nof the model follows a simple pipeline, which can be achieved in a few lines of Python\ncode. We describe the pipeline in Figure 1, which involves downloading and loading the\ndata as an ImageDataset using an ImageLoader , preprocessing using the various\nTransformers followed by training and evaluating the UNet.\nFig 2. Overview of the cell segmentation pipeline using DeepChem. The pipeline\nincludes data loading, pre-processing, model training, evaluation, and inference.\nUsers can download and process their data, and train models on it using just a few\nlines of code without having to refer to various packages or guides.\nFig 3. This is the DeepChem implementation of the Image Segmentation Pipeline as\nseen in Fig. 2.\nTable 2. Comparison of the lines of code written to implement our pipeline using\nDeepChem and using only PyTorch [13].\nDeepChem PyTorch\nLines of Code 9 250-300\nSeptember 13, 2024 4/8Results\nExperimental Setup\nWe test the UNet model on various open-source datasets. All code was run using\nGoogle Colab Pro, on 1 NVIDIA A100 GPU. We report results averaged across 5 runs\nof the datasets mentioned above. For preprocessing the data, we normalize the pixel\nintensities of all the input images and ensure that the segmentation masks were binary\nin nature. All images are scaled down to a height of 256 and the width was scaled in\nproportion to the image\u2019s original aspect ratio to the nearest multiple of 16. We use a\nbatch size of 2 and train the models for a total of 100 epochs. All models are trained\nusing Binary Cross Entropy Loss and the Adam optimizer with a learning rate of 10\u22124.\nThe models are evaluated using Intersection over Union (IoU), the F1 Score and the\nArea under ROC (AuROC) as metrics.\nExperimental Results\nModels trained on the BBBC003 and BBBC0039 datasets from the Broad Bioimage\nBenchmark Collection show good performance using an 80-20 train-test split. These\ndatasets were fairly small (16-200 images) compared to the Cell Tracking Challenge\u2019s\ndatasets.\nTable 3. Evaluation performance of the DeepChem UNet Model on BBBC003 and\nBBBC039.\nDataset Precision Recall F1 Score AuROC mIoU\nBBBC003 0.7624 0.8263 0.7930 0.9888 0.6571\nBBBC039 0.9086 0.9902 0.9477 0.9989 0.9006\nWe also trained the model on a subset of the Cell Tracking Challenge\u2019s datasets.\nEach dataset contains 2 sets of images of cells taken over a duration of time at regular\nintervals. As these datasets contain images of cells over a fixed duration of time, we\ntreated each frame as an independent image and randomly split the data for each\nsequence. We trained and tested the model on the 2 different sequences for each dataset\ndue to the unavailability of the test set\u2019s labels.\nTable 4. Evaluation performance of the DeepChem UNet Model on datasets from the Cell Tracking Challenge 2020. Each\nCell Tracking dataset has 2 captured sequences of images. We trained models on sequence 1 and tested on sequence 2 for each\ndataset, as the test set is not public. SOTA numbers are from the test set and are not directly comparable with our\nbenchmarks but serve as a useful comparison point.\nDataset Precision Recall F1 Score AuROC mIoU SOTA mIoU [14]\nDIC-C2DH-HeLa 0.8977 0.7841 0.8371 0.9327 0.7198 0.877\nFluo-C2DL-MSC 0.8375 0.8059 0.8214 0.9698 0.6969 0.687\nFluo-N2DH-GOWT1 0.9742 0.8970 0.9340 0.9943 0.8762 0.938\nFluo-N2DH-HeLa 0.9476 0.9451 0.9463 0.9932 0.8982 0.923\nFluo-N2DH-SIM 0.8709 0.5915 0.7045 0.9395 0.5438 0.832\nPhC-C2DH-U373 0.8854 0.9259 0.9051 0.9971 0.8267 0.931\nPhC-C2DL-PSC 0.9393 0.8468 0.8906 0.9960 0.8028 0.756\nSeptember 13, 2024 5/8Discussion\nThe addition of the UNet model to DeepChem, in addition to the improvement in the\nImageLoader ,ImageDataset and the ImageTransformer , come together to form the Cell\nsegmentation pipeline, which is easy and intuitive to use. Our benchmarking\nexperiments show that the model performs fairly well when compared to the best results\non several datasets but still leaves room for improvement. Importantly, the model\nhandled the diverse microscopy techniques and cell samples within the datasets\neffectively.\nFig 4. The above image compares the UNet model\u2019s predictions with the true\nsegmentation mask. We\u2019ve compared 1 random sample from the Fluo-N2DH-GOWT1,\nFluo-C2DL-MSC and PhC-C2DL-PSC datasets each.\nPerformance and Accuracy\nThe UNet model demonstrated robust performance across multiple open-source\nmicroscopy datasets. The high F1 Score and Jaccard Index (IoU) achieved in\nsegmentation tasks affirm the model\u2019s capability to handle diverse microscopy\ntechniques and biological subjects. This versatility is critical, as it ensures the model\u2019s\nSeptember 13, 2024 6/8applicability across different research domains and imaging modalities, from\nfluorescence microscopy to DIC microscopy.\nIntegration with DeepChem and Practical Implications\nIncorporating the UNet model into the DeepChem ecosystem expands the utility of this\npowerful open-source library. The tutorials and pre-configured pipelines provided within\nDeepChem facilitate ease of use, making advanced cell segmentation techniques\naccessible even to researchers with limited expertise in deep learning or image\nprocessing. The convenience and ease of use provided by the segmentation pipeline\nmakes it relatively easy to be able to perform segmentation tasks when compared to\nother frameworks like PyTorch [12] or TensorFlow [15], both of which require extensive\nknowledge of deep learning to be used effectively.\nOne of the primary advantages of our approach is the significant reduction in time\nand effort required for cell segmentation. Traditional manual segmentation is not only\nlabor-intensive but also subject to human error and variability. By helping to automate\nthis process, our pipeline not only accelerates the workflow but also enhances\nreproducibility and consistency in segmentation outcomes. This improvement is\nparticularly beneficial for large-scale studies where manual segmentation would be\nimpractical.\nIn drug discovery, precise cell segmentation can enhance the accuracy of cell counting\nand morphology analysis, leading to a better understanding of drug effects. In clinical\nsettings, automated segmentation of medical images such as MRIs or X-rays can aid in\ndiagnostics and treatment planning, potentially improving patient outcomes [16,17].\nLimitations and Future Work\nThere are several areas for future improvement. Our current implementation and\nevaluation are limited to a few datasets; expanding this to include a wider variety of\ndatasets could further validate the model\u2019s robustness. Training models on large\ndatasets could help generalize predictions.\nFuture work could also explore the integration of additional machine learning models\nwithin DeepChem to handle other types of biological data, further enhancing its\nversatility and applicability. Moreover, continuous updates and improvements to the\npre-processing and training pipelines could yield even better segmentation accuracy.\nConclusion\nIn conclusion, our work demonstrates a significant step forward in the field of\nautomated cell segmentation. By leveraging the power of the UNet model and\nintegrating it into the user-friendly DeepChem framework, we provide a valuable tool\nfor the scientific community. This advancement not only streamlines cell segmentation\ntasks but also opens up new possibilities for research and application in various\nbiological and medical fields. The success of this approach highlights the potential of\ncombining deep learning techniques with open-source scientific tools to drive innovation\nand efficiency in research workflows.\nReferences\n1. Bengtsson E, Wahlby C, Lindblad J. Robust cell image segmentation methods.\nPattern Recognition and Image Analysis c/c of Raspoznavaniye Obrazov I Analiz\nIzobrazhenii. 2004;14(2):157\u2013167.\nSeptember 13, 2024 7/82. Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for\nBiomedical Image Segmentation; 2015.\n3. Ramsundar B, Eastman P, Walters P, Pande V, Leswing K, Wu Z. Deep\nLearning for the Life Sciences. O\u2019Reilly Media; 2019.\n4. Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, et al..\nMoleculeNet: A Benchmark for Molecular Machine Learning; 2018.\n5. Gomes J, Ramsundar B, Feinberg EN, Pande VS. Atomic Convolutional\nNetworks for Predicting Protein-Ligand Binding Affinity; 2017.\n6. Frey NC, Gadepally V, Ramsundar B. FastFlows: Flow-Based Models for\nMolecular Graph Generation; 2022.\n7. Edlund C, Jackson TR, Khalid N, Bevan N, Dale T, Dengel A, et al.\nLIVECell\u2014A large-scale dataset for label-free live cell segmentation. Nature\nmethods. 2021;18(9):1038\u20131045.\n8. Ljosa V, Sokolnicki KL, Carpenter AE. Annotated high-throughput microscopy\nimage sets for validation. Nature methods. 2012;9(7):637\u2013637.\n9. Ma\u02c7 ska M, Ulman V, Delgado-Rodriguez P, G\u00b4 omez-de Mariscal E, Ne\u02c7 casov\u00b4 a T,\nGuerrero Pe\u02dc na FA, et al. The Cell Tracking Challenge: 10 years of objective\nbenchmarking. Nature Methods. 2023;20(7):1010\u20131020.\n10. Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE conference on computer vision and\npattern recognition; 2015. p. 3431\u20133440.\n11. Drozdzal M, Vorontsov E, Chartrand G, Kadoury S, Pal C. The importance of\nskip connections in biomedical image segmentation. In: International workshop\non deep learning in medical image analysis, international workshop on large-scale\nannotation of biomedical data and expert label synthesis. Springer; 2016. p.\n179\u2013187.\n12.Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. Pytorch: An\nimperative style, high-performance deep learning library. Advances in neural\ninformation processing systems. 2019;32.\n13. Buda M, Saha A, Mazurowski MA. Association of genomic subtypes of\nlower-grade gliomas with shape features automatically extracted by a deep\nlearning algorithm. Computers in Biology and Medicine. 2019;109.\ndoi:10.1016/j.compbiomed.2019.05.002.\n14. Cell Segmentation Benchmark;. Available from:\nhttps://celltrackingchallenge.net/latest-csb-results/ .\n15.Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al.. TensorFlow:\nLarge-Scale Machine Learning on Heterogeneous Systems; 2015. Available from:\nhttps://www.tensorflow.org/ .\n16. Starkuviene V, Pepperkok R. The potential of high-content high-throughput\nmicroscopy in drug discovery. British journal of pharmacology. 2007;152(1):62\u201371.\n17. Wang J, Zhu H, Wang SH, Zhang YD. A review of deep learning on medical\nimage analysis. Mobile Networks and Applications. 2021;26(1):351\u2013380.\nSeptember 13, 2024 8/8"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary of the Paper: \"On the Role of Context in Reading Time Prediction\"\n\n#### 1. Abstract\n\nThe paper presents a novel perspective on the integration of context during real-time language comprehension, building on surprisal theory. It introduces pointwise mutual information (PMI) as an alternative contextual predictor, which yields the same predictive power as surprisal when controlling for unigram frequency. The authors demonstrate that both PMI and surprisal are correlated with frequency and do not contain information about context alone. They propose a technique to orthogonalize surprisal relative to frequency, creating a new predictor that is decorrelated from frequency. The experiments show that the explained variance in reading times due to context alone is much smaller than previously estimated, suggesting an overestimation of the role of context in past studies.\n\n#### 2. Introduction\n\n**Context of Research:**\nSurprisal theory suggests that the cognitive processing effort of a linguistic unit in a given context is an affine function of its surprisal. This theory has garnered empirical support across various reading measurement methodologies and languages.\n\n**Research Questions/Hypotheses:**\n- Is surprisal the best predictor of context in reading time prediction?\n- Does context have an overestimated effect on reading times due to its correlation with frequency?\n\n**Literature Review and Positioning:**\nPrevious studies often use surprisal as the primary contextual predictor in language models, showing consistent effects on reading times. However, the dependency of surprisal and PMI on frequency has been underexplored. This study builds on prior works but challenges the special status of surprisal, proposing an alternative approach to isolate the effect of context from frequency.\n\n#### 3. Related Work\n\nThe paper reviews past literature on surprisal theory, empirical findings supporting it, and the role of frequency in language processing. It also references studies utilizing pointwise mutual information (PMI) and residualization methods to control for correlated variables. This paper advances the field by explicitly orthogonalizing contextual predictors from frequency to isolate their unique contributions.\n\n#### 4. Method\n\n**Detailed Description of the Proposed Approach:**\n- **Surprisal Theory:** The paper acknowledges surprisal (Hale, 2001; Levy, 2008) as the standard for predicting linguistic processing efforts.\n- **PMI as an Alternative Predictor:** PMI between a linguistic unit and its context is proposed as another predictor, linked linearly with frequency and surprisal.\n- **Orthogonalization Technique:** The authors propose a method to orthogonalize surprisal with respect to frequency. Mathematically, they project surprisal onto the orthogonal complement of frequency, ensuring zero correlation.\n\n**Mathematical Formulations and Algorithms:**\n- **Surprisal \\( \u03b9_H(c,u) \\):** Defined as the negative log probability of a unit given its context.\n- **PMI \\( \u03bc_H(c,u) \\):** Defined as the log ratio between context-conditioned probability and unigram probability.\n- **Orthogonalized Surprisal:** Computed by projecting surprisal onto the orthogonal complement of frequency.\n\nFormulas:\n\\[ \u03b9_H(c,u) = -\\log p_H(u|c) \\]\n\\[ \u03bc_H(c,u) = \\log \\frac{p_H(u|c)}{q_H(u)} \\]\n\\[ \\text{proj}_{Y^{\\perp}_H}(I_H) = I_H - \\frac{\\langle I_H, Y_H \\rangle}{\\langle Y_H, Y_H \\rangle} Y_H \\]\n\n**Architectural Details:**\n- **Language Models (LM):** Surprisal and PMI are derived using a language model \\( p_H \\) which follows a normalized prefix probability distribution.\n\n**Novel Techniques/Adaptations:**\nThe orthogonalization methodology is a key innovation, isolating the pure contextual effect by decorrelating surprisal from frequency.\n\n#### 5. Experimental Setup\n\n**Datasets:**\n- **Multilingual Eye-movement Corpus (MECO):** This dataset includes word-level eye-tracking measurements across several languages.\n\n**Hardware and Software:**\n- **Language Model:** mGPT, a GPT-2 based multilingual model.\n- **Frequency Data:** Derived from Speer's (2022) word frequency estimates.\n- **Software:** Standard statistical and machine learning libraries for data processing and regression analysis.\n\n**Evaluation Metrics:**\n- **Proportion of Explained Variance (LMG):** This metric averages the explained variance across all possible orderings of predictors, providing a comprehensive measure of each predictor's contribution.\n\n**Baselines and Comparison Methods:**\n- Models comparing surprisal, PMI, and orthogonalized surprisal along with control variables: frequency and word length.\n\n#### 6. Results and Analysis\n\n**Empirical Findings:**\n- **Explained Variance:** The explained variance dropped significantly when surprisal was orthogonalized, indicating that context alone plays a smaller role in reading times than previously estimated.\n- **Comparison Across Models:** Models incorporating PMI showed slightly less overestimation of the contextual effect compared to surprisal.\n\n**Detailed Descriptions of Figures:**\n- **Figure 1:** Visual representation of explained variance in reading times across languages and predictors. Error bars denote 95% confidence intervals.\n\n**Ablation Studies:**\n- The role of previous word frequency and length were analyzed to separate their independent effects from the main predictors.\n\n**Unexpected/Anomalous Results:**\n- Unique results were observed for Korean, possibly due to its distinct script (Hangul).\n\n#### 7. Discussion\n\n**Interpretation of Results:**\n- The findings suggest that traditional surprisal-based models overestimate the impact of context in influencing reading times.\n- PMI offers a middle ground but is still not entirely free from frequency effects.\n\n**Implications:**\nThe results advocate for more nuanced models that account for the collinearity between predictors to obtain more accurate estimates of contextual effects.\n\n**Limitations:**\n- Potential bias towards Indo-European languages in the dataset.\n- The need for more efficient orthogonalization techniques.\n- The challenge of high correlation between word length and frequency.\n\n#### 8. Conclusion\n\n**Key Contributions:**\n- Introduction of PMI as an alternative contextual predictor.\n- Orthogonalization technique to isolate contextual effects from non-contextual frequency.\n- Empirical evidence suggesting previous studies overestimate the role of context in reading times.\n\n**Future Research Directions:**\n- Extension to a wider variety of languages.\n- Refinement of orthogonalization methods.\n- Investigation of additional predictors such as intra-word saccades.\n\nThis comprehensive summary encapsulates the detailed methodology, empirical findings, and potential limitations of the paper. Equipped with this information, a researcher should be able to replicate the study or build upon this work effectively."}, {"role": "assistant", "content": "On the Role of Context in Reading Time Prediction\nAndreas Opedal1,2Eleanor Chodroff3Ryan Cotterell1Ethan Gotlieb Wilcox1\n1ETH Z\u00fcrich2Max Planck ETH Center for Learning Systems3University of Z\u00fcrich\n{andreas.opedal,ryan.cotterell,ethan.wilcox}@inf.ethz.ch\neleanor.chodroff@uzh.ch\nAbstract\nWe present a new perspective on how readers in-\ntegrate context during real-time language com-\nprehension. Our proposals build on surprisal\ntheory, which posits that the processing effort\nof a linguistic unit (e.g., a word) is an affine\nfunction of its in-context information content.\nWe first observe that surprisal is only one out of\nmany potential ways that a contextual predictor\ncan be derived from a language model. Another\none is the pointwise mutual information (PMI)\nbetween a unit and its context, which turns out\nto yield the same predictive power as surprisal\nwhen controlling for unigram frequency. More-\nover, both PMI and surprisal are correlated with\nfrequency. This means that neither PMI nor sur-\nprisal contains information about context alone.\nIn response to this, we propose a technique\nwhere we project surprisal onto the orthogo-\nnal complement of frequency, yielding a new\ncontextual predictor that is uncorrelated with\nfrequency. Our experiments show that the pro-\nportion of variance in reading times explained\nby context is a lot smaller when context is rep-\nresented by the orthogonalized predictor. From\nan interpretability standpoint, this indicates that\nprevious studies may have overstated the role\nthat context has in predicting reading times.\nhttps://github.com/rycolab/\ncontext-reading-time\n1 Introduction\nSurprisal theory (Hale, 2001; Levy, 2008) posits\nthat the amount of effort it takes to process a\nlinguistic unit is an affine function of its in-context\ninformation content, i.e., its surprisal. Numerous\nstudies have found empirical support for surprisal\ntheory across different reading measurement\nmethods, languages, and language models (Smith\nand Levy, 2013; Wilcox et al., 2020; Kuribayashi\net al., 2021; Meister et al., 2021; Wilcox et al.,\n2023; Shain et al., 2024), particularly when\ncontrolling for additional effects such as frequency.\nIn this work, we take a critical look at surprisal\ntheory as an adequate explanation for the role ofcontext in reading time prediction, starting from\na simple observation: Surprisal is but one quantity\nthat can be derived from a language model to\nrepresent the effect of context (Giulianelli et al.,\n2024). We first show that, as an alternative to\nsurprisal, one could take an association-based view\non real-time language comprehension and model it\nas a function of the pointwise mutual information\n(PMI) between a unit and its context. Because\nPMI, surprisal, and frequency are collinear, all\nlinear models with just two of these covariates\nare equivalent in terms of their predictive power.\nThis simple identity therefore implies that all\nempirical validation of surprisal theory based on\nlinear regression modeling also lends support for\nassociation-based theories of language processing.\nThis raises the question of whether there is\na more suitable way to estimate the effect that\ncontext has on reading time. We argue that, given\nthat frequency is known to play an important role\nin processing effort (Broadbent, 1967; Inhoff and\nRayner, 1986; Rayner and Duffy, 1986; Bybee,\n2006), a more interesting construct to analyze\nshould be what context contributes beyond what\nis already captured by frequency. To obtain a\npredictor that represents just that, we propose\na technique where we project surprisal onto the\northogonal complement of frequency, ensuring\nthat they are uncorrelated. This process effectively\ndisentangles the contextual and non-contextual\ninformation into different covariates in our\nregressions and closely resembles residualization.1\nTo test whether the choice of contextual predic-\ntor matters empirically, we measure how much the\nvariance in reading times explained by the contex-\ntual predictor changes when substituting surprisal\nfor the orthogonalized context predictor. We find\nthat our proposed predictor results in much smaller\nexplained variance. Our results suggest that em-\npirical work on surprisal theory has overestimated\nthe effect that context has on reading times.\n1See App. D for more discussion on residualization.\n1arXiv:2409.08160v1  [cs.CL]  12 Sep 20242 Predictive Models of Reading Behavior\nWe seek to model the cognitive processing diffi-\nculty of a unitu, e.g., a word, drawn from an al-\nphabet \u03a3. Additionally, we augment \u03a3to include\na unique EOS\u0338\u2208\u03a3symbol which indicates the end\nof an utterance; we further define \u03a3def= \u03a3\u222a {EOS}.\nLet\u03a3\u2217be the set of all strings over the alphabet \u03a3;\nwe write u\u2208\u03a3\u2217for a string, utfor the tthunit in\nu,|u|for the number of units in u, anduvfor the\nconcatenation of uwith another string v. Given a\nstring cu, we are interested in how u\u2019s processing\neffort is shaped by its context of preceding units c.\nA common psychometric proxy for the cognitive\nprocessing difficulty of uis the time it takes a\nhuman to read u, typically, as measured in an eye-\ntracking study (Rayner, 1998). In general terms,\nwe are interested in empirically assessing some\ntheory of cognitive processing difficulty, which can\nbe thought of as a collection of unit-level properties\nthat are implicated in determining processing\neffort as measured by reading times. The most\ncommon type of evidence adduced to support such\ntheories comes from (generalized) linear modeling.\nWe define a predictor function as a function\nof type X: \u03a3\u2217\u00d7\u03a3\u2192RD, i.e., a function that\nmaps a context\u2013unit pair to a D-dimensional real\nvector. We model the reading time measurements\nas a linear model f\u03b2conditioned on X(c,u), i.e.,\nr(c,u)\u223cf\u03b2(\u00b7 |X(c,u))where \u03b2\u2208RDis a\nreal-valued parameter vector. A model whose\nexpected value, br(c,u) =\u03b2\u22baX(c,u), achieves\nhigh likelihood on held-out data lends empirical\nsupport to the theory that the factors measured by\nthe predictors in Xunderlie the process of reading.\n2.1 Language Modeling Background\nWe are particularly interested in predictors that are\nderived from language models (LMs) pH, which\nare distributions over \u03a3\u2217.2A relevant construct\nis the probability that a string in \u03a3\u2217has a certain\nprefix c, called normalized prefix probability :\n\u03c0H(c) =1\nZ\u03c0HX\nu\u2208\u03a3\u2217pH(cu), (1)\nwhere the normalizing constant Z\u03c0His\nZ\u03c0H= 1 +X\nu\u2208\u03a3\u2217pH(u)|u| (2)\nSee Prop. 1 in App. A for a proof of Eq. (2). In\nwords, this identity says that the normalized prefix\n2We use \u00b7Hto suggest that pHis the human LM.probability exists when the expected string length\nis finite, which is the case for transformer-based\nLMs (Du et al., 2023). For simplicity, we further\nassume that pH(u)>0for all u\u2208\u03a3\u2217. This\nassumption holds true in practice due to the\nsoftmax function (Boltzmann, 1868; Gibbs, 1902),\nwhich enforces the probability estimates to be\nstrictly positive. Then, for all u\u2208\u03a3,\npH(u|c)def=\u03c0H(cu)\n\u03c0H(c). (3)\nThe EOS symbol is special in the sense that\npH(EOS|c)def=pH(c)\n\u03c0H(c). (4)\nThus, pH(u|c)is a probability distribution over\n\u03a3. Importantly, note that pH(u|c)isnotthe\nprobability of cuas an entire string given that we\nknowc, only that ufollows c.\n2.2 Frequency as a Predictor of Reading Time\nPrevious studies (e.g., Shain, 2019, 2024) have\ninvestigated the effect of frequency, operational-\nized as unigram surprisal , on reading time. A\nunigram LM qHis a distribution over \u03a3\u2217where,\nwhen a string is sampled autoregressively, each\nunit is conditionally independent of the context. In\nnotation, we write qH(u)for the probability of u\nindependent of context.\nWe now consider the unigram model that best\napproximates the human LM pHin the sense of the\nforward Kullback\u2013Leibler divergence KL(pH||q).\nWe can compute the minimizer qHin closed form.\nWe define the following function that counts the\nnumber of occurrences of a unit u\u2208\u03a3inu:\n#(u,u)def=|u|X\nt=11{u=ut}+ 1{u=EOS}.(5)\nThen, the minimizing unigram LM, factored\nautoregressively, is given by\nqH(u) =1\nZqHX\nu\u2208\u03a3\u2217pH(u)#(u,u), (6)\nwhere the normalizing constant ZqHis necessarily\nfinite for language models of finite expected\nlength.3Then, given an LM pHwith a unigram\n3As a counterexample, consider an LM pHwith \u03a3 =\n{a}andpH(an) =1\n\u03c02/6\u00b71\nn2forn\u2208Z>0, i.e., where the\nprobabilities are globally normalized by\u03c02\n6, the solution to\nthe Basel problem. The expected count of awould depend onP\u221e\nn=11\nn2\u00b7n, which is divergent. Thus, ZqH=\u221e.\n2LMqH, the unigram surprisal is given by\n\u03c5H(u)def=\u2212logqH(u). (7)\nWe will refer to unigram surprisal as frequency for\nthe remainder of this paper. Importantly, frequency\nis often considered as a control variable, rather than\nthe factor being investigated in support of a par-\nticular cognitive theory of language processing.4\n2.3 Surprisal as a Predictor of Reading Time\nA common claim is that reading is mediated by\ncontextual surprisal (Shannon, 1948), defined as\n\u03b9H(c,u)def=\u2212logpH(u|c). (8)\nIndeed, this claim has received much empirical sup-\nport (Hale, 2001; Demberg and Keller, 2008; Smith\nand Levy, 2008, inter alia ). Importantly, there is\nevidence that the particular functional relationship,\ncalled the linking function , between contextual\nsurprisal and reading time is affine5(Smith and\nLevy, 2013; Wilcox et al., 2023; Shain et al., 2024),\njustifying the use of linear regression modeling.\n2.4 PMI as a Predictor of Reading Time\nNext, we point out an alternative way of deriv-\ning a contextual predictor from an LM, namely, as\nthepointwise mutual information (PMI ; Fano,\n1961) between a unit and its context. PMI mea-\nsures association, and has been an important notion\nin NLP (Church and Hanks, 1990; Levy and Gold-\nberg, 2014) and, more recently, psycholinguistics\n(Wilcox et al., 2024). The PMI between a unit\nu\u2208\u03a3and its context c\u2208\u03a3\u2217is given by\n\u00b5H(c,u)def= logpH(u|c)\u03c0H(c)\n\u03c0H(c)qH(u). (9)\nThe probability that canduoccur together is ex-\npressed in the numerator (rewritten using Eqs. (3)\nand (4)). The denominator expresses what this\nprobability would be if canduwere independent.\nIf PMI is predictive of reading times, then that\nwould suggest a theory positing that the strength\nof association that the observed unit has with its\ncontext is part of what determines the effort it takes\nto process it. It turns out that many of the empirical\n4Additional common controls include unit length (as mea-\nsured, e.g., by its orthographic representation), as well as\nlength and frequency of previous units. The latter are included\nto account for spillover effects, where reading-time slowdowns\ntriggered by a particular unit appear after a time delay.\n5Previous work often refer to this affine function as linear .results that have been published in support of sur-\nprisal theory, actually, by courtesy of the assumed\naffine linking function, provide an equal amount\nof evidence for a PMI-based theory. To see this,\nfirst note that we can rewrite PMI as the difference\nbetween frequency and surprisal:\n\u00b5H(c,u) = logpH(u|c)\nqH(u)(10a)\n=\u03c5H(u)\u2212\u03b9H(c,u). (10b)\nThis equation shows that \u03c5H,\u03b9Hand\u00b5Hare linearly\ndependent in a certain Hilbert space, which we will\nintroduce in \u00a73. Now, under a linear model f\u03b2with\nonly surprisal and frequency as predictors, the ex-\npected value of f\u03b2, denoted by br(c,u), is given by\nbr(c,u) =\u03b20+\u03b2\u03c5H\u03c5H(u) +\u03b2\u03b9H\u03b9H(c,u).(11)\nBy adding and subtracting an additional \u03b2\u03b9H\u03c5H(u)\nterm, this can be rewritten as\nbr(c,u) =\u03b20+ (\u03b2\u03c5H+\u03b2\u03b9H)\u03c5H(u)\u2212\u03b2\u03b9H\u00b5H(c,u).\n(12)\n(We suppressed an intermediate step, given in\nApp. B.1.) Thus, it turns out that the very same co-\nefficient that is typically taken to indicate the effect\nof surprisal also has an alternative interpretation\nas the negative effect of PMI. Furthermore, the pre-\ndictive power of a linear model with surprisal and\nfrequency is the same as that of a linear model with\nPMI and frequency. In other words, if frequency\nis provided as a predictor, additionally adding\nsurprisal as a predictor is no more predictive than\nadding PMI ceteris paribus . However, two such\nmodels will differ in the coefficient assigned to fre-\nquency: \u03b2\u03c5Hin the surprisal and frequency model,\nversus \u03b2\u03c5H+\u03b2\u03b9Hin the PMI and frequency model.\nAs a consequence, they will also differ in terms of\nthe strength of the effect attributed to the predictor\nthat stands in for context, i.e., surprisal or PMI.\n3 Disentangling the Effect of Context\nAs there is a large and established body of work\nshowing that frequency plays a major role in ex-\nplaining the effort it takes to process words (see,\ne.g., Bybee, 2006), we argue that the interest of sur-\nprisal theory lies in understanding what additional\neffect there is of contextual information beyond fre-\nquency. The exposition above implies that neither\nsurprisal nor PMI should receive special status as\na measure of the effect of context. Moreover, both\n3surprisal and PMI are correlated with frequency6\nand all three are collinear.\nWe now present a simple technique to decorre-\nlate frequency from surprisal, resulting in a new\npredictor that is engineered to be disentangled from\nfrequency. Importantly, our technique attributes the\nshared effect of frequency and surprisal on reading\ntime to frequency, and then creates a new predictor\nwhich represents the effect of surprisal that is not\nshared with frequency. We argue that this new pre-\ndictor is more relevant to study than either surprisal\nor PMI since it represents only the effect of context.\nOur technical exposition starts with an underly-\ning probability space (\u03a3\u2217\u00d7\u03a3,P(\u03a3\u2217\u00d7\u03a3), \u03c0H\u00b7pH).\nNext, consider the following random variables un-\nder this probability space: IHencoding the dis-\ntribution over surprisals \u03b9H(c,u)of the next unit\ngiven a context, MHencoding the distribution over\nPMIs \u00b5H(c,u)between the next unit and a context,\nandYHencoding the distribution over frequencies\n\u03c5H(u)of a unit. Note that IH,MHandYHare real-\nvalued random variables and that YHisconstant\ninc. They are elements of a Hilbert space Hover\nRcontaining all random variables under the above\nprobability space that have finite second moment\n(Rudin, 1987). The inner product on His given by\n\u27e8X,Y\u27e9def=E[XY] (13)\n=X\nc\u2208\u03a3\u2217,\nu\u2208\u03a3\u03c0H(c)pH(u|c)X(c,u)Y(c,u).\nApp. C.1 provides further details on why His\nindeed a Hilbert space over Rwith the above inner\nproduct. With Hbeing a Hilbert space, we can\ntake projections on H. Taking the projection of IH\nonto the orthogonal complement of YHwe get\nprojY\u22a5\nH(IH) =IH\u2212\u27e8IH,YH\u27e9\n\u27e8YH,YH\u27e9YH. (14)\nProjecting in this manner results in an orthogonal-\nization in the sense that \u27e8YH,projY\u22a5\nH(IH)\u27e9= 0,\nas a consequence of the Hilbert projection theorem\n(Rudin, 1991, pp. 306-9). See App. C.2 for a proof.\nIf the expected values of at least one of the random\nvariables XandYis0, which can be achieved by\na simple mean-centering transformation, then\n\u27e8X,Y\u27e9=E[XY]\n= Cov( X,Y) +E[X]E[Y]|{z}\n=0. (15)\n6For example, in the dataset we use in our studies in \u00a74\nwe observe a Pearson correlation coefficient of r= 0.57;p <\n0.001for surprisal and r= 0.52;p <0.001for PMI.Thus, if YHandIHare mean-centered the\ncovariance between YHandprojY\u22a5\nH(IH)will be 0,\ni.e., they will be decorrelated. Thus, projY\u22a5\nH(IH)\nconstitutes a new predictor variable, which\nwe term orthogonalized surprisal . In words,\northogonalized surprisal represents the effect of\ncontext and is disentangled from frequency.7\n4 Variance Explained by Context\nWe now seek an empirical understanding of how\nthe new orthogonalized surprisal predictor influ-\nences the importance attributed to context in read-\ning time prediction. In addition to the experiment\npresented here, we also compared the predictive\npower for nonlinear models across different predic-\ntors. Those experiments are discussed in App. F.\nDataset and Predictors. We use gaze duration\ntimes from the Multilingual Eye-movement Corpus\n(MECO; Siegelman et al., 2022), which consists\nof word-level eye-tracking measurements from\nseveral languages. To obtain surprisal estimates,\nwe approximate pHwith mGPT (Shliazhko et al.,\n2024), which is a multilingual LM based on the\nGPT-2 architecture. The frequency estimates\nare from Speer (2022)8and PMI is computed\nthrough the decomposition given in Eq. (10b).\nOrthogonalized surprisal is obtained by approx-\nimating \u27e8IH,YH\u27e9and\u27e8YH,YH\u27e9on a training\ncorpus. More specifically, we transform surprisal\nand frequency to be mean-centered, and use the\ntransformed data to compute unbiased sample\nvariances and covariances. Those sample estimates\nare then plugged into Eq. (14). App. B.2 gives the\nformula for a linear model with orthogonalized\nsurprisal. We also include a word length predictor.\nWord length is correlated with frequency (Zipf,\n1949), so we orthogonalize it in the same manner\nas surprisal. App. E gives more details on the\ndataset and predictor variables.\nExperimental Setup. We fit three linear models\nusing ordinary least squares, with predictors:\n(i) surprisal, frequency, and length, (ii) PMI,\nfrequency, and length, and (iii) orthogonalized\nsurprisal, frequency, and orthogonalized length.\nWe include spillover effects from the previous word\n7The method presented can in principle be applied to any\npair of predictor variables that live in H, e.g., taking MH\ninstead of IHyields an equivalent context predictor. We also\nconsider orthogonalized unit length in our experiments in \u00a74.\n8See Nikkarinen et al. (2021) for a more nuanced estima-\ntion technique.\n4Dutch English Finnish German Greek Hebrew Italian Korean Russian Spanish TurkishSurprisal PMI Orthogonalized\nwtwt\u22121wtwt\u22121wtwt\u22121wtwt\u22121wtwt\u22121wtwt\u22121wtwt\u22121wtwt\u22121wtwt\u22121wtwt\u22121wtwt\u221210.00.10.20.30.4\n0.00.10.20.30.4\n0.00.20.40.6Explained Variance (LMG)\npredictor Frequency (Unigram Surprisal) Context (Contextual Surprisal / PMI / Orthogonalized Surprisal) Length / Orthogonalized LengthFigure 1: Proportion of total variance explained by the predictors (Kruskal, 1987), across languages and linear\nregression models. Summing the values represented by the four bars yields the coefficient of determination R2. Note\nthat the R2values are the same across the three models, as a consequence of the collinearity discussed in \u00a72.4. Error\nbars are 95% confidence intervals across folds of data. We observe only a small proportion of explained variance by\ncontext when excluding the variance it shares with frequency, as in the orthogonalized surprisal predictor.\nand fit models over ten folds of cross-validation.\nWe then measure the relative importance of predic-\ntors by averaging over the proportion of variance\nexplained by the predictors across orderings in\nwhich they are added to the model (Lindeman\net al., 1980; Kruskal, 1987), a technique known as\nLMG (Gr\u00f6mping, 2007).9One advantage of this\ntechnique, compared to previous methods (e.g.,\nDelta Log Likelihood; Goodkind and Bicknell,\n2018) is that LMG gives a better absolute sense\nof predictive power.\nResults. The LMG values for the different pre-\ndictors across these models are visualized in Fig. 1.\nComparing the plots on the bottom row to the plots\non the top row, we observe that the explained vari-\nance for orthogonalized surprisal is much lower\nthan for surprisal. These results are consistent\nacross languages. For most languages, when rank-\ning the importance of the predictors, orthogonal-\nization shifts the third most important predictor\n9The intuition behind LMG is simple: Given a response\nvariable yand two predictors x1andx2, the proportion of the\nvariance of yexplained by x1can be taken to either depend on\nthe correlation between x1andy(ifx1is added to the model\nbefore x2), or to depend on the partial correlation between x1\nandywhen controlling for x2(ifx1is added to the model\nafterx2). For a model with ppredictors, the LMG averages\nthe explained variance over all p!such orderings.from context to the previous word\u2019s frequency. Our\nresults suggest that using surprisal therefore overes-\ntimates the importance that context has on reading\ntime.10Finally, we observe values for PMI that\nfor most languages lie between those for surprisal\nand orthogonalized surprisal, indicating that the\nextent to which that PMI overestimates the context\neffect is smaller compared to surprisal. We observe\nthat the mean R2values of the models across LMG\norderings, i.e., the sums of the bars within each\nfacet, range between 0.6\u20130.8, indicating that the\nlinear models capture a fairly large proportion of\nthe variance observed during the reading process.\n5 Conclusion\nThis article discusses predictors that capture how\nthe processing effort of a unit is shaped by its con-\ntext. We made the observation that there exist alter-\nnatives to the widely used surprisal predictor. Sur-\nprisal is correlated with non-contextual frequency,\nso we provided a technique to disentangle contex-\ntual and non-contextual information in language\nmodels. In so doing, we found that the effect that\ncontext has on reading times appears to be small.\n10Our results may help explain why surprisal estimates\nfrom larger LMs provide poorer fits to reading than those from\nmedium-sized LMs (Oh and Schuler, 2023a); see App. G.\n5Limitations\nOur approach takes one predictor to remain\nuntouched (i.e., frequency), and modifies others\nto reflect effects that are disassociated from the\nfirst. It would thus be natural to ask what happens\nin the alternative setting, where surprisal remains\nuntouched and frequency is projected onto the\northogonal complement of surprisal. In App. F.1,\nwe provide such an analysis. It turns out that even\nwhen attributing the shared effect of frequency\nand surprisal on reading times to surprisal\u2014which\nis the case when replacing frequency by an\northogonalized frequency predictor\u2014the variance\nexplained by the frequency predictor is still\nhigher for most languages in comparison to the\nsurprisal predictor. This gives further support to\nour conclusion that context appears to play a small\nrole in reading time prediction.\nFurthermore, our presentation of ideas and\ndiscussion largely ignores effects of word lengths.\nWe find word lengths to explain the most variance\nin reading times in our surprisal and PMI models.\nIn addition, after residualizing word length\nagainst frequency, we find length to be the second\nstrongest predictor, with an explained variance\nranging from around 10%\u201321%. One hypothesis\nis that readers may make multiple saccades within\nfirst passes of longer words, and the time it takes\nto plan and execute these saccades could be the\nunderlying reason why orthogonalized word length\nremains explanatory even after residualization.\nFuture work could control for this by adding in the\nnumber of saccades within a word as an additional\npredictor into models.\nWe are unaware of any efficient algorithm to\ncompute \u27e8IH,YH\u27e9and\u27e8YH,YH\u27e9exactly, so in\npractical settings we must rely on estimation. Thus,\nit may be that the orthogonalized surprisal pre-\ndictor is only \u201cclose to\u201d being orthogonal to fre-\nquency in practice. The manner in which esti-\nmation is performed makes our technique similar\nto residualization\u2014see App. D for a discussion.\nMoreover, our method only provides guarantees\nfor predictor variables that live in H.\nAnother limitation of this work is that, while we\ninvestigate several languages, these are still biased\ntowards Indo-European languages. For example,\nwe present results from one language only for Fino-\nUralic, Semitic, Turkic, and Koreanic language\nfamilies, but seven Indo-European languages. Ex-\npanding these results to even more languages wouldfurther broaden the impact of this work. In addi-\ntion, we observe somewhat unique effects for Ko-\nrean, where, in orthogonalized models, frequency\naccounts for a lower proportion of the variance, and\nlength and context account for higher proportions,\nat least compared to other languages. One possi-\nble reason for this is the Korean script (Hangul),\nwhich combines features of both alphabetic and syl-\nlabic writing systems. Future work should conduct\nsimilar analyses on different Korean datasets to de-\ntermine whether this trend is a property of Korean,\nor just our particular Korean language dataset.\nEthical Considerations\nThis work uses previously collected human data\nfrom the MECO dataset. Please see the paper that\nintroduces this dataset (Siegelman et al., 2022) for\ninformation about the data collection procedure.\nThe authors foresee no ethical problems arising\nfrom the work presented here.\nAcknowledgments\nAndreas Opedal acknowledges funding from the\nMax Planck ETH Center for Learning Systems.\nReferences\nLudwig Boltzmann. 1868. Studien \u00fcber das Gle-\nichgewicht der lebendigen Kraft zwischen bewegten\nmateriellen Punkten . K.K. Hof und Staatsdruckerei.\nJames A. Breaugh. 2006. Rethinking the control of nui-\nsance variables in theory testing. Journal of Business\nand Psychology , 20(3):429\u2013443.\nDonald E. Broadbent. 1967. Word-frequency effect and\nresponse bias. Psychological Review , 74 1:1\u201315.\nJoan Bybee. 2006. Frequency of use and the organiza-\ntion of language . Oxford University Press.\nKenneth Ward Church and Patrick Hanks. 1990. Word\nassociation norms, mutual information, and lexicog-\nraphy. Computational Linguistics , 16(1):22\u201329.\nVera Demberg and Frank Keller. 2008. Data from eye-\ntracking corpora as evidence for theories of syntactic\nprocessing complexity. Cognition , 109(2):193\u2013210.\nLi Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara\nMeister, Jason Eisner, and Ryan Cotterell. 2023. A\nmeasure-theoretic characterization of tight language\nmodels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 9744\u20139770, Toronto,\nCanada. Association for Computational Linguistics.\n6Robert M. Fano. 1961. Transmission of Information:\nA Statistical Theory of Communication . MIT Press\nClassics. MIT Press.\nCatalina B. Garc\u00eda, Rom\u00e1n Salmer\u00f3n, Claudia Garc\u00eda,\nand Jos\u00e9 Garc\u00eda. 2019. Residualization: justifica-\ntion, properties and application. Journal of Applied\nStatistics , 47(11):1990\u20132010.\nJosiah Willard Gibbs. 1902. Elementary Principles in\nStatistical Mechanics . Charles Scribner\u2019s Sons.\nMario Giulianelli, Andreas Opedal, and Ryan Cotterell.\n2024. Generalized measures of anticipation and re-\nsponsivity in online language processing.\nAdam Goodkind and Klinton Bicknell. 2018. Predictive\npower of word surprisal for reading times is a linear\nfunction of language model quality. In Proceedings\nof the 8th Workshop on Cognitive Modeling and Com-\nputational Linguistics (CMCL 2018) , pages 10\u201318,\nSalt Lake City, Utah. Association for Computational\nLinguistics.\nUlrike Gr\u00f6mping. 2007. Estimators of relative impor-\ntance in linear regression based on variance decom-\nposition. The American Statistician , 61(2):139\u2013147.\nJohn Hale. 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Second Meeting of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics .\nJacob Louis Hoover, Morgan Sonderegger, Steven T.\nPiantadosi, and Timothy J. O\u2019Donnell. 2023. The\nPlausibility of Sampling as an Algorithmic Theory\nof Sentence Processing. Open Mind , 7:350\u2013391.\nAlbrecht Werner Inhoff and Keith Rayner. 1986.\nParafoveal word processing during eye fixations in\nreading: Effects of word frequency. Perception &\nPsychophysics , 40(6):431\u2013439.\nFlorian Jaeger. 2010. Redundancy and reduction:\nSpeakers manage syntactic information density. Cog-\nnitive Psychology , 61(1):23\u201362.\nWilliam Kruskal. 1987. Relative importance by av-\neraging over orderings. The American Statistician ,\n41(1):6\u201310.\nVictor Kuperman, Raymond Bertram, and R. Harald\nBaayen. 2008. Morphological dynamics in com-\npound processing. Language and Cognitive Pro-\ncesses , 23(7-8):1089\u20131132.\nTatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo\nYoshida, Masayuki Asahara, and Kentaro Inui. 2021.\nLower perplexity is not always human-like. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 5203\u20135217,\nOnline. Association for Computational Linguistics.Omer Levy and Yoav Goldberg. 2014. Neural word\nembedding as implicit matrix factorization. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 27. Curran Associates, Inc.\nRoger Levy. 2008. Expectation-based syntactic compre-\nhension. Cognition , 106(3):1126\u20131177.\nRichard H. Lindeman, Peter F. Merenda, and Ruth Z.\nGold. 1980. Introduction to bivariate and multivari-\nate analysis . Glenview (Ill.) : Scott.\nClara Meister, Tiago Pimentel, Patrick Haller, Lena\nJ\u00e4ger, Ryan Cotterell, and Roger Levy. 2021. Revisit-\ning the Uniform Information Density hypothesis. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages 963\u2013\n980, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nIrene Nikkarinen, Tiago Pimentel, Dami\u00e1n Blasi, and\nRyan Cotterell. 2021. Modeling the unigram dis-\ntribution. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3721\u20133729, Online. Association for Computational\nLinguistics.\nByung-Doh Oh and William Schuler. 2023a.\nTransformer-based language model surprisal\npredicts human reading times best with about\ntwo billion training tokens. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023 , pages 1915\u20131921, Singapore. Association for\nComputational Linguistics.\nByung-Doh Oh and William Schuler. 2023b. Why\ndoes surprisal from larger transformer-based lan-\nguage models provide a poorer fit to human reading\ntimes? Transactions of the Association for Computa-\ntional Linguistics , 11:336\u2013350.\nByung-Doh Oh, Shisen Yue, and William Schuler. 2024.\nFrequency explains the inverse correlation of large\nlanguage models\u2019 size, training data amount, and\nsurprisal\u2019s fit to reading times. In Proceedings of\nthe 18th Conference of the European Chapter of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 2644\u20132663, St. Julian\u2019s, Malta.\nAssociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research ,\n21(140):1\u201367.\nKeith Rayner. 1998. Eye movements in reading and\ninformation processing: 20 years of research. Psy-\nchological bulletin , 124 3:372\u2013422.\nKeith Rayner and Susan A. Duffy. 1986. Lexical com-\nplexity and fixation times in reading: Effects of word\nfrequency, verb complexity, and lexical ambiguity.\nMemory & Cognition , 14(3):191\u2013201.\n7Walter Rudin. 1987. Real and complex analysis, 3rd ed.\nMcGraw-Hill, Inc., USA.\nWalter Rudin. 1991. Functional Analysis . International\nseries in pure and applied mathematics. McGraw-\nHill.\nCory Shain. 2019. A large-scale study of the effects\nof word frequency and predictability in naturalistic\nreading. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4086\u20134094, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nCory Shain. 2024. Word Frequency and Predictabil-\nity Dissociate in Naturalistic Reading. Open Mind ,\n8:177\u2013201.\nCory Shain, Clara Meister, Tiago Pimentel, Ryan Cot-\nterell, and Roger Levy. 2024. Large-scale evidence\nfor logarithmic effects of word predictability on read-\ning time. Proceedings of the National Academy of\nSciences , 121(10):e2307876121.\nClaude E. Shannon. 1948. A mathematical theory of\ncommunication. The Bell System Technical Journal ,\n27(3):379\u2013423.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nAnastasia Kozlova, Vladislav Mikhailov, and Tatiana\nShavrina. 2024. mGPT: Few-Shot Learners Go Mul-\ntilingual. Transactions of the Association for Compu-\ntational Linguistics , 12:58\u201379.\nNoam Siegelman, Sascha Schroeder, Cengiz Acart\u00fcrk,\nHee-Don Ahn, Svetlana Alexeeva, Simona Amenta,\nRaymond Bertram, Rolando Bonandrini, Marc Brys-\nbaert, Daria Chernova, et al. 2022. Expanding hori-\nzons of cross-linguistic research on reading: The mul-\ntilingual eye-movement corpus (MECO). Behavior\nResearch Methods , 54(6):2843\u20132863.\nNathaniel J. Smith and Roger Levy. 2008. Optimal\nprocessing times in reading: A formal model and\nempirical investigation. In Proceedings of the 30th\nAnnual Conference of the Cognitive Science Society .\nNathaniel J. Smith and Roger Levy. 2013. The effect\nof word predictability on reading time is logarithmic.\nCognition , 128(3):302\u2013319.\nRobyn Speer. 2022. rspeer/wordfreq: v3.0.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger P. Levy. 2020. On the predictive\npower of neural language models for human real-\ntime comprehension behavior. In Proceedings of\nthe 42nd Annual Meeting of the Cognitive Science\nSociety , page 1707\u20131713.\nEthan Gotlieb Wilcox, Tiago Pimentel, Clara Meister,\nand Ryan Cotterell. 2024. An information-theoretic\nanalysis of targeted regressions during reading. Cog-\nnition , 249:105765.Ethan Gotlieb Wilcox, Tiago Pimentel, Clara Meister,\nRyan Cotterell, and Roger P. Levy. 2023. Testing\nthe Predictions of Surprisal Theory in 11 Languages.\nTransactions of the Association for Computational\nLinguistics , 11:1451\u20131470.\nLee H. Wurm and Sebastiano A. Fisicaro. 2014. What\nresidualizing predictors in regression analyses does\n(and what it does not do). Journal of Memory and\nLanguage , 72:37\u201348.\nRichard York. 2012. Residualization is not the answer:\nRethinking how to address multicollinearity. Social\nScience Research , 41(6):1379\u20131386.\nGeorge K. Zipf. 1949. Human Behaviour and the Prin-\nciple of Least Effort . Addison-Wesley.\n8A Normalizing the Prefix Probabilities\nHere we show that Z\u03c0Has defined in Eq. (2) is the normalizing constant for the prefix probabilities.\nProposition 1. LetZ\u03c0Hbe defined as in Eq. (2), i.e.,\nZ\u03c0H= 1 +X\nu\u2208\u03a3\u2217pH(u)|u|. (16)\nThen, Z\u03c0His the normalizing constant for \u03c0H, i.e.,\nX\nc\u2208\u03a3\u2217\u03c0H(c) = 1 . (17)\nProof. First, note that Eq. (2) can be rewritten in the following manner:\nZ\u03c0H= 1 +X\nu\u2208\u03a3\u2217pH(u)|u| (18a)\n=X\nu\u2208\u03a3\u2217pH(u) +X\nu\u2208\u03a3\u2217pH(u)|u| (18b)\n=X\nu\u2208\u03a3\u2217(1 +|u|)pH(u) (18c)\n=X\nc\u2208\u03a3\u2217X\nu\u2032\u2208\u03a3\u2217pH(cu\u2032). (18d)\nThe last step follows from the fact that a string ucan be segmented into two substrings candu\u2032in1 +|u|\nways. Note the two cases where either c=\u03b5andu\u2032=u, orc=uandu\u2032=\u03b5(with \u03b5denoting the\nempty string). It is then easy to see that Z\u03c0His a valid normalizing constant:\nX\nc\u2032\u2208\u03a3\u2217\u03c0H(c\u2032) =X\nc\u2032\u2208\u03a3\u2217P\nu\u2208\u03a3\u2217pH(c\u2032u)P\nc\u2208\u03a3\u2217P\nu\u2032\u2208\u03a3\u2217pH(cu\u2032)(19a)\n=P\nc\u2032\u2208\u03a3\u2217P\nu\u2208\u03a3\u2217pH(c\u2032u)P\nc\u2208\u03a3\u2217P\nu\u2032\u2208\u03a3\u2217pH(cu\u2032)(19b)\n= 1. (19c)\n\u25a0\nB Linear Models\nB.1 Rewriting Linear Models with Surprisal in Terms of PMI\nIn this section, we demonstrate the equivalence between two linear models, one with surprisal and\nfrequency as predictors, and the other with PMI and frequency as predictors. In particular, we consider\na linear model of reading times f\u03b2(\u00b7 |X(c,u)), in which X(c,u): \u03a3\u2217\u00d7\u03a3\u2192RDincludes surprisal,\nfrequency and additional baseline predictors Xb(c,u): \u03a3\u2217\u00d7\u03a3\u2192RD\u22123and show that it is equivalent to\na model in which surprisal is replaced with PMI. Consider the prediction br(c,u)which is the expected\nvalue of f\u03b2(\u00b7 |X(c,u)). To demonstrate the equivalence, we simply add and subtract an additional\nfrequency term:\nbr(c,u) =\u03b20+\u03b2\u03c5H\u03c5H(u) +\u03b2\u03b9H\u03b9H(c,u) +\u03b2\u22ba\nbXb(c,u) (20a)\n=\u03b20+ (\u03b2\u03c5H+\u03b2\u03b9H)\u03c5H(u)\u2212\u03b2\u03b9H(\u03c5H(u)\u2212\u03b9H(c,u)|{z }\n=\u00b5H(c,u)) +\u03b2\u22ba\nbXb(c,u) (20b)\n=\u03b20+ (\u03b2\u03c5H+\u03b2\u03b9H)\u03c5H(u)\u2212\u03b2\u03b9H\u00b5H(c,u) +\u03b2\u22ba\nbXb(c,u), (20c)\nwhere \u03b2= [\u03b20, \u03b2\u03c5H, \u03b2\u03b9H,\u03b2b]\u2208RD.\n9B.2 The Orthogonalized Surprisal Model\nUnder a linear model of reading time which includes orthogonal surprisal, frequency and (possibly) other\npredictors Xb(c,u), reading time predictions are obtained as follows:\nbr(c,u) =\u03b20+\u03b2\u03b9H\u0012\n\u03b9H(c,u)\u2212Cov(IH,YH)\nCov(YH,YH)\u03c5H(u)\u0013\n+\u03b2\u03c5H\u03c5H(u) +\u03b2\u22ba\nbXb(c,u) (21a)\n=\u03b20+\u03b2\u03b9H\u03b9H(c,u) +\u0012\n\u03b2\u03c5H\u2212\u03b2\u03b9HCov(IH,YH)\nCov(YH,YH)\u0013\n\u03c5H(u) +\u03b2\u22ba\nbXb(c,u). (21b)\nWe estimate Cov(IH,YH)(andCov(YH,YH)) through the unbiased sample covariance on the training\ncorpus,\n1\nN\u22121X\nu\u2208\u03a3(\u03b9H(c,u)\u2212\u02c6\u00b5\u03b9H)(\u03c5H(u)\u2212\u02c6\u00b5\u03c5H), (22)\nwhere \u02c6\u00b5\u03b9Hand\u02c6\u00b5\u03c5Hare the training corpus sample means for surprisal and frequency, respectivey.\nC Further Technical Details on the Hilbert Space\nIn this section we provide formal guarantees for the method presented in \u00a73. In App. C.1, we justify why\nthe Hilbert space introduced in \u00a73 is indeed a Hilbert space. In App"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Summary of \"Review of Recent Advances in Gaussian Process Regression Methods\" by Chenyi Lyu, Xingchi Liu, and Lyudmila Mihaylova\n\n#### 1. Abstract:\nThe paper reviews recent advancements in Gaussian Process (GP) regression methods, particularly for large-scale and sparse datasets. It emphasizes key advantages such as uncertainty assessment, efficient factorization-based implementations, and scalability. The review includes hierarchical off-diagonal low-rank approximation methods and GPs with Kronecker structures, concluding with performance evaluations of these methods.\n\n#### 2. Introduction:\n- **Context**:\n  - The proliferation of sensors and data necessitates autonomous real-time processing.\n  - Uncertainty quantification is crucial for decision-making in autonomous systems.\n  - Probabilistic machine learning methods, especially Gaussian Processes (GPs), are potent due to their data-driven nature and capacity to model uncertainty.\n\n- **Research Questions**:\n  - How can GPs be scaled for large datasets without compromising prediction quality?\n  - What recent advancements have been made in global GP approximation methods?\n\n- **Literature Review**:\n  - GPs define distributions over possible functions fitting given points, offering a trade-off between fitting and smoothing data.\n  - Big data poses computational challenges for GPs due to the size of the kernel matrix.\n  - Two global strategies for approximating the kernel matrix are inducing point methods and low-rank approximations.\n\n#### 3. Related Work:\n- **Sparse Gaussian Process Approximations**:\n  - Inducing points are critical for low-rank approximations (Quinonero-Candela and Rasmussen, 2005).\n  - Variational Free Energy (VFE) methods improve inducing point selection and approximation quality (Titsias, 2009).\n  \n- **Structured Kernel Approximations**:\n  - Kronecker and Toeplitz structures enable efficient matrix operations (Wilson et al., 2014).\n  - Structured Kernel Interpolation (SKI) allows for approximations even when data isn't grid-aligned (Wilson and Nickisch, 2015).\n  \n- **Hierarchical Matrix-based Approximations**:\n  - Hierarchical Off-Diagonal Low-Rank (HODLR) matrices offer scalable solutions for covariance matrices (Ambikasaran et al., 2015).\n\n#### 4. Method:\n- **Gaussian Process Fundamentals**:\n  - **GP Definition**:\n    - Joint Gaussian distribution over functions.\n    - Predictive distribution: mean and covariance are derived from training data and kernel functions.\n\n- **Sparse Gaussian Process Approximations**:\n  - **Prior Approximations (SPGP/FITC)**:\n    - Use pseudo-inputs for constructing low-rank approximations.\n    - FITC model removes dependencies between latent function values by conditioning on inducing points.\n  \n  - **Posterior Approximations (VFE)**:\n    - Use variational methods to optimize inducing inputs and hyperparameters.\n\n- **Structured Sparse Approximations**:\n  - **Kronecker and Toeplitz Structures**:\n    - Utilize properties of covariance matrices to expedite matrix operations and reduce storage.\n  \n  - **Structured Kernel Interpolation (SKI)**:\n    - Approximate non-grid-aligned data using grid-aligned inducing points and interpolation techniques.\n\n- **Hierarchical Matrix-based Approximations**:\n  - **HODLR Matrix Structure**:\n    - Recursive decomposition into low-rank blocks.\n    - SVD and other methods for low-rank approximations.\n  - **Fast Solving Algorithms**:\n    - Recursive application of the Sherman-Morrison-Woodbury formula.\n    - Cholesky decomposition for computational efficiency.\n\n#### 5. Experimental Setup:\n- **Datasets**:\n  - Mention of a toy example: \\( f(x) = 0.02x + \\text{sinc}(x) + \\epsilon \\) for performance comparison.\n  \n- **Hardware and Software**:\n  - Simulation results primarily generated using MATLAB toolbox GPML and HM-toolbox.\n  \n- **Evaluation Metrics**:\n  - Computational time and prediction accuracy.\n  \n- **Baselines**:\n  - Comparison against full GP.\n  - Sparse methods: FITC, VFE, SKI, and HODLR.\n\n#### 6. Results and Analysis:\n- **Empirical Findings**:\n  - FITC and VFE methods yield results close to full GP but vary in computational efficiency and overfitting risks.\n  - SKI shows variability due to interpolation errors with insufficient inducing points.\n  - HODLR provides accuracy comparable to full GP with controllable precision loss based on low-rank approximation.\n\n- **Graphs and Tables**:\n  - Figures illustrating predictive mean and confidence intervals, comparison to full GP, and inducing point optimization.\n  - Table showing computational time for various methods across dataset sizes.\n\n#### 7. Discussion:\n- **Interpretation of Results**:\n  - HODLR and structured approximations deliver efficient, scalable solutions with minor trade-offs in accuracy.\n  - Inducing point methods balance between computational cost and predictive quality.\n\n- **Implications**:\n  - Improved methods can handle large-scale data with better uncertainty modeling.\n  - Real-time applications benefit significantly from efficient GP methods.\n\n- **Limitations**:\n  - Numerical instabilities in SKI.\n  - Inducing point methods might lead to suboptimal predictions due to overfitting.\n\n#### 8. Conclusion:\n- **Key Contributions**:\n  - Comprehensive review of scalable GP methods: inducing points, structured approximations, HODLR.\n  - Empirical validations demonstrating efficiency and accuracy.\n\n- **Future Research**:\n  - Address numerical instabilities in structured approximations.\n  - Explore hybrid approaches combining multiple techniques for enhanced performance. \n\nBy following the detailed methodology and insights from this summary, researchers can replicate the study or advance the work on Gaussian Process regression methods for large-scale data."}, {"role": "assistant", "content": "arXiv:2409.08112v1  [stat.ME]  12 Sep 2024Review of Recent Advances in Gaussian Process\nRegression Methods\nChenyi Lyu, Xingchi Liu and Lyudmila Mihaylova\nThe University of She\ufb03eld, She\ufb03eld, United Kingdom\nEmail:{clyu5, xingchi.liu, l.s.mihaylova }@she\ufb03eld.ac.uk\nAbstract. Gaussian process (GP) methods have been widely studied\nrecently, especially for large-scale systems with big data and even more\nextreme cases when data is sparse. Key advantages of these me thods\nconsist in: 1) the ability to provide inherent ways to assess the impact\nof uncertainties (especially in the data, and environment) on the solu-\ntions, 2) have e\ufb03cient factorisation based implementation s and 3) can\nbe implemented easily in distributed manners and hence prov ide scalable\nsolutions. This paper reviews the recently developed key fa ctorised GP\nmethods such as the hierarchical o\ufb00-diagonal low-rank appr oximation\nmethods and GP with Kronecker structures. An example illust rates the\nperformance of these methods with respect to accuracy and co mputa-\ntional complexity.\nKeywords: Gaussian process, factorisation, covariance matrix, hierarchica l\no\ufb00-diagonal matrix, low-rank approximation\n1 Introduction\nSensors provide huge amounts of data that need autonomous pro cessing and of-\nten in real-time. Uncertainty quanti\ufb01cation is also necessary for de cision-making\nin autonomous systems. Probabilistic machine learning methods can \ufb01 ll this\ngap, especially with their data-driven nature and learning capabilities . They can\nprovide a framework for modelling uncertainty that facilitates robu st predic-\ntions over di\ufb00erent changes. Hence, this paper focuses on recen t advances in\nmachine learning methods, namely Gaussian Process (GP) regressio n [1,2]. GP\nis astochasticprocesswhich de\ufb01nes adistribution overpossiblefun ctions that \ufb01t\na set of points. This method often produces a \ufb01ne and precise trad e-o\ufb00 between\n\ufb01tting and smoothing the data. However, like most machine learning m ethods,\nbig data can pose challenges to GP methods since it contains much info rmation.\nSeveral scalable GP prediction methods have been developed to impr ove scala-\nbility without compromising prediction quality. This paper will review the latest\ndevelopments in the global GP approximation methods, which approx imate the\nwhole probability density function.\nGenerally, for the global GP approximation, there are two strateg ies to ap-\nproximate the kernel matrix with size n\u00d7nthrough global distillation. TheChenyi Lyu, Xingchi Liu and Lyudmila Mihaylova\n\ufb01rst strategy sets a subset of training data or m(m\u226an) inducing points to\nconstruct a low-rank approximation to the GP covariance matrix wit h a smaller\nkernel matrix in size m\u00d7m[3,4]. The capacity of these methods relies on the\nselection of the inducing point, which requires extra optimisation. In stead of se-\nlecting the inducing points, the other strategy uses the low-rank a pproximation\nmethods, which are based on the spectrum of the covariance matr ix to remove\nthe uncorrelated entries in covariance matrix [5,6]. Instead of the cube com-\nputation cost, most global approximation GP algorithms scale linearly in the\ntraining set size. They can o\ufb00er a close result with the full GP(stand ard GP\nwithout any approximation or factorisation) with enough inducing po ints [7].\nHowever, the representation of the global patterns (long-term spatial correla-\ntions) often excludes local patterns due to their limited global induc ing set [1].\nHaving appealing properties, GPs have been extensively developed, improved\nand applied to solve a wide range of arti\ufb01cial intelligence problems.\nThis paper aims to aid readers in selecting methods appropriate for t heir\napplications and further exploring the literature. We present a tho rough review\nof popular global approximation methods, including the methodologic al char-\nacteristics, the advantages, the risks and comparisons for bett er understand-\ning. Speci\ufb01cally, after a short review of GP regression in Section 2, S ection\n3 reviews the classic inducing point-based approximation methods. S ection 4\ncontains the Kronecker and Toeplitz-based structured approxim ation and the\nstructured kernel interpolation (SKI) GP methods. The hierarch ical o\ufb00-diagonal\nlow-rank (HODLR) matrix-based approximation method, which impro ves GPs\nin terms of scalability and capability, is reviewed in Section 5. Finally, Sec tion 6\nshows the simulation results.\n2 Gaussian Process Regression Revisited\nA Gaussian process (GP) is a collection of random variables, any \ufb01nite num-\nber of which have a joint Gaussian distribution. Given any \ufb01nite set of nin-\nputsX={x1,...,xn}and the corresponding set of latent function values\nf={f(x1),...f(xn)}. The relationship between the input data xiand the\nobserved noisy targets yiare given by [2] as\nyi=f(xi)+\u03b5i, \u03b5\u223c N/parenleftbig\n0,\u03c32/parenrightbig\n, (1)\nwhere\u03b5is the zero-mean Gaussian noise and \u03c32is the variance of the noise. The\nprior distribution over the latent function can be written as\np(f|X)\u223c N/parenleftbig\nf,K/parenrightbig\n, (2)\nwhere the n\u00d71 mean vector fis de\ufb01ned by mathematical expectation E[f(X)].\nKis an\u00d7ncovariance matrix which Kij=k(xi,xj) are based on the kernel\nfunction of GP. The kernel function kcontrols the smoothness of the GP.\nThe predictive distribution [3] of the function values f\u2217at the test set X\u2217\ncan be written as\np(f\u2217|X\u2217,y,X)\u223c N(\u00b5\u2217,\u03a3\u2217), (3)Review of Recent Advances in Gaussian Process Regression Me thods\nwhere the mean \u00b5\u2217and covariance \u03a3\u2217are calculated respectively as\n\u00b5\u2217=\u00affX\u2217+KX\u2217,X(KX,X+\u03c32I)\u22121y, (4)\n\u03a3\u2217=KX\u2217,X\u2217\u2212KX\u2217,X(KX,X+\u03c32I)\u22121KX,X\u2217, (5)\nwhere the \u00affX\u2217is then\u2217\u00d71 mean vector, n\u2217represents the number of test data,\nandIis the identity matrix. KX\u2217,X=k(X\u2217,X) denotes the n\u2217\u00d7nmatrix of\ncovariances between the GP evaluated at X\u2217andX. Also the KX,Xis then\u00d7n\ncovariance matrix evaluated at training inputs X. According to (4) and (5),\nthe computational bottleneck of GP is the inverse of the covarianc e matrix. For\nthis purpose, a standard procedure is to compute the Cholesky de composition of\nKX,X+\u03c32I, requiring O/parenleftbig\nn3/parenrightbig\noperations and O/parenleftbig\nn2/parenrightbig\nfor storage.Afterwards, the\npredictive mean and covariance respectively cost O(n) andO/parenleftbig\nn2/parenrightbig\nfor a single\ntest input.\nThe marginal likelihood as a function of the model hyperparameters \u03b8can\nbe utilised to learn the hyperparameters. The negative log marginal likelihood\ncan be written as\nlogp(y|\u03b8)\u221d \u2212y\u22a4/parenleftbig\nK\u03b8+\u03c32I/parenrightbig\u22121y+log/vextendsingle/vextendsingleK\u03b8+\u03c32I/vextendsingle/vextendsingle. (6)\nThe \ufb01rst term above evaluates the model \ufb01t and the second term p enalises\nthe model complexity. By maximising (6), the optimal hyperparamet ers can be\nlearned. However, this maximum likelihood estimation (MLE) for param eter-\n\ufb01tting given noisy observations is a computationally-demanding task since sim-\nilar to the prediction process (4) and (5), the bottleneck of the ad aptation of\nhyperparameters is the inverse and the determinant of the covar iance matrix.\n3 Sparse Gaussian Process Approximations\nWhile the GP approach presented above is powerful, it still faces cha llenges\nfor large data sets since cubic computing is required. In the last few years,\nmany sparse approximation approaches have been outputted to o vercome this\nlimitation [1,4]. For most global approximation methods, \ufb01rst de\ufb01ne a n extra\ndatasetZ={z1,z2,...,z m}insizem.Thedatacalledinducingpointsorpseudo\ninputs, can be mapped to a set of latent inducing variables fz. These inducing\npoints are introduced to summarise the dependence of the entire t raining set\nand achieve sparsity of the full kernel matrix KX,X. Based on this, the exact\nGP can be realised in two di\ufb00erent ways: prior approximation and post erior\napproximation.\n3.1 Prior Approximation\nDue to the consistency (conjugation) of GPs, the prior p(f\u2217,f) can be recov-\nered by simply marginalizing out fzfrom the joint GP prior p(f\u2217,f,fz). Then\na fundamental approximation would be introduced which is used in almo st allChenyi Lyu, Xingchi Liu and Lyudmila Mihaylova\nprior sparse approximations. It approximates the joint prior p(f\u2217,f) by presum-\ning that f\u2217andfare conditionally independent given fz, the equation cab be\nwritten as\np(f\u2217,f)\u2243q(f\u2217,f) =/integraldisplay\nq(f\u2217|fz)q(f|fz)p(fz)dz, (7)\nwherep(fz) =N(0,Kz,z) andKz,zare the covariances between the including\npointsZitself. The predictionfunction f\u2217can onlyconnectto fthroughinducing\npointsZ. Consequently, there are dependencies between the training and test\ncases.Indi\ufb00erent approaches,therearedi\ufb00erentadditionalas sumptionsbetween\ntwo approximate inducing conditionals q(f|fz),q(f\u2217|fz).\nSnelson [8] proposes a new likelihood approximation method to speed u p GP\nregression called the sparse pseudo input GP (SPGP) method. This m odel has a\nsophisticated likelihood approximation. Based on this SPGP method, t he train-\ning conditional can be taken as fully independent data, which leads to the fully\nindependent training conditional (FITC) approximation method [9]. I t removes\nall the dependencies between latent variable function values, which means the\nvalues of the latent variable function are conditionally fully independe nt of given\nthe inducing points Z. By using a diagonal matrix, the FITC prior based on the\ninducing points can be written as\nqFITC(f|fz) =N/parenleftBig\nKX,ZK\u22121\nZ,ZZ,diag[KX,X\u2212QX,X]/parenrightBig\n, (8)\nwhere diag[ .] denotes a diagonalmatrix and QX,X/definesKX,ZK\u22121\nZ,ZKZ,X. The prior\napproximationsrecoverthefullGPwhenthesizeofinducingpoints m=n.How-\never, this con\ufb01guration is not the global optimum when maximising log qFITC,\nwhich makes them philosophically troubling. Since the inducing point migh t be\ngreedilyselectedonebyone,learninginducingpointsviaoptimising(6) maypro-\nduce poor predictions. These issues will be addressed by the poste rior approx-\nimations reviewed below. However, compared to the posterior appr oximation,\nFITC will return better error-bar estimates. This method o\ufb00ers g ood accuracy\nand low computational cost.\n3.2 Posterior Approximations\nA sparse GP that utilises inducing variables must select the inducing po ints and\nthe kernel parameters [10]. Unfortunately, this method has two d isadvantages\nwhen searching for hyperparameters and inducing points. Due to t he change\nin the prior, the continuous optimisation of the maximum likelihood equa tion\n(6) for the location of inducing point zcannot approximate the GP model.\nFurthermore, over\ufb01tting is possible because the maximum likelihood is highly\nparametric in this situation (due to the extra hyperparameters as the locations\nof inducing points). When jointly optimising the matrix QX,X, since the prior\nhas changed, continuous optimisation ofmaximum likelihood for inducin g points\nzcannot yield a reliable estimate of the exact GP model.Review of Recent Advances in Gaussian Process Regression Me thods\nTitsias [11] presents a variational free-energy (VFE) method bas ed on set-\nting the variational parameters as inducing inputs rather than tra ining the in-\nducing inputs as new parameters as subset regression GP approac hes. Using\nthe greedy selection rules, the inducing points are chosen by minimisin g the\nKullback-Leibler divergence between the variational distribution an d the exact\nposterior distribution over the latent variable function values. It r esults in the\nevidence lower bound:\nlogp(y)\u2265logN/parenleftbig\ny|0,KXZK\u22121\nZZK\u22a4\nXZ+\u03c32I/parenrightbig\n\u22121\n2\u03c32tr(KXX\u2212QXX).(9)\nBy including extra variational parameters, a more computationally s calable up-\nper bound can be reached [12]. Matthew bridges the gap between t he variational\ninducing-pointsframeworkandthemoregeneralKLdivergencebe tweenstochas-\ntic processes and applies a standard variational bound L=Llower+KL(Q\u221dbardblP).\nwhereQis a variational GP approximation and Pis the true posterior GP.\nThe sparse variational Gaussian process (SVGP) based on inducing points has\nbeen further enhanced recently. In [13], a new variational inferen ce framework is\nproposed. The GP prior is decomposed as the sum of a low-rank appr oximation\nusing inducing points and a full-rank residual process. This method r educes the\ncomplexity of SVGP, which still scales cubically with the number of induc ing\npoints. Adam [14] proposes an improvement inference method for 1-dimensional\ninput space to combine the advantages of both sparse GP approxim ation and\nstate-space model representation, which results in a novel repr esentation of in-\nducing features as the state space components. The computatio nal complexity\ngrows linearly with the number of data and inducing points. Moreover , the vari-\national parameters that need to be optimised are also reduced. He nsman and\nDutordoir [15,16] attempt to improve the scalability of SVGP from a d i\ufb00erent\nangle. Instead of reducing the cubic complexity with the number of in ducing\npoints, it generates the inducing variables by projecting the GP ont o the Fourier\nbasis, thus obtaining inducing variables with higher global informative ness on\nthe predictions and reducing the number of inducing points needed f or SVGP.\nFurthermore,thediagonalcovariancematricesareobtained,wh ichfully bypasses\nthe needto computeexpensivematrixinverses.Forchoosingthein ducing points,\nBurt [17] discusses the minimum inducing point number to satisfy the g rowth\nof the data size. Uhrenholt [18] proposes a probabilistic paradigm f or balancing\nthe capacity and complexity of sparse Gaussian processes.\n4 Structured Sparse Approximation\nThe special structure of the covariancematrix can directly accele rate solving the\nmatrix inversion. It can be achieved through fast matrix-vector m ultiplication\n(MVM). When the covariance matrix has some algebraic structures , such as the\nKronecker structure, the MVMs can provide massive scalability. He re we will\nintroduce how to use this feature to improve the scalability of the st andard GP.Chenyi Lyu, Xingchi Liu and Lyudmila Mihaylova\n4.1 Kronecker and Toeplitz Structures\nAssume the multi-dimensional input xis on the Cartesian grid namely, x\u2208\nX1\u00d7\u00b7\u00b7\u00b7\u00d7XDwhereDis the dimension of the input space. The product kernel\nfunction across grid dimensions can be written as\nk(xi,xj) =D/productdisplay\nd=1k/parenleftBig\nx(d)\ni,x(d)\nj/parenrightBig\n. (10)\nBased on properties of the Kronecker product, the inverse of th e covariance\nmatrix can be e\ufb03ciently found from K1,...,Kd:\nK\u22121=/circlemultiplydisplayD\nd=1K\u22121\nd. (11)\nThese properties can be utilised to reduce the computational reso urces for the\ndata without noise. The calculation process would be more complex fo r the data\nset with noise. Since/parenleftbig\nK+\u03c32I/parenrightbig\ncannot be transferred to Kronecker structure.\nSpeci\ufb01cally, the covariance matrix K in Kronecker structure can be fast eigende-\ncomposed with low computational cost. For the one-dimensional tr aining input\nsituation, Toeplitz can be applied, which counteracts Kronecker\u2019s s hortcoming.\nWhen the inputs xis on a regularlyone-dimensionalgrid and the covariancema-\ntrixKis calculated from the stationary covariance kernel, the covarianc e matrix\ncan be written as k(x,x\u2032) =k(x\u2212x\u2032). The Toeplitz matrices have constant di-\nagonals: Ki,j=Ki+1,j+1=k(xi\u2212xj) and can be embedded into the circulant\nmatrices for the fast MVMs as the Kronecker structure [19]. It ca n reduce com-\nputational complexity to O(nlogn) and memory requirements to O(n) through\nthe exploitation of Toeplitz structure induced in the covariance mat rix, for any\nstationary kernel.\n4.2 Structured Kernel Interpolation Method\nThe inducing point-based approach combined with a sparse approxim ation is\npopular since it can be applied outside the data set without requiring a ny knowl-\nedge of its structure. However, in a large data set, when the numb er of inducing\npoints is mless than O/parenleftBig\nlogDn/parenrightBig\n, the predictive accuracy cannot be guaran-\nteed [17]. On the other hand, structure exploiting approaches are compelling\nbecause they provide incredible gains in scalability, with essentially no lo sses in\npredictive accuracy. However, this method requires the input dat a on the grid,\nwhich makes this approach inapplicable.\nWilson [20] outputs a SKI method to solve the limitation where the input\npoints are not on the girds. This approach allows us to approximate t hen\u00d7\nmmatrixKX,Zby interpolating on the m\u00d7mcovariance matrix KZ,Z. For\ninstance, to estimate k(xi,zj) for input point xiand inducing point zj, Then\n\ufb01nd two inducing points zaandzbwhich most closely bound xi:za\u2264xi\u2264zb\n. The kernel function can be written as\nk(xi,zj) =wik(za,zj)+(1\u2212wi)k(zb,zj), (12)Review of Recent Advances in Gaussian Process Regression Me thods\nwherewidenotes the linear interpolation weights and (1 \u2212wi) represents the\nrelative distances from xito points zaandzb. Theoretically, all the interpolation\nstrategies can be applied in this method, such as bilinear interpolation or cubic\ninterpolation [20].\nAfter the interpolation, same as the subset of regression, we set KX,Z\u2248\nWKZ,Z, whereWrepresent a n\u00d7minterpolation weights matrix. In this case\nthe weight matrix Wcan be extremely sparse. For example if it is a local linear\ninterpolation, Wonly contains 2 non-zero entries per row. Substituting this new\nexpression for KX,Zback to the SoR approximation for KX,Xthe equation can\nbe written as KX,X\u2248WKZ,ZW\u22a4whereKZ,Z\u2208Rm\u00d7mis the kernel matrix\nfor the set of mpoints on the dense d-dimensional grid. Plugging this approx-\nimation into the GP inference. Then, K\u22121ycan be solved by linear conjugate\ngradients, since MVMs only cost O(n). The detailed proof and introduction\nare shown in Chapter 5 of Saatchi [9]. Overall, in the Kronecker stru cture case\nthis equation only requires O/parenleftbig\nDm1+1/D/parenrightbig\n(Dis the dimension of the input and\nD >1) computations and O/parenleftbig\nn+Dm2/D/parenrightbig\nstorage [19], and the Toeplitz re-\nquireO(mlogm) operations and O(m) storage, where mis the number of grid\ndata points. However, based on the conjugate gradient method, the SKI method\nframe struggles with numerical instabilities in learning kernel hyperp arameters\nandpoortestlikelihoods.Flaxman[21]proposeanewscalableKrone ckermethod\nfor Gaussian processes with non-Gaussian likelihoods. Maddox [22] also raises a\nprescription for the conjugate gradient optimises to correct the se issues.\n5 Hierarchical Matrix-based Approximation\nBesides the inducing point-based methods, another GP method bas ed on the\nHODLR structure proposed in [6] has shown a very strong potentia l to solve the\nlarge data. Here we brie\ufb02y introduce how this structure can be inte grated into\nGP.\n5.1 Structure of the HODLR Matrix\nFirstly we will introduce the HODLR matrices [23]. Many mathematicians de-\nveloped strategies to rearrange the matrix to handle the big, dens e covariance\nmatrix. The HODLR matrix, as a sparse representation of the matr ix, is one of\nthe appealing structures to solve the covariance matrices. The HO DLR matrix\nhas many versions or variants with di\ufb00erent low-rank approximation methods.\nHowever, its main structure remains the same, built via a recursive b lock parti-\ntion [24]. This method aims to do the low-rankapproximationto the o\ufb00- diagonal\nblocks and remains the diagonal parts. According to the k-dimensio nal tree, sort\nthe data points recursively [25]. Here is an example of a two-level de composition\nin the HODLR matrix. A real symmetric matrix K\u2208Rn\u00d7ncan be decomposed\ninto a two-level HODLR matrix:\nK=/bracketleftbiggK1U1VT\n1\nV1UT\n1K2/bracketrightbigg\n, (13)Chenyi Lyu, Xingchi Liu and Lyudmila Mihaylova\nwhere the K1andK2are then/2j\u00d7n/2jdiagonal block matrices from the\noriginal matrix KandU(j),V(j)matrices are n/2j\u00d7rmatrices with r\u226an.j\nis the level of decomposition which are 2 in this example and rank ris depends\non the desired accuracy of the low-rank approximation. A higher ra nk results in\nless precision loss and a higher computational cost. The most signi\ufb01c ant step in\nconstructing a HODLR matrix is compressing the o\ufb00-diagonal blocks into low-\nrank small matrices U,V. Moreover, these \u201ctall\u201d and \u201cthin\u201d matrices are the\nmost time-consuming step in the factorisation of the HODLR method . The most\npopular decomposing method is the singular value decomposition (SVD ) [26].\nHowever, SVD is a very expensive computation method. For the squ are matrix\ninn\u00d7n, the SVD method costs O(n3) since it computes the whole dense matrix.\nAlthough it leads to reliable results, applying this method to accelerat e the GP\nis meaningless. Recently, in papers [23], more aggressive strategies have been\nproposed, such as partially pivoted adaptive cross approximation [ 27] or some\nanalytical techniques such as Chebyshev interpolation [28], which ca n further\nreduce computational costs to O(rn). This method allows us to construct the\nHODLR matrix in a linear scaling with the size of the training data [29]. In stead\nof the block-box inducing point model, the low-rank approximation giv es a rea-\nsonable approximation based on the spectrum of the covariance ma trix. The\n\ufb02exible choice of the HODLR matrix level and the low-rank approximat ion\u2019s\nprecision loss allows the model to be more e\ufb00ective in practical proble ms.\n5.2 Fast Solving Algorithm for Gaussian Process\nAfter constructing the HODLR matrix, there are di\ufb00erent strate gies to solve the\ninverse and determinant by HODLR matrix. The \ufb01rst strategy is the continuous\nmultiplication method [6,25]. In the case of a one-level factorization , we can\neasily write down the computation. Let the matrix Kwith the only step in the\ndecomposition is to factor out the terms K1,K2, giving:\nK=/bracketleftbiggK10\n0K2/bracketrightbigg/bracketleftbiggIn/2K\u22121\n1UVT\nK\u22121\n2VUTIn/2/bracketrightbigg\n. (14)\nThe needed computational time is proportional to the inverse of th e dense block\ndiagonal matrix to the correspondingrows in the remaining factor. Furthermore,\nsince the matrix UVTis low-rank, so is K\u22121\n1UVT. Unfortunately, a one-level\nfactorization such as this is still quite expensive: it required the dire ct inversion\nofK1andK2, each of which are n/2\u00d7n/2 matrices. The procedure must be\nrecursiveacrosslog nlevelstoachieveanearlyoptimalalgorithm.At \ufb01rstglance,\nit may look as though the computation of inverse is still expensive and will scale\nasO/parenleftbig\nn3/8/parenrightbig\n. However,ifthe inversesof K1,K1areknownand U,Varelow-rank\nmatrices, then the inverse of the full matrix can be computed rapid ly using the\nSherman-Morrison-Woodbury formula. By apply this formula to diag onal block\nof each level we will have the factorization equation K=KlKl\u22121\u00b7\u00b7\u00b7K1K0,\nwhere the lis the level of the HODLR matrix [6]. Since each term is block\ndiagonalor a block diagonallow-rankupdate to the identity matrix, eachinverseReview of Recent Advances in Gaussian Process Regression Me thods\nfactorization can be computed in O(nlogn) time, the total computational to\nsolve the linear system costs O/parenleftbig\nnlog2(n)/parenrightbig\n.\nThe second method relies on the Cholesky factorisation of the HODL R ma-\ntrix [30]. Due to low-rank o\ufb00-diagonal parts, computational costs can be signi\ufb01-\ncantly reduced. Ballani [31] shows the detailed calculation process f or Cholesky\ndecomposition which only results in O/parenleftbig\nnlog2(n)/parenrightbig\ncosts. Linear systems Ax=b\nwith the lower triangular HODLR matrix require additional cost of O(nlog(n)).\nO(n) to sum the diagonal elements of the determinant. Besides the fas t inverse\nand determinant, the hyperparameters are learned by maximising t he marginal\nlikelihood. Most optimisation methods, such as gradient estimation, r equire \ufb01rst\nand second-orderderivatives. The Cholesky decomposition with su ch a tractable\nstructure A=LL\u22a4also saves time for the hyperparameters learning [32].\n6 Performance Comparison\nTo verify all these sparse approximate GP methods, we test them b ased on the\nMatlab toolbox GPML and HM-toolbox [25,33]. Figure 1 gives results fr om the\nsparse approximate GP methods discussed above. A one-dimension al toy exam-\nple withf(x) = 0.02x+sinc(x)+\u01ebis used in the comparisonwhere \u01eb\u223c N(0,0.2).\nThe + symbols represent100trainingpoints. The top circlesrepres entthe initial\nlocations of 10 inducing points, whereas the bottom triangles denot e the opti-\nmised locations of inducing points. The dot red curve represents th e predicted\nmean of the full GP. The red shaded regions visualise the 95% con\ufb01de nce inter-\nval of the predictions of a full GP. The blue curves indicate the pred icted mean\nfrom the sparse GP algorithms. The black curves represent the 95 % con\ufb01dence\ninterval for the results from the sparse GP algorithms.\nFigure 1 shows that the FITC and VFE methods give results close to t he\nfull GP. However, the FITC method will lead to over\ufb01tting due to the greedy\nselection of the inducing points. The accuracy of these methods re lies on the\nnumber of the inducing points [17]. Figure 1 also shows that the SKI p roduces\nresults with some variability due to the interpolation between the insu \ufb03cient\ninducing points. The HCFGP has a result that is very close to the full G P in\nmost situations because the only part of the approximation is contr ollable by\nthe precision loss (rank remained) of the low-rankapproximation[6]. The results\nTable 1. Computational time of the considered GP methods\nTime(s)\nSize of data Full GP FITCVFESKIHCFGP\n100 0.0180.0860.0190.1260.049\n1000 0.1190.0990.0300.1880.124\n2500 0.8750.1060.0420.5800.753\n5000 4.9760.1260.0511.4242.999\n8000 17.8050.3190.1203.0667.556Chenyi Lyu, Xingchi Liu and Lyudmila Mihaylova\nFig.1.Estimated state and its con\ufb01dence intervals\npresented in Table 1 are from 100 repeated Monte Carlo runs. Table 1 shows\nthe computational time of the compared GP methods. For the big da ta case,\nthe computation cost of full GP will be cubic growth. FITC and VFE me thods\ntakesthe leasttime, whichrequireasmall computationcostwith asm allnumber\nof inducing points. The SKI and the HODLR with the Cholesky factoris ation\nGaussian process (HCFGP) method are slower than other inducing p oint-based\nmethods due to the time taken for the interpolation and construct ion of the\nspecial matrices. The HCFGP takes less time than SKI when the data size is\nless than 2500 points.\n7 Conclusions\nThis paper presents recently developed global sparse GP methods , including\nthe classic inducing point-based, structured approximation and ne w HODLR-\nbased methods. Understanding these methods will help solve the pr actical high-\ndimensional big data problems and assess the impact of uncertaintie s on the\ndevelopedsolutions.Theperformanceofthe consideredGPmetho dsisevaluated\nover a di\ufb00erent number of data and the e\ufb03ciency of the factorisat ion based\nmethods is demonstrated. Factorisation based GP methods can be used in real\ntime applications.Review of Recent Advances in Gaussian Process Regression Me thods\nAnother advantage of the considered sparse approximation GP me thods is\nthat they can still provide trustworthy solutions and assess the im pact of uncer-\ntainties on the developed solutions.\nAcknowledgements: We are grateful to UK EPSRC for funding this work\nthrough EP/T013265/1 project NSF-EPSRC:ShiRAS. Towards Saf e and Re-\nliable Autonomy in Sensor Driven Systems and UK Research and Innov ation\n(UKRI) Trustworthy Autonomous Systems (TAS) programme [EPS RC Ref:\nEP/V026747/1]. This work was also supported by the National Scien ce Foun-\ndation under Grant USA NSF ECCS 1903466. For the purpose of ope n access,\nthe authors have applied a Creative Commons Attribution (CC BY) lice nce to\nany Author Accepted Manuscript version arising.\nReferences\n1. H. Liu, Y.-S. Ong, X. Shen, and J. Cai, \u201cWhen Gaussian proce ss meets big data:\nA review of scalable GPs,\u201d IEEE Transactions on Neural Networks and Learning\nSystems, vol. 31, no. 11, pp. 4405\u20134423, 2020.\n2. C. K. Williams and C. E. Rasmussen, Gaussian Processes for Machine Learning .\nMIT Press Cambridge, MA, 2006.\n3. J. Quinonero-Candela and C. E. Rasmussen, \u201cA unifying vie w of sparse approx-\nimate Gaussian Process regression,\u201d The Journal of Machine Learning Research ,\nvol. 6, pp. 1939\u20131959, 2005.\n4. J. Quinonero-Candela, C. E. Rasmussen, and C. K. Williams , \u201cApproxima-\ntion methods for Gaussian Processes regression,\u201d in Large-scale kernel machines ,\npp. 203\u2013223, MIT Press, 2007.\n5. J. Q. Shi and T. Choi, Gaussian Process Regression Analysis for Functional Data .\nNew York: Chapman and Hall CRC Press, 2011.\n6. S. Ambikasaran, D. Foreman-Mackey, L. Greengard, D. W. Ho gg, and M. O\u2019Neil,\n\u201cFastdirectmethodsfor Gaussian processes,\u201d IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence , pp. 252\u2013265, 2015.\n7. Z. Dai, M. A. \u00b4Alvarez, and N. D. Lawrence, \u201cE\ufb03cient modeling of latent inf orma-\ntion in supervised learning using Gaussian Processes,\u201d CoRR, 2017.\n8. E. Snelson and Z. Ghahramani, \u201cSparse Gaussian Processes using pseudo-inputs,\u201d\nAdvances in Neural Information Processing Systems , vol. 18, p. 1257, 2006.\n9. Y. Saat\u00b8 ci, R. D. Turner, and C. E. Rasmussen, \u201cGaussian Pr ocess change point\nmodels,\u201d in Proc. of the International Conference on Machine Learning , 2010.\n10. D. M. Blei, A. Kucukelbir, and J. D. McAuli\ufb00e, \u201cVariation al inference: A review\nfor statisticians,\u201d Journal of the American Statistical Association , vol. 112, no. 518,\npp. 859\u2013877, 2017.\n11. M. Titsias, \u201cVariational learning of inducing variable s in sparse Gaussian Process-\neses,\u201d in Arti\ufb01cial Intelligence and Statistics , PMLR, 2009.\n12. A. G. d. G. Matthews, J. Hensman, R. Turner, and Z. Ghahram ani, \u201cOn sparse\nvariational methods and the Kullback-Leibler divergence b etween stochastic Pro-\ncesseses,\u201d in Arti\ufb01cial Intelligence and Statistics , pp. 231\u2013239, PMLR, 2016.\n13. J. Shi, M. Titsias, and A. Mnih, \u201cSparse orthogonal varia tional inference for Gaus-\nsian processes,\u201d in Proc. of the International Conf. on AI and Statistics , pp. 1932\u2013\n1942, PMLR, 2020.Chenyi Lyu, Xingchi Liu and Lyudmila Mihaylova\n14. V. Adam, S. Eleftheriadis, A. Artemev, N. Durrande, and J . Hensman, \u201cDoubly\nsparse variational Gaussian processes,\u201d in International Conference on Arti\ufb01cial\nIntelligence and Statistics , pp. 2874\u20132884, PMLR, 2020.\n15. J.Hensman,N.Durrande,A.Solin, et al.,\u201cVariational fourierfeaturesfor Gaussian\nprocesses.,\u201d J. Mach. Learn. Res. , vol. 18, no. 1, pp. 5537\u20135588, 2017.\n16. V. Dutordoir, N. Durrande, and J. Hensman, \u201cSparse gauss ian processes with\nspherical harmonic features,\u201d in Proc. of the International Conference on Machine\nLearning , pp. 2793\u20132802, PMLR, 2020.\n17. D. Burt, C. E. Rasmussen, and M. Van Der Wilk, \u201cRates of con vergence for sparse\nvariational Gaussian process regression,\u201d in Proc. of the International Conference\non Machine Learning , pp. 862\u2013871, PMLR, 2019.\n18. A. K. Uhrenholt, V. Charvet, and B. S. Jensen, \u201cProbabili stic selection of induc-\ning points in sparse gaussian processes,\u201d in Uncertainty in Arti\ufb01cial Intelligence ,\npp. 1035\u20131044, PMLR, 2021.\n19. A. G. Wilson, E. Gilboa, J. P. Cunningham, and A. Nehorai, \u201cFast kernel learning\nfor multidimensional pattern extrapolation.,\u201d in NIPS, pp. 3626\u20133634, 2014.\n20. A. Wilson and H. Nickisch, \u201cKernel interpolation for sca lable structured Gaussian\nProcesseses (KISS-GP),\u201d in Proc. of the International Conf. on Machine Learning ,\npp. 1775\u20131784, PMLR, 2015.\n21. S. Flaxman, A. Wilson, D. Neill, H. Nickisch, and A. Smola , \u201cFast kronecker in-\nference in gaussian processes with non-gaussian likelihoo ds,\u201d inProc. of the Inter-\nnational Conference on Machine Learning , pp. 607\u2013616, PMLR, 2015.\n22. W. J. Maddox, S. Kapoor, and A. G. Wilson, \u201cWhen are iterat ive gaussian pro-\ncesses reliably accurate?,\u201d arXiv preprint arXiv:2112.15246 , 2021.\n23. A. Aminfar, S. Ambikasaran, and E. Darve, \u201cA fast block lo w-rank dense solver\nwith applications to \ufb01nite-element matrices,\u201d Journal of Computational Physics ,\nvol. 304, pp. 170\u2013188, 2016.\n24. S. B\u00a8 orm, L. Grasedyck, and W. Hackbusch, \u201cHierarchical matrices,\u201d Lecture notes ,\nvol. 21, p. 2003, 2003.\n25. S. Massei, L. Robol, and D. Kressner, \u201chm-toolbox: Matla b software for HODLR\nand HSS matrices,\u201d SIAM J. on Scient. Comp. , vol. 42, no. 2, pp. C43\u2013C68, 2020.\n26. H. Andrews and C. Patterson, \u201cSingular value decomposit ion (svd) image coding,\u201d\nvol. 24, no. 4, pp. 425\u2013432, 1976.\n27. Y. Liu, W. Sid-Lakhdar, E. Rebrova, P. Ghysels, and X. S. L i, \u201cA parallel hier-\narchical blocked adaptive cross approximation algorithm, \u201dThe International J. of\nHigh Performance Computing Applications , vol. 34, no. 4, pp. 394\u2013408, 2020.\n28. C. E\ufb00enberger and D. Kressner, \u201cChebyshev interpolatio n for nonlinear eigenvalue\nproblems,\u201d BIT Numerical Mathematics , vol. 52, no. 4, 2012.\n29. S. Ambikasaran, Fast algorithms for dense numerical linear algebra and appl ica-\ntions. Stanford University, 2013.\n30. C. Lyu, X. Liu, and L. Mihaylova, \u201cE\ufb03cient factorisation -based Gaussian pro-\ncess approaches for online tracking,\u201d in Proc. of the 25th International Conf. on\nInformation Fusion , IEEE, 2022.\n31. J. Ballani and D. Kressner, \u201cMatrices with hierarchical low-rank structures,\u201d in\nExploiting hidden structure in matrix computations: algor ithms and applications ,\npp. 161\u2013209, Springer, 2016.\n32. A. Litvinenko, Y. Sun, M. G. Genton, and D. E. Keyes, \u201cLike lihood approximation\nwith hierarchical matrices for large spatial datasets,\u201d Computational Statistics &\nData Analysis , vol. 137, pp. 115\u2013132, 2019.\n33. C. E. Rasmussen and H. Nickisch, \u201cGaussian processes for machine learning (gpml)\ntoolbox,\u201d The Journal of Machine Learning Research , vol. 11, pp. 3011\u20133015, 2010.This figure \"HOLRD.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.08112v1"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "## 1. Abstract:\nThe paper explores a novel approach to developing a foundation model for network traffic analysis employing a graph-based paradigm. Unlike previous transformer model proposals that focus on tokenized hex-level packet data, this study introduces an efficient flow-level graph representation of network traffic. The model utilizes a self-supervised link prediction pretraining task to learn the spatio-temporal dynamics of the network. Through a few-shot learning experiment, the model shows a 6.87% performance increase over training from scratch in intrusion detection, traffic classification, and botnet classification tasks, underscoring its potential as a scalable, efficient foundational model for network traffic analysis.\n\n## 2. Introduction:\n### Context:\n- Foundation models have successfully been applied in various fields, notably in natural language processing.\n- Network traffic analysis requires understanding complex dynamics which can benefit from such foundation models.\n\n### Research Questions/Hypotheses:\n- Can a graph-based approach at the flow level efficiently capture network traffic dynamics?\n- How effective is this method compared to models using tokenized hex-level packet data?\n\n### Literature Review and Positioning:\n- Existing models often use transformer architectures on tokenized, encrypted packet data, assuming that encrypted traffic exhibits structural patterns that can be decoded via language models (e.g., BERT, T5).\n- This study proposes a graph-based flow-level approach, arguing it is more efficient and captures network relationships better than sequential tokenization.\n- The proposed model uses a spatio-temporal Graph Neural Network (GNN) architecture combined with a link prediction pretraining task to imbibe network traffic dynamics.\n\n## 3. Related Work:\n- Prior works, such as Lens, ET-BERT, and netFound, focused on transformer models with self-supervised tasks (e.g., token prediction, masked span prediction, packet order prediction) on tokenized hex-level packet data.\n- Limitations identified in these approaches:\n  1. Tokenized models fail to capture network structure accurately.\n  2. Packet-level models are less efficient than flow-level models.\n  3. Transformer models demand significant computational resources due to their complexity.\n\n## 4. Method:\n### Proposed Approach:\n- **Graph Representation:**\n  - Network traffic is represented using dynamic spatio-temporal graphs at the flow level.\n  - Nodes represent flows and IP addresses, linked by bidirectional edges, capturing spatial relationships.\n  - Temporal edges are added to capture the temporal ordering of flows.\n\n- **Model Architecture:**\n  - The graph neural network (GNN) model employs the GraphSAGE framework extended for temporal dynamics.\n  - The model contains approximately 732,000 parameters, significantly smaller than transformer-based models (1% of ET-BERT or netFound).\n\n- **Pretraining Task:**\n  - **Link Prediction:**\n    - Real and fake edges are used, where the GNN model classifies each edge.\n    - This task aims to teach the model the network's spatial and temporal dynamics.\n\n- **Finetuning Process:**\n  - Once pretrained, the model is finetuned on downstream tasks using limited labeled data.\n  - The sequence involves initializing the model with pretrained weights and attaching fully connected layers for flow classification tasks.\n\n## 5. Experimental Setup:\n### Datasets:\n- **Pretraining:** UNSW-NB15, ToN-IoT (unlabeled versions).\n- **Finetuning/Downstream Tasks:** BoT-IoT for intrusion detection, UPC-COMNET14 for traffic classification, and CTU13 for botnet classification.\n\n### Hardware and Software:\n- No specific hardware mentioned; typically, such setups use GPUs for training.\n- Software frameworks likely include PyTorch/DeepGraphLibrary for implementing GNNs.\n\n### Evaluation Metrics:\n- Accuracy, weighted F1 score, and macro F1 score for classification performance.\n- Metrics measure generalization capabilities via a few-shot learning framework using limited training epochs.\n\n### Baselines and Comparison:\n- Models trained from scratch are compared with finetuned models from the pretrained base, considering few-shot (limited data and training epochs) and optimal performance (full data and extensive training).\n\n## 6. Results and Analysis:\n### Empirical Findings:\n- **Intrusion Detection (BoT-IoT):** Pretrained models showed faster convergence and superior performance across accuracy, weighted F1, and macro F1 metrics.\n- **Traffic Classification (UPC-COMNET14):** Results showed improvement though less pronounced than intrusion detection, suggesting task-specific transfer learning capabilities.\n- **Botnet Detection (CTU13):** Similarly to intrusion detection, pretrained models exhibited strong generalization and performance gains.\n\n### Detailed Descriptions:\n- **Graphs and Figures:** Training loss, validation loss, and performance metrics are tracked and visualized to show model convergence and generalization speed.\n- **Ablation Studies:** Highlight model effectiveness in transferring learned knowledge to downstream tasks, even with minimal pretraining data.\n\n### Anomalous Results:\n- UPC-COMNET14\u2019s lower generalization improvement indicates the pretraining dataset's semantic distance affects transfer learning. Suggests increasing pretraining dataset variety for robustness.\n\n## 7. Discussion:\n### Interpretation:\n- **Pretraining Efficiency:** Demonstrates successful transfer of network dynamics understanding from pretrained to finetuned models.\n- **Implications:** This method shows potential for producing compact, efficient network traffic foundation models suitable for rapid deployment in diverse network environments.\n- **Limitations and Biases:** The scope of pretraining data is limited, affecting generalization in some contexts (e.g., UPC-COMNET14 traffic classification).\n\n## 8. Conclusion:\n### Key Contributions:\n- Introduces a graph-based approach for representing and analyzing network traffic at the flow level.\n- Demonstrates efficient learning of network dynamics via self-supervised link prediction on dynamic spatio-temporal graphs.\n- Shows significant performance improvement in downstream tasks through few-shot learning.\n\n### Future Research:\n- **Model Scaling:** Expand pretraining with larger, diverse datasets and additional pretraining tasks.\n- **Model Complexity:** Incorporate attention mechanisms for refined aggregation.\n- **Extended Applications:** Explore the model's applicability to graph and edge-level tasks beyond flow classification.\n\nThis detailed summary encapsulates the essence and specifics of the paper\u2019s methodology, findings, and implications, providing a roadmap for replicating the study or building upon the proposed approach."}, {"role": "assistant", "content": "Towards a graph-based foundation model for network\ntraffic analysis\nLouis Van Langendonck\nlouis.van.langendock@estudiantat.upc.edu\nUniversitat Polit\u00e9cnica de Catalunya\nBarcelona, SpainIsmael Castell-Uroz\nismael.castell@upc.edu\nUniversitat Polit\u00e9cnica de Catalunya\nBarcelona, Spain\nPere Barlet-Ros\npere.barlet@upc.edu\nUniversitat Polit\u00e9cnica de Catalunya\nBarcelona, Spain\nABSTRACT\nFoundation models have shown great promise in various\nfields of study. A potential application of such models is\nin computer network traffic analysis, where these models\ncan grasp the complexities of network traffic dynamics and\nadapt to any specific task or network environment with min-\nimal fine-tuning. Previous approaches have used tokenized\nhex-level packet data and the model architecture of large\nlanguage transformer models. We propose a new, efficient\ngraph-based alternative at the flow-level. Our approach rep-\nresents network traffic as a dynamic spatio-temporal graph,\nemploying a self-supervised link prediction pretraining task\nto capture the spatial and temporal dynamics in this network\ngraph framework. To evaluate the effectiveness of our ap-\nproach, we conduct a few-shot learning experiment for three\ndistinct downstream network tasks: intrusion detection, traf-\nfic classification, and botnet classification. Models finetuned\nfrom our pretrained base achieve an average performance\nincrease of 6.87% over training from scratch, demonstrat-\ning their ability to effectively learn general network traffic\ndynamics during pretraining. This success suggests the po-\ntential for a large-scale version to serve as an operational\nfoundational model.\nCCS CONCEPTS\n\u2022Security and privacy \u2192Network security ;\u2022Comput-\ning methodologies \u2192Artificial intelligence ;Model de-\nvelopment and analysis ;\u2022Networks\u2192Network monitor-\ning.\nKEYWORDS\nGraph Neural Network, Foundation Model, Traffic Analysis\n1 INTRODUCTION\nRecent proposals considering large-scale pretraining for task-\nagnostic foundation models have proven transformative inthe academic world and beyond. A variant of such model\napplied to computer network traffic could provide similar\nimpact, though a successful implementation has yet to be put\nforth. Such a network traffic foundation model should under-\nstand complex network traffic dynamics and thus effortlessly\nspecialize to any downstream task and any network setting\nwith minimal fine-tuning.\nThe key components of any foundation model are: a con-\nsistent data representation and model architecture that ef-\nficiently captures the dynamics of the application field, an\ninformative self-supervised pretraining task enabling gen-\neral knowledge acquisition, and using large amounts of data\nto scale up the pretraining step. Previous approaches to de-\nveloping a network traffic foundation model employ large\nlanguage transformer models to operate on tokenized hex-\nlevel packet data. In this paper we put forth a new, efficient\ngraph-based alternative for the first two key elements (data\nrepresentation & model architecture and pretraining task),\nhence laying the foundation for an eventual large-scale im-\nplementation.\nWe argue in favour of a graph-based approach for the\nfollowing reasons: (\ud835\udc56)Graphs are well-suited to represent\nthe structural characteristics of network traffic, capturing\nthe relationships and interactions between different nodes\nand connections more accurately than sequential tokeniza-\ntion [ 3,11,16].(\ud835\udc56\ud835\udc56)Working at the flow-level is more effi-\ncient and informative because it aggregates data, providing\na clearer and more meaningful representation of network\ntraffic compared to the limited insights available from en-\ncrypted packet-level data [ 5,11].(\ud835\udc56\ud835\udc56\ud835\udc56)The complex inter-\ndependencies and long-term relationships in packet-level\ndata requires extremely large and resource-intensive models\nfor effective processing. In contrast, GNNs are more efficient\nto train and deploy due to geometrical reuse of parameters,\nwith our model being on average only 1% the size of other net-\nwork traffic foundation model proposals [4, 8, 9, 14, 22, 23].arXiv:2409.08111v1  [cs.LG]  12 Sep 2024Van Langendonck et al.\nTo show that the method we put forth indeed effectively\nlearns general network traffic behaviour through pretrain-\ning, we set up a few-shot learning experiment for three dis-\ntinct down-stream network tasks: intrusion detection, traffic\nclassification and botnet classification. For each task, with\nlimited data and training epochs, one GNN model is trained\nfrom scratch while another GNN model is finetuned from\nour pretrained base, which is trained on two small unlabeled\ndatasets considering a completely different network setting.\nThe results show that the models using our pretrained base\nshow an average performance increase of 6.87 % compared to\ntraining from scratch. Although only a small amount of pre-\ntraining data is used, completely independent from the data\nused for fine-tuning, we conclude that knowledge is effec-\ntively transferred from pretrained base to finetuned models\nin unseen down-stream tasks, underlining the potential for\na large-scale foundation model.\nThe rest of the paper is organized as follows: Section 2\nreviews the background and the related work. Section 3\ndescribes the characteristics of our proposed model while\nSection 4 presents the few-shot experiment and its results.\nSection 5 concludes the paper.\n2 RELATED WORK\nThe few existing proposals for a network traffic founda-\ntion model typically consider a large language transformer\nmodel operating on tokens of raw, encrypted packet-level\ndata in hex-format [ 8,14,22]. They rely on the observa-\ntion that encrypted traffic is not perfectly random and both\nimplicit and structural patterns exist [ 14,21].Lens [22], pro-\nposed by Wang, et al., employs the T5 model, a popular\ntext-to-text encoder-decoder transformer model. Three self-\nsupervised pretraining tasks are proposed: masked span\nprediction, packet order prediction and homologous traf-\nfic prediction. They show that their finetuned models often\nperform better on several down-stream tasks than other\npacket-level models. ET-BERT [14], put forth by Lin, et al.,\ninstead use as model architecture the well know language\nfoundation model BERT (bidirectional Encoder Represen-\ntations from Transformers). They consider two pretraining\ntasks: token prediction and origin prediction. Their finetuned\nmodels generally outperform the state-of-the-art in differ-\nent encrypted classification tasks. netFound [8], proposed by\nGuthula, et al., increases complexity compared to the afore-\nmentioned works by accounting for mixed granularity levels\n(packets, bursts, flows) in the model architecture. Addition-\nally to tokenization, several statistical features are conjointly\nfed to the BERT-inspired model. Extra aggregation layers\nenable final representations at different granularity levels.\nThe only pretraining task they consider is token prediction.They outperform most other classification models operating\non packet-level data.\nAlthough these solutions are promising, we claim that\nthey fall short in several regards: (\ud835\udc56)They fail to capture\nthe structural characteristics of network traffic, as sequential\ntokenization does not accurately model relationships and\ndependencies between network elements. (\ud835\udc56\ud835\udc56)Working at\nthe packet level is less efficient and informative compared to\nstrictly working at the flow level, as encrypted packet data\noften lacks context and useful features. (\ud835\udc56\ud835\udc56\ud835\udc56)The complex\ninter-dependencies and long-term relationships in packet-\nlevel data require extremely large and resource-intensive\nmodels, making them impractical for effective processing.\n3 PROPOSAL\nIn this section we introduce the architecture and training\nroutine of the proposed computer network traffic founda-\ntion model. Figure 1 provides a high-level overview of the\nproposed workflow. During unsupervised pretraining, we\nuse unlabeled, \"cheap\" network data to teach a graph-based\nmodel the essential behaviour characterizing network traffic.\nContinuing from the pretrained model weights, the finetun-\ning step consists of further supervised training on different\ndown-stream network analysis tasks, using limited labeled\ndata. We use different datasets for pretraining and for fine-\ntuning, thus avoiding data leakage and allowing for a fair\ncomparison to down-stream models trained from scratch.\nNote that each down-stream model is finetuned indepen-\ndently though all starting from the same pretrained base.\nFirst we summarize the chosen network graph representa-\ntion and corresponding graph neural network architecture.\nAfterwards we go in to detail about the proposed pretraining\ntask. Finally we describe the finetuning process.\n3.1 Model Architecture\nA schematic representation of the network graph and cor-\nresponding graph neural network model is displayed in the\nfinetuning section of the general proposal overview depicted\nin Figure 1.\nWe opt for the line-graph representation of the network\ntraffic. This entails that for every flow, we represent both\nthe flow itself and the source and destination IP address\nas a node. Both source and destination IP are connected to\nthe flow node using bidirectional spatial edges. The initial\nfeatures of the flow node are the preprocessed flow features\nwhile the IP nodes are initialized with dummy one-vectors.\nIn order to include a temporal component, we build sev-\neral small graphs spanning a few second of network traffic\nand add temporal edges between these of reoccuring nodes.\nWithin each window, temporal edges are included for in-\ndicating the order of flow nodes from the same source orTowards a graph-based foundation model for network traffic analysis\nFigure 1: Overview of proposed model and workflow. A graph neural network model is pretrained using an unsupervised\nlink-prediction task. The resulting model is used for finetuning to different downstream tasks. The Graph Neural Network\nmodel incorporates spatio-temporal information and uses both structural and feature-based information to update node\nrepresentations.\ndestination IP. Further details can be found in our previous\nwork using the same graph-building method [13].\nWe opt for a spatio-temporal graph neural network model\nto capture the dynamic nature of the proposed network\ngraph. Generally speaking, this architecture can be consid-\nered a standard heterogeneous, convolutional graph neural\nnetwork, implemented using the GraphSAGE [ 9] framework,\nthat is extended to effectively handle the temporal compo-\nnent. Again, further details can be found in our previous\nwork employing the same model [13].\nHowever, it is important to note that alternative graph\nrepresentations and model architectures could also yield\ngood results and should be considered in future work. We\nmade the considered choice of this spatio-temporal approach\nbecause of the high expressiveness of the model.\nNote that most graph neural networks are relatively com-\npact compared to transformer-based models due to the reuse\nof parameters across the graph. Our proposed model con-\ntains about 732.000parameters, which on average is about\n1% of the size of the models of other proposed network traffic\nfoundation models ( ET-BERT :\u223c110Million [ 4,14],netFound :\n\u223c220Million [8], Lens:\u223c60\u2212770Million [20, 22])3.2 Pretraining\nEffective pretraining is crucial in developing a successful\nfoundation model. The pretraining task should be self-supervised,\nonly make use of standardized, unlabeled network data to\nlearn network traffic dynamics. Several graph-based, self-\nsupervised pretraining methods have recently been proposed\nranging from context prediction to node-level feature pre-\ndiction [ 7,10,15,17]. As a proof of concept, we opt for the\nsimple task of link prediction. However, note that a large-\nscale implementation, just like in the large language setting,\nwould benefit from multiple pretraining tasks combined in a\nsingle loss function [ 1,4]. Our proposed link-prediction task\nconsiders the following steps: (\ud835\udc56)Negative Sampling: For\nany edge type, fake edges are randomly added. In each graph,\nwe choose to have an equal amount of fake edges as real\nedges(\ud835\udc56\ud835\udc56)Link Prediction: The GNN model described in\nSection 3.1 is given the binary classification task to classify\nevery edge as real or not. This teaches the model the spatial\nand temporal dynamics of the network graph, learning the\ntypical structural and feature-based relations between differ-\nent node types and across different edge types. A schematic\nrepresentation of the link-prediction task is displayed in theVan Langendonck et al.\npretraining section of the general proposal overview depicted\nin Figure 1.\n3.3 Finetuning\nThe final graph representation obtained by pretrained model\ncan now be used for efficient finetuning to a downstream\ntask in network traffic analysis. First a graph neural network\nmodel is initialized with the weights from the pretrained\nmodel. Some simple fully-connected neural network lay-\ners are connected to the final pretrained representations\nof the flow nodes. Then, the entire model is finetuned in\na supervised manner using labeled data (of e.g. a network\nintrusion detection task). The acquired understanding of net-\nwork traffic dynamics during pretraining allows for limiting\nthe amount of labeled data and training epochs, generally\nmaking the finetuning process an inexpensive process.\nIn this work we consider three network traffic analysis\ntasks: network classification, network intrusion detection\nand botnet classification. Note that although each of these\ntasks consider flow classification, the pretrained base can just\nas well be used for graph-level or edge-level classification or\nregression tasks.\n4 EVALUATION\n4.1 Downstream tasks\nAs already explained, our objective is to set up a few-shot\nexperiment, comparing generalization capabilities training\nfrom scratch to fine-tuning from our pretrained model. Since\nthe pretraining and fine-tuning datasets are strictly sepa-\nrated and consider different network settings, the pretrained\nmodel needs to learn general network dynamics in order to\nimprove performance in fine-tuning compared to training\nfrom scratch. Therefore, the experiment is well-suited to as-\nsess the potential of the pre-trained, spatio-temporal graph\nneural networks as network traffic foundation model.\nThe three down-stream tasks that we will use for the few-\nshot study are:\n\u2022Intrusion Detection Systems : An IDS is a system\ntrained to detect different network attacks such as\nDenial of Services attacks or Network Scans. These\nsystems represent a key element for network security\nexperts that use them for early-detection of possible\nservice outages or system intrusion. In particular,\nwe use three different IDS datasets publicly available:\nUNSW-NB15 [ 19],ToN-IoT [ 18]andBoT-IoT [ 12].\nThe first two of them are used during the pretraining\nphase, while the last one is used as one of the down-\nstream tasks to study. The pretraining is done on the\nunlabeled version of the datasets.Table 1: Dataset/task details\nDataset Size (raw) #Flows #Labels Year Task\nCTU13 71.7GB 262K 7 2013Intrusion\nDetection\nUPC-COMNET14 54.2GB 882K 77 2014Traffic\nClassification\nUNSW-NB15 99.1GB 2M 10 2015 Pretraining\nBoT-IoT 69.3GB 72M 10 2018Botnet\nDetection\nToN-IoT 67.6GB 30M 10 2019 Pretraining\n\u2022Traffic Classification : In computer networks, traffic\nclassification refers to the identification of each net-\nwork packet or flow with the application that gener-\nates it. While network attacks are somewhat limited\nin number, a network trace may contain hundreds or\neven thousands of different applications. In this work\nwe use the UPC-COMNET14 [ 2]private dataset ex-\nclusively for the finetuning and downstream task of\ntraffic classification problem.\n\u2022Botnet Detection : Botnets are group of infected ma-\nchines controlled by a command center computer\nwhich can remotely execute tasks inside them. Usu-\nally, an attacker uses the command center to control\nthe remote computers and perform network attacks\nto specific targets. The dataset used to study botnet\ndetection as a downstream task is the publicly avail-\nableCTU13 [6] dataset.\nAll the datasets used are Netflow variants of the original\ndatasets generated directly from network capture pcap files.\nThe features collected are the same features of bidirectional\nflows explained in our previous work available at [ 13]. Table 1\nshows a summary of the characteristics of each dataset.\n4.2 Few-shot Experiment\nIn the few-shot setting, we limit the amount of epochs used\nfor training to 50. We slowly increase the amount of samples\navailable for training, each time training two models: one\ntrained from scratch and one finetuned from the pretrained\nbase. In each training routine, we select the best model based\non the multiclass macro f1 score, since it is the most challeng-\ning metric to optimize, taking into account both precision\nand recall as well as considering each class equally (includ-\ning minority classes). Then each model is evaluated on the\nsame dataset-specific test set, using three common multiclass\nclassification metrics: accuracy, weighted f1 score and macro\nf1 score.\nFinally, serving as reference performance, for each dataset,\na single model is trained using all data available for 200\nepochs. The final metric used to evaluate performance is, for\neach of the three metrics, the percentual performance lossTowards a graph-based foundation model for network traffic analysis\nFigure 2: Few shot learning experiment. For each of\nthe downstream tasks (columns), one model is trained from\nscratch (orange) and one is finetuned from the pretrained\nmodel (blue). Each data point in the graph represents a differ-\nent model. Training epochs are limited to 50. The results are\nexpressed in percentage loss of optimal model performance\nof that metric (row) on the test set.\nwith respect to this reference performance. The results of\nthis experiment are displayed in Figure 2.\nTo compare generalization speed between training from\nscratch and finetuning within each training cycle, we look at\nthe training and validation loss curves for each of the models\ntrained in the few-shot experiment. For direct comparison,\nwe proceed to normalize the train and validation curves of\neach experiment and then average those results per epoch,\nper dataset and per training strategy (pretraining or from\nscratch). The resulting curves are plotted in Figure 3.\nThe results of the few-shot experiment clearly show that\npretraining the spatio-temporal graph neural network in-\ncreases the model\u2019s generalization speed and performance in\ndifferent down-stream tasks. For each of the datasets, for at\nleast two of the metrics, the pretrained model approaches op-\ntimal performance sooner than without pretraining. The av-\nerage difference across all datasets and metrics is 6.87 %. The\nloss curves display similar tendencies: the training curves of\nthe pretrained models minimizes faster and more profoundly\nfor all three datasets. The validation curves show strong gen-\neralization in the pretrained case for both the NF-BoT-IoT\nandNF-CTU13 datasets though not seeing any significant dif-\nference between curves in the NF-UPC-COMNET14 case. We\nFigure 3: Average normalized loss curves. For each of\nthe downstream tasks (columns), the training and validation\ncurves of the training routines of the few-shot experiment\nare normalized and then averaged at each epoch. This allows\nfor a direct comparison of convergence speed and general-\nization capabilities between the finetuned models and the\nones trained from scratch.\nbelieve this is due to this finetuning task of common network\nclassification semantically being the furthest removed from\nthe cybersecurity context of our pretraining dataset. This\nsuggests adding more volume and variety to the pretraining\ndataset used in future work.\n5 CONCLUSIONS\nThis work presented a novel, graph-based approach to build-\ning network traffic foundation models. In contrast to pre-\nvious works, our proposed method effectively incorporates\nthe structural component of network traffic as well as reduc-\ning model size by about 99 %. Via a few-shot experiment we\nshow that by pretraining on a self-supervised link-prediction\ntask using a small, out-of-context data sample, our proposed\nmodel successfully learns general network traffic dynamics,\ntransferable to any down-stream networking task. Though\nsignificant up-scaling is required, the results offer a promis-\ning direction for operational network traffic foundation mod-\nels. Looking ahead, future work will focus on increasing\nmodel complexity by including an attention-mechanism in\nmessage aggregation, increasing pretraining scale by adding\nseveral new unlabeled datasets and increasing pretraining\ncomplexity by including and combining multiple pretraining\ntasks.\nACKNOWLEDGEMENTS\nThis work was supported by the CHISTERA grant CHIST-\nERA-22-SPiDDS-02 corresponding to the GRAPHS4SEC projectVan Langendonck et al.\n(reference n \u00baPCI2023-145974-2) funded by the Agencia Es-\ntatal de Investigaci\u00f3n through the PCI 2023 call. This work\nis also supported by the Catalan Institution for Research and\nAdvanced Studies (ICREA Academia).\nREFERENCES\n[1]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot\nLearners. arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/2005.14165\n[2]Tomasz Bujlow, Valent\u00edn Carela-Espa\u00f1ol, and Pere Barlet-Ros. 2015.\nIndependent comparison of popular DPI tools for traffic classification.\nComputer Networks 76 (2015), 75\u201389. https://doi.org/10.1016/j.comnet.\n2014.11.001\n[3]Evan Caville, Wai Weng Lo, Siamak Layeghy, and Marius Portmann.\n2022. Anomal-E: A self-supervised network intrusion detection system\nbased on graph neural networks. Knowledge-Based Systems 258 (2022),\n110030.\n[4]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n2019. BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv:1810.04805 [cs.CL] https://arxiv.org/\nabs/1810.04805\n[5]Paraskevi Dimou, Jan Fajfer, Nicolas M\u00fcller, Eva Papadogiannaki,\nEvangelos Rekleitis, and Franti\u0161ek St\u0159as\u00e1k. 2020. Encrypted Traffic\nAnalysis: Use Cases and Security Challenges. https://www.enisa.\neuropa.eu/publications/encrypted-traffic-analysis\n[6]Sebastian Garcia, Martin Grill, Jan Stiborek, and Alejandro Zunino.\n2014. An empirical comparison of botnet detection methods. computers\n& security 45 (2014), 100\u2013123.\n[7]Zhengyao Gu, Diego Troy Lopez, Lilas Alrahis, and Ozgur Sinanoglu.\n2024. Always be Pre-Training: Representation Learning for Network\nIntrusion Detection with GNNs. arXiv:2402.18986 [cs.CR] https:\n//arxiv.org/abs/2402.18986\n[8]Satyandra Guthula, Navya Battula, Roman Beltiukov, Wenbo Guo, and\nArpit Gupta. 2023. netFound: Foundation Model for Network Security.\narXiv:2310.17025 [cs.NI] https://arxiv.org/abs/2310.17025\n[9]William L. Hamilton, Rex Ying, and Jure Leskovec. 2018. Inductive\nRepresentation Learning on Large Graphs. arXiv:1706.02216 [cs.SI]\nhttps://arxiv.org/abs/1706.02216\n[10] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang,\nVijay Pande, and Jure Leskovec. 2019. Strategies for pre-training graph\nneural networks. arXiv preprint arXiv:1905.12265 (2019).\n[11] In-Su Jung, Yu-Rae Song, Lelisa Adeba Jilcha, Deuk-Hun Kim, Sun-\nYoung Im, Shin-Woo Shim, Young-Hwan Kim, and Jin Kwak. 2024.\nEnhanced Encrypted Traffic Analysis Leveraging Graph Neural Net-\nworks and Optimized Feature Dimensionality Reduction. Symmetry16, 6 (2024). https://doi.org/10.3390/sym16060733\n[12] Nickolaos Koroniotis, Nour Moustafa, Elena Sitnikova, and Benjamin\nTurnbull. 2018. Towards the Development of Realistic Botnet Dataset\nin the Internet of Things for Network Forensic Analytics: Bot-IoT\nDataset. arXiv:1811.00701 [cs.CR]\n[13] Louis Van Langendonck, Ismael Castell-Uroz, and Pere Barlet-Ros.\n2024. PPT-GNN: A Practical Pre-Trained Spatio-Temporal Graph\nNeural Network for Network Security. arXiv:2406.13365 [cs.LG]\nhttps://arxiv.org/abs/2406.13365\n[14] Xinjie Lin, Gang Xiong, Gaopeng Gou, Zhen Li, Junzheng Shi, and\nJing Yu. 2022. ET-BERT: A Contextualized Datagram Representation\nwith Pre-training Transformers for Encrypted Traffic Classification.\nInProceedings of the ACM Web Conference 2022 (WWW \u201922) . ACM.\nhttps://doi.org/10.1145/3485447.3512217\n[15] Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei\nZhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S Yu, et al .2023. To-\nwards graph foundation models: A survey and beyond. arXiv preprint\narXiv:2310.11829 (2023).\n[16] Wai Weng Lo, Siamak Layeghy, Mohanad Sarhan, Marcus Gallagher,\nand Marius Portmann. 2022. E-graphsage: A graph neural network\nbased intrusion detection system for iot. In NOMS 2022-2022 IEEE/IFIP\nNetwork Operations and Management Symposium . IEEE, 1\u20139.\n[17] Yuanfu Lu, Xunqiang Jiang, Yuan Fang, and Chuan Shi. 2021. Learn-\ning to Pre-train Graph Neural Networks. Proceedings of the AAAI\nConference on Artificial Intelligence 35, 5 (May 2021), 4276\u20134284.\nhttps://doi.org/10.1609/aaai.v35i5.16552\n[18] Nour Moustafa. 2021. A new distributed architecture for evaluating\nAI-based security systems at the edge: Network TON_IoT datasets.\nSustainable Cities and Society 72 (2021), 102994. https://doi.org/10.\n1016/j.scs.2021.102994\n[19] Nour Moustafa and Jill Slay. 2015. UNSW-NB15: a compre-\nhensive data set for network intrusion detection systems\n(UNSW-NB15 network data set). In 2015 Military Communi-\ncations and Information Systems Conference (MilCIS) . IEEE,\nhttps://research.unsw.edu.au/projects/unsw-nb15-dataset, 1\u20136.\nhttps://doi.org/10.1109/MilCIS.2015.7348942\n[20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023.\nExploring the Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. arXiv:1910.10683 [cs.LG] https://arxiv.org/abs/1910.\n10683\n[21] Satadal Sengupta, Niloy Ganguly, Pradipta De, and Sandip\nChakraborty. 2019. Exploiting Diversity in Android TLS Implemen-\ntations for Mobile App Traffic Classification. In The World Wide\nWeb Conference (San Francisco, CA, USA) (WWW \u201919) . Association\nfor Computing Machinery, New York, NY, USA, 1657\u20131668. https:\n//doi.org/10.1145/3308558.3313738\n[22] Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, and Huajie Shao.\n2024. Lens: A Foundation Model for Network Traffic in Cybersecurity.\narXiv:2402.03646 [cs.LG] https://arxiv.org/abs/2402.03646\n[23] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How\npowerful are graph neural networks? arXiv preprint arXiv:1810.00826\n(2018)."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "## Comprehensive Summary of WhisperNER: Unified Open Named Entity and Speech Recognition\n\n### 1. Abstract\n\nThe paper introduces WhisperNER, a novel model combining automatic speech recognition (ASR) and named entity recognition (NER) into a single system. WhisperNER leverages recent advancements in open NER to manage a diverse range of entity types during inference. The model incorporates synthetic datasets augmented with synthetic speech samples to train for a variety of NER tags. The evaluation involves generating synthetic speech datasets for commonly used NER benchmarks and assessing the model's performance against existing ASR datasets annotated with open NER tags. Empirical results indicate that WhisperNER surpasses conventional baselines, particularly in out-of-domain open NER and supervised fine-tuning scenarios.\n\n### 2. Introduction\n\nRecent advancements in speech processing and natural language processing (NLP) have significantly improved ASR systems and NLP tasks like NER. Traditionally, speech-driven applications implement a pipeline approach where ASR transcribes speech to text, which is then processed by NLP models. This method often leads to cumulative errors, notably affecting tasks requiring high accuracy such as NER. Integrating speech recognition with direct speech processing models aims to eliminate intermediate steps, thus reducing error propagation.\n\nOpen NER research focuses on generalizing recognition to new and unseen entities, making it flexible compared to traditional systems bound by fixed entity types. Despite advancements, integrating open NER directly into ASR systems remains unexplored. WhisperNER introduces a unified model enabling joint ASR and open NER, decreasing the risk of error propagation in speech-driven applications.\n\n### 3. Related Work\n\nThe paper builds upon recent technologies:\n\n- **Whisper ASR Model**: Enhances transcription accuracy through large-scale weak supervision.\n- **Spoken Language Understanding Models**: Perform tasks such as intent detection and NER directly from speech, bypassing traditional ASR stages.\n- **Open NER**: Methods that include distillation from large language models, small transformer encoders for NER, and importance of \"negative instances\" for improving performance.\n\nWhisperNER integrates speech transcription and open-type NER in a novel way by prompting the model with NER labels during training.\n\n### 4. Method\n\n**Model Architecture**:\n- Utilizes Whisper ASR as the base model.\n- Extends the model by conditioning decoding on a set of NER tags and generating output sequences containing both transcribed text and tagged entities.\n\n**Training Approach**:\n- Inputs are audio features \\(x\\) fed into an encoder that produces hidden states \\(h\\).\n- Decoder takes the previous tokens, hidden states, and entity tags \\(t\\) to generate output sequence \\(y\\).\n- Uses cross-entropy loss for optimization:\n\\[ L(y, y^*) = -\\sum_{t=1}^{n} \\log P(y_t = y^*_t | y_1:t-1, h, t) \\]\n\n**Entity Marking Strategies**:\n- **BIO Tagging**: Tags entities by beginning (B), inside (I), and outside (O) markers.\n- **Boundary Markers**: Tags entities with start and end markers, e.g., \\(\\text{<location>}national park\\text{</location>}\\).\n\n**Negative Sampling and Entity Type Dropout**:\n- **Negative Sampling**: Involves prompting the model with entity labels not present in the speech, enhancing generalization.\n- **Entity Type Dropout**: Randomly eliminates a subset of entity types during training, reducing hallucinations during inference.\n\n### 5. Experimental Setup\n\n**Datasets**:\n- **Synthetic Dataset**: 350K samples, ~1.8 million unique entity types, generated using scripts.\n- **Speech Datasets**: VoxPopuli-NER, LibriSpeech-NER, Fleurs-NER, LibriSpeech-clean.\n- **NER Benchmarks**: MIT-Movie and MIT-Restaurant.\n\n**Baselines**:\n- **GLiNER**: Transformer encoder-based architecture for zero-shot NER.\n- **NuNER**: Uses synthetic datasets for training.\n- **GNER**: Generative model tailored for NER.\n- Comparison models are evaluated for zero-shot performance, closed set NER, and supervised finetuning.\n\n**Evaluation Metrics**:\n- **Word Error Rate (WER)**: Measures speech recognition accuracy.\n- **F1 Score**: Evaluates NER effectiveness, requiring both correct transcription and accurate tagging.\n\n**Hardware and Software**:\n- Implemented and trained using deep learning frameworks (e.g., PyTorch).\n- Hardware specifications include high-performance GPUs for extensive training cycles.\n\n### 6. Results and Analysis\n\n**Zero-Shot Open Type NER**:\n- WhisperNER models outperform baselines on three key datasets: VoxPopuli-NER, LibriSpeech-NER, and Fleurs-NER.\n- Notably higher NER F1 scores with minimal increase in WER.\n\n**Zero-Shot NER**:\n- WhisperNER shows limitations compared to baselines in closed-set environments such as MIT NER benchmarks.\n\n**Supervised Fine-Tuning**:\n- WhisperNER achieves the highest performance on MIT-Movie and MIT-Restaurant datasets.\n- Demonstrates superior results in both NER F1 and WER in supervised settings.\n\n**Ablation Studies**:\n- Effect of different negative sampling approaches and entity type dropout analyzed.\n- Optimal results achieved through balanced negative samples and entity type dropout.\n\n### 7. Discussion\n\n**Interpretation of Results**:\n- WhisperNER significantly reduces the error propagation common in traditional pipeline approaches.\n- Achieves better generalization in open-type NER contexts.\n- Limitations in closed-set zero-shot NER adaptability identified.\n\n**Implications**:\n- Shows potential for more integrated and efficient speech-driven applications.\n- Opens avenues for further research in direct speech processing models.\n\n**Critical Examination**:\n- Highlight areas for improvement, particularly in adapting to closed-set NER tasks.\n- Calls for more in-depth analysis and refinements.\n\n### 8. Conclusion\n\n**Key Contributions**:\n- Introduces WhisperNER, an integrated ASR and NER model supporting open-type entity recognition.\n- Demonstrates improved performance over traditional methods, reducing error propagation and enhancing accuracy.\n- Provides new benchmarks and datasets to support further research.\n\n**Future Directions**:\n- Focus on enhancing closed-set NER performance.\n- Further exploration into the impact of entity type dropout and negative sampling strategies.\n- Release of source code, datasets, and models to encourage reproducibility and future research.\n\nThis comprehensive summary provides sufficient information for researchers to replicate WhisperNER or build on its advancements in joint ASR and open NER tasks."}, {"role": "assistant", "content": "WhisperNER: Unified Open Named Entity and\nSpeech Recognition\nGil Ayache*, Menachem Pirchi*, Aviv Navon, Aviv Shamsian, Gill Hetz, Joseph Keshet\naiOla Research, Israel\ngil@aiola.com\nAbstract \u2014Integrating named entity recognition (NER) with\nautomatic speech recognition (ASR) can significantly enhance\ntranscription accuracy and informativeness. In this paper, we\nintroduce WhisperNER, a novel model that allows joint speech\ntranscription and entity recognition. WhisperNER supports\nopen-type NER, enabling recognition of diverse and evolving enti-\nties at inference. Building on recent advancements in open NER\nresearch, we augment a large synthetic dataset with synthetic\nspeech samples. This allows us to train WhisperNER on a large\nnumber of examples with diverse NER tags. During training,\nthe model is prompted with NER labels and optimized to output\nthe transcribed utterance along with the corresponding tagged\nentities. To evaluate WhisperNER, we generate synthetic speech\nfor commonly used NER benchmarks and annotate existing ASR\ndatasets with open NER tags. Our experiments demonstrate\nthat WhisperNER outperforms natural baselines on both out-\nof-domain open type NER and supervised finetuning.\nIndex Terms \u2014Open NER, Speech Recognition\nI. I NTRODUCTION\nRecent advancements in both speech processing and natural\nlanguage processing (NLP) have dramatically expanded the\ncapabilities of automatic speech recognition (ASR) systems.\nThese improvements, driven by large-scale models like Whis-\nper [1], have significantly enhanced transcription accuracy.\nMeanwhile, the emergence of powerful NLP models, particu-\nlarly large language models (LLMs), has led to major break-\nthroughs in tasks like language understanding, text generation,\nand named entity recognition (NER).\nTraditionally, speech-driven applications have relied on\npipeline architectures, where ASR systems transcribe speech\ninto text, which is then processed by NLP models for tasks\nsuch as sentiment analysis, question answering, and NER.\nWhile this approach has proven effective across a range of\napplications, it suffers from error accumulation \u2014 where\ntranscription errors in the ASR stage propagate through the\npipeline, reducing the performance of downstream NLP tasks.\nThis challenge is particularly evident in complex tasks requir-\ning high accuracy, such as NER.\nA growing area of interest is the integration of speech mod-\nels into a variety of NLP tasks beyond transcription, allowing\nmodels to process spoken language directly and eliminate\nintermediate stages [2]\u2013[5]. For instance, spoken language\nunderstanding models have been developed to perform tasks\nlike intent detection and NER [2], [5], [6] directly from\nspeech, bypassing the need for separate ASR components.\n*Equal contribution.\nFig. 1. WhisperNER: A set of entity types is provided as prompt to\nthe decoder, which is optimized to output the transcription along with the\ncorresponding entity tags.\nThese end-to-end models have shown promising results by\njointly optimizing for both speech and language understanding.\nRecently, open NER [7]\u2013[9] has gained significant atten-\ntion for its ability to generalize to new and unseen entities,\noffering a flexible alternative to traditional NER systems that\nare restricted by fixed entity types. [7] introduce a targeted\ndistillation method, where student models are trained through\ninstruction tuning. [8], [9] suggested using a tailored, small\nsize transformer encoders, designed specifically for the NER\nproblem, and showed that this models can outperform LLM\nbased NER models. [10] propose a method for improving\ninformation extraction results through annotation guidelines,\nand [11] explored the importance of \u201cnegative instances\u201d, i.e,\nnon-entity spans, for improving contextual information and\nlabel boundaries. However, integrating open type NER directly\ninto ASR systems remains an unexplored area.\nIn this paper, we propose WhisperNER, a unified model\nthat simultaneously performs speech transcription and named\nentity recognition, based on the Whisper ASR model [1]. By\nintegrating NER directly into the ASR process, WhisperNER\neliminates the need for separate ASR and NER components,\nthereby reducing the risk of error propagation. The model\nsupports open type NER, enabling it to recognize and tagarXiv:2409.08107v1  [cs.CL]  12 Sep 2024TABLE I\nZERO-SHOT ,OUT-OF-DOMAIN EVALUATION ON THREE OPEN TYPE NER SPEECH BENCHMARKS . THEPARAMS COLUMN INDICATE THE EXTRA\nPARAMETERS OF BASELINE METHODS W .R.TWHISPER NER. W E BOLD THE BEST RESULTS AND UNDERLINE THE SECOND -BEST RESULTS .\nV oxPopuli-NER LibriSpeech-NER Fleurs-NER Average\nF1\u2191 WER\u2193 F1\u2191 WER\u2193 F1\u2191 WER\u2193 F1\u2191 WER\u2193 Params \u2193\nGNER-T5-base 45.31 8.32 35.68 5.51 47.15 8 .52 42.72 7.45 +248 M\nNuNER 52.18 8.32 43.65 5.51 53.11 8 .52 49.64 7.45 +459 M\nGLiNER 53.55 8.32 48.97 5.51 54.35 8.52 52.29 7.45 +459 M\nWhisperNER-BIO 55.79 9.51 50 .40 5.95 54 .42 8 .22 53.53 7.89 \u2212\nWhisperNER 57 .24 9.24 50 .84 5.82 53.50 10 .26 53 .86 8.44 \u2212\na wide range of entities during inference, offering a more\naccurate and flexible approach to entity recognition from\nspeech.\nTo build WhisperNER, we leverage a large-scale synthetic\ndataset released from [9]. We augment a subset of the textual\nopen NER dataset with corresponding synthetic audio samples.\nThe final dataset contains 350K samples and \u223c1.8M unique\nentity types. Our approach draws motivation from recent\napproaches which incorporates prior information and context\ninto the decoding process by prompting the model on domain\ninformation [12] or keyword phrases [13], [14]. We train\nWhisperNER by prompting the model with NER type, for the\ntask of joint transcription and NER tagging (See Figure 1).\nAt inference time, WhisperNER can generalize to novel entity\ntypes not observed during training.\nThis paper makes the following contributions: (i) Propose\nWhisperNER, a novel model for joint open NER and ASR; (ii)\nIntroducing new speech and text NER and open-NER bench-\nmarks (iii) Conduct extensive experiments to demonstrate the\nsuperiority of WhisperNER.\nTo encourage future research and reproducibility, we will\nmake our source code, datasets, and models publicly available.\nII. O URMETHOD\nIn this section we describe our approach, WhisperNER,\nwhich extends the Whisper [1] ASR model to allow for joint\nopen NER and speech recognition. The model is designed to\noutput a tagged transcription sequence given an audio input,\neffectively performing both tasks simultaneously.\nA. Model Architecture and Training\nLetxdenote the input audio features, which are fed into\nthe encoder. The encoder produces a sequence of hidden\nstates h=Encoder (x), which are then used to condition the\ngeneration process.\nIn our approach, we further condition the decoding process\non a set of entity tags t= [t1, t2, . . . , t k](where each ti\nrepresents a specific entity type such as person ,location ,\norfood ). The decoder then generates the output sequence\ny= [y1, y2, . . . , y n], which includes both the transcribed text\nand the corresponding entity tags, see Figure 1.\nFormally, the decoder generates each token ytbased on the\nprevious tokens and the encoder\u2019s hidden states,\nyt=Decoder (y1:t\u22121,h,t).The model is trained to minimize the standard cross-entropy\nloss between the predicted output sequence yand the ground\ntruth sequence y\u2217, which includes both the correct transcrip-\ntion and the correct entity tags:\nL(y,y\u2217) =\u2212nX\nt=1logP(yt=y\u2217\nt|y1:t\u22121,h,t).\nTo mark the entities in the output sequence, we\nexplore two approaches. The second approach uses the\nuses the BIO tagging scheme, similar to the approach\nused in [11], i.e., ...walks(O) in(O) the(O)\nnational(B-location) park(I-location) . The\nsecond marks entities with start and end markers as follow\n<ti>yi1...yin<ti>>, for example, <location>national\npark<location>> (see Figure 1). While the former may\ninduce greater supervision and improve generalization [11],\nthis approach increases the length of the generated sequence,\nhence it is potentially less compute and time efficient.\nOur prompting based training procedure, allows Whis-\nperNER to handle both traditional (i.e., predefined set of\nentities) and open-type NER tasks. By exposing the model\nto diverse entity types during training, we ensure that it can\ngeneralize well to new and unseen entities during inference,\nmaking it highly adaptable to a wide range of applications.\nB. Negative Sampling and Entity Type Dropout\nIn our experiments, we find that including negative samples,\ni.e., prompting the model with entity labels not presented\nin the speech utterance, to significantly enhance the model\u2019s\ngeneralization performance, in line with findings in previous\nliterature on open-type NER [8]. We explored various negative\nsampling approaches and chose to use the complete entity set\nfrom a negative example, randomly selected from the training\nset, which proved most effective.\nAdditionally, we applied random entity type dropout during\ntraining, as inspired by Sainz et al. [10]. This involves ran-\ndomly eliminating a subset of entity types from the prompt\nand output sequence, compelling the model to focus on the\nremaining defined entity types. We found this approach to\nreduce entity hallucination risks during inference \u2013 where\nthe model output entities not presented in the input prompt.\nAlso, we incorporate random shuffling of entities in the input\nprompt to further promote generalization across diverse inputFig. 2. Negative samples and entity dropping: The effect of incorporating\nnegative entity tags and entity tag dropping.\nsequences. For a detailed analysis of these methodologies and\ntheir impacts, see Section III-D.\nIII. E XPERIMENTS\nIn this section, we evaluate WhisperNER against naturall,\npipeline based baselines, using NER and open type NER\nspeech datasets.\nDatasets. Our evaluation setup consists of open type NER\nbenchmarks and commonly used NER benchmarks. For the\nopen type NER benchmarks we utilize three frequently used\nspeech datasets with diverse linguistic and contextual proper-\nties. These datasets include V oxPopuli [15], LibriSpeech [16],\nand Fleurs [17], each offering unique challenges and op-\nportunities for evaluating NER models, particularly in open-\ntype NER contexts. For the multilingual datasets, we focus\nspecifically on the English subset of the data. As this ASR\ndatasets do not include NER labels, we adopt an LLM based\nannotation approach, in line with the method described in [9].\nWe utilize the prompts derived approach to extract open type\nNER from the raw transcriptions of each dataset. See [9]\nfor full details of the approach and the specific prompt.\nThis approach results with a wide and diverse set of NER\nannotations, allowing us to test the capabilities of our approach\nto identify a broader range of entity types that are not confined\nto traditional categories; Thus, enhancing our understanding of\nthe model\u2019s generalization in real-world applications.\nThis annotation scheme results with the following ASR-\nNER datasets: (i) VoxPopuli-NER : This dataset comprises\n879 samples. It is annotated with 2469 unique entity types,\nproviding a comprehensive basis for political and geograph-\nical entity recognition ; (ii) LibriSpeech-NER : Based on the\nLibriSpeech-clean dataset, with 1604 samples derived from\naudio book readings, includes annotations for 3674 unique\nentity types. This dataset is particularly valuable for testing\nthe model\u2019s ability to identify personal names and locations\nwithin narrative contexts; (iii) Fleurs-NER : Consists of 441\nsamples, annotated for 1440 unique entity types.\nIn addition, we utilize commonly used, textual NER\nbenchmarks [8]\u2013[11], namely MIT-Movie [18] and MIT-\nRestaurant [18]. These datasets consists of \u223c14K and \u223c10K,\nwith 12and8entity types, respectively. To enhance these\ntext-based datasets for speech recognition, we generated corre-\nsponding audio data using the AWS Polly text-to-speech (TTS)\nFig. 3. Effect of the number of negatives example. The x-axis describes the\nnumber of negatives examples used to construct negative NER labels.\nservice. To support future research on joint ASR and NER, we\nwill release all datasets.\nBaselines. We evaluate our WhisperNER againt natural\ntwo-stage baselines, in which we initially employ a Whisper\nlarge-v2 model for speech-to-text transcription, followed by\na NER model. We constrain our evaluation to recent small-\nscale NER models (up to 0.5 billion parameters) to ensure\ncompetitiveness and maintain parity in terms of the num-\nber of parameters and computational requirements, aligning\nwith our end-to-end approach. The baselines include: (i)\nGLiNER [8]: a transformer encoder based architecture with\nzero-shot NER capabilities. We use the large variant, GLiNER-\nL; (ii) NuNER [9]: since the original NuNER model is not\nsuitable for the zero-shot NER task, we utilize a GLiNER\nbased architecture model, trained with the synthetic dataset\nof [9], named NuNER zero-span .; (iii) GNER [11]: Dif-\nferent from the previous two benchmarks, this approach uses\ngenerative models specifically adopted for the NER task. We\nuse the flan-t5-base variant; (iv) WhisperNER-BIO : Our\napproach with complete BIO annotations; (v) WhisperNER :\nOur approach with NER span boundary annotations (Figure 1).\nEvaluation metrics. We assess the performance of our\nsystem using two evaluation metrics: Word Error Rate (WER),\nwhich quantifies the accuracy of the automatic speech recogni-\ntion transcription, and F1 Score, which evaluates the effective-\nness of named entity recognition. For a NER span prediction\nto be deemed correct, both the transcription must be accurate\nand the assigned NER label must match the true entity. This\ndual requirement ensures that our evaluation reflects both the\nprecision of the speech recognition and the correctness of the\nentity identification. For the baseline methods, we compute the\nWER metric on the ASR\u2019s output.\nTraining details. The WhisperNER models are trained\nusing a subset of 350K samples from the dataset proposed\nin [9]. For each example, we generate a corresponding syn-\nthetic audio sample. Unless stated otherwise, we train the\nWhisperNER models for 250K steps, with learning rate 1e\u22126\nand linear decay learning rate scheduler. During training, we\nfreeze the Whisper encoder and only modify the decoder.TABLE II\nZERO-SHOT PERFORMANCE ON THE MIT NER B ENCHMARKS .\nMovie Restaurant\nF1\u2191 WER\u2193 F1\u2191 WER\u2193\nGNER-T5-base 39.98 9 .92 30 .45 11.10\nNuNER 44 .64 9.92 34 .04 11.10\nGLiNER 40.44 9.92 29 .12 11 .10\nWhisperNER-BIO 30.16 8.10 26.67 8.44\nWhisperNER 30.55 8 .39 28.22 9 .64\nA. Zero-shot Open Type NER\nWe first evaluate WhisperNER on the three open-type NER\nbenchmarks, namely V oxPopuli-NER, LibriSpeech-NER and\nFleurs-NER. Each test example is annotated with positive\nNER tags based on the annotation scheme of [9]. To construct\nnegative samples, we randomly sample negative NER labels\nfrom the entire dataset. We ensure that the number of negative\nNER labels matches the number of positive labels, resulting\nin a balanced dataset. The results are presented in Table I.\nboth WhisperNER variants outperform all baseline methods in\nterms of NER F1 score, with only a slight decrease in WER,\nwhile being more parameter-efficient.\nB. Zero-shot NER\nNext, we evaluate the zero-shot NER performance of our\nWhisperNER and WhisperNER-BIO models on the closed set\nentity sets of the MIT NER benchmarks [18]. The results\nare summarized in Table II. In this setup, WhisperNER does\nnot perform as well as the baselines in terms of NER F1,\nshowing a higher tendency to incorrectly tag non-entity spans.\nRecognizing these limitations, we identify areas for refine-\nment and believe that a more in-depth analysis, and further\nenhancements, hold potential for significant improvements. We\nleave these pursuits for future research, focusing on optimizing\nWhisperNER to better handle closed set zero-shot scenarios.\nC. Supervised finetuning\nFinally, we evaluate the supervised fine-tuning performance\nof our WhisperNER model on common NER benchmarks,\nwith results summarized in Table III. For this experiment,\nthe methods were fine-tuned on labeled datasets to assess\ntheir ability to recognize and tag entities in a supervised\nsetting. For the baseline methods, we independently finetune\nboth the Whipser model, and the NER baseline. Our method\noutperforms all baselines, achieving the best performance in\nboth WER and NER F1 scores. These improvements demon-\nstrate the effectiveness of our approach in jointly handling\ntranscription and entity recognition tasks, further validating\nWhisperNER\u2019s capability to generalize to structured NER\ndatasets in a supervised setting.\nD. Ablation\nTo better understand the impact of negative sampling strate-\ngies and entity type dropout on model performance, we con-\nducted an ablation study. We analyzed varying the proportionTABLE III\nSUPERVISED FT P ERFORMANCE ON THE MIT NER B ENCHMARKS .\nMovie Restaurant\nF1\u2191 WER\u2193 F1\u2191 WER\u2193\nGNER-T5-base 61.12 3 .34 57 .90 5 .48\nNuNER 75.25 3 .34 71 .40 5 .48\nGLiNER 75.34 3 .34 71 .09 5 .48\nWhisperNER-BIO 80.91 2.64 72 .74 4.74\nWhisperNER 81 .35 2 .31 71.51 4.02\nTABLE IV\nEFFECT OF NEGATIVE SAMPLING APPROACH : W E EVALUATE\nWHISPER NER MODELS TRAINED USING THE NUNER DATASET FOR 25K\nSTEPS ,ON THE ZERO -SHOT TASK OVER THE MIT NER B ENCHMARKS .\nF1\u2191 WER\u2193\nRandom NER Type 20.22 8 .36\nHard Negatives 25 .91 8.46\nRandom Sample 25.83 8.09\nof negative samples, the method for negative sampling, and the\ninclusion of random entity type dropout during training. For\nefficiency and consistency, all models were trained for 25K\nsteps using the augmented NuNER dataset [9] and evaluated\non the MIT NER benchmarks.\nTable IV shows that using full entity sets from negative\nexamples, \u201crandom sample\u201d, consistently outperforms random\nnegative sampling of NER types, likely due to the contextual\nconsistency of the selected examples. We also tested a \u201chard\nnegative\u201d approach, where negative examples are chosen based\non semantic similarity to positive examples. While this method\nperformed comparably, it introduced additional overhead in\nquerying the dataset, hence we opt for random sampling from\nthe training set for efficiency.\nFigure 3 demonstrates that using two negative samples\n(approximately 66% negative entity types) provides the best\nF1 and WER results. Figure 2 highlights the effectiveness\nof random entity type dropout, which helps reducing entity\ntags hallucinations during inference. Based on these findings,\nwe train WhisperNER with entity type dropout and negative\nsampling based on the entity set from two negative examples.\nIV. C ONCLUSION\nIn this paper, we propose WhisperNER, a novel approach\nfor joint open type named entity recognition and automatic\nspeech recognition. Our model is designed to generalize to\nnovel entities at inference, effectively handling a diverse range\nof entity types. We demonstrate that WhisperNER not only\noutperforms traditional pipeline-based baselines in terms of\nentity recognition accuracy but also achieves this with minimal\nto no increase in transcription errors. This work marks a step\nforward in the integration of speech and language processing,\nsignificantly improving both the effectiveness and precision of\nspeech-driven applications.REFERENCES\n[1] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine\nMcLeavey, and Ilya Sutskever, \u201cRobust speech recognition via large-\nscale weak supervision,\u201d in International conference on machine\nlearning . PMLR, 2023, pp. 28492\u201328518.\n[2] Quentin Meeus, Marie-Francine Moens, and Hugo Van Hamme,\n\u201cWhisper-slu: Extending a pretrained speech-to-text transformer for low\nresource spoken language understanding,\u201d in 2023 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU) . IEEE, 2023,\npp. 1\u20136.\n[3] Siddhant Arora, Ankita Pasad, Chung-Ming Chien, Jionghao Han,\nRoshan Sharma, Jee-weon Jung, Hira Dhamyal, William Chen, Suwon\nShon, Hung-yi Lee, et al., \u201cOn the evaluation of speech foun-\ndation models for spoken language understanding,\u201d arXiv preprint\narXiv:2406.10083 , 2024.\n[4] Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan\nSharma, Yosuke Kashiwagi, Emiru Tsunoo, Karen Livescu, and Shinji\nWatanabe, \u201cUniverslu: Universal spoken language understanding for\ndiverse tasks with natural language instructions,\u201d in Proceedings of the\n2024 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Volume 1:\nLong Papers) , 2024, pp. 2754\u20132774.\n[5] Mohan Li, Simon Keizer, and Rama Doddipatla, \u201cPrompting whisper for\nqa-driven zero-shot end-to-end spoken language understanding,\u201d arXiv\npreprint arXiv:2406.15209 , 2024.\n[6] Hao Yang, Min Zhang, Shimin Tao, Miaomiao Ma, and Ying Qin,\n\u201cChinese asr and ner improvement based on whisper fine-tuning,\u201d\nin2023 25th International Conference on Advanced Communication\nTechnology (ICACT) . IEEE, 2023, pp. 213\u2013217.\n[7] Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon,\n\u201cUniversalner: Targeted distillation from large language models for open\nnamed entity recognition,\u201d arXiv preprint arXiv:2308.03279 , 2023.\n[8] Urchade Zaratiana, Nadi Tomeh, Pierre Holat, and Thierry Charnois,\n\u201cGliner: Generalist model for named entity recognition using bidirec-\ntional transformer,\u201d arXiv preprint arXiv:2311.08526 , 2023.\n[9] Sergei Bogdanov, Alexandre Constantin, Timoth \u00b4ee Bernard, Benoit\nCrabb \u00b4e, and Etienne Bernard, \u201cNuner: Entity recognition encoder pre-\ntraining via llm-annotated data,\u201d arXiv preprint arXiv:2402.15343 , 2024.\n[10] Oscar Sainz, Iker Garc \u00b4\u0131a-Ferrero, Rodrigo Agerri, Oier Lopez de La-\ncalle, German Rigau, and Eneko Agirre, \u201cGollie: Annotation\nguidelines improve zero-shot information-extraction,\u201d arXiv preprint\narXiv:2310.03668 , 2023.\n[11] Yuyang Ding, Juntao Li, Pinzheng Wang, Zecheng Tang, Bowen Yan,\nand Min Zhang, \u201cRethinking negative instances for generative named\nentity recognition,\u201d arXiv preprint arXiv:2402.16602 , 2024.\n[12] Feng-Ting Liao, Yung-Chieh Chan, Yi-Chang Chen, Chan-Jan Hsu,\nand Da-shan Shiu, \u201cZero-shot domain-sensitive speech recognition\nwith prompt-conditioning fine-tuning,\u201d in 2023 IEEE Automatic Speech\nRecognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1\u20138.\n[13] Aviv Shamsian, Aviv Navon, Neta Glazer, Gill Hetz, and Joseph Keshet,\n\u201cKeyword-guided adaptation of automatic speech recognition,\u201d arXiv\npreprint arXiv:2406.02649 , 2024.\n[14] Yuang Li, Min Zhang, Chang Su, Yinglu Li, Xiaosong Qiao, Mengxin\nRen, Miaomiao Ma, Daimeng Wei, Shimin Tao, and Hao Yang, \u201cA\nmultitask training approach to enhance whisper with open-vocabulary\nkeyword spotting,\u201d Interspeech 2024 , 2023.\n[15] Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya\nTalnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel\nDupoux, \u201cV oxpopuli: A large-scale multilingual speech corpus for\nrepresentation learning, semi-supervised learning and interpretation,\u201d\narXiv preprint arXiv:2101.00390 , 2021.\n[16] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur,\n\u201cLibrispeech: an asr corpus based on public domain audio books,\u201d\ninAcoustics, Speech, and Signal Processing (ICASSP), 2015 IEEE\nInternational Conference on . IEEE, 2015, pp. 5206\u20135210.\n[17] Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod,\nSiddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna, \u201cFleurs:\nFew-shot learning evaluation of universal representations of speech,\u201d in\n2022 IEEE Spoken Language Technology Workshop (SLT) . IEEE, 2023,\npp. 798\u2013805.\n[18] Jingjing Liu, Panupong Pasupat, Scott Cyphers, and Jim Glass, \u201cAsgard:\nA portable architecture for multilingual dialogue systems,\u201d in 2013 IEEEInternational Conference on Acoustics, Speech and Signal Processing .\nIEEE, 2013, pp. 8386\u20138390."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**1. Abstract:**\nThe paper introduces DEMAU (Decompose, Explore, Model & Analyse Uncertainties), an open-source tool designed for visualizing and analyzing various types of uncertainty in machine learning classification models. DEMAU supports the decomposition of uncertainty into epistemic (reducible) and aleatoric (irreducible) components and offers customization options for datasets, models, and uncertainty quantification methods. The tool is educational and exploratory, aimed at enhancing model performance understanding and decision-making processes.\n\n**2. Introduction:**\nMachine learning practitioners often need to visualize uncertainties, particularly in active and adaptive learning scenarios. Understanding uncertainty aids in assessing model reliability and prediction confidence. Traditional measures like Shannon\u2019s entropy have evolved to include decompositions into epistemic and aleatoric uncertainties. The paper proposes DEMAU, which aligns with recent research trends by providing an open-source interface for visualizing and exploring uncertainty types in classification models. This tool serves as a learning aid for newcomers and a research asset for practitioners.\n\n**3. Related Work:**\nThe paper refers to foundational works on active learning and uncertainty quantification and decomposition. The contributions from Settles (2009), Aggarwal et al. (2014), and Hacohen et al. (2022) on active learning are acknowledged. The paper also cites Kendall and Gal (2017) for decomposing uncertainty and other frameworks by Hora (1996) and H\u00fcllermeier and Waegeman (2021). Various uncertainty reasoning frameworks like probabilities, credal sets, and belief functions are acknowledged, noting the significance of Dempster (1967) and Shafer (1976). DEMAU advances the state of the art by integrating these methods into a user-friendly, modular tool.\n\n**4. Method:**\nDEMAU offers a graphical interface with several panels for exploring dataset features and model uncertainties. Key features include:\n   - Visualization Tools: Red-to-blue gradients indicate uncertainty levels. \n   - Customizable Models and Datasets: Users can upload their datasets (CSV format) and models.\n   - Built-in Models: Options like K-Nearest Neighbors, Support Vector Machines, Naive Bayes, and Random Forest.\n   - Uncertainty Quantification: Built-in methods include Gini criteria, Shannon\u2019s entropy, and Least-Confident measures. Users can input further methods.\n   - Decomposition Techniques: Methods to estimate aleatoric and epistemic uncertainties, non-specificity, and discord are provided, with bibliographic references integrated.\n\n**5. Experimental Setup:**\nDEMAU\u2019s experimentation involves popular datasets like Fisher\u2019s Iris dataset. Users can add other datasets by placing CSV files in the tool\u2019s dataset folder:\n   - Hardware and Software: The application is primarily designed in Python, suitable for various configurations.\n   - Evaluation Metrics: Metrics include accuracy, confidence intervals, and visual representations of uncertainties.\n   - Comparative Models: Experiments are performed with built-in models such as K-NN, SVM, Naive Bayes, and Random Forest, allowing comparative analysis.\n\n**6. Results and Analysis:**\nThe empirical results highlight the tool's effectiveness in depicting uncertainty areas:\n   - Different models display varied areas of high and low uncertainty, visually analyzed.\n   - Gini, Entropy, and Least-Confident measures show the same areas' representation.\n   - Decomposition (e.g., aleatoric and epistemic uncertainties) offers deeper insight into model reliability.\n   - Figures illustrate uncertainty distributions (e.g., Figure 2 for model uncertainty areas).\n\n**7. Discussion:**\nThe paper interprets results by emphasizing how clearer visualizations and decompositions can elucidate model performance and decision empowerment:\n   - The ability to distinguish between reducible and irreducible uncertainties helps in refining models.\n   - The modular approach enables customization and extension.\n   - Unexpected results, like discrepancies between similar uncertainty measures, prompt further investigation.\n   - Limitations include the tool's unsuitability for production use, leaving room for performance optimization.\n\n**8. Conclusion:**\nThe paper sums up DEMAU's contributions as providing a user-friendly means for visualizing and decomposing classification model uncertainties:\n   - It supports research and educational objectives, facilitating better model interpretation.\n   - Future work includes enhancing performance, expanding uncertainty quantification methods, and applying the tool to new datasets.\n   - DEMAU\u2019s open-source nature and modular design promote transparency and further development in uncertainty quantification research.\n\nThis summary offers all essential details for replicating the study, understanding the methodology, and utilizing DEMAU for educational or research purposes."}, {"role": "assistant", "content": "DEMAU: Decompose, Explore, Model & Analyse\nUncertainties\nArthur Hoarau\u2217, Vincent Lemaire\u2217\u2217\n\u2217Universit\u00e9 de Rennes, CNRS, IRISA, DRUID, France\n\u2217\u2217Orange innovation, Lannion, France\nAbstract. Recent research in machine learning has given rise to a flourishing\nliterature on the quantification and decomposition of model uncertainty. This\ninformation can be very useful during interactions with the learner, such as in\nactive learning or adaptive learning, and especially in uncertainty sampling. To\nallow a simple representation of these total, epistemic (reducible) and aleatoric\n(irreducible) uncertainties, we offer DEMAU, an open-source educational, ex-\nploratory and analytical tool allowing to visualize and explore several types of\nuncertainty for classification models in machine learning.\n1 Introduction\nIn practical scenarios, the need to visualize uncertainty arises across various fields of ma-\nchine learning, particularly during interactions with the learner, such as in active learning\nor adaptive learning, and especially in uncertainty sampling Settles (2009); Aggarwal et al.\n(2014); Hacohen et al. (2022). It offers insights into the reliability of predictions and the\nconfidence associated with model outputs. Commonly, uncertainty in probabilistic models has\nbeen represented using Shannon\u2019s entropy Shannon (1948). However, the toolkit for uncer-\ntainty representation has expanded significantly. Researchers have proposed decomposing un-\ncertainty into reducible (epistemic) and irreducible (aleatoric) components Hora (1996); H\u00fcl-\nlermeier and Waegeman (2021); Kendall and Gal (2017); Senge et al. (2014); Charpentier\net al. (2020). Additionally, various frameworks for uncertainty reasoning have emerged Abdar\net al. (2021); Huang et al. (2023), including probabilities, credal sets, possibilities, and belief\nfunctions (Dempster, 1967; Shafer, 1976).\nThe diversity of uncertainty forms and their implications necessitate effective visualization\ntools. Visualizing uncertainties can provide researchers and practitioners with deeper insights\ninto model performance and decision-making processes. The interface proposed facilitates the\nexploration and interpretation of uncertainty in machine learning models, allowing users to\ncomprehend the nuances of uncertainty representation across different frameworks.\nTherefore, we propose DEMAU, an open-source educational, exploratory and analytical\ntool allowing to visualize and explore several types of uncertainty for classification models in\nmachine learning, including recent techniques for computing epistemic and aleatoric uncer-\ntainties, while leaving the possibility for the user to customize the dataset, the model or even\nthe method for quantifying uncertainty as they wish. The tool is mainly developed for researcharXiv:2409.08105v1  [cs.LG]  12 Sep 2024DEMAU: Decompose, Explore, Model & Analyse Uncertainties\nFIGURE 1 \u2013 DEMAU interface\nand educational purposes, and may not be suited for production use. It can be used to explore\nnew datasets, to analyze or develop uncertainty quantification metrics or even to visualize and\ndecompose the uncertainty of a machine learning model. It may also be used to facilitate the\nundertanding for newcomers in the field of uncertainty quantification, such as PhD students.\n2 DEMAU to explore and decompose uncertainties\n2.1 Overview\nThe interface is presented in Figure 1, with two main panels including the dataset currently\nstudied (in panel (1)) along with the areas of uncertainty (in panel (2)) with a gradient from\nred to blue. More red indicates an area of high uncertainty for the model while a blue area\nindicates high certainty in the prediction. With the drop-down menu (3) the user can select\none of the pre-loaded datasets, or use its own (see section 2.5). In the double drop-down menu\n(4) two variables, among the ones in the dataset, can be selected for a 2D representation,\nalong with reduction techniques, according to the checkboxes (5). DEMAU also comes with\ndifferent possible machine learning models in the drop-down menu (6) (see section 2.2) and\nseveral uncertainties in the drop-down menu (7) can be computed (see section 2.3 and 2.4). InA. Hoarau, V . Lemaire\n(a)K-NN\n (b)SVM\n (c)Naive Bayes\n (d)Random Forest\nFIGURE 2 \u2013 Uncertainty for several models.\n(a)Dataset\n (b)Gini\n (c)Entropy\n (d)Least Confident\nFIGURE 3 \u2013 Different uncertainty quantifications.\naddition, you can simply add your own datasets, models and uncertainty quantification methods\n(see section 2.5).\n2.2 Uncertainty from machine learning models\nDEMAU simply allows to represent areas of uncertainty from your model. Figure 2 shows\ndifferent areas of uncertainty for several machine learning models according to Fisher\u2019s Iris\ndataset, presented in Figure 1. Built-in models are the K-Nearest Neighbors (2a), Support Vec-\ntor Machine (2b), Naive Bayes classifier (2c) and Random Forest (2d), but the user is free to\nuse its own model.\n2.3 Use of different methods\nFigure 3 shows different computations of the model uncertainty, with Gini criteria (3b),\nShannon\u2019s entropy (3c) and a Least-Confident measure (3d). Although these measurements\nappear to represent similar areas, they may vary and any other measurement can also be simply\nadded for different results.\n2.4 Decomposition of uncertainty\nRecent methods for estimating the uncertainty of the model rely on the decomposition into\nreducible and irreducible uncertainties. Figure 4 shows the areas of uncertaintiy corresponding\nto three different methods. Built-in computations allow to represent an estimation of the alea-\ntroic uncertainty (4b), the epistemic uncertainty (4c), the non-specificity (4d) and the discordDEMAU: Decompose, Explore, Model & Analyse Uncertainties\n(a)Dataset\n (b)Aleatoric\n (c)Epistemic\n (d)Non-Specicifity\nFIGURE 4 \u2013 Decomposition of uncertaintiy.\nof the model. Note that the bibliographic references associated with each method are directly\ndisplayed within the interface.\n2.5 Modularity\nNew dataset can be added by simply deposing a CSV file into the datasets folder. It is not\nneeded to reload the app, just press the refresh button. Machine learning models can be added\nin the models file and uncertainty quantification methods in the uncertainties file. A built-in\nfuction also allows to reduce the grid size for faster computation on very large datasets, light\nconfigurations or greedy strategies.\n3 Conclusion\nBy offering a user-friendly interface, DEMAU empowers researchers and practitioners to\ngain deeper insights into the uncertainties associated with their models, thereby enhancing\ndecision-making processes and model performance assessment. This information can be very\nuseful during interactions with the learner, such as in active learning or adaptive learning, and\nespecially in uncertainty sampling. The modular design of DEMAU allows for easy custo-\nmization and extension, facilitating its utility across various applications and datasets. Moving\nforward, DEMAU serves as a valuable tool for advancing research and education in uncertainty\nquantification, while also promoting transparency and reproducibility in machine learning ex-\nperiments.\nAccess : DEMAU is an open-source Python software for educational, exploratory and analy-\ntical purposes allowing to visualize and explore several types of uncertainty for classification\nmodels in machine learning. Link to the project : https://github.com/ArthurHoa/\nDEMAU .\nA presentation video of DEMAU is available at : https://youtu.be/2xHwqlJJTm4\nR\u00e9f\u00e9rences\nAbdar, M., F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh, P. Fieguth,\nX. Cao, A. Khosravi, U. R. Acharya, V . Makarenkov, and S. Nahavandi (2021). A reviewA. Hoarau, V . Lemaire\nof uncertainty quantification in deep learning : Techniques, applications and challenges.\nInformation Fusion 76 , 243\u2013297.\nAggarwal, C., X. Kong, Q. Gu, J. Han, and P. Yu (2014). Active Learning : A Survey, Data\nClassification : Algorithms and Applications . CRC Press.\nCharpentier, B., D. Z\u00fcgner, and S. G\u00fcnnemann (2020). Posterior network : Uncertainty estima-\ntion without ood samples via density-based pseudo-counts. In H. Larochelle, M. Ranzato,\nR. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information Processing Sys-\ntems, V olume 33, pp. 1356\u20131367. Curran Associates, Inc.\nDempster, A. P. (1967). Upper and Lower Probabilities Induced by a Multivalued Mapping.\nThe Annals of Mathematical Statistics 38 (2), 325 \u2013 339.\nHacohen, G., A. Dekel, and D. Weinshall (2022). Active learning on a budget : Opposite strate-\ngies suit high and low budgets. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv\u00e1ri, G. Niu,\nand S. Sabato (Eds.), International Conference on Machine Learning, 2022, Baltimore, Ma-\nryland, USA , V olume 162 of Proceedings of Machine Learning Research , pp. 8175\u20138195.\nPMLR.\nHora, S. C. (1996). Aleatory and epistemic uncertainty in probability elicitation with an\nexample from hazardous waste management. Reliability Engineering & System Safety 54 (2),\n217\u2013223. Treatment of Aleatory and Epistemic Uncertainty.\nHuang, L., S. Ruan, Y . Xing, and M. Feng (2023). A review of uncertainty quantification in\nmedical image analysis : probabilistic and non-probabilistic methods.\nH\u00fcllermeier, E. and W. Waegeman (2021). Aleatoric and epistemic uncertainty in machine\nlearning : An introduction to concepts and methods. Machine Learning 110 .\nKendall, A. and Y . Gal (2017). What uncertainties do we need in bayesian deep learning for\ncomputer vision ? In NIPS .\nSenge, R., S. B\u00f6sner, K. Dembczynski, J. Haasenritter, O. Hirsch, N. Donner-Banzhoff, and\nE. H\u00fcllermeier (2014). Reliable classification : Learning classifiers that distinguish aleatoric\nand epistemic uncertainty. Inf. Sci. 255 , 16\u201329.\nSettles, B. (2009). Active learning literature survey. Computer Sciences Technical Report\n1648, University of Wisconsin\u2013Madison.\nShafer, G. (1976). A Mathematical Theory of Evidence . Princeton University Press.\nShannon, C. E. (1948). A mathematical theory of communication. Bell System Technical\nJournal 27 (3), 379\u2013423.\nR\u00e9sum\u00e9\nRecent research in machine learning has given rise to a flourishing literature on the quan-\ntification and decomposition of model uncertainty. This information can be very useful during\ninteractions with the learner, such as in active learning or adaptive learning, and especially\nin uncertainty sampling. To allow a simple representation of these total, epistemic (reducible)\nand aleatoric (irreducible) uncertainties, we offer DEMAU, an open-source educational, ex-\nploratory and analytical tool allowing to visualize and explore several types of uncertainty for\nclassification models in machine learning."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Summary of the Academic Paper \"The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal\"\n\n#### 1. Abstract:\nThe paper focuses on the integration of technological innovation to improve access to justice by predicting case outcomes from the UK Employment Tribunal (UKET). It introduces the CLC-UKET dataset, which consists of approximately 19,000 UKET cases annotated using a large language model (LLM). The dataset includes comprehensive legal information such as facts, claims, case outcomes, and statutory references. The study establishes a benchmark for multi-class case outcome prediction tasks and demonstrates that fine-tuned transformer models outperform zero-shot and few-shot LLMs. The CLC-UKET dataset is proposed as a valuable resource for employment-related dispute resolution research.\n\n#### 2. Introduction:\nThe paper explores the utilization of natural language processing (NLP) techniques in the legal domain, with a particular focus on predicting outcomes of employment-related disputes in the UK Employment Tribunal. The UKET is pivotal for enforcing employment rights and improving access to justice. Despite advances in legal judgment prediction (LJP) using NLP, the UK judicial decisions, specifically in employment law, remain under-explored. The paper aims to address this gap by creating a detailed dataset and benchmarking prediction models to enhance the predictability of UKET outcomes, which could facilitate better dispute resolution.\n\n#### 3. Related Work:\nThe paper reviews existing literature on legal judgments, focusing on quantitative and NLP methods applied in various legal domains. Earlier studies, like those by Grunbaum and Newhouse (1965) and Field and Holley (1982), mainly relied on manual extraction and analysis. Recent advances involve deep learning models and large-scale legal datasets (e.g., Xiao et al., 2018; Henderson et al., 2022; Ma et al., 2021). However, there is limited research on the UK legal system, with Strickson and De La Iglesia (2020) being a notable pre-LLM study. The paper contributes by filling this research gap with the development and benchmarking of the CLC-UKET dataset.\n\n#### 4. Method:\n- **Dataset Curation:**\n  - **Preparation:** The raw UKET subset of the Cambridge Law Corpus (CLC) contains 52,339 cases. Cases with only procedural decisions were excluded, yielding 19,090 cases with substantial legal information.\n  - **Annotations with LLMs:** Employing the GPT-4-turbo model, legal annotations were automatically extracted using a carefully refined prompt. The dataset includes metadata, facts, claims, statutory references, precedent references, outcomes, detailed orders, and remedies.\n\n- **Prediction Task Definition:**\n  - Focuses on predicting the outcomes based on concatenated fact and claim statements.\n  - Uses four outcome categories: \"claimant wins,\" \"claimant loses,\" \"claimant partly wins,\" and \"other.\"\n\n- **Models Employed:**\n  - **Transformer-based Models:** BERT and T5 were fine-tuned on the dataset.\n  - **LLM-based Models:** GPT-3.5-turbo and GPT-4-turbo were evaluated in zero-shot and few-shot settings.\n\n#### 5. Experimental Setup:\n- **Datasets:**\n  - **Training, Validation, and Testing Sets:** CLC-UKET pred consists of 14,582 cases split into training (11,838 cases), validation (1,373 cases), and testing (1,371 cases).\n  - Training and validation annotations were generated by GPT-4, whereas testing outcome labels were manually annotated by legal experts.\n  \n- **Specifications:**\n  - **Hardware and Software:** Models were trained using Python and relevant ML frameworks. Specific hardware details were not provided but likely include GPUs for training transformer models.\n  \n- **Evaluation Metrics:**\n  - Used accuracy, precision, recall, and F-score.\n  - Baseline human predictions were collected for comparison.\n\n#### 6. Results and Analysis:\n- **Overall Performance:**\n  - Fine-tuned T5 model achieved the highest F-score (0.564), outperforming other baseline models.\n  - Human performance had the highest F-score (0.672), indicating a strong benchmark.\n  \n- **Key Observations:**\n  - Human annotators demonstrated significantly higher prediction accuracy compared to models.\n  - GPT-4-turbo (few-shot with jurisdiction-coded examples) showed marginal improvement over its zero-shot counterpart.\n  \n- **Detailed Results:**\n  - Detailed evaluation metrics for each model and human predictions were provided.\n  - Performance varied significantly with label category, revealing potential areas of improvement in model training and dataset augmentation.\n\n#### 7. Discussion:\n- **Interpretation of Results:**\n  - Models show substantial improvements over random baselines but still lag behind human performance.\n  - Limited information and biases in the annotated facts might restrict accurate predictions, especially for complex case categories like \"partly wins\" and \"other.\"\n\n- **Limitations:**\n  - Potential biases due to using post-judgment fact and claim extractions.\n  - Quality of GPT-4 annotations might not fully capture intricate legal details, suggesting a potential need for manual review or improved automatic annotation methods.\n\n#### 8. Conclusion:\n- The paper presents the CLC-UKET dataset as a comprehensive resource for predicting UKET case outcomes, enhancing access to justice.\n- Fine-tuned transformer models show promise, yet there's a clear gap to human prediction performance.\n- Future research should explore better annotation techniques, incorporate more detailed legal provisions, and refine models to bridge the performance gap.\n\n### Detailed Replication Guide:\n\n1. **Dataset Curation and Preparation:**\n   - **Collect raw case data** from the Cambridge Law Corpus (CLC).\n   - **Filter cases** to exclude those with only procedural information, retaining cases with substantial legal content.\n   - **Extract metadata** (e.g., case identifier, hearing date, jurisdiction codes) for 19,090 remaining cases.\n\n2. **Automatic Annotation:**\n   - Utilize the **GPT-4-turbo model** to extract and annotate cases with facts, claims, statutory references, precedent references, case outcomes, orders, and remedies.\n   - **Refine LLM prompts** through iterative testing to ensure quality extraction (see study\u2019s Appendix A.2 for final prompt used).\n\n3. **Model Training and Evaluation:**\n   - **Transformer-based Models:** Fine-tune BERT and T5 using the Adam optimizer (learning rate 1e-4, batch size 32) over 5 epochs.\n   - **LLM-based Models:** Test GPT-3.5-turbo and GPT-4-turbo with zero-shot and few-shot settings.\n\n4. **Experimental Setup:**\n   - Use the **training (11,838 cases), validation (1,373 cases), and testing (1,371 cases)** splits.\n   - Ensure models only access fact and claim statements.\n   - Compare model performance using accuracy, precision, recall, and F-score.\n\n5. **Results Analysis:**\n   - **Compile results** with detailed metrics across all models and human baseline annotations.\n   - Conduct **ablation studies** by aggregating certain prediction categories to analyze effects on model performance.\n  \n6. **Benchmark and Improve:**\n   - Provide detailed **discussions on limitations** and suggest methods to improve annotation quality and model prediction accuracy.\n   - Make the **CLC-UKET dataset publicly available** to encourage further research and benchmarking.\n\nThe comprehensive methodology and detailed dataset annotations are pivotal in facilitating the replicability and extension of this study. Researchers can build on this work to further refine prediction models and contribute to the legal domain\u2019s NLP applications."}, {"role": "assistant", "content": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the\nUK Employment Tribunal\nHuiyuan Xie1Felix Steffek2Joana Ribeiro de Faria2Christine Carter2Jonathan Rutherford2\n1Department of Computer Science and Technology, University of Cambridge\n2Faculty of Law, University of Cambridge\nAbstract\nThis paper explores the intersection of tech-\nnological innovation and access to justice by\ndeveloping a benchmark for predicting case\noutcomes in the UK Employment Tribunal\n(UKET). To address the challenge of extensive\nmanual annotation, the study employs a large\nlanguage model (LLM) for automatic annota-\ntion, resulting in the creation of the CLC-UKET\ndataset. The dataset consists of approximately\n19,000 UKET cases and their metadata. Com-\nprehensive legal annotations cover facts, claims,\nprecedent references, statutory references, case\noutcomes, reasons and jurisdiction codes. Fa-\ncilitated by the CLC-UKET data, we examine\na multi-class case outcome prediction task in\nthe UKET. Human predictions are collected to\nestablish a performance reference for model\ncomparison. Empirical results from baseline\nmodels indicate that finetuned transformer mod-\nels outperform zero-shot and few-shot LLMs\non the UKET prediction task. The performance\nof zero-shot LLMs can be enhanced by integrat-\ning task-related information into few-shot ex-\namples. We hope that the CLC-UKET dataset,\nalong with human annotations and empirical\nfindings, can serve as a valuable benchmark for\nemployment-related dispute resolution.\n1 Introduction\nIn recent years, there has been great interest in\nadapting natural language processing techniques in\nthe legal domain. One notable application is the\nprediction of outcomes for legal disputes in various\njurisdictions (Xiao et al., 2018; Poudyal et al., 2020;\nHwang et al., 2022; Henderson et al., 2022; Niklaus\net al., 2023). However, the AI-based prediction of\nUK court decisions is still under-explored.\nThis paper investigates the prediction of dispute\noutcomes in the UK Employment Tribunal (UKET).\nThe UKET serves a crucial function in the UK jus-\ntice system, specifically dealing with employment-\nrelated disputes. Cases heard at the UKET covera wide range of issues, such as unfair dismissal,\ndiscrimination and breach of contract. The possi-\nbility to apply to the UKET for a decision ensures\nthat employment rights can be enforced. Knowing\nthe likely outcome of a court procedure improves\naccess to justice and facilitates amicable dispute\nresolution.\nThe contributions of this paper are as follows:\n1.We constructed a large-scale CLC-UKET\ndataset based on the Cambridge Law Corpus\n(CLC) (\u00d6stling et al., 2023). CLC-UKET in-\ncludes two components: CLC-UKET anno and\nCLC-UKET pred. CLC-UKET anno consists\nof a selection of 19,090 UKET case judg-\nments heard between 2011 and 2023 (inclu-\nsive). All cases come with metadata includ-\ning a unique case identifier, the hearing date\nand jurisdiction codes. We further provided\ndetailed legal annotations for all cases, includ-\ning (a) facts, (b) claims, (c) references to le-\ngal statutes, acts, regulations, provisions and\nrules, (d) references to precedents and other\ncourt decisions, (e) general case outcome and\n(f) detailed order and remedies. We further cu-\nrated CLC-UKET pred, specifically designed\nto facilitate a multi-class case outcome predic-\ntion task. CLC-UKET predconsists of 14,582\ncases, each supplemented with statements de-\ntailing the facts, claims and the general out-\ncomes of the cases.\n2.We presented human performance on the\nUKET outcome prediction task on CLC-\nUKET pred with the aim of setting a human\nperformance reference to calibrate prediction\nmodels.\n3.We experimented with a range of baseline\nmodels to predict the general case outcomes\nbased on information about facts and claims\nof UKET cases.arXiv:2409.08098v1  [cs.CL]  12 Sep 2024The CLC-UKET dataset and the empirical explo-\nrations aim to supplement the standard CLC dataset\nand facilitate future research on employment-\nrelated dispute resolution in the UK legal system.\nWe will make the CLC-UKET dataset available\nwith the final publication of this paper.\n2 UK Legal System and UKET\nThe UK has a special category of judicial body,\nthe employment tribunals, which deal exclusively\nwith employment disputes. The UKET is one of\nthe three largest tribunals in the greater tribunals\nsystem (Judicial Office, 2016). The UKET aims\nto provide a procedure which is easily accessible,\ninformal, speedy and inexpensive (BEIS, 2020, p.\n23). Claimants must comply with procedural and\nsubstantive requirements to be successful. For in-\nstance, claimants must submit their claims on time,\ncomply with the orders of the tribunal, present re-\nquired evidence or information in a timely man-\nner, and avoid scandalous, unreasonable or vexa-\ntious conduct (which makes a fair trial impossi-\nble). These are usually considered as procedural\nrequirements. Claimants must also comply with\nthe substantial requirements of the rules supporting\nthe claims. For example, in order to be success-\nful with a discrimination claim on grounds of dis-\nability, the claimant must prove their status as an\nemployee, demonstrate their disability and show\nthat they faced discrimination, which are consid-\nered as substantial requirements of the case. These\nprocedural and substantial requirements are not\nnecessarily determined at one final hearing or in-\ncluded in one final judgment. Instead, they may be\niteratively decided at different stages, which can\nresult in multiple decisions.\nThe employee (claimant) and the employer (re-\nspondent) submit their claims and responses, re-\nspectively, through a standardised form (Rules 8\nand 16 of the Employment Tribunals Rules of Pro-\ncedure 2013, hereinafter referred to as Rules). The\ntribunal considers these forms and may dismiss a\nclaim for procedural or substantial reasons, e.g., for\nlack of jurisdiction or for lack of any reasonable\nprospect of success (Rules, r. 27). At any stage\nof the proceedings, the tribunal can determine a\npreliminary issue, make a procedural order ( e.g.,\na deposit order or require the presentation of ad-\nditional documents) or make a final decision ( e.g.,\nstrike out the claim, Rules, r. 37). There may be\nmultiple final hearings for different issues, for ex-ample, one hearing to determine whether or not\na party is liable, another hearing to determine the\nremedy and another to determine who pays what\ncosts (Rules, r. 57). Each of these hearings results\nin a separate judgment, written out in a separate\ndocument. Finally, a party may request a reconsid-\neration of a previous judgment, which will lead to\nanother judgment (Rules, r. 70). As a consequence,\nthe resolution of a dispute may not be covered by\none judgment only, but may be determined by it-\nerative multiple decisions resulting in various case\ndocuments.\nEach decision is linked to one or multiple juris-\ndiction codes. In the case of UKET, there are 54\njurisdiction codes in total, which are used to iden-\ntify the matter of disputes. By way of example, the\njurisdiction code \u201cunfair dismissal\u201d is used when\nclaimants argue that they have been unfairly dis-\nmissed. This jurisdiction code is often employed in\naddition to other jurisdiction codes, such as unlaw-\nful deduction from wages, redundancy, protective\naward, breach of contract, and working time regu-\nlations.\nIn stark contrast with typical UK judgments,\nUKET decisions are relatively clearly structured,\nnot only because there are no dissenting opinions,\nbut also because there are specific rulings that\nset out which elements a judgment must contain\n(Rules, r. 62(5)). Nevertheless, UKET judgments\nare not always consistent since there are no formal\nrules on the style to be used in drafting a decision.\nMost English judgments summarise their decisions\nin a paragraph, although this summary does not\nneed to respect any particular form (d\u2019\u00c9tat, 2012,\np. 136). In the case of UKET, the summary is\noften found at the beginning of the judgment. How-\never, judgments on multiple claims are sometimes\ndivided into chapters, each analysing one claim\ncontaining the relevant decision. Also, while a\njudgment may contain an initial statement that the\nclaimant is successful, it may not be clear which\nclaim(s) this relates to in cases where there are\nmultiple claims.\n3 Related Work\n3.1 Analysis of Employment Judgments\nQuantitative methods for analysing legal judgments\nhave long been explored. In relation to employ-\nment law, Grunbaum and Newhouse (1965) anal-\nysed 20 US Supreme Court judgments to identify\nthe variables which impacted outcomes. Similarly,Field and Holley (1982) identified factors which\ninfluenced outcomes of performance appraisal judg-\nments. Brudney et al. (1999) analysed the extent to\nwhich extradoctrinal factors such as political party,\ngender and professional experience influenced out-\ncomes.\nMoreover, several studies explored correlations\nbetween specific demographic groups and the abil-\nity to pursue their employment rights in tribunal. In\nthe US, Schuster and Miller (1984) analysed 153\nfederal court cases, focusing on age discrimination,\nwhilst Schultz and Petterson (1992) investigated\nrace and sex discrimination. In the UK, Barnard\nand Ludlow (2016) investigated whether EU-8 mi-\ngrant workers were able to enforce their rights by\nbringing claims before the UKET.\nMany of these studies occurred before judgments\nwere published online, and therefore not only en-\ntailed costly journeys to the registers, but also re-\nquired manual extraction and tagging of specific\nelements of court decisions. More recently, Black-\nham (2021) conducted quantitative analyses of em-\nployment decisions, but despite having access to\nonline judgments, some of their tasks still required\nmanual labour.\n3.2 Legal Judgment Prediction\nThe advance of deep learning models alongside\nthe development of large-scale legal datasets has\ngreatly advanced the research on legal judgment\nprediction (LJP) (Xiao et al., 2018; Chalkidis\net al., 2019; O\u2019Sullivan and Beel, 2019; Ma et al.,\n2021; Chalkidis et al., 2023). A large number of\ndatasets have been created for both civil law sys-\ntems (Poudyal et al., 2020; Yamada et al., 2022)\nand common law systems (Caselaw Access Project,\n2018; Henderson et al., 2022; \u00d6stling et al., 2023;\nButler, 2024).\nFacilitated by large-scale datasets, recent years\nhave witnessed a surge in the application of deep\nlearning models to LJP. Zhong et al. (2018) intro-\nduced TopJudge to address LJP using multi-task\nlearning that combines three aspects: law articles,\ncharges and terms of penalty. Another notable con-\ntribution is the work of Ma et al. (2021) where an\nend-to-end framework was built to predict dispute\noutcomes using multi-task supervision and multi-\nstage representation learning. To the best of our\nknowledge, the only notable LJP paper on UK law\nis Strickson and De La Iglesia (2020), which dates\nbefore the emergence of LLMs and is limited to the\nbinary task of UK Supreme Court judges allowingor rejecting an appeal.\n4 The CLC-UKET Dataset\nWe curated a large-scale dataset focusing on UK\nemployment-related dispute resolution. The result-\ning CLC-UKET dataset consists of two compo-\nnents: CLC-UKET anno consisting of 19,090 cases\nwith detailed legal annotations and CLC-UKET pred\nwith 14,582 cases curated for case outcome pre-\ndiction for the UKET. The CLC-UKET dataset\nis constructed based on the UKET subset of the\nCLC (\u00d6stling et al., 2023) by adding annotations\nfor selected UKET cases. A common practice for\ncollecting legal annotations is to ask legal experts\nto manually annotate texts. However, this can be\ncostly and time-consuming. To alleviate the burden\nof manual annotation, we explored utilising large\nlanguage models (LLMs) for automatic annotation.\nThe dataset curation pipeline of CLC-UKET anno\nconsists of two steps: a case preparation module\nand an LLM-aided case annotation module.\n4.1 Case Preparation\nThe raw UKET subset of the CLC contains 52,339\ncases in total, covering employment-related cases\nheard at the UKET from January 2011 to August\n2023 (inclusive).1After analysing these cases, we\nnoticed that many cases only consist of one page\nas regards the tribunal decision. These cases are\nmostly the result of straightforward procedural de-\ncisions, for example when claimants withdraw their\ncases or respondents do not respond at all such that\na default judgment is made. As these cases do not\ncontain substantial information on the facts and\nthe substantive reasons (de Faria et al., 2024), we\nexcluded them at the case preparation step.\nAfter this filtering step, we obtained a collec-\ntion of 19,090 cases containing more than one page\nin their court decision files. For each case, we\ncollected a list of metadata, including a unique\ncase identifier used in the UKET records, date of\nfiling, date of decision, place of hearing, judges,\nclaimant(s), respondent(s) and appearances at the\nhearing. We also obtained jurisdiction codes for all\ncases from the UKET website.2In legal contexts, a\njurisdiction code typically refers to a numerical or\nalphanumeric code assigned to a specific legal ju-\nrisdiction, a certain subject matter or a geographic\n1The hearing dates of the cases in the UKET subset of\nthe CLC range from 2011 to 2023, although some cases were\nsubmitted before 2011.\n2https://www.gov.uk/employment-tribunal-decisions.area. Legal jurisdictions are defined areas with a\ndistinct set of laws and regulations. In the case of\nUKET, there are a total of 54 jurisdiction codes,\nwhich are used to identify the dispute matter. Each\nUKET case can be associated to multiple jurisdic-\ntion codes that indicate the categorical areas of the\ncase. As an example, the code unfair dismissal\nis used when claimants argue that they have been\nunfairly dismissed (challenging the procedure of\ndismissal or the reason for dismissal) and submit\na claim for payment of a certain sum, e.g., basic\naward, compensatory award (lack of notice pay\nand loss of earnings between a period) and injury\nto feelings award. A full list of jurisdiction codes\nin the UKET is presented in Appendix B.3.\n4.2 LLM-aided Case Annotation\nThe CLC provides raw texts of the decisions of\nUKET cases. These documents usually contain en-\ntangled statements about facts provided by parties\nand their lawyers, reasoning towards a decision,\nlegal statutes and precedents applied to justify the\nreasoning and final decisions regarding the case\noutcome. In this step, we followed similar lines\nto de Faria et al. (2024) and utilised the GPT-4-\nturbo model (Achiam et al., 2023) to automatically\nextract legal information from UKET decisions.\nWe applied an iterative development process to\nfind the optimal prompt for the purpose of legal\ninformation extraction. The final prompt that we\nopted for yielded the best results in terms of the\naccuracy of information extracted, the adequacy\nof necessary information contained and the level\nof detail. The final prompt that we used for LLM-\naided case annotation is presented in Appendix\nA.2. After automatic annotation, we obtained de-\ntailed annotations on important legal factors for\n19,090 CLC-UKET cases, covering (1) facts, (2)\nclaims, (3) references to legal statutes, (4) refer-\nences to precedents, (5) general case outcomes,\n(6) general case outcomes labelled as \u201cclaimant\nwins\u201d, \u201cclaimant loses\u201d, \u201cclaimant partly wins\u201d and\n\u201cother\u201d, (7) detailed orders and remedies and (8)\nreasons.\n5 Case Outcome Prediction\nThe annotated CLC-UKET data ( i.e., CLC-\nUKET anno) provides a large collection of court\ndecisions augmented with rich legal annotations,\nwhich can readily be used for downstream legal AI\ntasks. In this paper, we showcase a use case of theCLC-UKET data by investigating a classic task in\nlegal AI, i.e., case outcome prediction.\n5.1 Task Definition\nGiven a set of facts and claims of a UKET case, the\ntask of case outcome prediction aims to automati-\ncally generate an outcome label falling into one of\nthe four categories of \u201cclaimant wins\u201d, \u201cclaimant\nloses\u201d, \u201cclaimant partly wins\u201d and \u201cother\u201d. The\nfacts and the claims are the judges\u2019 summarisation\nof the statements provided by the claimant(s) and\nrespondent(s) prior to or during a hearing.\nMore formally, given a set of facts F=\nf1, f2,\u00b7\u00b7\u00b7, fmand a set of claims C =\nc1, c2,\u00b7\u00b7\u00b7, cnfor a UKET case, a prediction model\nCLSoutputs a label gfor the general case outcome:\ng=CLS(F, C )\nwhere g\u2208[\u201cclaimant wins\u201d, \u201cclaimant loses\u201d,\n\u201cclaimant partly wins\u201d and \u201cother\u201d].\nNote that there is a debate concerning the differ-\nence between the legal judgment prediction (LJP)\ntask and the case outcome classification (COC)\ntask (Medvedeva et al., 2021; Santosh et al., 2022;\nMedvedeva et al., 2023; Medvedeva and Mcbride,\n2023). In this paper, we opted for the terminol-\nogy \u201cprediction\u201d over \u201cclassification\u201d as we delib-\nerately excluded explicit information about case\noutcomes from the input of the prediction task, and\nonly kept descriptions of facts and claims in the\ninput. As such, this task focuses on predicting case\noutcomes based solely on information about case\nfacts and applicants\u2019 claims. Similarly, the legal ex-\nperts predicting outcomes had only access to facts\nand claims.\n5.2 Data Preparation for the Prediction Task\nWe tailored the CLC-UKET anno data to construct\na case outcome prediction task for UKET. Three\ntypes of legal factors are needed for the prediction\ntask, namely facts ,claims andgeneral case out-\ncomes . The input to the prediction models is a se-\nquence of fact statements concatenated with claim\nstatements, in the form of \u201c fact 1,fact 2,\u00b7\u00b7\u00b7,fact n\n[SEP] claim 1,claim 2,\u00b7\u00b7\u00b7,claim m\u201d. The target out-\nput of the prediction task is a general outcome label,\nwhich is a categorical variable labelling potential\ncase outcomes as claimant wins ,claimant partly\nwins,claimant loses andother .train val test\n#Cases 11,838 1,373 1,371\n#AvgFactLen 79 85 88\n#MaxFactLen 409 463 321\n#AvgClaimLen 34 34 34\n#MaxClaimLen 187 164 150\nTable 1: Data statistics of the CLC-UKET pred dataset.\n#Cases denotes the number of cases. #AvgFactLen de-\nnotes the average number of words per fact statement.\n#MaxFactLen denotes the maximum length of fact state-\nments. #AvgClaimLen denotes the average number of\nwords per claim statement. #MaxClaimLen denotes the\nmaximum length of claim statements.\n5.3 Data Statistics\nFrom the 19,090 cases in the CLC-UKET anno\ndataset, we filtered out cases where no substantial\ninformation about facts andclaims was extracted\nby GPT-4 at the LLM-aided case annotation step.\nAfter the filtering, we obtained a set of 14,582\nUKET cases, supplemented with fact and claim\nstatements extracted by GPT-4. We denote this\nprediction dataset as CLC-UKET pred. Following\ngeneral practice in machine learning research, we\ndivided the 14,582 CLC-UKET predcases into three\nsplits: training ,validation andtesting . The details\non data statistics of the train/val/test sets for CLC-\nUKET predare summarised in Table 1.\nNote that for the training andvalidation sets, all\nthree legal factors - facts, claims and outcomes -\nwere sourced from information automatically ex-\ntracted by GPT-4, as detailed in Section 4.2. For\nthetesting set, facts and claims were automatically\nextracted by GPT-4, whilst the case outcome labels\nwere manually annotated by a legal expert3. The\nexpert annotator carefully analysed the full court\njudgments and summarised the judges\u2019 decisions\ninto general case outcome labels. These manu-\nally annotated outcome labels for the test cases\nrepresent the actual judicial decisions, serving as\ngold-standard references for prediction evaluation.\n6 Experiments and Results\n6.1 Baseline Models\nWe experimented with two classes of baseline mod-\nels:\n1.Transformer-based (Vaswani et al., 2017)\nmodels, including BERT (Devlin et al., 2019)\nand T5 (Raffel et al., 2020);\n3The legal annotator is a PhD Candidate in Law.2.LLM-based models, including GPT-3.5 (Ope-\nnAI, 2022) and GPT-4 (Achiam et al., 2023).\nThe two Transformer-based models were fine-\ntuned on our CLC-UKET preddata, whilst GPT-3.5\nand GPT-4 were tested using zero-shot and few-\nshot settings without dedicated fine-tuning. Im-\nplementation details of the baseline models are\npresented in Appendix A.1.\nBERT . We fine-tuned BERT (Devlin et al., 2019)\non the training set of CLC-UKET pred with the\nAdam optimiser (Kingma and Ba, 2014) with a\nlearning rate of 1e-4 and a batch size of 32. The\nfinal checkpoint was obtained after training the\nmodel for 5 epochs.\nT5. The T5 model (Raffel et al., 2020) is also\nfine-tuned on the training set of CLC-UKET pred.\nThe model is optimised with a learning rate of 1e-4\nfor 5 epochs.\nGPT-3.5-turbo andGPT-4-turbo . We tested\nGPT-based models with diverse settings, including\n(1) zero-shot prediction, (2) few-shot prediction\nwith randomly selected examples and (3) few-shot\nprediction with examples selected according to ju-\nrisdiction codes. The prompts that we used for\nLLM experiments are presented in Appendix A.2.\n\u2022Zero-shot prediction . In this setting, the GPT-\nbased models are directly asked to predict an\noutcome based on information about facts and\nclaims of a case. No examples are provided\nfor the models in the prompts.\n\u2022Few-shot prediction with randomly selected\nexamples . We randomly selected a few exam-\nples from the training set and included them\nin the prompt to GPT-based models. We also\ninvestigated the effects of the number of ex-\namples on prediction performance by experi-\nmenting with two numbers ( i.e., 2 and 5) for\nexamples included in the prompts.\n\u2022Few-shot prediction with examples selected\nusing jurisdiction codes . This setting differs\nfrom the above few-shot setting in that we\ndeliberately sampled case examples according\nto jurisdiction code similarity. In other words,\ngiven a target case for which a case outcome\nis to be predicted, we first identified the set\nof jurisdiction codes associated with it. Next,\nwe gathered a collection of cases that share at\nleast one jurisdiction code with the target case.\nFrom this collection, we sampled a specifiednumber (similarly, 2 and 5) of example cases\nto include in the few-shot prompt.\n6.2 Human Prediction\nWe further investigated how well legal experts\ncan predict UKET case outcomes given facts and\nclaims. This investigation is of paramount impor-\ntance, as human performance can establish a refer-\nence to calibrate model performance.\nTwo legal experts conducted the human predic-\ntion exercise. They are PhD candidates in Law\nwith a focus on UK employment law. They were\nsupervised by a professor of law based in the UK.\nEach test case in CLC-UKET predwas separately\nannotated by the two legal experts. We asked anno-\ntators to indicate what they think is the most likely\ncase outcome after reading facts and claims of a\ncase. They were also asked to indicate whether\na prediction is of low confidence. Cases labelled\nwith low confidence are usually cases that are hard\nto predict due to insufficient information contained\nin the given facts or claims or due to the intrinsic\ncomplexity of a case.\nAt the beginning of the annotation process, both\nannotators were provided with annotation guide-\nlines (see Appendix D for details). The annotation\nguidelines are consistent with our overarching ex-\nperimentation design for the prediction task. An-\nnotators were asked to make their judgments sep-\narately, avoiding discussions amongst themselves.\nWe emphasised that human predictions should be\nmade based on the same facts and claims that pre-\ndiction models were evaluated on. Annotators were\nrequired to not search for the cases they were an-\nnotating on the internet.4Whenever questions re-\ngarding the implementation of the annotation arose\nduring the annotation process, the annotators were\nprovided with clarification by the supervisor.\nAfter annotating, we obtained two independent\nsets of predicted case outcome labels for the 1,371\ntest cases. The Cohen\u2019s Kappa score for all annota-\ntions is 0.421.\n6.3 Results\nOverall results . Table 2 presents the overall evalua-\ntion results for the CLC-UKET predprediction task.\nThe experiment findings reveal several key insights\nregarding the performance of different models. All\nmodels tested significantly outperform the random\n4However, annotators were free to research other informa-\ntion that might be helpful for the annotations, for example,\ninformation on the applicable law.Baseline Accuracy Precision Recall F-score\nRandom 0.241 0.340 0.241 0.276\nBERT 0.446 0.623 0.446 0.427\nT5 0.624 0.602 0.624 0.564\nGPT-3.5 zero 0.535 0.553 0.535 0.525\nGPT-3.5 rand 2 0.540 0.567 0.540 0.535\nGPT-3.5 rand 5 0.532 0.561 0.532 0.532\nGPT-3.5 juris 2 0.544 0.568 0.544 0.542\nGPT-3.5 juris 5 0.549 0.570 0.549 0.550\nGPT-4 zero 0.545 0.623 0.545 0.549\nGPT-4 rand 2 0.518 0.612 0.518 0.530\nGPT-4 rand 5 0.539 0.614 0.539 0.547\nGPT-4 juris 2 0.540 0.619 0.540 0.551\nGPT-4 juris 5 0.536 0.617 0.536 0.546\nHuman 0.693 0.680 0.693 0.672\nTable 2: Overall evaluation results for the multi-\nclass CLC-UKET pred prediction task. Precision ,\nrecall and F-score report the weighted average of\nprecision/recall/F-score obtained across labels, account-\ning for label imbalance. Random refers to random guess .\nHuman refers to the averaged scores of the outcome la-\nbels predicted by two human experts. All predicted\noutcomes were evaluated against gold-standard case\noutcome labels directly extracted from court decisions.\nguess baseline, indicating the models\u2019 efficacy on\nthis task. Among the models, the fine-tuned T5\nemerges as the best performer overall, achieving\nthe highest F-score. There is a noticeable gap be-\ntween machine and human performance, with hu-\nman expert predictions obtaining a 19.1% higher F-\nscore compared to the fine-tuned T5, highlighting\nthe superiority of human judgment in this domain\nin a baseline setting.\nIn terms of the two GPT-based models, GPT-\n4 generally outperforms GPT-3.5, reinforcing the\nadvancements made in this newer model version.\nHowever, the margin of GPT-4\u2019s outperformance is\nrather small. The inclusion of few-shot examples\nproves beneficial for improving GPT-3.5\u2019s predic-\ntion performance. Specifically, using examples\nthat share similar jurisdiction codes with the target\ncase enhances the F-score of GPT-3.5\u2019s predictions\nmore effectively than randomly sampled examples,\nvalidating the positive impact of incorporating task-\nspecific information on GPT-3.5\u2019s prediction per-\nformance. In addition, the marginal gains observed\nwhen varying the number of few-shot examples\nprovided to GPT-based models suggest that sim-\nply increasing the number of examples is not suffi-\ncient to significantly boost performance. Moreover,\nGPT-4, in its zero-shot setting, already achieves\nthe highest precision among all baseline models.\nProviding two similar cases in the juris-2 few-shotBaseline Label Precision Recall F-score\nBERTwins 0.459 0.828 0.591\nloses 0.869 0.215 0.345\npartly 0.381 0.364 0.372\nother 0.036 0.455 0.067\nT5wins 0.595 0.716 0.650\nloses 0.647 0.846 0.734\npartly 0.541 0.066 0.117\nother 0 0 0\nGPT-3.5 juris 5wins 0.515 0.700 0.594\nloses 0.720 0.565 0.633\npartly 0.362 0.305 0.331\nother 0.143 0.455 0.217\nGPT-4 juris 2wins 0.588 0.700 0.639\nloses 0.778 0.430 0.554\npartly 0.359 0.541 0.431\nother 0.082 0.364 0.133\nHumanwins 0.627 0.815 0.708\nloses 0.792 0.812 0.802\npartly 0.554 0.302 0.391\nother 0 0 0\nTable 3: Evaluation scores obtained by baseline models\nand human predictions for the four label categories:\nclaimant wins ,claimant loses ,claimant partly wins and\nother . The numbers of cases for the four labels are 437,\n618, 305 and 11, respectively. For GPT-3.5 and GPT-4,\nthe variants that achieved the highest F-sclores across\nrelevant settings are presented.\nsetting improves GPT-4\u2019s F-score compared to the\nzero-shot setting.\nResults for individual classes . In Table 3 we\nreport the individual scores achieved by baseline\nmodels and human predictions across various label\ncategories. Most baseline models demonstrate a\nhigh recall and a relatively low precision when pre-\ndicting \u201cclaimant wins\u201d and in contrast achieve a\nhigh precision and a relatively low recall when pre-\ndicting the \u201cclaimant loses\u201d label. These findings\nunderscore the distinct trade-offs that prediction\nmodels make between precision and recall. Hu-\nman predictions exhibit strong performance in the\n\u201cclaimant wins\u201d and \u201cclaimant loses\u201d categories,\nwhere the F-scores are consistently high. The la-\nbels \u201cpartly wins\u201d and \u201cother\u201d consistently receive\nlower evaluation scores across all models and the\nhuman predictors, which may be attributed to the\ninherent difficulty of identifying cases within these\ntwo categories, compounded by the imbalanced\ndistribution of cases across four categories.\nAblation study . This paper explores a fine-\ngrained prediction setting that differentiates be-\ntween cases where the claimant wins outright and\nthose where the claimant partially wins. This dis-\ntinction inherently creates a more challenging pre-Baseline Accuracy Precision Recall F-score\nRandom 0.380 0.487 0.380 0.420\nBERT 0.589 0.728 0.589 0.554\nT5 0.718 0.735 0.718 0.713\nGPT-3.5 juris 5 0.699 0.710 0.699 0.697\nGPT-4 juris 2 0.675 0.713 0.675 0.663\nHuman 0.812 0.807 0.812 0.810\nTable 4: Evaluation results obtained by baseline mod-\nels and human predictions when the labels \u201cwins\u201d and\n\u201cpartly wins\u201d are aggregated. Random refers to ran-\ndom guess .Human refers to the averaged scores of the\noutcome labels predicted by two human experts. For\nGPT-3.5 and GPT-4, the variants that achieved the high-\nest F-scores across relevant settings are presented.\ndiction task, as accurately predicting partly wins\nrequires a nuanced assessment of the claimant\u2019s ini-\ntial claims and the most likely outcomes for each\nindividual claim. To understand the added diffi-\nculty of our setting, we aggregated the judgments\nwith outcomes of \u201cwins\u201d and \u201cpartly wins\u201d ( i.e.,\ntreating both labels as \u201cwins\u201d) and evaluated per-\nformance under this simplified setting. The overall\nevaluation results are presented in Table 4.5A com-\nparison of the results in Table 2 and Table 4 shows\nthat all baseline models exhibit consistent improve-\nments in prediction performance across all metrics\nin the simplified setting, with T5 achieving the best\noverall performance. Human predictions achieved\na precision of 0.807 and a recall of 0.812, indicat-\ning that human annotators can effectively predict\ncase outcomes when there is no requirement to fur-\nther distinguish between the two winning-related\ncategories.\n7 Further Discussions\n7.1 Relevance of Scores\nWe would like to emphasise that the evaluation\nscores reported for the CLC-UKET predprediction\ntask are baseline results. Both the transformer-\nbased and the LLM-based models could be im-\nproved further for the task at hand. For example,\nthe latter could be further enhanced by incorporat-\ning retrieval-augmented generation (Lewis et al.,\n2020; Gao et al., 2023) or chain-of-thought (Wei\net al., 2022; Diao et al., 2023; Kim et al., 2023).\nSimilarly, human experts could invest more time\nand research into their predictions. Those inter-\nested in the legal domain are, therefore, encour-\naged to apply caution when drawing conclusions\n5We also present results for individual categories ( i.e.,\n\u201cwins\u201d, \u201closes\u201d and \u201cother\u201d) in Appendix C.for legal practice.\nAgainst this background, it is worth discussing a\nfew patterns in the scores. First, both models and\nlegal experts achieve higher recall than precision\nscores for \u201cwins\u201d and higher precision scores than\nrecall scores for \u201closes\u201d. Precision is a useful mea-\nsure when the costs of a wrongly predicted positive\nare high. In a litigation context, this is the case\nwhen the costs of initiating litigation ( e.g., fees for\nlegal and other advisers, court fees, time and nerves\ninvested etc) are high. Likewise, recall is a useful\nmeasure if the costs of missing a true positive are\nhigh. In the context of court proceedings, this is\nthe case when the opportunity cost of not initiating\nlikely successful litigation is high, for example, if\nthe expected remedy has a high monetary value\nor otherwise has a high relevance for the poten-\ntial claimant ( e.g., for emotional reasons). Hence,\nit depends on the specific situation of a potential\nclaimant whether precision or recall provides bet-\nter guidance. Since the UKET currently does not\ncharge fees and claimants can represent themselves\n(thereby saving costs), recall may be the preferable\nscore if the claim matters to the potential claimant.\nSecond, it is worth noting that the F-score of GPT-\n4juris 2for \u201cpartly wins\u201d outperforms the human\npredictors. This may indicate the LLM\u2019s ability to\nnavigate more complex litigation, which involves\nmultiple claims or multiple parties on either side.\n7.2 Possible Reasons for Errors\nModels and annotators, based on the extracted facts\nand claims, cannot always determine whether a tri-\nbunal\u2019s decision will finally resolve the claim or\nonly address a preliminary issue. For example, in\na disability discrimination case, the tribunal might\nfirst issue a judgment confirming the claimant\u2019s\ndisability (preliminary issue), followed by a sec-\nond judgment addressing the actual discrimination\nclaim. The first judgment (which the claimant may\nwin) is a necessary step but does not resolve the\nfinal claim, while the second judgment might con-\nclude that there was no discrimination (such that\nthe claimant ultimately loses). Preliminary issues\nare often contested, and some applications may\nsolely seek a tribunal declaration on such issues\n(e.g., confirming the claimant is an employee or\ndisabled). The possibility of such multi-step pro-\nceedings increases the complexity of predictions\nand has likely had a negative effect on the results\nfor both the models and the human predictors.\nFurther difficulties arise in cases where UKETrenders a procedural decision instead of deciding\non the substance of the claim. Such cases are clas-\nsified as \u201cother\u201d. However, both models and hu-\nman annotators may predict a substantive instead\nof a procedural decision and, therefore, suggest\n\u201cclaimant wins\u201d or \u201cclaimant loses\u201d. According to\nour annotation guidelines, this affects, in particu-\nlar, the categories of \u201cclaimant partly wins\u201d and\n\u201cother\u201d. This complexity may have contributed to\nlow evaluation scores for \u201cclaimant partly wins\u201d\nand \u201cother\u201d.\nMore generally, the extracted facts, which are\nthe basis for both the models\u2019 and the humans\u2019 pre-\ndictions, may not include all the elements needed\nto form a prediction. This may be the result of\nGPT-4 not including all details in the facts sec-\ntion when extracting the facts from the underlying\nUKET judgments. For example, when there is an\napplication for costs, which is highly dependent on\nthe parties\u2019 behaviour, the models and legal experts\nmay be limited in their prediction due to factual\ndetails missing. Additionally, certain outcomes\nmay hinge on factors like the respondent\u2019s failure\nto challenge the claim or produce evidence, which\nmight not be reflected in the extracted facts, lead-\ning to incorrect predictions. Although extracted\nfacts may include procedural aspects, they do not\nalways capture procedural facts that determine the\noutcome, such as the timing of a claim that is dis-\nmissed due to late submissions.\n8 Conclusion\nThis paper explores the prediction of dispute out-\ncomes for the UK Employment Tribunal (UKET).\nThe paper demonstrates efforts on employing\nLLMs for automatic annotation practice to allevi-\nate the burden of massive manual annotation. With\nLLM-aided annotation, we curated the CLC-UKET\ndataset with comprehensive, high-quality legal an-\nnotations. We showcased how the CLC-UKET\ndata can be used to construct a prediction task to\ncategorise case outcomes based on sequences of\nfacts and claims. We fine-tuned and evaluated two\nwidely used Transformer-based models on this pre-\ndiction task. We also evaluated LLMs on the pre-\ndiction task with a range of settings, and reported\nhuman performance on the task to facilitate model\ncalibration. These empirical efforts serve as a use-\nful benchmark for the UKET prediction task. We\nwill make the CLC-UKET dataset publicly avail-\nable with the publication of this paper.Ethics Statement\nThe curated dataset is developed on the basis of the\nCambridge Law Corpus (CLC), which aggregates\npublicly available UK legal judgments. Both the\ndecisions in the CLC and the jurisdiction codes of\nUKET are licensed for use under the Open Gov-\nernment Licence. This licence grants a worldwide,\nroyalty-free, perpetual and non-exclusive licence.\nAccess to the CLC is restricted to researchers with\nconfirmed ethical clearance and requires compli-\nance with the DPA and UK GDPR. While UK legal\njudgments are not anonymised, Rule 50 of the Em-\nployment Tribunal Rules ensures that sensitive per-\nsonal information is anonymised when necessary.\nAdditionally, Schedule 2, Part 5 of the DPA pro-\nvides derogations for academic research, alleviat-\ning the burden of notifying all individuals involved\nin judgments.\nOur dataset does not go beyond publicly avail-\nable information and includes established proce-\ndures for data removal if requested. Like the origi-\nnal CLC, access to the dataset created for this paper\nis limited to qualified researchers who adhere to\nthe relevant ethical and legal standards. Given the\npublic availability of the data and our efforts to\ndemocratise access to legal information, we be-\nlieve that we meet the ethical requirements for this\nresearch.\nFor more details on the legal and ethical consid-\nerations concerning the underlying CLC dataset,\nsee \u00d6stling et al. (2023).\nLimitations\nWhile our study provides valuable insights into the\nprediction of dispute outcomes for the UK Em-\nployment Tribunal, it is important to acknowledge\ncertain limitations of our findings.\nAccess to the actual facts and claims of the\ncases. The facts and claims used in this paper were\nextracted from tribunal decisions. This was nec-\nessary given the impossibility of obtaining actual\nfacts and claims in the number necessary for this\npaper. Consequently, we employed the extracted\nfacts and claims from the court judgments as a prac-\ntical substitute, providing a tangible foundation for\nour judgment prediction models.\nThis approach could potentially introduce infor-\nmation biases at the input stage of the prediction\ntask. The facts and claims that we used in the CLC-\nUKET dataset were derived from the judges\u2019 writ-\nten decisions at the end of the proceedings. Sincethe judges know the result of the case at this stage\nof the process, the texts they write may inherently\ncontain biased information (Sargeant and Magnus-\nson, 2024). For example, sentiment words in the\njudges\u2019 statements might implicitly reveal their in-\nclinations towards certain decisions. The models\nmight incorporate such factors when making pre-\ndictions related to case outcomes. Similarly, the\nlegal experts may have picked up such sentiments.\nIn subsequent research, we will explore alter-\nnative methods of identifying facts and claims to\nbetter approximate the original submissions to the\ncourt, thus fostering a more realistic modelling of\njudgment prediction.\nAutomatic information extraction. Manual\nannotation of legal texts requires extensive expert\nknowledge and can be costly. To alleviate these\nchallenges, this research utilised GPT-4 for au-\ntomatic information extraction. While the use\nof GPT-4 offers notable advantages in terms of\ntime and cost efficiency, and the extraction results\nare generally satisfactory according to the quality\ncheck conducted by legal experts in a related study\n(de Faria et al., 2024), this annotation practice is\nnot without flaws. The quality of legal annotations\ncould be further improved in future explorations.\nThere is also room to explore the effect of extract-\ning and providing more detailed facts compared to\nthe relatively concise fact statements present in the\ncurrent CLC-UKET dataset.\nDataset and evolution of law over time. We do\nnot know whether the datasets employed are repre-\nsentative or include all decisions by the UKET in\nthe relevant period. The dataset providing the cases\nto be predicted by the models and human experts\ncovers the years 2011 to 2023. During this time,\nboth employment and procedural law has evolved.\nPredicting a case outcome without knowing the\nprecise decision date may lead to mistakes. Models\nand human predictors did not have direct access to\nthe date at which the underlying case was decided.\nHowever, they may have inferred the decision date\nfrom the case identifier, which contains the year of\nthe decision.\nAcknowledgements\nThis project received funding support from the\nCambridge Centre for Data-Driven Discovery and\nAccelerate Programme for Scientific Discovery,\nmade possible by a donation from Schmidt Fu-\ntures. We are grateful for the helpful commentsfrom Ludwig Bull, Mateja Jamnik, Leif Jonsson,\nChikako Kanki, M\u00e5ns Magnusson, Holli Sargeant,\nTakenobu Tokunaga, Arvid Wenestam and the par-\nticipants of the \u201cArtificial Intelligence and Law\u201d\nworkshop at Hitotsubashi University in December\n2023.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. GPT-4 Technical Re-\nport. arXiv preprint arXiv:2303.08774 .\nCatherine Barnard and Amy Ludlow. 2016. Enforce-\nment of"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "# Summary of the Paper: \"Optimizing Falsification for Learning-Based Control Systems: A Multi-Fidelity Bayesian Approach\"\n\n## 1. Abstract\nThe paper addresses the falsification problem in learning-based closed-loop control systems, focusing on identifying counterexamples that violate safety requirements using simulations. Recognizing the computational expense of high-fidelity simulators, the authors propose a multi-fidelity Bayesian optimization (BO) framework which utilizes simulators of varying fidelity levels to enhance efficiency. Experiments conducted across different Gym environments reveal that the multi-fidelity approach is more computationally efficient compared to full-fidelity BO and other baseline methods. The proposed algorithm is available at [GitHub](https://github.com/SAILRIT/MFBO_Falsification).\n\n## 2. Introduction\n**Context**: The rise of cyber-physical systems, including autonomous vehicles and robotics, necessitates rigorous testing to ensure controllers operate safely and reliably. Simulation-based tests play a critical role in identifying potential failures and evaluating safety compliance.\n\n**Research Questions**:\n1. Can we efficiently identify counterexamples that violate safety requirements in learning-based control systems?\n2. How can we reduce the computational costs associated with using high-fidelity simulators for falsification?\n\n**Literature Review and Positioning**:\n- Different testing methods such as knowledge-based, data-driven, and search-based approaches.\n- Traditional optimization-based algorithms for falsification include ant colony optimization, stochastic local search, and simulated annealing, which do not fully leverage prior simulation information.\n- Application of Bayesian optimization (BO) in various domains has shown promise in black-box optimization but remains computationally expensive in high-fidelity simulations.\n- Multi-fidelity BO presents a promising approach for cost reduction by leveraging simulators with different fidelities.\n\n## 3. Related Work\n- Existing methods: Knowledge-based, data-driven, and search-based methods for testing autonomous vehicles.\n- Prior work on falsification: Optimization-based algorithms, black-box testing methodologies, and BO applications.\n- Limitations noted: High computational costs associated with using high-fidelity simulators.\n- Advances by this paper: Introducing a multi-fidelity BO framework that balances using lower fidelity for cost efficiency and higher fidelity for accuracy, thus improving the overall computational efficiency of the falsification process.\n\n## 4. Method\n**Proposed Approach**:\n- Multi-fidelity Bayesian Optimization Framework: Combines data from multiple simulators with different fidelity levels to identify likely counterexamples efficiently.\n\n**Mathematical Formulation**:\n- Robustness Value ($\\rho_\\phi(\\xi(e))$): Quantifies the degree of satisfaction or violation of the safety specification.\n- Optimization Problem: \n  \\[\n  \\arg\\min_e \\rho_\\phi(\\xi(e))\n  \\]\n\n**Gaussian Processes (GPs)**:\n- Utilized for approximating the objective function\u2019s model by continuously updating posterior estimates with new data.\n- Equations governing the GP\u2019s mean and covariance functions (refer to Eq. 7-9).\n\n**Multi-Fidelity Model**:\n- Linear relation between robustness values of different fidelity levels:\n  \\[\n  \\rho_i^\\phi(e) = \\eta_i \\rho_{i-1}^\\phi(e) + \\rho_{i,\\text{gap}}(e)\n  \\]\n- Auto-regression model to transfer knowledge across fidelities.\n\n**Acquisition Function**:\n- Modified Entropy Search:\n  \\[\n  e^*, i = \\arg\\max_{e \\in E, i \\in \\{1, \\ldots, q\\}} \\frac{\\alpha^{ES}_i(e)}{\\lambda_i}\n  \\]\n\n**Algorithm**:\n- Algorithm 1 demonstrates the multi-fidelity BO for falsification, iteratively updating the GP model and selecting next environment parameters and fidelity levels based on acquisition function values.\n\n## 5. Experimental Setup\n**Datasets**:\n- Various Gym environments including cart-pole, lunar lander, highway, merge, and roundabout scenarios.\n\n**Hardware and Software**:\n- Emukit toolkit for implementation.\n- Details of hardware not specified but assumed to involve standard computing setups for simulation and optimization tasks.\n\n**Evaluation Metrics**:\n- Reliability percentage of discovered counterexamples.\n- Computational cost, measured in terms of time and resources consumed per counterexample detection.\n\n**Baselines**:\n- Standard BO on high, middle, and low-fidelity simulators.\n- TuRBO-1 and \u03c0BO methods.\n\n## 6. Results and Analysis\n**Empirical Findings**:\n- Multi-fidelity BO demonstrated superior computational efficiency and reliability in detecting counterexamples compared to full-fidelity BO and other baseline methods.\n\n**Reliability Percentage**:\n- The highest reliability observed with multi-fidelity BO (both two and three-fidelity levels) across different benchmarks.\n\n**Computational Costs**:\n- Notable reduction in cost per counterexample using multi-fidelity approaches.\n\n**Figures and Tables**:\n- Fig. 2 shows different fidelity levels in the highway case study.\n- Fig. 3 compares the reliability percentages.\n- Fig. 4 and Fig. 5 depict the average cost per counterexample for cart-pole and lunar lander case studies, respectively.\n\n**Ablation Studies**:\n- Analysis of different fidelity level combinations (two-fidelity vs. three-fidelity) and their impact on efficiency and reliability.\n\n## 7. Discussion\n**Implications**:\n- The multi-fidelity BO framework improves efficiency and reliability in safety-critical system falsification by effectively balancing exploration and exploitation across different fidelity simulators.\n\n**Limitations**:\n- Complexity in determining realistic cost ratios.\n- Potential biases in manual assignment of costs and the need for automated, systematic approaches.\n\n**Future Directions**:\n- Extending multi-fidelity BO frameworks to more complex and higher-dimensional control systems.\n- Automating the cost measurement process for robust and reliable multi-fidelity BO applications.\n\n## 8. Conclusion\n**Key Contributions**:\n- Introduction of a multi-fidelity BO framework for falsification in learning-based control systems.\n- Demonstrated improvements in computational efficiency and reliability over full-fidelity BO and other baselines.\n\n**Future Research**:\n- Investigating automated cost assignment methods.\n- Expanding applicability to more complex cyber-physical systems and real-world scenarios.\n\nThis detailed summary provides a thorough understanding and replication roadmap for the paper, maintaining high technical precision and encompassing all critical aspects necessary for reproduction and further research."}, {"role": "assistant", "content": "Optimizing Falsification for Learning-Based Control Systems:\nA Multi-Fidelity Bayesian Approach\nZahra Shahrooei1, Mykel J. Kochenderfer2, and Ali Baheri1\nAbstract \u2014 Testing controllers in safety-critical systems is\nvital for ensuring their safety and preventing failures. In this\npaper, we address the falsification problem within learning-\nbased closed-loop control systems through simulation. This\nproblem involves the identification of counterexamples that\nviolate system safety requirements and can be formulated as\nan optimization task based on these requirements. Using full-\nfidelity simulator data in this optimization problem can be\ncomputationally expensive. To improve efficiency, we propose\na multi-fidelity Bayesian optimization falsification framework\nthat harnesses simulators with varying levels of accuracy. Our\nproposed framework can transition between different simu-\nlators and establish meaningful relationships between them.\nThrough multi-fidelity Bayesian optimization, we determine\nboth the optimal system input likely to be a counterexample\nand the appropriate fidelity level for assessment. We evaluated\nour approach across various Gym environments, each featuring\ndifferent levels of fidelity. Our experiments demonstrate that\nmulti-fidelity Bayesian optimization is more computationally\nefficient than full-fidelity Bayesian optimization and other\nbaseline methods in detecting counterexamples. A Python\nimplementation of the algorithm is available at https://\ngithub.com/SAILRIT/MFBO_Falsification .\nI. I NTRODUCTION AND RELATED WORKS\nThe rapid development of cyber-physical systems, includ-\ning autonomous vehicles and robotics, has enhanced the need\nfor controllers to function both safely and reliably. Ensuring\nthe safety of these systems is crucial to prevent catastrophic\nfailures. This involves using simulation-based tests to iden-\ntify potential failures, determine the most likely failure mode,\nand estimate system failure likelihood. Various methods have\nbeen developed for generating test scenarios for autonomous\nvehicles, including knowledge-based [1], [2], data-driven\n[3], [4], and search-based approaches [5], [6]. Knowledge-\nbased methods use formal models to create scenarios based\non traffic rules, while data-driven approaches rely on real-\nworld driving data to identify critical situations. On the other\nhand, search-based techniques explore possible scenarios by\nsystematically testing different driving conditions to ensure\ncomprehensive coverage.\nFalsification is a search-based testing procedure that tries\nto disprove safety by discovering an input (a counterexample)\nthat leads to a safety specification violation. A safety spec-\nification is a set of requirements that must be met by the\nsystem during execution. For falsification, the specification\nis often expressed in signal temporal logic [7] or metric\n1Zahra Shahrooei and Ali Baheri are with the Department of Mechanical\nEngineering at Rochester Institute of Technology. zs9580@rit.edu\nandakbeme@rit.edu\n2Mykel J. Kochenderfer is with the Department of Aeronautics &\nAstronautics at Stanford University. mykel@stanford.eduinterval temporal logic [8]. Various algorithms have been\ndeveloped to solve the falsification problem. Among these,\noptimization-based algorithms are designed to guide the\nfalsification process by defining a suitable objective function\nthat guides the search towards an environment trace where\nthe specification is not satisfied. For tackling this global\noptimization problem, black-box methods offer a structured\napproach for uncovering safety failures in systems without\nthe need for understanding their internals. Black-box testing\ninvolves searching the uncertainty space, where the param-\neters are system inputs and the goal is to detect failures.\nSequential search algorithms include ant colony optimization\n[9], stochastic local search [10], and simulated annealing\n[11]. While these algorithms excel at efficiently navigating\nthe uncertainty space, they do not make effective use of the\ninformation gained from prior simulations.\nIn this paper, we use Bayesian optimization (BO), a\nblack-box algorithm to globally optimize an unknown ob-\njective function by approximating its model using Gaussian\nprocesses (GPs) [12] and continuously updating posterior\nestimates to help determine the next point to evaluate. BO\nhas seen successful applications ranging from robotics [13],\n[14] and control [15] to design optimization problems [16],\n[17] and hyper-parameter tuning in machine learning [18].\nAdditionally, BO has been extensively applied in safety\ntesting [19]\u2013[24]. These studies typically focus on safely\noptimizing an unknown function [25]. For instance, Ghosh\net al. [21] use BO to address the falsification problem\nin uncertain closed-loop control systems. They model the\nunknown specification by breaking it down into a parse\ntree with individual constraints represented by nodes and\neach constraint modeled using GPs to reduce number of\nthe required queries for falsification. Deshmukh et al. [19]\nproposed the use of random embedding BO (REMBO) [26]\nto tackle the falsification problem in cyber-physical systems\nwith high-dimensional input spaces. Ramezani et al. [23]\napplied two other extensions of BO to falsification, trust\nregion BO (TuRBO) [27] and \u03c0BO [28]. TuRBO makes use\nof several local GPs, each within its trust region that shrinks\nor expands based on the success or failure of solutions.\n\u03c0BO, on the other hand, focuses on injecting the expert prior\nknowledge of the falsifying points to the BO framework. All\nthese studies apply BO directly on the full-fidelity simulator.\nA significant drawback of using BO for simulation-based\nfalsification is the high computational cost of evaluating the\nobjective function on full-fidelity simulators.\nMulti-fidelity BO is a promising approach for cost re-\nduction when leveraging rich, high-fidelity simulators. ThisarXiv:2409.08097v1  [eess.SY]  12 Sep 2024framework has been successfully applied to engineering de-\nsign problems, scientific discovery, and hyperparameter op-\ntimization [29]\u2013[31]. Multi-fidelity BO exploits information\nfrom various fidelities to achieve higher efficiency than using\nfull-fidelity. Fidelity level in simulators refers to how closely\na simulator replicates the real system. High-fidelity and low-\nfidelity simulators can differ widely based on various factors\nacross different applications. For instance, model fidelity\nrefers to the simplification of the mathematical representation\nof the physical system, typically by simplifying the differ-\nential equations being solved or the numerical model. Ad-\nditionally, sensor fidelity is related to having more accurate\nsensors within the simulator while environmental fidelity is\nassociated with considering the effect of environment factors\nsuch as weather condition or disturbances in simulator.\nThere are also works on multi-fidelity BO in control\nsystems [32]\u2013[35]. For example, Marco et al. [34] incor-\nporate two degrees of fidelity involving both simulations\nand physical experiments in order to optimize controller\nparameters more efficiently. They propose an acquisition\nfunction aimed at striking a balance between cost and accu-\nracy. However, they assume that the costs of simulation and\nreal-world experimentation are predefined constants. Hence,\nthe proposed algorithm suffers from lack of robustness to\ncost variation.\nThis paper is an extension of our conference publication\n[36] that proposes the first multi-fidelity BO application\nto the computationally demanding task of falsification for\nclosed-loop control systems using data from different levels\nof fidelity. The framework has been shown to successfully\ncut down the cost of falsification in comparison to using full-\nfidelity simulator. However, we used predefined costs for the\nsimulators with varying levels of fidelity, chosen manually\nand without any systematic measurement or analysis even\nthough these costs directly influence the acquisition function\nand, consequently, the selection of the optimal fidelity level\nfor evaluation. This paper expands the results of [36] through\nthe following contributions:\n\u2022Unlike the previous paper, which focused solely on\na two-fidelity setting, we investigate both two-fidelity\nand three-fidelity levels. We explore the role of a third\nfidelity level in reducing the cost of falsification, en-\nhancing the applicability and flexibility of our approach.\nAdditionally, we provide a detailed analysis of the\ncost changes associated with transitioning from a two-\nfidelity setting to a three-fidelity setup, highlighting the\ntrade-offs involved.\n\u2022To avoid using manually assigned costs associated with\ndifferent fidelity levels which can degrade the multi-\nfidelity BO performance, we propose a method to\ncalculate the cost proportions of various fidelity degrees\nby measuring specific parameters, which offers a more\nrealistic and adaptable cost model making multi-fidelity\nBO framework more robust and reliable in various\napplications.\n\u2022Unlike previous work, we extend the use of multi-\nfidelity BO to falsification of complex systems, specif-ically in the context of autonomous driving to demon-\nstrate the applicability of multi-fidelity BO approach.\n\u2022We study the trade-off between cost and accuracy in\nthe context of falsification. We demonstrate that using\nmulti-fidelity BO can improve accuracy compared to\nusing low-fidelity simulation alone and assess the effi-\ncacy of multi-fidelity BO in cost reduction compared\nto standard BO and other extensions of BO that rely\nexclusively on high-fidelity simulators in several case\nstudies.\nThis paper is organized as follows. Section II specifies\nthe problem formulation in terms of safety requirements and\noutlines the BO framework. Section III explains how we use\nmulti-fidelity BO for falsification. Section IV describes how\nwe implement multi-fidelity BO on four case studies. Section\nV discusses our empirical results. Section VI concludes our\nwork and provides potential future directions.\nII. P ROBLEM FORMULATION AND PRELIMINARIES\nThis section briefly reviews our notation, system safety\nspecification, quantitative semantics, and BO concepts.\nA. Problem Formulation\nWe assume that we have access to a set of simulators with\nvarying degrees of fidelity specified by S1toSqfor a system\nof interest S. All these qsimulators operate in a stochastic\nenvironment Ethat includes various types of uncertainty. We\ncan parameterize each simulator with a set of environment\nparameters e\u2208E. Hence, each simulator takes an instance\nof the environment eas an input and returns a finite-length\ntrajectory \u03be(e). Also, we have a controller \u03c0trained for the\nhighest-fidelity simulator Sq. We aim to test this controller\nperformance against the system specification \u03c6.\nFor this purpose, we use quantitative semantics to estimate\nthe robustness value of safety specification satisfaction (dis-\nsatisfaction) \u03c1\u03c6(\u03be(e)). To falsify the controller, we explore\nsearch space for cases that correspond to negative robustness\nvalues ( \u03c1\u03c6(\u03be(e))<0). This can be considered as an opti-\nmization problem with the robustness value as the objective\nfunction\nargmin\ne\u03c1\u03c6(\u03be(e)) (1)\nTypically, solving this optimization problem using high-\nfidelity data is costly, but we combine the information\nfrom simulators with different levels of fidelity with limited\nexperiments on the highest-fidelity simulator to identify the\nenvironment parameters that are likely to be counterexam-\nples. In particular, we use BO to effectively direct the search\nover the environment and choose which simulator to use.\nB. Safety Specification and Quantitative Semantics\nA specification \u03c6is defined with respect to system\ntrajectories. We conceptualize \u03c6as the set of all finite-\nhorizon trajectories of the system that meet the system-\nlevel safety specification. A system trajectory \u03beS(e)for an\nenvironment parameter esatisfies the specification if and\nonly if \u03beS(e)\u2208\u03c6. In fact, \u03c6evaluates a system trajectoryto true if \u03beS|=\u03c6. The specification is composed of several\nindividual constraints known as predicates. These predicates\nserve as the fundamental building blocks of the logic, which\ncan be joined together using specific logical operations. We\nassume that a predicate \u00b5is a smooth function along a\nsystem trajectory \u03beS(e). We say a predicate is satisfied\nif\u00b5(\u03beS(e))is greater than 0or falsified otherwise. The\nlogical operations \u00ac(negation), \u2227(conjunction, which means\n\u201cand\u201d), and \u2228(disjunction, which means \u201cor\u201d) are used\nto combine these predicates. Rather than just checking the\nBoolean satisfaction of a predicate, the concept of robust\nor quantitative semantics is defined to quantify the degree of\nsatisfaction. This allows us to move beyond binary notions of\n\u201csafe\u201d or \u201cunsafe\u201d by associating each predicate with a real-\nvalued function \u03c1\u00b5(\u03beS(e))of the trajectory \u03beS(e), which\nprovides a \u201cmeasure\u201d of the margin by which \u00b5(e)is met.\nThis robustness value \u03c1\u00b5(\u03beS(e))is defined such that\n\u03beS(e)|=\u00b5\u2194\u03c1\u00b5(\u03beS(e))>0 (2)\nSpecification Monitoring \nBayesian Optimization Policy Fidelity 1 \nFidelity 2 \nFidelity q robustness value trajectory \nenvironment parameter   fidelity level \nenvironment \nFig. 1: Overview of multi-fidelity BO falsification. The algo-\nrithm begins by performing random experiments at different\nfidelity levels, with fewer experiments conducted at higher\nfidelity levels. The robustness values across trajectories are\nrecorded, and a GP model is initialized. The algorithm\nthen optimizes entropy search over this model to select a\ncandidate environment parameter e, along with the fidelity\nindex i\u2208 {1, . . . , q }at which the next experiment should\nbe performed. The GP model is then updated with the new\ndata.\nThe quantitative semantics of the predicates are used to\nestablish the quantitative semantics of the safety specification\n\u03c6using the following rules:\n\u03c1\u00b5>0:=\u00b5(\u03beS(e)) (3)\n\u03c1\u00ac\u00b5(\u03beS(e)) :=\u2212\u00b5(\u03beS(e)) (4)\n\u03c1\u03c6\u2227\u03c8(\u03beS(e)) := min ( \u03c1\u03c6(\u03beS(e)), \u03c1\u03c8(\u03beS(e))) (5)\n\u03c1\u03c6\u2228\u03c8(\u03beS(e)) := max ( \u03c1\u03c6(\u03beS(e)), \u03c1\u03c8(\u03beS(e))) (6)\nBy evaluating the robustness value for a specific environ-\nment parameter \u03c1\u03c6(\u03beS(e)), we can determine whether the\nsystem behaves safely in the corresponding e. The sign of\u03c1\u03c6(\u03beS(e))serves as an indicator of whether the specification\n\u03c6is satisfied (non-negative value) or not satisfied (negative\nvalue). Additionally, robust semantics enables us to estab-\nlish a hierarchy among failure cases. When \u03c1\u03c6(\u03beS(e1))<\n\u03c1\u03c6(\u03beS(e2)), failure case e1is comparatively more severe\nthane2. To simplify the notation, we use \u03c1\u03c6(e)instead of\n\u03c1\u03c6(\u03beS(e))throughout the rest of paper.\nC. Bayesian Optimization\nThere have been several applications of BO for testing\nlearning-based control systems. For black-box systems in\ngeneral, it is not known in advance how the robustness value\n\u03c1\u03c6(e)depends on the environment parameter e. To address\nthis, we use GPs to estimate evaluations of robustness\nvalues within the environment parameters based on prior\nevaluations to aid in the optimization [12]. GPs are defined\nby a mean function m:Rn\u2192Rand a positive semi-\ndefinite covariance kernel function k:Rn\u00d7Rn\u2192R.\nIn this manner, the unknown nonlinear robustness function\n\u03c1\u03c6(e) :E\u2192Ris represented as random variables, such that\nany finite number of them follows a multivariate Gaussian\ndistribution. In this paper, the prior mean of the distribution\nm(e)is set to 0. The kernel function k(e,e\u2032)expresses the\ncovariance between function values \u03c1\u03c6(e)and\u03c1\u03c6(e\u2032)at two\nenvironment parameters eande\u2032. Suppose that we have a\nset of nobservations: yn= [\u02c6\u03c1\u03c6(e1),\u02c6\u03c1\u03c6(e2), . . . , \u02c6\u03c1\u03c6(en)]\nobserved with independent Gaussian noise \u03c9\u223cN(0, \u03c32)at\nenvironment parameters En= [e1,e2, . . . ,en], i.e., \u02c6\u03c1\u03c6(e) =\n\u03c1\u03c6(e) +\u03c9. By treating the outputs as random variables, the\nposterior distribution of \u03c1\u03c6(e)is characterized by\nmn(e) =kn(e)(Kn+In\u03c32)\u22121yn (7)\nkn(e,e\u2032) =k(e,e\u2032)\u2212kn(e)(Kn+In\u03c32)\u22121kT\nn(e\u2032)(8)\n\u03c32\nn(e) =kn(e,e\u2032) (9)\nnoting that the vector kn(e) = [ k(e,e1), . . . , k (e,en)],\n\u03c32\nn(e)is variance, Inis the identity matrix, and Knis the\npositive definite kernel matrix [k(e,e\u2032)]e,e\u2032\u2208En.\nSearch for an optimum in a GP is guided by an acquisition\nmethod, such as probability of improvement [37], expected\nimprovement [38], upper confidence bound [39], and entropy\nsearch [40]. These functions balance between exploring new\nareas and exploiting promising ones. In areas where the\nobjective function is predicted to be optimal or in areas that\nhave not yet been explored, the acquisition function assigns\na higher value.\nEntropy search, as an information-theoretic acquisition\nfunction, seeks to maximize the expected information gain\nfrom observing a subsequent query regarding the optimum,\ndenoted as e\u2217. We represent the nth probability distribution\novere\u2217asPn(e\u2217)and its associated level of uncertainty as\nits entropy, denoted by H(Pn(e\u2217)). Similarly, we can define\nH(Pn(e\u2217|\u00afe,\u00af\u03c1\u03c6(\u00afe)))as the entropy of what the (n+ 1) th\nprobability distribution over e\u2217would be if we were to\nobserve it at an environment instance represented by \u00afeand\nobtain a corresponding value of \u00af\u03c1\u03c6(\u00afe). This measure of\nuncertainty depends on the specific observation value \u00af\u03c1\u03c6(\u00afe).Algorithm 1 Multi-Fidelity BO for falsification\nRequire: simulators Si, relative costs of simulators \u03bbi,i= 1, . . . , q\nuncertainty space E, policy \u03c0, safety specification \u03c6, number of BO iterations n\n1:Initialize GP over the nested training inputs from qsimulators and corresponding robustness values\n2:fort= 1,2, . . . , n do\n3: Compute the acquisition function \u03b1ES(e)over GP model, as defined by Eq. 10\n4: e\u2217, i= argmax\ne\u2208E,i\u2208{1,...,q}\u03b1ES\ni(e)/\u03bbi\n5: Execute the scenario on Sifor input e\u2217and monitor the resulting trajectory \u03beSi\n6: Derive the safety specification robustness value \u03c1i\n\u03c6(e\u2217)across the trajectory\n7: Update GP model with new data\n8: if\u03c1i\n\u03c6(e\u2217)<0then\n9: Counterexample e\u2217detected on fidelity i\nWe can quantify the reduction in uncertainty resulting from\nmaking such an observation using\n\u03b1ES(\u00afe) =H(Pn(e\u2217))\u2212Eh\nH\u0010\nPn\u0000\ne\u2217|\u00af\u03c1\u03c6(\u00afe)\u0001\u0011i\n(10)\nwhere Erepresents the expected value. Maximizing \u03b1ES(\u00afe)\ntranslates to minimizing the uncertainty in our posterior after\nincorporating new observations.\nIII. M ETHODOLOGY\nFull-fidelity simulators for a system of interest offer a\nvariety of settings, the adjustment of which can signifi-\ncantly impact the simulator\u2019s fidelity. Starting with a high-\nfidelity simulator, lower-fidelity versions can be developed\nusing techniques such as model order reduction, physical\nsimplification, or data-driven approximations. Model order\nreduction simplifies the system\u2019s equations while preserving\nkey dynamics, physical simplification approximates or ig-\nnores certain phenomena to expedite simulations, and data-\ndriven approaches employ surrogate models to replicate high-\nfidelity outputs at reduced computational costs. In this work,\nwe rely on domain expertise to develop low-fidelity simula-\ntors customized specifically to our application. Therefore,\nwe consider qsimulators with different levels of fidelity\nfor a system of interest specified by S1, . . . , S q. Here, S1\nrepresents the lowest fidelity simulator (the cheapest, fastest,\nand least accurate one), while Sqis the highest fidelity\nsimulator (the most expensive, slowest, and most accurate\none).\nThe relative query costs associated with these simulators,\nexpressed by \u03bb1, . . . , \u03bb q, where 1< \u03bb 1< . . . < \u03bb q\nare determined by conducting some specific measurements.\nFor each simulator, we can monitor trajectories and derive\nrobustness values \u03c1i\n\u03c6(e)fori\u2208 {1, . . . , q }. Our goal is to\nestimate the black-box function \u03c1q\n\u03c6(e)given the observation\ninformation across different fidelities. Our first step is to es-\ntablish a relationship between the robustness values obtained\nfrom various simulators. For this purpose, we use multi-\noutput GPs that allow us to model a set of vector-valued\noutputs (the robustness values) based on a corresponding\nset of inputs (environment parameters) and represent the\noutputs as a multivariate normal distribution [41]. For usingmulti-output GPs, we also add a dimension to the input\nspace to represent the fidelity index i\u2208 {1, . . . , q }. Doing\nso, the multi-output GP can be seen as a single-output GP\non the extended input space. Let Eibe the environment\nconfigurations of fidelity ifor initializing GP model. To\nimprove computational efficiency [42], we suppose that these\ninitialization input data have a nested structure, i.e., the\ninitialization input data for a simulator with a higher fidelity\nindex is a subset of the input data for a simulator with a lower\nfidelity index ( Eq\u2286Eq\u22121\u2286. . .\u2286E1). We assume that\nthere is a linear correlation between the robustness values\nof different fidelity levels, as it facilitates straightforward\nintegration of data from multiple fidelities. We use the\nauto-regression model [41] to express the relation between\nfidelities\n\u03c1i\n\u03c6(e) =\u03b7i\u03c1i\u22121\n\u03c6(e) +\u03c1i\ngap(e) (11)\nHere, \u03b7iis a constant parameter that transfers knowledge\nlinearly from the lower fidelity level to the higher one,\nand\u03c1i\ngap(e)is the bias term between fidelities, which is\nan independent GP with its own mean function mi\ngapand\nkernel function ki\ngap(e,e\u2032). We assume \u03c1i\u22121\n\u03c6(e)and\u03c1i\ngap(e)\nare independent processes linked only by the above equation.\nIn this equation, while the linear term captures the overall\ntrend between fidelity levels, the GP gap term is capable\nof capturing any residual non-linear relationships. These\nrobustness values take the joint Gaussian distribution of the\nform\u0014\u03c1i\u22121\n\u03c6\n\u03c1i\n\u03c6\u0015\n\u223cGP\u0010\u00140\n0\u0015\n,\u0014ki\u22121 \u03b7iki\u22121\n\u03b7iki\u22121\u03b72\niki\u22121+ki\ngap\u0015\u0011\n(12)\nThe choice of kernel function is problem-dependent and en-\ncodes assumptions about smoothness and the rate of change\nof the robustness values. We use the radial basis function\n(RBF) kernel for both the error ki\ngapand lower fidelity\nsimulator indicated by ki\u22121.\nIt is worth noting that considering linear relations between\nthe outputs of different fidelity levels can simplify the\ncomputational modeling process but it is often insufficient\nfor complex models. There are versions of multi-fidelity\nBO that assume nonlinear relationships between fidelities,\nas mentioned in [43]. However, these approaches also havelimitations for use in certain contexts. One significant limita-\ntion is that these methods require lower-fidelity levels even\nduring prediction to maintain a nested training set, which\ncan be restrictive. Additionally, they often assume that the\noutputs are noiseless, which is challenging to meet.\nUpon constructing the GP model, we use an acquisition\nfunction to select both the fidelity index and the environment\nparameter that leads to the minimum robustness value. We\nuse a modified entropy search acquisition function:\ne\u2217, i= argmax\ne\u2208E,i\u2208{1,...,q}\u03b1ES\ni(e)/\u03bbi (13)\nEq. 13 allows us to switch between different fidelity levels to\nreduce the computational costs of our experiments. Addition-\nally, it performs two exploration and exploitation trade-offs\nat the same time: one at each fidelity level to search over\nthat specific fidelity, and the other between different fidelity\nlevels to build a more accurate GP model.\nAlgorithm 1 outlines multi-fidelity BO for falsification.\nThe algorithm is designed to model the robustness values\nusing high-accuracy data from simulators with higher fi-\ndelity levels, as well as the data richness of simulators\nwith lower fidelity levels. Once the prior GP over these q\nsimulators is obtained, BO is performed iteratively until a\nstopping criterion is met. On each iteration, the algorithm\nseeks to find the worst counterexample by maximizing the\nacquisition function in Eq. 13. Depending on the gap and\ncost ratios between simulators, the algorithm chooses the\ncandidate environment parameter and fidelity index to run\nthe next experiment. Then, the GP is updated with the\nnew environment instance and its corresponding robustness\nvalue on that specific simulator. The algorithm outputs the\nidentified counterexamples and their corresponding fidelity\nindices. Fig. 1 shows multi-fidelity BO approach applied to\nfalsification.\nIV. E XPERIMENTS\nIn this section, we provide the implementation details and\noutline the methodology used to measure the cost ratios be-\ntween different fidelity levels. We then discuss the baselines\nused for comparison. Finally, we describe our benchmarks.\nFor each of these case studies, we identify the variables that\ninfluence the fidelity of simulator. These variables can be of\nvarious types: discrete, continuous, or categorical. Once these\nvariables are identified, we explore different combinations of\nthese variables to adjust and control the simulator\u2019s fidelity.\nWe develop low-, middle-, and high-fidelity simulators and\nexamine both two- and three-fidelity BO approaches. The\nsource code for our experiments is online.1\nA. Cost Ratios Measurement\nSelecting the next fidelity level for evaluation in Eq. 13\ndepends on the relative costs of simulators. Hence, we avoid\nallocating pre-determined constant costs to simulators and\nconsider that the cost ratios of the high-fidelity simulator to\nthe other two simulators can be determined by measuring\n1https://github.com/SAILRIT/MFBO Falsification\nX (a)\nX (b)\nFig. 2: Fidelity levels in the highway case study. (a) Low-\nfidelity highway simulator at t=t0andt= 12 t0. (b)\nHigh-fidelity highway simulator at t=t0andt > 16t0.\nThe simulation frequencies in the low-fidelity simulator and\nthe high-fidelity simulator are 11and15; respectively. For\nan identical scenario, the high-fidelity simulator is slower.\nAdditionally, the high-fidelity simulator features more cars\non the road.\ntwo factors: simulation time and output similarity. Simu-\nlation time measures how long it takes for simulators to\nrun a similar experiment, and output similarity quantifies\nhow closely the trajectories of different simulators align.\nNumerous tests are available to determine the similarity\nbetween trajectories of the low-, middle-, and high-fidelity\nsimulators subjected to identical inputs. However, we employ\ncosine similarity due to its straightforwardness and clarity.\nEq. 14 demonstrates the cost ratios between high-, low-, and\nmiddle-fidelity simulators:\n\u03bbhf\n\u03bblf=thf\ntlf\u00d7shf\nlf\n\u03bbhf\n\u03bbmf=thf\ntmf\u00d7shf\nmf(14)\nwhere thf,tmf, and tlfare the time for executing an identi-\ncal experiment on the high-fidelity, low-fidelity, and middle-\nfidelity simulators. Further, cosine similarities between tra-\njectories of the high-fidelity simulator and trajectories of the\nlow- and middle-fidelity simulators are\nshf\nlf=\u03behf\u00b7\u03belf\n\u2225\u03behf\u2225\u2225\u03belf\u2225\nshf\nmf=\u03behf\u00b7\u03bemf\n\u2225\u03behf\u2225\u2225\u03bemf\u2225(15)\nTo provide more precise cost ratios, we compute the\naverage similarity among ten trajectories for each of our case\nstudies, along with the mean simulation time resulting from\nten identical experiments conducted at various fidelity levels.\nB. Baselines\nWe compare the performance of multi-fidelity BO with\nstandard BO across high-, middle-, and low-fidelity sim-\nulators. In addition to these individual fidelity levels, we\nincluded two other baselines in our analysis: TuRBO- 1and\n\u03c0BO.\n1)TuRBO- 1:TuRBO- 12is a variant of the TuRBO\nalgorithm [27] that focuses on using a single trust region\nfor optimization. In TuRBO- 1, the optimization is performed\n2https://github.com/uber-research/TuRBOTABLE I: Multi-fidelity settings for case studies\nCase Study Setting Type Low-fidelity Middle-fidelity High-fidelity\nCart-PoleKinematic integrator Categorical Euler Euler semi-implicit Euler\nForce magnitude Continuous 10 15 20\nPosition sensor Noise Continuous N(0,0.25) N/A N/A\nSensors precision Discrete 2digits 6digits 8digits\nEpisode length Discrete 150 300 450\nLunar LanderWind power Continuous 0 6 19 .9\nTurbulence power Continuous 0 0 1 .99\nEpisode length Discrete 200 400 800\nHighwaySimulation frequency Discrete 11 13 15\nNumber of road cars Discrete 23 24 25\nPosition sensor noise Continuous N(0.1,0.2) N(0.1,0.1) N/A\nSpeed sensor noise Continuous N(0.5,0.5) N(0.3,0.3) N/A\nMergeMax comfort acceleration Continuous 3 3 .5 4\nMax comfort deceleration Continuous 5 5 .25 5 .5\nTime gap Continuous 1.5 1 .25 1\nRoundaboutMax imposed braking Continuous 1.8 3 4\nLane change delay Continuous 0.84 0 .82 0 .8\nPoliteness Continuous 0.1 0 .05 0\nwithin a hyperrectangular trust region, which is centered\nat the current best solution, denoted by e\u2217. To balance\nbetween exploration and exploitation, the side lengths of\nthis region are adjusted based on the success of previous\noptimization steps. In fact, the trust region\u2019s size Lis initially\nset to Linit. It expands or contracts according to the num-\nber of consecutive successful or unsuccessful evaluations.\nSpecifically, after \u03c4succsuccessful evaluations, the region\u2019s\nsize is doubled, L\u2190min(Lmax,2L). Conversely, after\n\u03c4failunsuccessful evaluations, the size is reduced by half,\nL\u2190L/2. If the size Lfalls below a minimum threshold\nLmin, the trust region is terminated and reinitialized. The\nacquisition function \u03b1(e, D), where Ddenotes the dataset of\nobservations, guides the selection of new points within the\ntrust region, and the optimization continues until convergence\nor a predefined budget is reached. The flexibility of TuRBO-\n1to change the trust region size helps avoid getting stuck in\nlocal optima and increases optimization process efficiency.\nIn our implementation, the trust region size is initialized to\nLinit= 0.75, can expand to a maximum of Lmax= 1.5, and is\nreduced down to a minimum of Lmin= 0.57when necessary.\n2)\u03c0BO:In\u03c0BO [28], expert knowledge is represented as\na probability distribution \u03c0(e)over the potential location of\nthe optimum. This prior information is used to weight the ac-\nquisition function and guide the optimization process towards\npromising regions. The acquisition function \u03b1(e, Dn\u22121),\nwhere Dn\u22121={(e1, \u03c1\u03c6(e1)), . . . , (en\u22121, \u03c1\u03c6(en\u22121))}, is\nmodified by multiplying it with \u03c0(e)\u03b2\u2217/n, where \u03b2\u2217is a\nhyperparameter indicating the expert\u2019s confidence in the prior\nknowledge, and nrepresents the current BO iteration. This\nis mathematically represented as:\ne\u2217\u2208arg max\ne\u2208E\u03b1(e, Dn\u22121)\u00b7\u03c0(e)\u03b2\u2217/n(16)\nAs more data is collected through iterations, the exponent\n\u03b2\u2217/nbecomes smaller, which reduces the impact of the prior\u03c0(e)and shifts the focus to the data-driven surrogate model.\nFor\u03c0BO baseline, we consider the falsified points to be\nlocated on the edges of uncertainty space [44]. Therefore,\nthe prior knowledge of expert is a U-shaped distribution\nwhere the edges are weighted more than the inside area. In\npractical terms, we modeled the prior distribution at each\ndimension of uncertainty space as a combination of two\nGaussian distributions, each centered at the edges. The value\nof\u03b2\u2217is set to 0.7.\nThe rationale for selecting TuRBO- 1and\u03c0BO as base-\nlines in our study is based on their common foundation in\nBO and their unique strategies for improving optimization\nperformance. TuRBO- 1serves as a baseline to investigate\nwhether we should concentrate on high-fidelity models to\nenhance optimization or incorporate information from un-\ncertain sources to potentially improve efficiency and reduce\ncomputational costs. On the other hand, by contrasting our\nmulti-fidelity method with \u03c0BO, we aim to highlight the\nbenefits of using available lower-fidelity models instead of\nrelying solely on expert knowledge, especially in situations\nwhere such knowledge may be limited or uncertain.\nC. Case Studies\nTo evaluate our proposed approach, we implemented\nmulti-fidelity BO using Emukit toolkit [45] and conducted\nexperiments on five benchmarks provided by Gym [46]. For\neach case study, we elaborate on safety specifications, system\ncontroller, and how different levels of fidelity are developed.\nFor policy training, we used OpenAI baselines3on the high-\nfidelity simulator.\n1)Classic Control\u2014Cart-Pole :The cart-pole environ-\nment comprises a cart and a vertical pole attached to the\ncart using a passive pivot joint. The objective of this task is\nto keep the vertical pole balanced while the cart moves to the\nright or left along a frictionless track. The state vector of the\n3https://github.com/openai/baselinesLFBO MFBO 2FBO 3FBO020406080100Reliability Percentage33.5936.6859.2763.68\nLFBO MFBO 2FBO 3FBO020406080100\n64.10\n60.5493.76 93.79\nLFBO MFBO 2FBO 3FBO020406080100\n11.20 11.6125.67\n23.21\nLFBO MFBO 2FBO 3FBO020406080100\n59.58\n12.8594.37\n80.79\nLFBO MFBO 2FBO 3FBO020406080100\n18.5523.4066.4473.94Low-fidelity Middle-fidelity High-fidelityFig. 3: Average reliability percentage of discovered counterexamples over 750tests using standard BO on low- and middle-\nfidelity simulators, two-fidelity BO and three-fidelity BO methods for cart-pole, lunar lander, highway, merge, and roundabout\ncase studies.\nsystem consists of the cart position x, cart velocity v, pole\nangle \u03b8, and pole angular velocity \u02d9\u03b8. The agent can perform\ntwo actions: apply force to move the cart left or right.\nThe environment comes with six sources of uncertainty:\nE= [\u22122,2]\u00d7[\u22120.05,0.05]\u00d7[\u22120.2,0.2]\u00d7[\u22120.05,0.05]\u00d7\n[0.05,0.15]\u00d7[0.4,0.6]. The first four uncertainty intervals\nrepresent the possible perturbations to the position, velocity,\nangle, and angular velocity of the system. Following that,\nthe last two intervals are for the mass and length of the pole,\nrespectively. A system\u2019s trajectory is a series of states across\ntime, i.e., \u03be= (x(t), v(t), \u03b8(t),\u02d9\u03b8(t)). Given an instance\ne\u2208E, the trajectory of the system is uniquely defined.\nWe consider the primary cart-pole offered by Gym as low-\nfidelity simulator, which has a force magnitude of 10. To\ncreate a high-fidelity simulator, we attempt to make the game\nmore difficult and unstable by increasing the force magnitude\napplied to the pole. We also use a stronger strategy to solve\nthe kinematic equations using the semi-implicit Euler method\nrather than Euler method for the low-fidelity simulator. The\nparameters chosen for multi-fidelity experiments are shown\nin Table I. Moreover, as per our measurements, the cost\nratios amount to \u03bbhf/\u03bbmf= 2.71and\u03bbhf/\u03bblf= 20 .81.\nWe trained a controller for the high-fidelity simulator using\nproximal policy optimization (PPO) [47].\nThe controller for this case study must meet certain safety\nrequirements. First, the cart position xshould be within the\nrange [\u22121,1](\u00b51). Second, the absolute momentum of the\ncart should be less than 1(\u00b52). Third, the cart-pole angle\nshould be less than 9degrees from vertical ( \u00b53). Hence, thespecification could be written as: \u03c6=\u00b51\u2227\u00b52\u2227\u00b53.\n2)Classic Control\u2014Lunar Lander :This environment\nfeatures a lander that must safely descend onto the moon\u2019s\nsurface under conditions of low gravity. The primary purpose\nis to guide the agent to the landing pad as softly and\nfuel-efficiently as possible. For this environment, there are\neight state variables including the coordinates (x, y)of the\nlander, the horizontal and vertical velocities (vx, vy), the\norientation \u03b8in space, the angular velocity v\u03b8, and two\nBoolean parameters indicating whether the left or right leg\nhas made contact with the ground. Four possible actions\ninclude activating the left orientation engine, activating the\nright orientation engine, activating the main engine, and\ntaking no action. We consider four sources of uncertainty.\nThe coordinate perturbations are such that \u03b4x\u2208[\u22120.5,0.5]\nand\u03b4y\u2208[0,3], the velocity perturbations are such that \u03b4vx\u2208\n[\u22122,2]and\u03b4vy\u2208[0,2]. A trajectory of this system could be\nrepresented as \u03be= (x(t), y(t), vx(t), vy(t), \u03b8(t),\u02d9\u03b8(t)).\nThe high-fidelity simulator uses two sources of wind\ndisturbance within the environment. In the middle-fidelity\nsimulator, we replicate one of these disturbances, while the\nlow-fidelity counterpart disregards both disturbances. The\nlow-fidelity simulator includes inaccurate sensors capable\nof providing measurements up to two decimal points. The\ndetailed distinctions in fidelity levels for the lunar lander case\nstudy are presented in Table I. Based on this configuration,\nthe cost ratios are \u03bbhf/\u03bbmf= 2.02and\u03bbhf/\u03bblf= 4.53.\nWe employ the deep deterministic policy gradient (DDPG)\nalgorithm [48] to train a controller for the high-fidelity sim-ulator. A trajectory is safe when it satisfies these conditions:\nFirst, the lander must keep its horizontal coordinate ( x)\nwithin a certain range of the origin ( \u00b51). Second, the lander\u2019s\ntilt angle must not exceed \u03c0/4(\u00b52), and third, the lander does\nnot spin faster than 0.2radians per second ( \u00b53). Thus, the\nsafety specification is \u03c6=\u00b51\u2227\u00b52\u2227\u00b53.\n1 2 3 4 5\nExperiment number020406080100120140Cost per counterexample\n3FBO 2FBO HF TuRBO-1 \u03c0BO\nFig. 4: Comparison between average cost of finding a\ncounterexample through 200BO iterations on cart-pole case\nstudy.\n3)Autonomous Driving\u2014Highway, Merge, and Round-\nabout Scenarios :To thoroughly evaluate the effectiveness\nof our approach, we concentrate on three driving scenarios:\nhighway driving, merging, and roundabout navigation. In\nthese scenarios, the state space includes the (x, y)longi-\ntudinal and lateral coordinates, as well as longitudinal and\nlateral velocities (vx, vy)of both the ego vehicle and the four\nclosest cars. We consider five discrete meta-actions: moving\nto the left lane, maintaining an idle state, shifting to the right\nlane, accelerating, and decelerating. The trajectories of these\nscenarios comprise the coordination of both the ego vehicle\nand the four nearest cars, represented as\n\u03be= (xego(t), yego(t), xob1(t), yob1(t),\nxob2(t), yob2(t), xob3(t), yob3(t), xob4(t), yob4(t)).\nFor these scenarios, we use deep Q-learning (DQN) [49]\nfor the ego vehicle and both intelligent driver model (IDM)\n[50] and MOBIL (minimizing overall braking induced by\nlane change) model [51] for longitudinal and lateral be-\nhaviors of other vehicles on the road, respectively. While\nMOBIL is a lane-changing model that uses safety and in-\ncentive criteria\u2014considering the braking of the new and old\nneighbors, as well as a politeness factor\u2014to determine when\na lane change is safe and beneficial, IDM is a parametric car-\nfollowing model that regulates the acceleration of vehicles.\nIn this model, the vehicle\u2019s acceleration is described by the\nfollowing kinematic equation:\n\u02d9v\u03b1=a\"\n1\u2212\u0012v\u03b1\nv0\u00134\n\u2212\u0012s\u2217(v\u03b1,\u2206v\u03b1)\ns\u03b1\u00132#\n(17)\nwhere the acceleration \u02d9v\u03b1depends on the current speed v\u03b1,\nthe gap to the leading vehicle s\u03b1, and the relative speeddifference \u2206v\u03b1between the ego vehicle and the vehicle\nahead. The deceleration factor is influenced by the ratio of\nthe desired minimum gap s\u2217(v\u03b1,\u2206v\u03b1)to the actual gap s\u03b1,\nwhere s\u2217(v\u03b1,\u2206v\u03b1) =s0+vT+v\u2206v\n2\u221a\nab. Here, s0represents\nthe minimum gap in dense traffic, vTis the gap maintained\nwhile following the leading vehicle at a constant time gap\nT, and aandbcorrespond to the maximum comfortable\nacceleration and deceleration, respectively.\nHighway: This environment involves a vehicle on a multi-\nlane highway [52]. The agent controls speed and lane tran-\nsitions. The aim is to achieve high speeds without colliding\nwith adjacent vehicles while maintaining a position on the\nright side of the road. The motion of the ego vehicle is\nrepresented using kinematic bicycle model [53]. We evaluate\nnine potential sources of uncertainty, arising from the initial\nproximity of the ego vehicle to its four nearest neighbors as\nwell as their initial velocities. The initial spacing between\nthe ego vehicle and the four nearest vehicles falls within\nthe range \u03b4x\u2208[20,30]. Meanwhile, the ego vehicle\u2019s initial\nvelocity is limited to the range vego\u2208[20,40]. Similarly,\nthe initial velocities of other four cars are denoted as vobi\u2208\n[20,30]fori= 1, . . . , 4.\n1 2 3 4 5\nExperiment number0.02.55.07.510.012.515.017.520.0Cost per counterexample\n3FBO 2FBO HFBO TuRBO-1 \u03c0BO\nFig. 5: Comparison between average cost of finding a coun-\nterexample through 200 BO iterations on lunar lander case\nstudy.\nWe developed our simulators to have different levels\nof fidelity. The high-fidelity simulator, with a simulation\nfrequency of 15, is more accurate than the low- and middle-\nfidelity simulators, which operate at frequencies of 11and\n13, respectively. Our low- and middle-fidelity simulators\nintroduce noise into the sensors, limiting the ego vehicle\u2019s\nability to precisely predict the location and velocity of\nneighboring vehicles while also maintaining fewer cars on\nthe road, as shown in Fig. 2. Based on these settings, we\nfound the cost ratios to be \u03bbhf/\u03bbmf= 3.75and\u03bbhf/\u03bblf=\n14.5. Safe trajectories are established by ensuring that an\nabsolute distance of over 0.5meters is maintained between\nthe ego car and both the leading (front) and following (back)\nvehicles positioned in the same line. Accidents involving\nother vehicles on the road, not including the ego vehicle, are\nnot considered unsafe modes. Hence, our safety specificationcan be written as \u03c6= (|xego\u2212xobi| \u22120.5)fori= 1, . . . , 4.\n1 2 3 4 5\nExperiment number100020003000400050006000Cost per counterexample\n3FBO 2"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**1. Abstract:**\n\nThe paper introduces TravelAgent, an AI-powered travel planning system designed to generate personalized, comprehensive, and rational travel itineraries using large language models (LLMs). It addresses limitations in existing systems that fail to fully meet the criteria of rationality, comprehensiveness, and personalization. TravelAgent is composed of four main modules: Tool-Usage, Recommendation, Planning, and Memory Modules. The system\u2019s effectiveness is demonstrated through evaluations with human and simulated users, showing significant improvements in rationality, comprehensiveness, and personalization over current methods.\n\n**2. Introduction:**\n\n- **Context and Motivation:** As global tourism grows, the need for sophisticated travel planning services increases. Users demand personalized and diverse travel experiences.\n- **Research Gap:** Existing travel planning systems either provide static guides or rely heavily on manual selection without the ability to generate and adapt itineraries dynamically. Recent LLMs, while effective in various tasks, struggle with real-world applications requiring multi-constraint management.\n- **Research Objectives:** The study sets out to create a travel planning system that excels in rationality, comprehensiveness, and personalization. It establishes three critical evaluation criteria: \n  1. Rationality: Modeling real-world constraints.\n  2. Comprehensiveness: Providing fine-grained, dynamic updates.\n  3. Personalization: Leveraging user-specific data for customized recommendations.\n- **Literature Review:** \n  - Early travel platforms lacked recommendation capabilities.\n  - LLM advancements in other fields (like education, healthcare) hint at their potential in travel but face hurdles with dynamic planning.\n  - Existing benchmarks (TravelPlanner) expose the inadequacies of state-of-the-art LLMs in dynamic scenarios.\n- **Positioning:** TravelAgent aims to address these gaps by integrating advanced data processing and LLM capabilities into a cohesive system structured around user-centric constraints.\n\n**3. Related Work:**\n\n- **LLM-based Agents:** Recent LLMs show prowess in multi-step tasks across various fields but require enhancements in dynamic task management, such as task decomposition and memory construction.\n- **Recommendation Systems with LLMs:** Two primary methods: direct recommendation and encode-decode approach. Both face challenges in capturing long-term user preferences, which TravelAgent\u2019s design seeks to overcome.\n- **Tool Usage and Memory Integration:** Prior work has explored using real-time data and external tools, which form the backbone of TravelAgent\u2019s comprehensiveness.\n\n**4. Method:**\n\n- **System Design:** TravelAgent comprises four interlinked modules:\n  - **Tool-Usage Module:** Integrates real-time data from APIs (SerpAPI, Google Maps) and arithmetic tools for distance and time calculations.\n  - **Recommendation Module:** Uses LLMs for personalized travel suggestions, leveraging multiple data sources: user-specific constraints, real-time info, historical insights, and generalized knowledge.\n  - **Planning Module:** Divided into Budget Planner and Route Planner, integrates budget elements into itinerary generation using spatiotemporal-aware algorithms.\n  - **Memory Module:** Divided into short-term (current interactions) and long-term (historical data) memory, enabling continual learning for personalized recommendations.\n\n- **Constraints Modeling:** \n  - **Personal Constraints:** Hard (direct user input like dates, group composition) and soft constraints (inferred from historical data).\n  - **Commonsense Constraints:** General travel knowledge applicable to any user.\n\n- **Algorithm and Scoring Details for Route Planning:** \n  - **Travel Time Calculation:** Integrates travel and visit durations for efficient route planning.\n  - **Scoring Mechanism:** Scores attractions based on alignment with remaining time, preferred visit windows, and efficient use of remaining time.\n\n**5. Experimental Setup:**\n\n- **Datasets Used:** Included various travel scenarios and user profiles.\n- **Hardware and Software Specifications:** Detailed hardware setup and software frameworks used for running and evaluating TravelAgent.\n- **Evaluation Metrics:** Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for personalization accuracy, alongside user feedback ratings on different criteria.\n- **Baselines:** Comparisons with typical agent systems, notably GPT-4+ agent, to benchmark performance improvements.\n\n**6. Results and Analysis:**\n\n- **Empirical Findings:**\n  - **Human Evaluation Scores:** TravelAgent outperformed the GPT-4+ agent significantly across all criteria in 20 travel cases (Rationality: 9.56 vs. 8.16, Comprehensiveness: 8.87 vs. 6.25, Personalization: 8.44 vs. 4.31).\n  - **Simulated User Interactions:** Demonstrated high prediction accuracy and effective performance in personalization tasks. Graphs showed lower error rates in MAE and RMSE trends, validating the system\u2019s continuous learning capability.\n- **Figure and Table Details:** \n  - Extensive graphs and tables detailing MAE, RMSE trends, and comparative scores.\n  - Figures showcasing examples of travel plans and module operations.\n\n**7. Discussion:**\n\n- **Interpretation of Results:** TravelAgent demonstrated superior performance in aligning travel itineraries with user-specific constraints and preferences. Its ability to dynamically update and personalize recommendations marks a significant advancement over static or less adaptive systems.\n- **Implications:** The system's architecture opens avenues for more user-focused, adaptive travel planning applications, aligning well with broader trends in AI-personalization.\n- **Limitations:** \n  - Dependence on data quality from real-time sources might affect recommendation accuracy.\n  - Need for deeper personalization insights and more detailed user preference models for further enhancement.\n\n**8. Conclusion:**\n\n- **Key Contributions:**\n  - Introduction of a travel planning system meeting rationality, comprehensiveness, and personalization criteria.\n  - Development of advanced spatiotemporal-aware algorithms and a unique recommendation framework leveraging LLMs.\n- **Future Directions:** Improvements in real-time data quality, exploring richer personalization features, and expanding the system to accommodate a broader range of travel scenarios.\n\nBy meticulously capturing every detail of the system's design, methodologies, and empirical evaluations, this summary provides sufficient groundwork for a researcher to replicate or extend the study, ensuring the nuanced innovations and findings of TravelAgent are preserved and actionable."}, {"role": "assistant", "content": "TravelAgent: An AI Assistant for Personalized Travel Planning\nAili Chen\u2660, Xuyang Ge\u2660, Ziquan Fu\u2661, Yanghua Xiao\u2660, Jiangjie Chen\u2660\n\u2660Fudan University\u2661System Inc.\n{alchen20,xyge20,shawyh,jjchen19}@fudan.edu.cn frank@system.com\nAbstract\nAs global tourism expands and artificial intelli-\ngence technology advances, intelligent travel\nplanning services have emerged as a signif-\nicant research focus. Within dynamic real-\nworld travel scenarios with multi-dimensional\nconstraints, services that support users in au-\ntomatically creating practical and customized\ntravel itineraries must address three key ob-\njectives: Rationality, Comprehensiveness, and\nPersonalization. However, existing systems\nwith rule-based combinations or LLM-based\nplanning methods struggle to fully satisfy these\ncriteria. To overcome the challenges, we intro-\nduce TravelAgent1, a travel planning system\npowered by large language models (LLMs) de-\nsigned to provide reasonable, comprehensive,\nand personalized travel itineraries grounded in\ndynamic scenarios. TravelAgent comprises\nfour modules: Tool-usage, Recommendation,\nPlanning, and Memory Module. We evaluate\nTravelAgent \u2019s performance with human and\nsimulated users, demonstrating its overall ef-\nfectiveness in three criteria and confirming the\naccuracy of personalized recommendations.\n1 Introduction\nWith the growth of global tourism, the demand for\nautomated travel itinerary generation services has\nsurged. Concurrently, travelers seek diverse and\npersonalized experiences more and more. Early\nplatforms (Wikitravellers, 2024; FrommerMedia\nLLC, 2024) offer static guidance without recom-\nmendation or planning. Later systems (Roadtrip-\npers, LLC, 2024; Expedia Group, 2024; Booking\nHoldings Inc., 2024) provide categorized recom-\nmendation and booking services but heavily rely\non manual selection and rule-based combinations,\nunable to autonomously generate itineraries and\nadapt to dynamic scenarios.\nRecent advancements in artificial intelligence,\nparticularly LLMs, have promoted the develop-\n1https://travel.okagent.orgment of intelligent travel services. However,\ndespite their effectiveness in traditional tasks,\nLLMs face challenges in dynamic, real-world ap-\nplications such as personalized travel planning,\nwhich requires considering multiple constraints\nand real-time information. The TravelPlanner\nbenchmark (Xie et al., 2024) assesses language\nagents\u2019 capabilities in managing these constraints\nusing a dataset of travel scenarios. Results demon-\nstrate that even state-of-the-art LLMs (OpenAI,\n2022; Achiam et al., 2023; Team et al., 2023; Jiang\net al., 2023, 2024) and planning strategies (Wei\net al., 2022; Yao et al., 2022; Shinn et al., 2024)\nstruggle to meet all constraints and produce viable\nplans.\nDespite rationality considerations, travel ser-\nvices designed for real-world scenarios require\ncomprehensive itineraries incorporating dynamic\nupdates, fine-grained details, and extensive con-\ntextual information. Additionally, personalization\nis also essential for enhancing the quality of ser-\nvices. In this study, we establish three evaluation\ncriteria for itineraries generated by travel planning:\nRationality ,Comprehensiveness , and Personal-\nization . Each category introduces implementa-\ntion challenges that travel service systems must\naddress:\n1.Rationalily - How to model constraints in\ntravel scenarios and plan rational itineraries\nunder these constraints?\n2.Comprehensiveness - How to provide com-\nprehensive travel services, including recom-\nmendation and planning, make real-time, fine-\ngrained, and interesting itineraries?\n3.Personalization - How to uncover and lever-\nage implicit user personalization information\nto deliver personalized recommendations and\nplanning services?\nIn this paper, we propose TravelAgent , a novel\ntravel service system powered by LLMs and an\n1arXiv:2409.08069v1  [cs.AI]  12 Sep 2024advanced data processing framework to tackle the\noutlined core challenges and meet the established\nevaluation criteria. TravelAgent features four dis-\ntinct modules: the Tool-Usage Module ,Recom-\nmendation Module ,Planning Module , and Mem-\nory Module ; each is meticulously designed and\ncollaborates seamlessly with others. We model the\ntravel constraints and devise efficient algorithms\nto enhance the Rationality of the planning stage.\nAdditionally, the Tool-usage Module encompasses\na pool of real-time tools; we address the compre-\nhension issue with history insights in the Memory\nModule. Finally, we present a new personalized\nrecommendation framework for LLM-based agents\nto promote Personalization.\nFurthermore, we rigorously evaluate the effec-\ntiveness and quality of TravelAgent through two\ndistinct methodologies: Overall Evaluation with\nhuman users andPersonalization Evaluation\nwith simulated users . Overall Evaluation assesses\nTravelAgent in terms of the three criteria through\ndetailed case studies. Personalization Evaluation\ndelves deeper into Personalization, using simu-\nlated users to produce streaming behavioral data\nfor predictive tasks in attraction recommendations.\nFindings indicate that TravelAgent consistently\ndelivers high performance in Rationality, Compre-\nhensiveness, and Personalization across multiple\ntravel scenarios.\nOurcontributions include:\n\u2022(System) We define three objective criteria\nfor travel services and create TravelAgent ,\na novel AI-driven system designed for auto-\nmated personalized travel planning service to\naddress the objectives.\n\u2022(Method) For Rationality, we model travel\nconstraints and develop a spatiotemporal-\naware planning algorithm. For Comprehen-\nsiveness, we establish a real-time data mech-\nanism enhanced by multiple tools. For Per-\nsonalization, we introduce an agent-based per-\nsonalized recommendation framework with\ncontinuous learning of implicit information.\n\u2022(Evaluation) We conduct human case stud-\nies to evaluate the overall performance of\nTravelAgent , which surpasses the baseline\nGPT-4+ agent across three criteria. Further,\nwe undertake behavioral prediction tasks with\nsimulated users and find that our personalized\nrecommendation framework exhibits lower\nerror rates than baseline LLM-based recom-mendation methods.\n2 Related Work\nLLM-based Agent LLM-based agents have\ndemonstrated significant capability in executing\nmulti-step tasks (Wang et al., 2024b), showcasing\ntheir potential in fields such as education (Wang\net al., 2024b; Swan et al., 2023), healthcare (Zhang\net al., 2023), and emotional support (Hasan et al.,\n2023). To enhance their planning ability, some\nresearchers propose task decomposition meth-\nods (Shen et al., 2024; Wang et al., 2023; Singh\net al., 2023), while others focus on step-by-step\nreasoning policies for dynamic planning (Wei\net al., 2022; Yao et al., 2024; Besta et al., 2024;\nChen et al., 2022; Yao et al., 2022). Additionally,\nthrough memory construction (Zhong et al., 2024;\nLiu et al., 2023b; Hu et al., 2023; Zheng et al.,\n2023; Huang et al., 2023) and tool use (Schick\net al., 2024; Qin et al., 2023; Lu et al., 2024; Wang\net al., 2024a; Yuan et al., 2024), LLM-based agents\ncan leverage historical and external resources to\nbetter adaptation, making it a promising tool for\ntravel services.\nLLM as Recommendation System With the ad-\nvent of LLMs, researchers endeavor to leverage\nLLMs for direct recommendations. Two main\napproaches have emerged: Direct Recommenda-\ntion (Dai et al., 2023; Hou et al., 2024; Kang et al.,\n2023; Wang and Lim, 2023) and Decode and En-\ncode (Zhou et al., 2024; Yang et al., 2023; Liu et al.,\n2023a). The former prompts LLMs with users\u2019 be-\nhavioral histories to recommend directly, while\nthe latter decodes a one-time user profile from the\nlast interaction for subsequent recommendations.\nHowever, both approaches have limitations in cap-\nturing long-term user preferences through multiple\ninteractions, resulting in suboptimal effectiveness.\n3 The TravelAgent System Design\nTravelAgent system is architected into five key\nmodules: (1) Tool-usage Module , which inte-\ngrates various utilities drawing from real-time dy-\nnamic data sources or arithmetic algorithms; (2)\nRecommendation Module , which delivers per-\nsonalized travel recommendations tailored to indi-\nvidual preferences; (3) Planning Module , which\nincorporates budget and route planning functionali-\nties, leveraging budget plan result to guide detailed\nroute plans sensitive to time and geographic con-\nstraints; (4) Memory Module , which help to con-\n2I'm organizing a 3-day trip from Beijing to Shanghai, scheduled from April 1 to April 4, 2024, for two adults and one 12-year-old child. Can you give some recommendations and help devising a feasible travel plan?\"Personal Constraints 2 - Soft ConstraintsPersonal Constraints 1 - Hard Constraints\nCommonsense Constraints\n1. Group Composition: 2 adults and 1 12-year-old child2. Departure & Destination: Bejing & Shanghai3. Outbound & Return Date: 2024-04-01 & 2024-04-04\n4. Budget: To be determined1. User Travel Preferences: Art & History\u2026\u20262. User Characters & Interests: Explorer & Extrovert\u2026\u20263. User Spending Power: Medium1. A sight or a restaurant can\u2019t be visited twice.2. Positions of hotels and restaurants should be reasonable3. \u2026\u2026.Initialized by Memory & To be UncoveredInitialized by User Input\nInitialized by Memory & To be UncoveredFigure 1: Initial constraints modeling with user input\nand Memory Module.\ntinually learn user preferences as a comprehensive\nrepository of travel knowledge and user profiles.\n3.1 Constraints Modeling\nThe core challenge in travel services lies in han-\ndling multi-dimensional constraints in complex\nreal-world travel scenarios. In this work, we cate-\ngorize these constraints into - (1) Personal Con-\nstraints , which are unique to individual users and\ninclude Hard Constraints and Soft Constraints; (2)\nCommonsense Constraints , which encompass\ngeneral travel knowledge applicable across vari-\nous scenarios and beneficial to all users. Figure 1\nillustrates an initial constraints example.\n\u2022Personal Constraints 1 - Hard Constraints\nare both user and scenario-specific, which\nmust be strictly followed. These non-\nnegotiable elements, such as the outbound-\n/return date, are directly provided by users\nin the current scenario and reflect their travel\nobjectives, which are essential for Rationality\nand Personalization of TravelAgent .\n\u2022Personal Constraints 2 - Soft Constraints\nare user-specific but cross-scenarios, initiated\nby historical insights from Memory Module.\nInstead of explicitly provided by users, they\nare uncovered gradually by TravelAgent\nacross multiple scenarios and evolve with user\ninteractions. These personalized elements are\nPersonal Constraints - Hard Constraints1. Group Composition: 2 adults and 1 12-year-old child2. Departure & Destination: Bejing & Shanghai3. Outbound & Return Date: 2024-04-01 & 2024-04-044. Budget: $1000\nparams = {         \"check_in_date\": 2024-04-01,         \"check_out_date\": 2024-04-04,         \"adults\": 2         \"children\":1,         \"children_ages\":12,         \"price\": HOTEL_FUND }Parameter Extraction \nparams = {         \"city\": Shanghai, China,         \"check_in_date\": 2024-04-01,         \"check_out_date\": 2024-04-04,         \"adults\": 2         \"children\":1,         \"children_ages\":12,         \"price\": HOTEL_FUND }Parameter Extraction Hotel Tool CallHotel 3Hotel 1Hotel 2\u2026\u2026Real-time Hotel InfoItem ListFigure 2: An example of Hotel Tool call. Appendix\nA provides a comprehensive overview of all types and\nfunctions of tools.\nnot mandatory but significantly affect Person-\nalization and user satisfaction.\n\u2022Commonsense Constraints are general\nknowledge applicable to users and scenarios,\nwhich are initiated by Memory Module and\ncontinually learned by TravelAgent . They\nare crucial for Rationality of itineraries and\ntravel experiences, such as preventing the rep-\netition of dining restaurants.\n3.2 Tool-usage Module\nTravelAgent integrates various real-time data\nsources to ensure up-to-date recommendations and\nplans. Tool-usage Module, which underpins Ratio-\nnality and Personalization and serves as the core\ncomponent for enhancing Comprehensiveness, is\ndivided into two categories: real-time API tools\nandarithmetic algorithm tools . The first category,\npowered by SerpAPI2and Google Maps API3, in-\ncludes specialized utilities such as Attraction Tool,\ndesigned to fetch high-quality real-time data. The\nsecond category comprises tools like Distance and\nTime Tools, which employ arithmetic algorithms\nto ensure precise route planning.\nAs shown in Figure 2, after initial constraints\nmodeling, Tool-usage Module extracts structured\nparameters from Hard Constraints, serving as the\nprincipal filter at the tool-call stage. Subsequently,\nit invokes specific tools according to the targeted\nrecommendation or planning point. The filtered\ninformation is then transmitted to Recommenda-\n2https://serpapi.com/integrations/python#\ngoogle-search-api-capability\n3https://developers.google.com/maps\n3RecommendationMemory\nIn-Context Constraints\nHistory InsightsTool-usage\nReal-time InformationUser Interface\nNew InsightsUser Feedback AttractionsEdited Attractions(A)\n(B)(C)(D)(E)(E)(F)(G)Figure 3: The process of Attraction Recommendation.\ntion Module for additional refinement and finally,\nconveyed to Planning Module.\n3.3 Recommendation Module\nRecommendation Module serves as the pri-\nmary component to achieve Personalization of\nTravelAgent . It diverges from traditional\nparadigms of recommendation systems by di-\nrectly leveraging LLM-based agents to perform\nonline recommendations. Inspired by some re-\nlated work for online Personalization construc-\ntion (Chidlovskii, 2015; Dalvi et al., 2022; Baek\net al., 2024; Ma et al., 2024), we form a data-\ndriven recommendation framework comprises Bud-\nget, City, Flight, Hotel, Restaurant, and Attraction\nRecommender, which continually learns implicit\npersonalization information through multiple inter-\nactions.\nRecommendation Module capitalizes on four\ndistinct data sources for recommendations:\n\u2022In-context Travel Constraints , sourced from\nthe user interface, refers to Hard Constraints\nprovided by users, reflecting their specific\nneeds for the current travel scenario.\n\u2022Real-time Travel Information , derived from\nTool-usage Module , encompasses up-to-date\ndetails on various travel options, each charac-\nterized by multi-dimensional attributes.\n\u2022History Insights , retrieved from Memory\nModule , comprises learned commonsense\nknowledge, user personas, and past interac-\ntions records. These insights dynamically\nevolve through new interactions and finally\nre-storing in Memory Module for future use.\n\u2022Generalized Knowledge , sourced from\nLLMs , incorporates a wide range of funda-\nmental travel-related knowledge.\nTake attraction recommendation as an instance,the whole worflow is shown in Figure 3 - (A) In-\ncontext Constraints Extraction : At the start of\na travel scenario, Hard Constraints are initialized\nbased on the user\u2019s input from the user interface;\n(B) History Insights Retrieval : Concurrently, the\nsystem retrieves relevant historical insights from\nMemory Module, initializing implicit constraints,\nincluding both soft and commonsense constraints;\n(C) Real-time Information Gathering : At the\nattraction recommendation stage, Attraction Tool\nuses Hard Constraints to fetch up-to-date attraction\ninformation; (D) Data-driven Recommendation :\nRecommendation Module synthesizes all data and\nleverage LLMs to generate personalized attraction\nrecommendations and visit suggestions; (E) User\nBehavioral Interactions : Recommendation list,\naccompanied by contextual information, are pre-\nsented on the structured user interface for user feed-\nback (like/pass); (F) Items Re-rank Based on user\nfeedback, the Recommendation Module re-ranks\nthe attractions, finalizing the selection for route\nplanning; (G) Implicit Insight Learning : Post-\ninteraction, the system draws new short-term in-\nsights relevant to users\u2019 persona and commonsense\nknowledge, updating long-term insights in Mem-\nory Module. With this pipeline, TravelAgent can\ncontinually learn implicit knowledge and dynami-\ncally retain a tailored insight of each user.\n3.4 Planning Module\nPlanning Module comprises Budget Planner and\nRoute Planner . We innovatively integrate bud-\ngetary elements into the planning process to en-\nhance the itinerary\u2019s Rationality and Comprehen-\nsiveness, empowering planning with LLMs and a\nspatiotemporal-aware route algorithm.\nBudget Planner After initial constraint mod-\neling, Budget Recommender engages LLMs to\ngenerate a preliminary budget estimate. Once\nusers confirm their budget, Budget Planner re-\nevaluates users\u2019 financial situation and strategi-\ncally distributes the budget across six categories:\nAccommodation, Attractions, Restaurants, Trans-\nportation, Other Expenses, and a Reserve Fund,\nprimarily adheres to commonsense knowledge and\ndynamically adjusts according to other constraints\nto ensure that the travel plans are economically\nviable and tailored to the context.\nRoute Planner As shown in Figure 4, guided\nby the budget plan, multi-day itineraries are dy-\nnamically generated using an attraction set pro-\n4Personal Constraints (Hard / Soft)Commonsense Constraints\nUser Input & History Insight in Memory\nDay 1\n\u202208:00 to 10:10: \ufb02ight\n\u202210:10 to 14:00: \nCheck-in at Dayin \nInternational Youth \nHostel\n\u202216:00 to 18:00: Have \ndinner at Xiao Yang \nSheng Jian\u2026\u2026\nDay 2\n\u202209:10 to 11:10: Visit \nYu Garden.\n\u202211:10 to 13:10: Have \nlunch at Yang's Fried \nDumpling.\n\u202213:20 to 15:20: Visit \nShanghai Ocean \nAquarium\u2026\u2026\nDay 3\n\u202209:10 to 10:10: Visit \nPeople's Square.\n\u202210:10 to 12:10: Have \nlunch at Lost Heaven \nBar.\n\u202212:10 to 12:20: Travel \nto Former Residence of \nSun Yat-Sen.,,,,,\nDay 4\n\u202209:00 to 11:25: Hotel \ncheck-out and prepare \nfor the return \ufb02ight.\n\u202211:25 to 13:40: Return \n\ufb02ight (CZ 8880)\nPlan A Day\nFull Travel Plan\n(A) City Map (B) Timeline\n(Current Position, Current Time)1. Initialize Position & Time\n2. Pre-de\ufb01ne the End Time\n3. Pre-de\ufb01ne Special Event Set  \n4. Initialize Available Attractions\n5. Planning Loop:\nWhile (Remain Attractions & Time):\n\u2022Plan the Route Point (cur_p, cur_t)\n\u2022Update Position & Time\nPlan A Route Point\n(Current Position, Current Time)\nCheck  the Point\nif Special Event\n else\n(A) Handle Event (B) Visit Attraction\n\u2022Calculate Scores\n\u2022Score Analysis\n\u2022Select Attraction\n\u2022Visit Attraction A\n\u2022Remove A\u2022Tool-usage\n\u2022Recommend\n\u2022User Interaction\n\u2022Memory UpdateFigure 4: The whole workflow of Route Planner illustrates the dynamic generation process of multi-day itineraries.\nvided by Recommendation Module. Each day\u2019s\nplanning starts by setting initial conditions, includ-\ning current position pcur, current time tcur, and\nan end time. In addition, we pre-defined some\nanticipated event routines according to Common-\nsense Constraints to ensure rational planning, such\nas dining or check-ins. The daily planning pro-\ncess iterates as long as time permits and attractions\nare available, with each iteration re-evaluating the\nroute point characterized by the current coordinates\n(pcur, tcur). If the current coordinates align with a\nspecial event, corresponding actions are executed,\nsuch as selecting an optimal nearby restaurant. Oth-\nerwise, the planner evaluates the available attrac-\ntions and selects one for a visit. After each route\npoint, the coordinates (pcur, tcur)are updated to\nalign subsequent activities with the strategic tem-\nporal and spatial logistics.\nAttraction Scores Calculation For better assess-\nment at (pcur, tcur), we devise Algorithm 1 to\nscore available attractions. For each attraction Ai,\nfirstly check the distance difrompcurto the attrac-\ntionpito compute three time-(1) the travel time\ntcost,i, including both the travel duration at speed\nvand the recommended visit duration \u03b4trec,i; (2)\nthe arrival time Tarr,i (3) the leave time Tdep,i. The\nscoring mechanism involves three components:\n\u2022Return Time Score Sret,i, which checks if\nthe total travel time and duration at the Aifit\nwithin the remaining day time tleft.\n\u2022Optimal Visit Time Score Sopt,i, which eval-\nuates how well the visit aligns with the pre-\nferred visiting window \u03c4i.\n\u2022Reserve Time Score Sleft,i, which optimizes\nthe use of the remaining time, prioritizing\nefficient day planning.Algorithm 1 Calculate Score for Attraction Ai\nRequire:\nCurrent Position pcur,Current Time tcur\nAvailable Attraction Ai= (pi, \u03b4ti, \u03c4i)\n\u25b7Attraction Position Ai\n\u25b7Recommended Duration \u03b4ti\n\u25b7Recommended Visit Time Window \u03c4i\nLeft Time tleft,Travel Speed v\nEnsure: Si\n1:di\u2190 \u2225pcur\u2212pi\u2225 \u25b7Calculate distance\n2:tcost,i\u2190di\nv+\u03b4trec,i \u25b7Calculate the travel time\n3:Tarr,i\u2190tcur+di\nv\u25b7Calculate the arrival time\n4:Tdep,i\u2190Tarr,i +\u03b4ti \u25b7Calculate the leave time\n5:Sret,i\u2190RETURN TIMESCORE (tleft, tcost,i )\n6:Sopt,i\u2190VISITTIMESCORE ((Tarr,i, Tdep,i ), \u03c4i)\n7:Sleft,i\u2190RESERVE TIMESCORE (tleft, tcost,i )\n8:Si\u2190Sret,i +Sopt,i +Sleft,i \u25b7Final score\n9:return Si\nThe total score Si=Sret,i+Sopt,i+Sleft,iguides\nthe selection of the next attraction, ensuring the\nitinerary is balanced, enjoyable, and logistically\nsound. This method dynamically adapts to daily\ncircumstances, aligning travel plans with real-time\nconstraints and preferences.\n3.5 Memory Module\nMemory Module is designed to promote Person-\nalization and dynamic data processing and con-\nsists of two primary components: (1) Short-term\nMemory stores new user interactions and captures\nthe immediate contextual travel data, constraints,\nand plans, adapting to the latest user preferences.\n(2)Long-term Memory retains user data over ex-\ntended periods, accumulating a comprehensive per-\nsona based on historical interactions. It stores sta-\nble user preferences, such as a user\u2019s consistent\ninterest in eco-friendly attractions or specific di-\netary requirements. Historical insights allow the\nsystem to provide recommendations that align with\n5/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013\n/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000038/uni00000056/uni00000048/uni00000055/uni00000003/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni00000024/uni00000045/uni00000056/uni00000052/uni0000004f/uni00000058/uni00000057/uni00000048/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000030/uni00000024/uni00000028/uni0000000c/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000027/uni0000005c/uni00000051/uni00000044/uni00000050/uni0000004c/uni00000046/uni00000003/uni00000030/uni00000024/uni00000028/uni00000003/uni00000037/uni00000055/uni00000048/uni00000051/uni00000047/uni00000056/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013\n/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000038/uni00000056/uni00000048/uni00000055/uni00000003/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000035/uni00000052/uni00000052/uni00000057/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni00000036/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000035/uni00000030/uni00000036/uni00000028/uni0000000c/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000027/uni0000005c/uni00000051/uni00000044/uni00000050/uni0000004c/uni00000046/uni00000003/uni00000035/uni00000030/uni00000036/uni00000028/uni00000003/uni00000037/uni00000055/uni00000048/uni00000051/uni00000047/uni00000056/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000005a/uni00000003/uni00000025/uni00000048/uni0000004b/uni00000044/uni00000059/uni0000004c/uni00000052/uni00000058/uni00000055 /uni00000027/uni00000048/uni00000046/uni00000052/uni00000047/uni00000048/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni00000028/uni00000051/uni00000046/uni00000052/uni00000047/uni00000048/uni00000003/uni0000005a/uni00000003/uni00000025/uni00000048/uni0000004b/uni00000044/uni00000059/uni0000004c/uni00000052/uni00000058/uni00000055 /uni00000027/uni0000004c/uni00000055/uni00000048/uni00000046/uni00000057/uni00000003/uni00000035/uni00000048/uni00000046/uni00000052/uni00000050/uni00000048/uni00000051/uni00000047/uni00000003/uni0000005a/uni00000003/uni00000025/uni00000048/uni0000004b/uni00000044/uni00000059/uni0000004c/uni00000052/uni00000058/uni00000055 /uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000025/uni00000048/uni0000004b/uni00000044/uni00000059/uni0000004c/uni00000052/uni00000058/uni00000055 /uni00000027/uni0000004c/uni00000055/uni00000048/uni00000046/uni00000057/uni00000003/uni00000035/uni00000048/uni00000046/uni00000052/uni00000050/uni00000048/uni00000051/uni00000047/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000025/uni00000048/uni0000004b/uni00000044/uni00000059/uni0000004c/uni00000052/uni00000058/uni00000055Figure 5: MAE and RMSE trends over simulated user interactions. When with user behavior history, our methods\ndemonstrate superior cold-start performance and excellent error control. When without user behavior history, our\nmethods mitigate performance degradation effectively compared to baselines. Prompts are shown in Appendix C.\nCriteria TravelAgent GPT-4+ agent\nRationality 9.56 8.16\nComprehensiveness 8.87 6.25\nPersonalization 8.44 4.31\nTable 1: Average human evaluation scores of Ra-\ntionality, Comprehensiveness, and Personalization for\nTravelAgent and GPT-4+ agent across 20 travel cases.\nlong-term travel habits and preferences. Examples\nof long-term insights are in Appendix B.\n4 Evaluation\nWe conduct simulated and human user evaluations\nofTravelAgent to verify its performance against\nthe three criteria: Rationality, Comprehensiveness,\nand Personalization.\n4.1 Overall Evaluation\nThis section presents the results from case stud-\nies that evaluated TravelAgent using 20 distinct\ntravel scenarios and user input forms (shown in\nAppendix D). To establish a baseline, these case\nstudies were also conducted with a typical agent\nsystem, the GPT-4+ agent (Achiam et al., 2023),\nto showcase the performance of the proposed\nTravelAgent . In this section, human users interact\nwith TravelAgent and the GPT-4+ agent online\nusing these 20 input forms. Throughout the entire\nworkflow and analysis of generated itineraries, hu-\nman users evaluated three criteria on a scale from 1\nto 10: Rationality, Comprehensiveness, and Person-\nalization. Each criterion was assessed according to\na predefined set of evaluation dimensions detailed\nin Appendix E. The average scores are presented\nin Table 1, and comparative case study examples\nare detailed in Appendices F and G.4.2 Personalization Evaluation\nTo assess Personalization Module of TravelAgent\nand the performance of our agent-based online\nrecommendation framework, we experiment with\n100 simulated users based on GPT-3.5-turbo-0125\n(OpenAI, 2022) and 10 travel scenarios and in-\nstruct Recommendation Module to suggest 20 at-\ntractions per city to each user. Simulated users rate\nitems based on preferences, and TravelAgent pre-\ndicts these ratings while continually learning over\ninteractions. We compared our method with two\npopular LLM-based recommendation frameworks:\n(1)Direct Recommend and (2) Decode and En-\ncode , both lacking memory and continual learning.\nEvaluation metrics included Mean Absolute Error\n(MAE) and Root Mean Squared Error (RMSE).\nFigure 5 shows the trend results over user interac-\ntions and our methods\u2019 high prediction accuracy,\nverifying TravelAgent \u2019s Personalization.\n5 Conclusion\nIn this work, we introduced TravelAgent , a novel\ntravel service system leveraging LLMs and real-\ntime data for automated itinerary generation in real-\nworld scenarios with multi-dimensional constraints.\nWe establish three criteria for travel planning and\nmodel travel constraints. To address the challenges,\nwe developed tailored spatiotemporal-aware algo-\nrithms and a novel agent-based personalized on-\nline recommendation framework enhanced by a\nreal-time data processing mechanism and dynamic\nmemory. We assess TravelAgent through simu-\nlated user rating prediction and real-world human\ncase studies. The system delivers rational, compre-\nhensive, and personalized travel services. We hope\nour work will inspire further exploration of travel\nservices and LLM-based agent applications.\n6Limitations\nWhile TravelAgent we proposed aims to address\nrational and personalized travel itinerary genera-\ntion for real-world travel scenarios, it is essential\nto acknowledge certain limitations in our study.\nFirstly, the system heavily relies on the quality\nof dynamic real-time data sources; inaccuracies,\nsuch as outdated flight information, can result in\nsuboptimal recommendations and impractical final\nplans. Future research should focus on enhancing\nthe quality and reliability of these data sources to\nimprove adaptability across different travel scenar-\nios. Secondly, a deeper exploration of personalized\nactivities in travel itineraries based on more de-\ntailed user personalization information is needed\nfor further improvement.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 .\nJinheon Baek, Nirupama Chandrasekaran, Silviu\nCucerzan, Allen Herring, and Sujay Kumar Jauhar.\n2024. Knowledge-augmented large language mod-\nels for personalized contextual query suggestion. In\nProceedings of the ACM on Web Conference 2024 ,\npages 3355\u20133366.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Michal Podstawski, Lukas Gianinazzi,\nJoanna Gajda, Tomasz Lehmann, Hubert Niewiadom-\nski, Piotr Nyczyk, et al. 2024. Graph of thoughts:\nSolving elaborate problems with large language mod-\nels. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence , volume 38, pages 17682\u201317690.\nBooking Holdings Inc. 2024. Booking.com | official\nsite | the best hotels, flights, car rentals & accommo-\ndations. https://www.booking.com/ . Accessed\non August 5, 2024.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588 .\nBoris Chidlovskii. 2015. Improved trip planning by\nlearning from travelers\u2019 choices. In Proceedings of\nthe 2nd International Conference on Mining Urban\nData-Volume 1392 , pages 17\u201326.\nSunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu,\nZihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang,\nand Jun Xu. 2023. Uncovering chatgpt\u2019s capabilities\nin recommender systems. In Proceedings of the 17thACM Conference on Recommender Systems , pages\n1126\u20131132.\nBhavana Dalvi, Oyvind Tafjord, and Peter Clark. 2022.\nTowards teachable reasoning systems: Using a dy-\nnamic memory of user feedback for continual system\nimprovement. In Proceedings of the 2022 conference\non empirical methods in natural language process-\ning, pages 9465\u20139480.\nExpedia Group. 2024. Expedia travel: Vacation homes,\nhotels, car rentals, flights & more. https://www.\nexpedia.com/ . Accessed on August 5, 2024.\nFrommerMedia LLC. 2024. Frommer\u2019s travel guides:\nTrip ideas, inspiration & deals. https://www.\nfrommers.com/ . Accessed on August 5, 2024.\nMasum Hasan, Cengiz Ozel, Sammy Potter, and Ehsan\nHoque. 2023. Sapien: affective virtual agents pow-\nered by large language models. In 2023 11th Inter-\nnational Conference on Affective Computing and In-\ntelligent Interaction Workshops and Demos (ACIIW) ,\npages 1\u20133. IEEE.\nYupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu,\nRuobing Xie, Julian McAuley, and Wayne Xin Zhao.\n2024. Large language models are zero-shot rankers\nfor recommender systems. In European Conference\non Information Retrieval , pages 364\u2013381. Springer.\nChenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo\nZhao, and Hang Zhao. 2023. Chatdb: Augmenting\nllms with databases as their symbolic memory. arXiv\npreprint arXiv:2306.03901 .\nZiheng Huang, Sebastian Gutierrez, Hemanth Kamana,\nand Stephen MacNeil. 2023. Memory sandbox:\nTransparent and interactive memory management\nfor conversational agents. In Adjunct Proceedings of\nthe 36th Annual ACM Symposium on User Interface\nSoftware and Technology , pages 1\u20133.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, et al. 2023. Mis-\ntral 7b. arXiv preprint arXiv:2310.06825 .\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand, et al.\n2024. Mixtral of experts. arXiv preprint\narXiv:2401.04088 .\nWang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Mah-\neswaran Sathiamoorthy, Lichan Hong, Ed Chi, and\nDerek Zhiyuan Cheng. 2023. Do llms understand\nuser preferences? evaluating llms on user rating pre-\ndiction. arXiv preprint arXiv:2305.06474 .\nJunling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang\nZhou, and Yan Zhang. 2023a. Is chatgpt a good\nrecommender? a preliminary study. arXiv preprint\narXiv:2304.10149 .\n7Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang\nZhang, Jinjie Gu, and Guannan Zhang. 2023b.\nThink-in-memory: Recalling and post-thinking en-\nable llms with long-term memory. arXiv preprint\narXiv:2311.08719 .\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and\nJianfeng Gao. 2024. Chameleon: Plug-and-play\ncompositional reasoning with large language models.\nAdvances in Neural Information Processing Systems ,\n36.\nXiao Ma, Swaroop Mishra, Ariel Liu, Sophie Ying\nSu, Jilin Chen, Chinmay Kulkarni, Heng-Tze Cheng,\nQuoc Le, and Ed Chi. 2024. Beyond chatbots: Ex-\nplorellm for structured thoughts and personalized\nmodel responses. In Extended Abstracts of the CHI\nConference on Human Factors in Computing Sys-\ntems, pages 1\u201312.\nOpenAI. 2022. Chatgpt. https://openai.com/blog/\nchatgpt . Accessed: 2024-08-06.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789 .\nRoadtrippers, LLC. 2024. Road trip planner \u2013 build\nyour itinerary and find the best stops. https://\nroadtrippers.com/ . Accessed on August 5, 2024.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. 2024.\nToolformer: Language models can teach themselves\nto use tools. Advances in Neural Information Pro-\ncessing Systems , 36.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2024. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends\nin hugging face. Advances in Neural Information\nProcessing Systems , 36.\nNoah Shinn, Federico Cassano, Ashwin G"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Summary of the Paper: \"Self-Supervised Learning of Iterative Solvers for Constrained Optimization\"\n\n#### 1. Abstract:\nThis paper introduces a novel learning-based iterative solver, named LISCO, for constrained optimization problems, which combines a neural network predictor and a learned neural network iterative solver. The method can rapidly produce highly accurate solutions by first predicting an initial primal-dual solution and then refining it iteratively. Notably, the training of both neural networks is self-supervised, leveraging a new loss function based on modified Karush-Kuhn-Tucker (KKT) conditions, thus eliminating the need for pre-sampled optimizer solutions. Empirical evaluations show that the proposed method outperforms other learning-based and state-of-the-art solvers in terms of speed and accuracy.\n\n#### 2. Introduction:\n- **Context**: The efficient and accurate resolution of constrained optimization problems against varying parameters is vital in fields like control, optimal power flow, and planning. Traditional solvers face significant challenges in meeting the requirements of real-time application demands for high precision.\n- **Research Questions**: How can neural networks be employed to accelerate and improve the accuracy of solutions for parametric constrained optimization problems? How can self-supervised learning be used to train these neural networks effectively?\n- **Literature Review**: Previous techniques include fixed neural network predictors and iterative approaches reliant on manually sampled solutions, which suffer from approximation errors and constraint violations. Techniques like DC3 and PDL address these challenges to some extent but still show limitations in terms of training data cost and solution accuracy.\n- **Positioning**: This work introduces a unique two-step solver (predictor + iterative solver) fully trained in a self-supervised manner, promising superior speed and accuracy over existing methods.\n\n#### 3. Related Work:\n- **Learning-based Optimization**: Techniques like DC3 focus on directly predicting solutions but suffer from constraint violations and require expensive correction steps. PDL offers advances in self-supervised learning for optimization by training without prior solution data.\n- **Iterative Solvers**: Methods like those in the work of L\u00a8uken and Lucia (2024) show initial success in iterative solving using neural networks but exhibit limitations due to poor initial values and inadequate optimality condition handling.\n- **Contributions**: LISCO advances these methods by integrating an initial predictor with an iterative solver, using novel modified KKT-based loss functions for accurate, constraint-satisfying solutions without the need for pre-sampled solutions.\n\n#### 4. Method:\n- **Approach**: LISCO combines a neural network predictor and a learned iterative solver. The process begins with a neural network predicting an approximate initial value which is then refined by an iterative solver neural network.\n- **Mathematical Formulation**:\n  - **Optimization Problem**: \\( y^*(x) = \\arg\\min_y f(y;x) \\quad \\text{s.t.} \\quad h_j(y;x) = 0, \\quad g_i(y;x) \\leq 0 \\)\n  - **Primal-Dual Solution**: \\( z^*(x) = (y^*, \\nu^*, \\lambda^*) \\) with \\( \\nu \\) and \\( \\lambda \\) as Lagrange multipliers.\n  - **Lagrangian**: \\( L(y;x) = f(y;x) + \\sum_{j \\in H} \\nu_j \\cdot h_j(y;x) + \\sum_{i \\in G} \\lambda_i \\cdot g_i(y;x) \\)\n  - **KKT Conditions**: Required conditions \\( F_{KKT}(z^*(x);x) = 0 \\)\n  - **Modified Conditions**: Using the Fischer-Burmeister equation to transform inequalities into smooth equations.\n- **Algorithm**: The iterative solver neural network computes steps \\( \\Delta z_k \\) based on residuals of the KKT conditions and the current solution estimates.\n  - **Predictor**: \\( \\hat{z}_{\\text{Pred.}} = \\Pi_{\\theta,\\text{Pred.}}(x) \\)\n  - **Solver**: \\( \\Delta \\hat{z}_k = \\Pi_{\\theta,\\text{Solver}}(F_k, x) \\)\n  - **Training**: Both networks are trained using self-supervised losses based on the squared norm of the KKT residuals.\n- **Convexification for Non-Convex Problems**: Linearizing non-convex components with quadratic penalty terms during training to avoid local maxima or saddle points.\n\n#### 5. Experimental Setup:\n- **Datasets**: The paper uses several parametric constrained optimization problem instances with convex and non-convex objectives including quadratic programming (QP), non-convex QP, and Rosenbrock problems.\n- **Hardware and Software**: Experiments conducted on AMD Threadripper 1920x CPU, Nvidia RTX 3080Ti GPU, neural network training in PyTorch, optimization formulations in CasADi (non-convex) and CVXPY (convex QP).\n- **Evaluation Metrics**: Success rates, optimality gaps, constraint violations, and computational time.\n- **Baselines**: Comparisons with traditional solvers like OSQP and IPOPT, and recent learning-based methods like PDL and DC3.\n\n#### 6. Results and Analysis:\n- **Graphs and Tables**: Provided to illustrate convergence behavior, success rates, compute times, and errors on optimality conditions.\n- **Convergence Behavior**: Showed faster and more reliable convergence with the predictor, reaching significantly lower optimality gaps and constraint violations.\n- **Accuracy and Speed**: LISCO with predictor demonstrated orders of magnitude improvements in accuracy and speed comparable to OSQP and IPOPT, especially notable in non-convex cases.\n- **Comparison with Baselines**:\n  - **Predictor Alone**: Performed competitively with existing methods but showed superiority in refinement through the iterative solver.\n  - **LISCO vs. SOTA Solvers**: Outperformed IPOPT in non-convex problems with speedups up to 30x, demonstrating superior GPU parallelization potential.\n\n#### 7. Discussion:\n- **Implications**: The findings suggest that learning-based iterative solvers like LISCO can significantly impact applications with tight real-time constraints, offering both high accuracy and rapid solution capabilities.\n- **Limitations**: Current results are based on well-conditioned and relatively small-scale problems; further work is needed to scale the method to larger, more complex, or ill-conditioned optimization problems.\n- **Potential Biases**: The performance may vary with problem conditioning and the specific configurations of the neural networks used.\n\n#### 8. Conclusion:\n- **Key Contributions**: Introduction of LISCO, a novel integration of predictor and iterative neural network solvers for constrained optimization; development of self-supervised training using modified KKT conditions; significant improvements in solution accuracy and speed over traditional and learning-based methods.\n- **Future Research**: Emphasizes the exploration of LISCO in larger-scale and ill-conditioned problems, potential extensions for non-smooth optimization problems, and providing formal convergence guarantees.\n\nThis detailed summary includes the comprehensive technical content required to replicate the study or extend it directly. All essential components, from the formulation of the methodology through the experimental validation, are thoroughly detailed to ensure clarity and precision."}, {"role": "assistant", "content": "Self-Supervised Learning of Iterative Solvers for Constrained Optimization\nLukas L \u00a8uken, Sergio Lucia\nChair of Process Automation Systems, TU Dortmund University\nEmil-Figge-Str. 70, 44227 Dortmund, Germany\nlukas.lueken@tu-dortmund.de,sergio.lucia@tu-dortmund.de\nAbstract\nObtaining the solution of constrained optimization problems\nas a function of parameters is very important in a multitude\nof applications, such as control and planning. Solving such\nparametric optimization problems in real time can present\nsignificant challenges, particularly when it is necessary to\nobtain highly accurate solutions or batches of solutions. To\nsolve these challenges, we propose a learning-based iterative\nsolver for constrained optimization which can obtain very fast\nand accurate solutions by customizing the solver to a spe-\ncific parametric optimization problem. For a given set of pa-\nrameters of the constrained optimization problem, we pro-\npose a first step with a neural network predictor that outputs\nprimal-dual solutions of a reasonable degree of accuracy. This\nprimal-dual solution is then improved to a very high degree\nof accuracy in a second step by a learned iterative solver in\nthe form of a neural network. A novel loss function based\non the Karush-Kuhn-Tucker conditions of optimality is intro-\nduced, enabling fully self-supervised training of both neural\nnetworks without the necessity of prior sampling of optimizer\nsolutions. The evaluation of a variety of quadratic and non-\nlinear parametric test problems demonstrates that the predic-\ntor alone is already competitive with recent self-supervised\nschemes for approximating optimal solutions. The second\nstep of our proposed learning-based iterative constrained op-\ntimizer achieves solutions with orders of magnitude better\naccuracy than other learning-based approaches, while being\nfaster to evaluate than state-of-the-art solvers and natively al-\nlowing for GPU parallelization.\n1 Introduction\nThe accurate and fast solution of constrained optimization\nproblems as a function of their parameters is of great impor-\ntance for a variety of applications, such as control (Arango\net al. 2023), optimal power flow (Donti, Rolnick, and Kolter\n2021; Park and Van Hentenryck 2023) or planning and\nscheduling (Kotary, Fioretto, and Hentenryck 2022). The\nreal-time requirements pose a major challenge, especially\nwhen very accurate solutions or batches of solutions are to\nbe determined.\nIn the case of convex quadratic optimization problems\n(QPs), there are already very fast solvers such as OSQP\n(Stellato et al. 2020). In the nonlinear case, solvers such as\nIPOPT (W \u00a8achter and Biegler 2006) are also very successful,\nbut might be too slow for many applications. Furthermore,\nthese approaches cannot be parallelized natively.An alternative to the use of traditional iterative solvers are\nlearning-based approaches that predict optimal solutions di-\nrectly using neural networks. These are used, for example,\nfor optimal power flow problems (Donti, Rolnick, and Kolter\n2021) or for control problems (Lucia et al. 2021) and enable\nvery high inference speeds, as often only one neural network\nneeds to be evaluated.\nHowever, these approaches have two major disadvan-\ntages. First, the approximation error of the neural networks\nhas a negative effect on the solution quality. Especially\nfor constrained problems, these approximation errors lead\nto constraint violations. Second, for the classical super-\nvised training of these learning-based predictors, optimiza-\ntion data is required as training data, which can become very\ncostly to sample. For instance, DC3 by Donti, Rolnick, and\nKolter (2021), addresses the first problem by supplementing\nthe neural network prediction with downstream completion\nand correction steps, such as solving nonlinear systems of\nequations, to ensure constraint satisfaction, but at the cost\nof reducing the optimality of the solution and adding signif-\nicant additional computational overhead due to the correc-\ntion and completion steps. The recent work on Primal-Dual\nLearning (PDL) by Park and Van Hentenryck (2023), for\nexample, addresses the problem of training data availabil-\nity through a fully self-supervised training approach. It uses\na primal and a dual network that predict primal and dual\nsolutions and are alternately trained with a training algo-\nrithm based on an augmented Lagrangian method. With this\nmethod, neural networks can be trained completely without\nprior-sampled optimal solutions, predicting solution approx-\nimations of higher accuracy than other supervised or self-\nsupervised learning approaches. However, constraint viola-\ntions and non-zero optimality gaps are also present here.\nTo solve these two challenges in the context of model pre-\ndictive control (MPC), (L \u00a8uken and Lucia 2024) recently pre-\nsented an approach based on learning of an iterative solver\nwhere the iteration steps are predicted by a neural network.\nIn addition, the authors present an unsupervised or self-\nsupervised training approach that allows this iterative solver\nto be trained without prior-sampled optimal solutions. For a\nsmall nonlinear MPC test problem, this approach finds so-\nlutions that have orders of magnitude better accuracy with\nrespect to the optimality criteria of the optimization prob-\nlem than other learning-based approaches that predict thearXiv:2409.08066v1  [cs.LG]  12 Sep 2024solutions directly instead of doing so in an iterative process.\nHowever, this approach has some limitations. The ran-\ndomly selected initial values are relatively poor and there-\nfore require additional iterations of the learned solver, which\nincreases the computation time. The loss function used is\nbased on trying to learn Newton steps on the first-order nec-\nessary conditions of optimality. Since these conditions are\nnot sufficient for non-convex problems, this solver might be\ntrained to predict steps in direction of local maximizers or\nsaddle points and further cannot predict steps better than\nNewton steps, which artificially limits this approach. More-\nover, this approach has been demonstrated only on a small,\nsparse control problem with 3 parameters.\nTo overcome these aforementioned challenges, we\npresent a learning-based iterative solver for constrained op-\ntimization (LISCO). This approach consists of a neural net-\nwork predictor that predicts initial values for a neural net-\nwork solver. This iterative neural network solver takes into\naccount the error on the optimality conditions in the form of\nKarush-Kuhn-Tucker (KKT) conditions and predicts steps\nthat minimize these errors. Based on modified KKT condi-\ntions, self-supervised training approaches are presented for\nboth the neural network predictor and the neural network\nsolver, allowing both neural networks to be trained com-\npletely without previously sampled optimal solutions. With\nthis approach, predictions can be obtained that are orders\nof magnitude more accurate than those of other learning-\nbased optimization approaches, which means that negligi-\nbly small optimality gaps and virtually no constraint viola-\ntions can be realized. Furthermore, we show on a set of para-\nmetric test problems that this approach is able to determine\nsolutions for parametric constrained optimization problems\nmuch faster than state-of-the-art solvers such as IPOPT and\nalso allows for fully parallelized execution on the GPU, en-\nabling the sampling of large batches of solutions at the eval-\nuation time of one IPOPT solve. Moreover, we consider a\nsimple convexification strategy to apply the proposed train-\ning loss function directly to non-convex problems.\nThe paper is organized as follows: In Section 2, we give\nan overview of related work in learning-based optimization.\nSection 4 describes the background of the modified opti-\nmality criteria, which are necessary for this work in many\nplaces. In Section 5, we introduce the new LISCO approach,\nwhich is evaluated in Section 6 on a set of parametric test\nproblems after which we conclude this work in Section 7.\n2 Related Work\nIn recent years, there has been considerable growth in the\narea of Learning to Optimize (L2O or LtO) , which fo-\ncuses on improving solvers for specific classes of optimiza-\ntion problems such as neural network training (Li and Ma-\nlik 2016; Chen et al. 2022b). These approaches usually\naim at improving update strategies of existing solver such\nas (stochastic) gradient descent or by learning novel up-\ndate rules for the specific problem classes. While these ap-\nproaches usually do not consider constrained parametric op-\ntimization problems, in which strong requirements on the\noptimality of a solution are made, recently in the specific\ncontext of MPC, L \u00a8uken and Lucia (2024) have demonstratedthe use of learned iterative solvers to quickly solve nonlin-\near constrained optimization problems, achieving very low\noptimality error.\nIn the context of parametric constrained optimization,\nother approaches which can be described as predictors ,\namortized optimization (Amos et al. 2023) or optimization\nproxies (Park and Van Hentenryck 2023) have become pop-\nular. These methods predict optimal solutions directly on\nthe basis of problem parameters and have found applica-\ntion in several areas, including optimal power flow problems\n(Donti, Rolnick, and Kolter 2021; Park and Van Henten-\nryck 2023) and MPC. In the MPC context, these techniques\nare often referred to as approximate MPC (Karg and Lucia\n2020a; Hertneck et al. 2018) or neural network-based opti-\nmization (Arango et al. 2023). Moreover, approaches such as\nDC3 by Donti, Rolnick, and Kolter (2021) or the works by\nKarg and Lucia (2020b) and Chen et al. (2022a) include ad-\nditional downstream calculations to achieve constraint sat-\nisfaction of the predictions. Approaches to self-supervised\nlearning of predictors include for instance PDL by Park and\nVan Hentenryck (2023) and differentiable predictive control\n(DPC) by Drgo \u02c7na et al. (2022).\n3 Notations\nVectors are defined in bold lowercase, e.g. y\u2208Rny. Ma-\ntrices are described by using bold uppercase letters, e.g.\nA\u2208Rm\u00d7n. Iterations are indicated by subscript k, e.g.zk.\n4 Preliminaries\nIn this section, we first formalize the parametric optimiza-\ntion problem with constraints and its optimality conditions.\nWe then describe the foundations of recent work on learn-\ning iterative solvers based on these optimality criteria, which\nform the basis for the approach in this paper.\nParametric Constrained Optimization\nA general parametric constrained optimization problem can\nbe described as follows:\ny\u2217(x) = arg min\ny\u2208Yf(y;x) (1a)\ns.t. h j(y;x) = 0 ,\u2200j\u2208 H, (1b)\ngi(y;x)\u22640,\u2200i\u2208 G. (1c)\nThe goal is to minimize an objective function f:Y \u00d7\nX \u2192Rby optimizing over the decision variables y\u2208 Y \u2286\nRny, where x\u2208 X \u2286 Rnxare parameters of the optimiza-\ntion problem, with YandXdescribing the domains of y\nandx, respectively. The dimensions of yandxare given by\nnyandnx. In addition, this problem formulation contains\nequality and inequality constraints, hj:Y \u00d7 X \u2192 Rand\ngi:Y \u00d7 X \u2192 R, which must be satisfied, where jandi\ndefine the indices of the constraints as part of the index sets\nHandG. The objective function fand the constraint func-\ntions hjandgican be potentially nonlinear, non-convex,\nand non-smooth. In this paper, however, we only consider\nsmooth objectives and constraints and leave the considera-\ntion of non-smooth problems to future work. The optimiza-\ntion problem (1) is parametric in the sense that f,hj, andgipotentially depend on x. Therefore, the optimal solution\ny\u2217(x)is a function of x.\nA primal-dual solution of optimization problem (1) can be\ndenoted by z\u2217(x) = (y\u2217, \u03bd\u2217, \u03bb\u2217), with the Lagrange multi-\npliers \u03bdjand\u03bbicorresponding to the equality and inequality\nconstraints hjandgi, respectively.\nConsequently, the Lagrangian Lis defined as\nL(y;x) =f(y;x) +X\nj\u2208H\u03bdj\u00b7hj(y;x) +X\ni\u2208G\u03bbi\u00b7gi(y;x).\n(2)\nForz\u2217(x)to be an optimal solution, the first-order nec-\nessary conditions of optimality, the Karush-Kuhn-Tucker\n(KKT) conditions, have to be satisfied. These conditions are\nformulated as follows (Nocedal and Wright 2006),\nFKKT(z\u2217(x);x) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\u2207yL(z\u2217;x) = 0 ,\nhj(y\u2217;x) = 0 ,\u2200j\u2208 H,\ngi(y\u2217;x)\u22640,\u2200i\u2208 G,\n\u03bb\u2217\ni\u22650,\u2200i\u2208 G,\n\u03bb\u2217\ni\u00b7gi(y\u2217;x) = 0 ,\u2200i\u2208 G,(3a)\n(3b)\n(3c)\n(3d)\n(3e)\nwith stationarity condition (3a), primal and dual feasibili-\nties (3b),(3c),(3d) and complementary slackness (3e). Given\nthat the optimization problem is (1) convex, i.e. the objective\nfunction fand the inequality constraints giare convex, and\nthe equality constraints hjare affine in yfor all x, then these\nconditions are not only necessary but also sufficient for opti-\nmality, meaning that any point satisfying these conditions is\na global minimizer (Boyd and Vandenberghe 2004; Nocedal\nand Wright 2006). However, if the optimization problem is\nnon-convex for certain instantiations of x, then there might\nexist multiple local solutions and local maximizers or saddle\npoints.\nReformulation of the Optimality Conditions\nMany solution approaches to problem (1) are based on a re-\nformulation of the KKT conditions (3) into a nonlinear sys-\ntem of equations Fwhich has its roots at the primal-dual\nsolutions F(z\u2217(x);x) = 0 , such as active-set methods or\ninterior-point methods (Nocedal and Wright 2006). Specif-\nically suited for the application to learning-based optimiza-\ntion is the Fischer-Burmeister reformulation, as it does not\nrequire updating an active-set and is able to handle infeasi-\nble starting points (Fischer 1992; Liao-McPherson and Kol-\nmanovsky 2020; L \u00a8uken and Lucia 2024).\nThe Fischer-Burmeister equation is defined as the follow-\ning implicit function, that replaces the inequality constraints\n(3c), dual feasibility (3d) and the complementary slackness\n(3e) and implicitly satisfies these equations as long as the\nresidual is zero (Fischer 1992):\n\u03d5(\u03bbi, gi) =\u03bbi\u2212gi\u2212q\n\u03bb2\ni+g2\ni+\u03f52= 0. (4)\nHere, \u03f5represents a small smoothing parameter, e.g. 10\u22126.\nThe nonlinear system of equation representing the optimal-ity conditions then reads as follows:\nF(z\u2217(x);x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\u2207yL(z\u2217;x) = 0 ,\nhj(y\u2217;x) = 0 ,\u2200j\u2208 H,\n\u03d5(\u03bb\u2217\ni, gi(y\u2217;x)) = 0 ,\u2200i\u2208 G.(5a)\n(5b)\n(5c)\nLearning Iterative Solvers\nIn general, a closed-form solution to problem (1) does not\nexist. Therefore, numerical solvers are applied to solve this\nproblem iteratively, which are based on the following update\nrule,\nzk+1=zk+\u03b1\u2206zk, (6)\nwhich is repeated until a maximum number of steps or an\noptimality tolerance is reached, with zkdenoting the value\nofzat iteration k,\u03b1describing the step size and \u2206zkthe\nstep (Nocedal and Wright 2006). The update rule (6) can\nthen be defined as a rootfinding step of (3) or (5), e.g. via\nNewtons method or Quasi-Newton methods (Nocedal and\nWright 2006). However, computing these steps can be very\ncostly, especially for large optimization problems.\nTo this end, L \u00a8uken and Lucia (2024) propose to use a\nneural network to predict the steps, replacing the expensive\ncomputation of \u2206zke.g. via Newtons method. For a given\nparameter vector x, this approach predicts a step \u2206\u02c6zkat a\ncurrent iteration kwith a neural network \u03a0\u03b8, with weights\n\u03b8, based on the residual Fkof the modified optimality con-\nditions (5) as follows:\n\u2206\u02c6zk= \u03a0 \u03b8(Fk,x). (7)\nThe neural network \u03a0\u03b8is trained by introducing a loss func-\ntion based on the linearization of the modified optimality\nconditions around the current iterate zk:\n\u2225Fk+\u2207zFk\u2206\u02c6zk\u22252\n2. (8)\nAs the loss function is only dependent on the current it-\nerates zk, the predicted steps \u2206\u02c6zkand the parameters of the\noptimization problem instances x, it is not reliant on pre-\nviously sampled optimal solutions. As a result, the training\ncan be carried out completely self-supervised. First, initial\nvalues for zk=0and values for the parameters xrandomly\nsampled. Subsequently, the predicted steps are employed to\ncompute the loss and to train the NN using standard gra-\ndient descent methods. Additionally, current iterates zkare\nupdated using the predicted steps \u2206\u02c6zkup to a fixed num-\nber of iterations, after which new points are sampled. In this\nway, the learned solver improves and subsequentially pro-\nduces iterates of with decreasing residual on the optimality\nconditions during training. For further details, please refer to\nthe work of L \u00a8uken and Lucia (2024).\n5 Learning-Based Iterative Solver for\nConstrained Optimization\nThe following section presents our proposed approach:\nlearning-based iterative solver for constrained optimization\n(LISCO). The fundamental concept of this methodology is\nthe integration of a neural network predictor with a neural\nnetwork solver, thereby extending the approach proposed byAlgorithm 1: LISCO: Learning-based Iterative Solver for\nConstrained Optimization\nInput :x,\u03a0\u03b8,Pred.,\u03a0\u03b8,Solver,Nmax,\u03c4= 10\u22128,\u03b1= 1.0,\u03c9=\n10,\u03b2= 0.95\nOutput :\u02c6z\n1:Letk= 0.\n2:z0\u2190\u03a0\u03b8,Pred.(x)(9)\n3:F0=F(z0,x)(5)\n4:while k < N maxdo\n5: \u2206\u02c6zk= \u03a0 \u03b8,Solver(Fk,x)(10)\n6:zk\u2190zk+\u03b1\u2206\u02c6zk(6)\n7: Fk=F(zk,x)(5)\n8: if||Fk||< \u03c4then\n9: break {Convergence achieved }\n10: end if\n11: if||Fk||<||Fbest||then\n12: zbest\u2190zk\n13: Fbest\u2190Fk\n14: end if\n15: if||Fk||> \u03c9||Fbest||then\n16: zk\u2190zbest\n17: \u03b1\u2190\u03b1\u00b7\u03b2\n18: end if\n19: k\u2190k+ 1\n20:end while\n21:return \u02c6z\u2190zk\nL\u00a8uken and Lucia (2024) to obtain rapid solutions with high\naccuracy for parametric constrained optimization problems.\nThe predictor takes the parameters xas inputs and pre-\ndicts an estimate of the primal-dual solution \u02c6zPred.of the op-\ntimization problem using a feedforward neural network:\n\u02c6zPred.= \u03a0 \u03b8,Pred.(x). (9)\nThe neural network solver, a learned iterative solver, calcu-\nlates iterative steps \u2206\u02c6zkbased on the predicted starting val-\nuesz0=\u02c6zPred.in order to improve the accuracy of the solu-\ntion. The error on the modified optimality criteria (5) for the\ngiven iteration Fk=F(zk,x)is determined and given to-\ngether with the parameters of the optimization problem xas\ninputs to a feedforward neural network, which predicts the\nsteps\n\u2206\u02c6zk,Solver=\u2225Fk\u22252\u00b7\u03a0\u03b8,Solver(\u02dcFk,x), (10)\nwith the input scaling \u02dcFk= [Fk/\u2225Fk\u22252,log(\u2225Fk\u22252)].As in\nthe work of L \u00a8uken and Lucia (2024), we normalize Fkin the\ninput of the neural network solver and supplement it with\nthe logarithmically scaled norm of Fkin order to map the\ndirection and magnitude of the residual vector Fkin values\nof a similar order of magnitude. In addition, we scale the\noutput with the norm \u2225Fk\u22252so that the predicted step scales\nwith the magnitude of the error to allow for very small steps\nclose to the optimal solutions.\nLISCO is summarized in Algorithm 1, whereby a simple\nline-search used, which resets the current iterate and alsoreduces the step size \u03b1by a factor \u03b2in the event of an ex-\ncessive divergence of the iterates, specified with the factor\n\u03c9. The algorithm terminates when a specified tolerance of\nthe modified KKT conditions \u03c4is reached or a maximum\nnumber of iterations Nmaxis exceeded. This line-search ap-\nproach makes it possible, on the one hand, to perform steps\nthat temporarily worsen the iterates if significantly better\nsteps are performed afterwards and, on the other hand, to be\nperformed in parallel for a batch of data, e.g. on the GPU,\nbecause the step sizes do not have to be \u201ctested\u201d first, such\nas in backtracking line search methods (Nocedal and Wright\n2006), which means stopping the iterations and possibly per-\nforming several calculations of F. Since both the predictor\nand the solver are mapped by neural networks and the pre-\ndiction only requires the calculation of the modified KKT\nconditions (5), which only contain first-order derivatives of\nthe optimization problem, the prediction steps of LISCO can\nbe executed very quickly and can also be parallelized on the\nGPU.\nSelf-Supervised Training of LISCO\nTo enable self-supervised training for both the neural net-\nwork predictor and the neural network solver, we propose to\nuse the following metric Tto evaluate the optimality of the\niterates zk:\nT(z,x) = 1 /2||F(z,x)||2\n2 (11)\nHere, we use the squared Euclidean norm of the modified\nKKT conditions (5). This is a modification of (8), whereby\nthe linearization is omitted in order to reduce artificial re-\nstrictions on the achievable quality of the calculated steps\nand facilitate the application to the predictor training.\nUsing (11), the training loss functions for the neural net-\nwork predictor for a batch of Nbatchparameter vectors xtcan\nbe set up as follows,\nLLISCO\nPred. =1\nNbatchNbatchX\nt=1T(\u02c6zt\nPred.,xt), (12)\nwhere only the predictions \u02c6zPred.of the neural network pre-\ndictor and the parameters xof optimization problem (1) are\nnecessary, so that the training can be conducted completely\nself-supervised with e.g. (stochastic) gradient descent meth-\nods.\nSince the neural network solver is expected to achieve\nvery high accuracy and therefore also values for T, which\ncan vary over several orders of magnitude during training,\nwe use logarithmic scaling:\nLLISCO\nSolver =1\nNbatchNbatchX\nt=1log10\u0000\nT(zt+ \u2206\u02c6zt\nSolver,xt)\u0001\n.(13)\nWe train the neural network solver by sampling a batch\nof random values for x\u2208 X and either evaluating a previ-\nously trained neural network predictor or sampling random\nstarting values for z0. Based on these values, the residual\nFkis first determined and a step \u2206\u02c6zkis predicted with the\nneural network solver. After the weights \u03b8of the neural net-\nwork have been updated using loss (13), the step is applied\nto the iterates and the next batch of iterates zkis determined,with which the next training step is carried out. If the metric\nTkfalls below a tolerance \u03c4(e.g.10\u22128)or the iterations k\nreach the maximum number of iterations Nmax,train , the cor-\nresponding data points of the batch are resampled. The pro-\ncedure is repeated for a fixed number of training steps.\nConvexification\nOne challenge of this approach is that the KKT conditions\n(3) in the non-convex case are only necessary for a lo-\ncal minimum but not sufficient. This means that a solution\nF(z\u2217;x) = 0 could be a maximizer, minimizer or a sad-\ndle point. To avoid running into stationary points, which are\nnot minimizers, we apply a simple convexification strategy,\nwhich is based on linearizing the non-convex objective func-\ntions (and potentially nonlinear constraints) and adding a\nquadratic penalty term with penalty parameter \u03c1 >0, which\nlimits steps reaching too far away from the point of lineariza-\ntion,\n\u02dcf(y;x) =fk+\u2207yfk(y\u2212yk)\n+\u03c1(y\u2212yk)\u22a4(y\u2212yk),(14)\nwhere yk\u2208zkis the current iterate around which lineariza-\ntion is performed and y\u2208zdescribes the point predicted\nby the solver or predictor, so that if a solution is reached and\ny=ykapplies, the error due to the linearization disappears.\n6 Experiments\nIn the following, we examine the performance of our pro-\nposed LISCO approach and compare it with recent results\nfrom the literature. We consider convex and non-convex QPs\nused by Donti, Rolnick, and Kolter (2021) and Park and\nVan Hentenryck (2023) as well as a variation of these exam-\nples with the Rosenbrock test function (Rosenbrock 1960).\nWe investigate the convergence behavior of LISCO, the so-\nlution accuracy in terms of constraint violations as well as\noptimality, the evaluation speed compared to state-of-the-art\nsolvers such as OSQP and IPOPT.\nCase Study\nThe parametric constrained optimization problem examples\nare defined as follows:\nmin\ny\u2208Rnyf(y;x) (15a)\ns.t.Ay=x, (15b)\nGy\u2264h, (15c)\nwith decision variables y\u2208Rny, matrices A\u2208Rnh\u00d7ny\nandG\u2208Rng\u00d7nyfor equality and inequality constraints,\nrespectively, as well as the vector h\u2208Rngof the inequal-\nity constraints. The parameters x\u2208[\u22121,1]nxare included\nin the equality constraints as affine terms, whereby the di-\nmension of the parameters is correspondingly equal to the\ndimension of the equality constraints: nx=nh. We thus\nadopt this formulation of the constraints as well as the in-\nstantiation of the matrices AandGand the vector hfrom\n(Donti, Rolnick, and Kolter 2021). This instantiation ensures\nthat for all x\u2208[\u22121,1]nxa feasible solution exists and the\noptimization problem can be solved.Three different cases are considered as objective func-\ntions:\nfQP=1\n2y\u22a4Qy+p\u22a4y (16a)\nfNonconv.=1\n2y\u22a4Qy+p\u22a4sin(y) (16b)\nfRosenbrock=n\u22121X\ni=1\u0002\n(yi+1\u2212y2\ni)2+ 0.01(1\u2212yi)2\u0003\n+ 5pTy (16c)\nThe instantiations of Q\u2208Rny\u00d7nyandp\u2208Rnyare per-\nformed as described by Donti, Rolnick, and Kolter (2021).\nThe linear term 5pTywas added to the Rosenbrock problem\nto avoid solutions where no inequality constraints are active.\nFor each of the case studies, we create 5 instances of\nthe optimization problems with 100 decision variables, 50\nequality constraints and parameters and 50 inequality con-\nstraints. The nonlinear optimization problems are modeled\nin CasADi (Andersson et al. 2019) and the convex QP in\nCVXPY (Diamond and Boyd 2016). For comparison, the\noptimizers IPOPT for the non-convex QP and the Rosen-\nbrock problem and OSQP for the convex QP are used\nand 1000 optimal solutions are sampled for each instance,\nwhereby the tolerance of IPOPT was set to 10\u221210and the\ndefault settings in CVXPY are used by OSPQ. The exper-\niments were performed on an AMD Threadripper 1920x\nCPU and an Nvidia RTX 3080Ti GPU. The neural network\ntraining as well as the modified KKT conditions (5) were\nimplemented in PyTorch and, if applicable, accelerated by\nJIT-compilation for the inference (Paszke et al. 2019).\nBoth the neural network predictor and neural network\nsolver are modeled as a feedforward neural network with\na hidden layer with 2048 neurons and leaky relu (Maas et al.\n2013) activation function. A batch size of 4096 was selected.\nAdamW (Loshchilov and Hutter 2018) with a weight decay\nof10\u22123is used as the optimizer. The predictor training starts\nat a learning rate of 10\u22123, which is reduced by a factor of 10\nwhen a plateau of the loss is reached. In addition, the training\nis stopped early if the learning rate would go below 10\u22128.\nTraining is carried out up to a maximum number of epochs\nof 150k for the predictor and 100k epochs for the solvers.\nThe solver training uses a fixed learning rate of 10\u22124and\nconsiders a maximum number of steps after a data point is\nresampled if the tolerance \u03c4= 10\u22128has not been reached.\nFirst, the predictor (Pred.) is trained, which is then also used\nfor the prediction of the start values in neural network solver\ntraining (LISCO with Pred.). For comparison, we also train\nneural network solvers without predictor (LISCO w/o Pred.)\nwith randomly sampled starting values z0\u223c N(0,1). In the\nnon-convex case, the convexification strategy (14) was ap-\nplied.\nResults\nFigure 1 shows the convergence behavior of the LISCO ap-\nproach with and without predictor. It can be seen that both\nformulations converge to very accurate solutions in almost\nall cases for the various optimization problems. However,0 50 100 150 200 250\nIteration0.00.20.40.60.81.0Fraction of converged solutionsQP\n||Fk||2\n2\u22641e-06\n||Fk||2\n2\u22641e-08\nLISCO w/o Pred.\nLISCO with Pred.\n0 50 100 150 200 250\nIteration0.00.20.40.60.81.0Fraction of converged solutionsRosenbrock\n||Fk||2\n2\u22641e-06\n||Fk||2\n2\u22641e-08\nLISCO w/o Pred.\nLISCO with Pred.\n0 50 100 150 200 250\nIteration0.00.20.40.60.81.0Fraction of converged solutionsRosenbrock\n||Fk||2\n2\u22641e-06\n||Fk||2\n2\u22641e-08\nLISCO w/o Pred.\nLISCO with Pred.Figure 1: Convergence behaviour of LISCO with and with-\nout the use of a Predictor. Here, the fraction iterates zkof all\nruns at iteration k, that have reached errors on the norm of\nthe optimality conditions \u2225Fk\u22252\n2lower than a a given toler-\nance\u03c4\u2208[10\u22126; 10\u22128]is given.\nit can be seen that with the use of the predictor, LISCO\nrequires significantly fewer iterations than if it is omitted\nand random starting values are used instead and, in addition,\nLISCO with predictor converges more reliably to very ac-\ncurate solutions with an optimality error \u2225Fk\u22252\n2\u226410\u22128, as\ncan be seen in the case of the Rosenbrock problem.\nTable 1 contains the results of the LISCO and predictor\nevaluations on the test data. A solver run is considered not\nsuccessful if a tolerance of \u2225Fk\u22252\n2\u226410\u22128has not been\nreached after 500 iterations. It can be seen that the predic-\ntor already has relatively low optimality gaps and constraint\nviolations and is qualitatively comparable to the PDL ap-\nproach of Park and Van Hentenryck (2023). And what is\nmore important, our proposed LISCO approach leads to sig-\nnificantly higher accuracy and thus, for example, LISCO\nwith predictor in the case of the QP and Rosenbrock prob-\nlems has no optimality gaps or constraint violations larger\nthan10\u22123. Exceptions are individual runs in the non-convex\nQP example, where the maximum optimality gap is rela-\ntively high, but this may also be due to the fact that the\nproblem is non-convex and the solver may have run into\nworse local minima and individual runs have not convergedbecause the initial value prediction with the predictor was\ntoo poor in individual cases. In this case, a larger NN for the\npredictor might be helpful, as LISCO without predictor per-\nforms very well here and does not violate any constraints,\nwhile the optimality gap is low. The LISCO without pre-\ndictor runs for the Rosenbrock problem, which did not con-\nverge, explain the rather large deviation here.\nTable 2 compares the success rates, solve times and it-\neration counts of the LISCO approach with the OSQP and\nIPOPT optimizers, whereby all approaches were performed\nindividually on the CPU for each data point of the test set.\nFor the solve times, only solver runs are shown that are con-\nverged with a tolerance T\u226410\u22128. In the QP case, OSQP\nis very fast and can solve problems faster than LISCO. In\nthe median case, however, this difference is small. In con-\ntrast to OSQP though, LISCO is parallelizable on the GPU,\nallowing to solve large batches in parallel, while only be-\ning slightly slower than solving a single datapoint on the\nCPU. For non-convex QP and Rosenbrock problems, LISCO\noutperforms IPOPT with and without a predictor, whereby\nLISCO with a predictor is even 30 times faster than IPOPT\nin the median, which represents a considerable acceleration.\nLISCO with predictor also requires fewer steps than IPOPT\nat the median, highlighting the benefit of the predictor.\nIt is also interesting to note that the individual LISCO\nsteps (error calculation and neural network inference) exe-\ncuted on a GPU for a batch size of 1000 with a duration\nof approximately 1 ms only require twice as long as these\nsteps implemented on the CPU for a single data point. This\nshows the great potential of LISCO to calculate orders of\nmagnitude more optimal solutions by exploiting the inherent\nGPU parallelization capability of this approach. The train-\ning times can be roughly given as a few hundred seconds\nto a maximum of 20 minutes for the predictor and approxi-\nmately 30-40 minutes for the training of the solver.\n7 Conclusion\nWe propose LISCO, a novel approach to learning-based it-\nerative solvers for constrained optimization that combines\na neural network predictor and a neural network solver to\nefficiently solve parametric constrained optimization prob-\nlems. This approach enables a fully self-supervised train-\ning procedure without the need for pre-calculated optimal\nsolutions. Moreover, very high accuracies with negligible\noptimality gaps and constraint violations and very low er-\nrors on the optimality conditions are achieved with this ap-\nproach, showing superior performance compared to exist-\ning learning-based methods such as PDL and DC3, pro-\nvided that the LISCO neural network solver is repeatedly\nevaluated. For the considered non-convex constrained opti-\nmization problems, LISCO outperforms the state-of-the-art\nsolver IPOPT in terms of solution times, being up to 30 times\nfaster, while showing a strong potential for GPU paralleliza-\ntion, enabling fast solution of large batches of parametric\nconstrained optimization problems.\nWhile these outcomes are promising, it should be noted\nthat thus far, only well-conditioned test problems have\nbeen considered on a small scale. Further research willMethod Max. Opt. Gap(%) Opt. Gap(%) Max. Eq. Max. Ineq. Mean Eq. Mean Ineq.\nQP\nPred. 0.191 (0.082) 0.039 (0.032) 0.006 (0.003) 0.015 (0.014) 0.000 (0.000) 0.000 (0.000)\nLISCO w/o Pred. 0.006 (0.011) 0.000 (0.000) 0.000 (0.000) 0.002 (0.003) 0.000 (0.000) 0.000 (0.000)\nLISCO with Pred. 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000)\nPDL - 0.176 (0.054) 0.005 (0.001) 0.001 (0.000) 0.002 (0.000) 0.000 (0.000)\nDC3 - 6.219 (0.098) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000)\nNonconvex-QP\nPred. 0.503 (0.807) 0.105 (0.190) 0.004 (0.002) 0.021 (0.011) 0.000 (0.000) 0.000 (0.000)\nLISCO w/o Pred. 0.273 (0.423) 0.008 (0.013) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000)\nLISCO with Pred. 0.754 (1.534) 0.021 (0.043) 0.000 (0.001) 0.001 (0.003) 0.000 (0.000) 0.000 (0.000)\nPDL - 0.324 (0.051) 0.004 (0.001) 0.001 (0.000) 0.001 (0.000) 0.000 (0.000)\nDC3 - 4.103 (0.151) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000)\nRosenbrock\nPred. 0.062 (0.020) 0.009 (0.004) 0.016 (0.004) 0.131 (0.171) 0.001 (0.000) 0.000 (0.000)\nLISCO w/o Pred. 35.097 (78.480) 0.048 (0.107) 0.106 (0.236) 0.100 (0.223) 0.000 (0.000) 0.000 (0.000)\nLISCO with Pred. 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000)\nTable 1: Comparison of optimization performance metrics for predictor (Pred.), LISCO without predictor, and LISCO with\npredictor across the QP, Rosenbrock, and non-convex QP Problems, each with 100 decision variables, 50 equality, and 50\ninequality constraints. The performance metrics include the maximum and average constraint violations for the equality (Eq.)\nand inequality (Ineq.) constraints as well as the maximum and average optimality gaps (%) of the predicted solutions compared\nto the optimizer solutions: (f(y)\u2212f(y\u2217))/|f(y\u2217)|. Lower values indicate better performance for all metrics. The best results\nare marked in bold , while bad results which are specifically discussed are underlined . The results for DC3 and PDL were\ntaken from (Park and Van Hentenryck 2023), in which one instantiation of the convex and non-convex QPs is considered.\nAlthough the exact instantiations used in their work and the code for the PDL approach were not available to us, we repeat our\nexperiments for 5 different instantiations of optimization problems (i.e., Q,p,G,A,hmatrices and vectors). We report the\nstandard deviations across the different instantiations in parentheses.\nProblem Method Solve Time (s) Success Iteration Count\n50th Max. Rate 50th Max.\nQPLISCO w/o Pred. 0.028 (0.006) 0.121 (0.033) 0.999 (0.001) 57.000 (13.509) 257.800 (72.933)\nLISCO with Pred. 0.008 (0.005) 0.100 (0.049) 0.999 (0.002) 16.200 (10.826) 219.200 (106.985)\nOSQP 0.005 (0.001) 0.008 (0.000) - 55.000 (11.180) 70.000 (11.180)\nNon-\nconvex-\nQPLISCO w/o Pred. 0.022 (0.005) 0.148 (0.045) 0.979 (0.033) 42.500 (9.203) 304.333 (97.971)\nLISCO with Pred. 0.010 (0.004) 0.086 (0.046) 0.977 (0.035) 18.167 (7.960) 176.833 (104.549)\nIPOPT 0.150 (0.004) 0.259 (0.059) - 15.833 (0.408) 24.500 (3.507)\nRosen-\nbrockLISCO w/o Pred. 0.016 (0.008) 0.084 (0.107) 0.877 (0.275) 24.400 (14.046) 141.200 (186.405)\nLISCO with Pred. 0.004 (0.000) 0.037 (0.005) 1.000 (0.000) 4.800 (0.837) 61.200 (8.497)\nIPOPT 0.120 (0.002) 0.193 (0.010) - 14.800 (0.447) 21.800 (0.447)\nTable 2: Performance comparison of LISCO with and without predictor, and standard solvers (OSQP/IPOPT) across the QP,\nRosenbrock, and non-convex QP problems. Each problem consists of 100 decision variables, 50 equality and 50 inequality\nconstraints. The results show median (50th percentile) and maximum solve times, success rates, and iteration counts. The\nfastest solve times are marked in bold . Each evaluation is conducted per datapoint (batch size 1). Only convergent runs were\ntaken into account for the solution times and iteration counts (the proportion is indicated by the success rate), as divergent runs\nusually diverge in the first iterations. Standard deviations are presented in parentheses.\nexamine the scalability of this approach to larger and ill-\nconditioned optimization problems, e.g. by exploiting spar-\nsity and learning-based conditioning approaches. Modifica-tions to LISCO that would provide convergence guarantees,\nwhich are currently lacking, will also prove an avenue wor-\nthy of further investigation.References\nAmos, B.; et al. 2023. Tutorial on Amortized Optimization.\nFoundations and Trends\u00ae in Machine Learning , 16(5): 592\u2013\n732.\n"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "Here's a comprehensive summary of the academic paper *AI-accelerated discovery of high critical temperature superconductors*:\n\n**1. Abstract:**\nThe paper presents an AI-driven search engine developed to discover high critical temperature (T_c) superconductors. By integrating deep model pre-training, fine-tuning techniques, diffusion models, and physics-based approaches like first-principles electronic structure calculations, the study predicts 74 dynamically stable materials with T_c\u226515 K, none of which are in existing datasets. The results demonstrate AI's significant potential in accelerating the discovery of new high-T_c superconductors.\n\n**2. Introduction:**\nThe research aims to address the limitations of conventional methods in discovering new high-T_c superconductors. Traditional approaches rely heavily on physical intuition and existing databases, which only represent a small fraction of potential materials. Recent advances in AI, particularly machine learning, offer new avenues for material discovery. This research focuses on integrating AI methods, such as deep learning and diffusion models, with first-principles calculations to identify new high-T_c superconductors.\n\n**3. Related Work:**\nThe paper reviews past efforts in high-T_c superconductor discovery, citing works such as Wines et al. (2023), which used crystal diffusion variational auto-encoders, and Choudhary and Garrity (2022), who employed electron-phonon coupling calculations enhanced by deep learning. These studies primarily used chemical formula searches based on existing datasets and lacked detailed atomic structure information. This paper pushes the boundary by incorporating detailed structural data and going beyond known databases using an AI-driven approach.\n\n**4. Method:**\nThe proposed approach involves several AI techniques and DFT calculations integrated into a search engine framework. Key components include:\n\n  - **Generative Model**: Inspired by the DiffCSP model and using diffusion generative models and equivalent graph neural networks (GNNs), it generates new superconducting crystal structures. The model defines a symmetry-constrained process for crystal generation.\n  \n  - **Superconducting Classification Model**: A pre-trained GNN informed by material crystal structures, refined through high-throughput screening and binary classification, achieving a 99.04% success rate in predicting superconducting properties.\n  \n  - **Formation Energy Prediction Model**: Using an improved MEGNET architecture with a focus on long-range atomic interactions, predicting formation energies with a mean absolute error (MAE) of 21 meV per atom.\n  \n  - **Active Learning Techniques**: Incorporating discovered superconductors into the training set iteratively to enhance predictive capabilities.\n\n**5. Experimental Setup:**\nDatasets include 105 known BCS superconductors and 380,000 crystal structures from GNoME and Materials Project. The hardware setup includes high-performance computing resources at Renmin University of China. Evaluation metrics focus on critical temperature, formation energy, and dynamic stability. Baseline methods for comparison include previous AI-driven and DFT-based superconductor discovery approaches.\n\n**6. Results and Analysis:**\n  - **Empirical Findings**: 74 new materials with predicted T_c\u226515 K. Notable materials include B_4CN_3 and B_5CN_2 with T_c of 24.08 K and 15.93 K, respectively.\n  - **Figures and Tables**: Containing electronic structures, phonon properties, specific crystal structures, and EPC analysis.\n  - **Detailed Analysis**: Validating predictions with DFT calculations, incorporating band structures, density of states (DOS), and phonon spectrum.\n  - **Ablation Studies**: Indicating significant precision improvements in formation energy prediction and the efficacy of the integrated AI methods.\n\n**7. Discussion:**\n  - **Interpretation of Results**: AI techniques demonstrated considerable efficacy in predicting new superconductors, validated against known empirical data.\n  - **Implications**: Highlighting AI's potential to radically accelerate and extend the search for high-T_c materials, far beyond traditional databases and methods.\n  - **Limitations**: The necessity of advanced hardware for computations, potential biases in AI models due to limited initial datasets, and challenges in validating high-pressure stabilizations.\n\n**8. Conclusion:**\nThe study showcases an advanced AI search engine for discovering high-T_c superconductors, achieving significant results with 74 new materials, including ideal candidates like B_4CN_3 and B_5CN_2. The proposed approach's adaptability and integration of generative models, enhanced formation energy predictions, and deep learning techniques mark substantial progress in materials science, paving the way for further explorations across various functional materials. Future research should focus on refining models, expanding datasets, and exploring other material properties for broader scientific and applied advancements.\n\nThis detailed summary includes all technical aspects necessary for replicating the study, from the methodological framework to the experimental setup, analysis, and concluding insights crucial for future research directions."}, {"role": "assistant", "content": "AI-accelerated discovery of high critical temperature superconductors\nXiao-Qi Han1,2,\u2217Zhenfeng Ouyang1,2,\u2217Peng-Jie Guo1,2, Hao Sun4, Ze-Feng Gao1,2,\u2020and Zhong-Yi Lu1,2,3\u2021\n1. Department of Physics and Beijing Key Laboratory of Opto-electronic Functional\nMaterials &Micro-nano Devices. Renmin University of China, Beijing 100872, China\n2. Key Laboratory of Quantum State Construction and Manipulation (Ministry of Education),\nRenmin University of China, Beijing 100872, China\n3. Hefei National Laboratory, Hefei 230088, China and\n4. Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\n(Dated: September 13, 2024)\nThe discovery of new superconducting materials, particularly those exhibiting high critical tem-\nperature ( Tc), has been a vibrant area of study within the field of condensed matter physics. Con-\nventional approaches primarily rely on physical intuition to search for potential superconductors\nwithin the existing databases. However, the known materials only scratch the surface of the exten-\nsive array of possibilities within the realm of materials. Here, we develop an AI search engine that\nintegrates deep model pre-training and fine-tuning techniques, diffusion models, and physics-based\napproaches (e.g., first-principles electronic structure calculation) for discovery of high- Tcsupercon-\nductors. Utilizing this AI search engine, we have obtained 74 dynamically stable materials with\ncritical temperatures predicted by the AI model to be Tc\u226515 K based on a very small set of sam-\nples. Notably, these materials are not contained in any existing dataset. Furthermore, we analyze\ntrends in our dataset and individual materials including B 4CN3and B 5CN2whose Tcs are 24.08\nK and 15.93 K, respectively. We demonstrate that AI technique can discover a set of new high-\nTcsuperconductors, outline its potential for accelerating discovery of the materials with targeted\nproperties.\nIntroduction. Superconducting materials have numer-\nous applications in modern society since it was dis-\ncovered [1], particularly in magnetic resonance imag-\ning [2], fueling advances in nuclear fusion technology [3].\nSuperconductor-based devices are demonstrating poten-\ntial for achieving scalable quantum information proces-\nsors, advanced sensors, and efficient communication sys-\ntems [4\u20136]. Many of these devices use conventional\nBardeen-Cooper-Schrieffer (BCS) superconductors [7],\nwhich demand costly helium-based cooling. Hence,\nsearching superconductors with high superconducting\ncritical temperature ( Tc) is vital for propelling techno-\nlogical progress in these dynamic areas.\nOver the past decade, substantial advancements have\nbeen achieved in searching high- Tcsuperconductors. For\nexample, a superconducting transition with Tc\u223c36\nK was experimentally observed in high-pressured Scan-\ndium, which is the highest record for elemental super-\nconductors [8]. The discovery of superconductivity in bi-\nlayer La 3Ni2O7under pressure raises superconducting Tc\nof nickelates to the liquid-nitrogen temperature zone [9].\nAnd lots of theoretical work predicted superconductiv-\nity in hydrides [10\u201315], where superconductivity in H 3S\nunder pressure was experimentally confirmed [16].\nRecently, machine learning-based methods have be-\ncome increasingly popular in searching potential high- Tc\nsuperconductors [17\u201322]. Wines et al [23]. have em-\nployed crystal diffusion variational auto-encoder (CD-\nVAE) [24] to generate data based on the JARVIS-DFT\ndatabase [25], subsequently employing the atomistic line\ngraph neural network (ALIGNN) [26] for Tcforecasting.\nUsing high-throughput density functional theory (DFT)calculations, 34 dynamically stable 2D superconductors\nwith Tc\u22655 K from over 1000 candidates in the JARVIS-\nDFT database were identified [18]. Moreover, Choud-\nhary and Garrity [17] leveraged electron-phonon coupling\n(EPC) calculations, assistanted by deep-learning models\nfor efficient prediction of superconducting properties, to\nidentify 105 conventional superconductors with Tc\u22655\nK from a pre-screened set of 1736 materials. While nu-\nmerous studies have highlighted the application of ma-\nchine learning in this field, these approaches primarily\nrely on chemical formulas or searches based on the exist-\ning datasets. They often lack the intricate atomic struc-\nture details crucial for understanding superconducting\nbehavior and are limited in exploring crystal materials\nbeyond known databases. To truly advance the discov-\nery of new superconductors, it is essential to incorporate\ndetailed structural information and broaden the scope\nbeyond existing data. So far, the conventional meth-\nods (such as elemental substitution or physical insight)\nhave limited success in finding new high- Tcsuperconduc-\ntors among the existing data. The rise of Al technology\nbrings a transformative approach, potentially reshaping\nour path to solving this challenge.\nIn this work, we developed an AI search engine to ex-\nplore high- TcBCS superconductors, integrating diffusion\nmodel, formation energy prediction model, ALIGNN,\npre-training and fine-tuning technique, atom docking\nbased on pre-trained model, active learning technique,\nand physics-based methods ( e.g., first-principles elec-\ntronic structure calculations), and meanwhile sufficiently\nincorporating detailed structural information. Lever-\naging a limited dataset of high- TcBCS superconduc-arXiv:2409.08065v1  [cond-mat.supr-con]  12 Sep 20242\n(a)\nPretrain \nModelClassification \nLayer\nFormation Energy \nPrediction ModelDFTActive Learning\nDistribution of the number of \natoms in superconductor cell\nRandom Cell (Position,\nTypes, Lattice matrix)Generation Process\nDiffusion Process\nM=(A, X, L)(b)\nSCGM\nALIGNN\nEDGNN\nFIG. 1. (a). The AI search engine workflow overview.\nAI-accelerated discovery of high- Tcsuperconductors includes\ngenerative model for predicting crystal structures, pre-trained\nmodel for superconductivity classification, formation energy\nprediction model, screening model for superconducting tran-\nsition temperature prediction, and validation using DFT cal-\nculation. (b). Symmetry-constrained crystal generation\nmodel. The generation of superconducting crystals defines\ntwo Markov processes: the black arrows represent the gradual\naddition of noise to a BCS superconducting crystal, resulting\nin a random unit cell, while the red arrows indicate the grad-\nual denoising from a prior atomic distribution to generate the\noriginal superconducting crystal structure.\ntors (105 superconductors with Tc\u22655 K [17]), we have\nobtained 74 dynamically stable materials exhibiting crit-\nical temperatures predicted by the AI model to be Tc\u2265\n15 K. Furthermore, we analyze trends in our results, fo-\ncusing on specific materials such as B 4CN3and B 5CN2,\nwith Tcof 24.08 K and 15.93 K, respectively. Our AI\nsearch engine stands out for its unique capability to ob-\ntain crystal structures absent from the existing material\ndatabases, effectively pioneering new avenues in the quest\nfor high- Tcsuperconductors. Its adaptability allows itself\nto be tailored for a diverse array of functional materials,\neach with specific desired properties, thereby greatly ex-\npanding its utility across the field of materials science.\nOverview. Our AI search engine utilizes multiple AI\nmethods and DFT calculations (Fig 1(a)) for generat-\ning and screening high- Tcsuperconductors. Specifically,\ninspired by the DiffCSP model [27], we constructed a\nsymmetry-constrained superconducting crystal genera-\ntion model, based on diffusion generative models [28] andequivalent graph neural networks [29]. This model gen-\nerates new superconducting structures. We also built a\nsuperconducting classification model using pre-training\ntechniques [30], graph auto-encoder architectures [31],\nand optimal transport theory [32] to determine whether\nor not the generated crystals exhibit superconducting\nproperties. To further assess the stability of the mate-\nrials, we retrained a formation energy prediction model\nbased on the MEGNET architecture [33] with improve-\nments. Next, we used ALIGNN [26] to predict the su-\nperconducting transition temperatures of these materials\nand screened for high- Tcsuperconducting candidates. Fi-\nnally, for the top-5 candidates, We validated our predic-\ntions with convergence tests and verified superconducting\ntransition temperatures using DFT calculation. Adopt-\ning active learning, we incorporated the discovered su-\nperconductors into training set.\nSymmetry-constrained crystal generation model. In\ncrystal structures (Fig 1(b)), the atoms exhibit a peri-\nodic distribution, with the smallest repeating unit be-\ning the unit cell, denoted by M, which can be repre-\nsented as M= (A,X,L). Here, A= [a1,a2, ...,aN]\u2208\nRh\u00d7Ndenotes the atomic types within the unit cell,\nX= [x1,x2, ...,xN]\u2208R3\u00d7Nrepresents the Cartesian\ncoordinates of each atom, and L= [l1,l2,l3]\u2208R3\u00d73is\nthe lattice matrix used to describe the periodicity of the\ncrystal. We employed an ab initio crystal generation ap-\nproach to generate superconducting crystal structures.\nSpecifically, this involves generating a superconducting\ncrystal Mfrom a given number of atoms Nwithin the\nunit cell, with a sampling distribution defined as:\np(M, N) =p(N)p(M|N), (1)\nwhere Nremains unchanged during the generation pro-\ncess. The distribution p(N) is calculated from the\ntraining set, while p(M|N) is generated based on\nthe model. Standard denoising diffusion probabilistic\nmodel (DDPM) [28] can be used to generate LandA,\nand their loss functions take the same form as:\nLL/A=E\u03f5\u223cN(0,I)[\u2225\u03f5\u2212\u02c6\u03f5L/A(Mt, t)\u22252\n2]. (2)\nThe denoising terms \u02c6\u03f5L(Mt, t) and \u02c6\u03f5A(Mt, t) are pre-\ndicted by an equivalent denoising graph neural networks\n(short as EDGNN Fig 1(b)), and N(0,I) represents a\nstandard normal distribution. Given the periodicity of\nX, it is generated using a score-matching based frame-\nwork [34]. Details are in the supplementary material\n(SM). Utilizing 105 BCS superconductors [17], we trained\nthe model to generate novel crystal structures, exclud-\ning those in the training set and with overlapping com-\npositions in the Materials Project (MP) database [35].\nSince generative models often produce non-ground-state\nstructures, we performed geometry optimization using\nthe neural networks atomic simulation environment [36]\nand L-BFGS algorithm [37] to refine the generated struc-\ntures.3\nData Size\nGNoME   380,000MP 60,000\nR=8\u00c5Node Edge State\nl\nije\nl\niv\nlu\n1l\nije+\n1l\niv+\n1lu+\n\u00d7N\nConcat  & MLP\nFormation Energy(c)\n(d)(e)\nPre-trained Model\nLatent\nFeature\nCassification  \nLayerMaterials ProjectInsulator & \nmagnetic materialsConductors &\nSemiconductorsHigh-throughput \nscreening +(a)\n(b)\n105 BCS \nSuperconductorsInitial\nGNN\nFIG. 2. Superconducting classification model: (a) High-\nthroughput screening process for pre-training and fine-tuning\ndata. (b) Graph auto-encoder architecture based on a graph\nneural network. Formation energy prediction model: (c)\nTraining data sources. (d) Crystal data representation us-\ning atomic graphs with an 8 \u02daA cutoff radius. (e) Interaction\nof node, edge, and global state representations in the model\u2019s\narchitecture.\nSuperconducting classification model. Initially, we\nextract 144,595 crystal data entries from the MP\ndatabase [35]. We first classified the materials into two\ngroups: magnetic and non-magnetic. Subsequently, we\nrefined the non-magnetic category into conductors, semi-\nconductors, and insulators. Then, we designated insu-\nlators and magnetic materials as negative samples, and\nconductors and semiconductors as positive samples, as il-\nlustrated in Fig 2 a. The model is based on a pre-trained\ngraph neural network (GNN) that utilizes material crys-\ntal structure information to predict materials [31, 38],\nconsists of a graph convolutional network encoder and\na decoder that reconstructs the graph features based on\noptimal transport theory (see Fig 2 band SM). To ob-\ntain hidden layer representations related to supercon-\nductivity, we pre-trained the model using the positive\nsamples. During the fine-tuning stage, we employed the\npre-trained encoder and used up-sampling techniques to\nbalance the number of the BCS superconductors and\nnegative samples for binary classification model. Sub-\nsequently, we obtained the classifier model that achieved\na discrimination success rate of 99 .04% for the 105 BCSsuperconductors. Utilizing this model , we evaluated the\ncandidate structures generated by the generative model.\nFormation energy prediction model. To further as-\nsess the stability of potential superconductors, we pre-\ndict the formation energy of crystals as an indicator of\ntheir stability. The AI algorithms like CGCNN [31] and\nSchNet [39], while fast, lack the required precision for for-\nmation energy predictions. Inspired by MEGNET [33],\nwe trained the model using 380,000 crystal structures\nfrom GNoME [40] and 60,000 crystal structures [35].\nNext, we increased the cutoff radius for constructing\natomic graphs from 5 \u02daA to 8 \u02daA, enabling the model to\ncapture more long-range atomic interactions to more ac-\ncurately simulate atomic interactions. Recognizing the\ndirect correlation between crystal formation energy and\natomic bonding strength, we have incorporated eight new\natomic features into our prediction model. This enhance-\nment offers a more comprehensive representation of crys-\ntal data, as elaborated in the SM. The original MEG-\nNET benchmark reported a mean absolute error (MAE)\nof 28 meV per atom, while our improved model achieved\nan MAE of 21 meV per atom. Since we are particu-\nlarly interested in high- Tcsuperconducting materials, we\nused ALIGNN [26] to predict the superconducting tran-\nsition temperatures of these materials and applied a 15\nK threshold, resulting in top-5 candidate high- Tcsuper-\nconductors.\nPredicted high- Tcmaterials. By performing the DFT\ncalculations, we studied the electronic structures, phonon\nproperties, and EPC of B 5CN2and B 4CN3(See SM for\ncrystal structures and additional results.). In Fig. 3, we\nshow the band structure of B 5CN2and B 4CN3(5 GPa).\nThe results of DFT calculations and Wannier projec-\ntion show good consistence and suggest that B 5CN2and\nB4CN3(5 GPa) are metallic. The atomic-orbital resolved\ndensity of states (DOS) shows that the 2 porbitals of B,\nC, and N atoms mainly contribute the Fermi surfaces.\nNext, we investigate the dynamical stability of B 5CN2\nand B 4CN3. At ambient pressure, we find that B 5CN2\nis dynamically stable, while B 4CN3shows a maximum\nimaginary-frequency phonon of \u223c \u22127.7 meV along the\nR-Zpath. By applying pressure of 5 GPa, the imag-\ninary phonons of B 4CN3disappear. Hence, we show\nthe phonon spectrum of B 5CN2and B 4CN3(5 GPa) in\nFig. 4(a) and (c) and further study the EPC of these two\nmaterials. The calculated Eliashberg spectral function\n\u03b12F(\u03c9) and accumulated EPC constant \u03bb(\u03c9) are exhib-\nited in Fig. 4(b) and (d). And the mode-resolved \u03bbq\u03bdis\nadded in the phonon spectrum. The EPC constants \u03bb\nof B 5CN2and B 4CN3(5 GPa) are integrated to be 0.61\nand 0.72, respectively. Using the McMillan-Allen-Dynes\nformula [41, 42]\nTc=\u03c9log\n1.2exp[\u22121.04(1 + \u03bb)\n\u03bb(1\u22120.62\u00b5\u2217)\u2212\u00b5\u2217], (3)\nthe superconducting Tcof B 5CN2and B 4CN3(5 GPa) is4\n(c)(a)-\n3-2-10123Energy (eV)TotalD\nOSB-pC\n-pN\n-p(b)-\n3-2-10123Energy (eV)(d)D\nOS/s915XM/s915 ZAHZ /s915\nYMX/s915 ZVTRZ\nFIG. 3. (a)-(b) Electronic structure and DOS of B 5CN2at\nambient pressure. (c)-(d) Electronic structure and DOS of\nB4CN3under 5 GPa. The blue soild lines and red circles rep-\nresent the bands obtained by DFT and Wannier projection,\nrespectively. The Fermi level is set to be zero.\nrespectively estimated to be 15.93 K and 24.08 K when\nthe Coulomb pseudopotential \u00b5\u2217is set to be 0.1.\nDiscussion. Recently, several studies have utilized gen-\nerative models to explore high- Tcsuperconductors [17,\n18, 20, 23]. Wines et al. [23] employed CDVAE to gener-\nate data on the JARVES-DFT dataset [25], subsequently\nemploying the ALIGNN [26] for Tcforecasting. Com-\npared to existing methods, our proposed AI search en-\ngine has seen improvements in three aspects. Firstly, our\nmethod is capable of effective generation based on a few\npositive samples ( i.e., 105 samples with Tc\u22655 K). Unlike\nCDVAE [24], which randomly generates chemical formu-\nlas before predicting structures, our approach directly\ngenerates structures. By directly generating structural\nconfigurations, our method adeptly navigates the spa-\ntial intricacies of superconductors, facilitating the gene-\nsis of plausible chemical entities. Secondly, we have inte-\ngrated a sophisticated post-processing phase employing\nthe DPA-2 model [43] for atom docking. This step metic-\nulously circumvents atomic clashes, refines bond lengths\nto more rational values, and guarantees the equilibrium\nof forces exerted on each atomic constituent. Existing\nmethods often predict superconducting transition tem-\nperatures without aforehand confirming the materials\u2019\nsuperconductivity, which is problematic. We address this\nby introducing a superconducting classification model.\nWe improved the formation energy prediction model un-\nder GNoME [40], increasing its precision from 28 meV\n04080120/s969/s32/s40/s109/s101/s86/s410\n.00 .8/s955qv0\n.00 .8/s955qv(a)/s915\nXM/s915 ZAHZ /s915\nY0.00.4/s945\n2/s70/s40/s969/s41/s44/s32/s955/s40/s969/s41/s9452/s70/s40/s969/s41/s955\n/s40/s969/s41(b)0\n4080120160/s969/s32/s40/s109/s101/s86/s41(c)M\nX/s915 ZVTRZ0.00.40.8/s945\n2/s70/s40/s969/s41/s44/s32/s955/s40/s969/s41(d)FIG. 4. (a)-(b) Phonon spectrum with a color representation\nof\u03bbq\u03bd, Eliashberg spectral function \u03b12F(\u03c9), and accumulated\nEPC constant \u03bb(\u03c9) for B 5CN2at ambient pressure under 5\nGPa. (c)-(d) Phonon spectrum with a color representation of\n\u03bbq\u03bd, Eliashberg spectral function \u03b12F(\u03c9), and accumulated\nEPC constant \u03bb(\u03c9) for B 4CN3at ambient pressure under 5\nGPa. The scale of \u03b12F(\u03c9) is omitted.\nto 21 meV. Lastly, active learning progressively expands\nthe chemical space of high- Tcsuperconducting materials\nin iterative reinforcement generative learning. These re-\nfinements enhance our method\u2019s effectiveness and estab-\nlish a new standard for exploring and predicting high- Tc\nsuperconductors.\nIn conclusion, our proposed AI search engine integrates\na suite of advanced methodologies, including generative\nmodel, formation energy prediction model, pre-training\nand fine-tuning strategy, ALIGNN, and first-principles\nelectronic structure calculations. This AI search engine\nhas not only predicted 74 superconducting material can-\ndidates with Tc>15 K based on a modest set of positive\nsamples (105 samples with Tc\u22655 K), but also identi-\nfied two ideal high- Tccandidates: B 5CN2(Tc=15.93 K)\nand B 4CN3(Tc=24.08 K). Notably, this engine is capa-\nble of discovering crystal structures that are not yet doc-\numented in existing material dataset, thereby opening\nup new horizons in the search for high- Tcsuperconduc-\ntors. Moreover, the AI search engine\u2019s versatility allows\nit to be adapted for exploring a wide range of functional\nmaterials with various target properties, significantly ex-\npanding its potential applications in materials science.5\nACKNOWLEDGMENTS\nThis work was financially supported by the Na-\ntional Natural Science Foundation of China (Grant\nNo.62476278, No.62206299, No.11934020, and\nNo.12204533). Computational resources have been\nprovided by the Physical Laboratory of High Perfor-\nmance Computing at Renmin University of China.\n\u2217These authors contributed equally to this work.\n\u2020zfgao@ruc.edu.cn\n\u2021zlu@ruc.edu.cn\n[1] H. K. Onnes, The resistance of pure mercury at helium\ntemperatures, Commun. Phys. Lab. Univ. Leiden, b 120\n(1911).\n[2] Y. Lvovsky, E. W. Stautner, and T. Zhang, Novel tech-\nnologies and configurations of superconducting magnets\nfor mri, Supercond. Sci. Technol. 26, 093001 (2013).\n[3] P. Bruzzone, W. H. Fietz, J. V. Minervini, M. Novikov,\nN. Yanagi, Y. Zhai, and J. Zheng, High temperature\nsuperconductors for fusion magnets, Nucl. Fusion 58,\n103001 (2018).\n[4] M. Mirhosseini, A. Sipahigil, M. Kalaee, and O. Painter,\nSuperconducting qubit to optical photon transduction,\nNature 588, 599 (2020).\n[5] J. M. Gambetta, J. M. Chow, and M. Steffen, Building\nlogical qubits in a superconducting quantum computing\nsystem, NPJ Quantum Inf. 3, 2 (2017).\n[6] C. L. Degen, F. Reinhard, and P. Cappellaro, Quantum\nsensing, Rev. Mod. Phys. 89, 035002 (2017).\n[7] J. Bardeen, L. N. Cooper, and J. R. Schrieffer, Theory of\nsuperconductivity, Phys. Rev. 108, 1175 (1957).\n[8] J. Ying, S. Liu, Q. Lu, X. Wen, Z. Gui, Y. Zhang,\nX. Wang, J. Sun, and X. Chen, Record high 36 k transi-\ntion temperature to the superconducting state of elemen-\ntal scandium at a pressure of 260 gpa, Phys. Rev. Lett.\n130, 256002 (2023).\n[9] H. Sun, M. Huo, X. Hu, J. Li, Z. Liu, Y. Han, L. Tang,\nZ. Mao, P. Yang, B. Wang, J. Cheng, D.-X. Yao, G.-M.\nZhang, and M. Wang, Signatures of superconductivity\nnear 80 K in a nickelate under high pressure, Nature 621,\n493 (2023).\n[10] D. Duan, Y. Liu, F. Tian, D. Li, X. Huang, Z. Zhao,\nH. Yu, B. Liu, W. Tian, and T. Cui, Pressure-induced\nmetallization of dense (H 2S)2H2with high-T csupercon-\nductivity, Sci. Rep. 4, 6968 (2014).\n[11] H. Liu, I. I. Naumov, R. Hoffmann, N. W. Ashcroft, and\nR. J. Hemley, Potential high-T csuperconducting lan-\nthanum and yttrium hydrides at high pressure, Proc.\nNatl. Acad. Sci. U.S.A 114, 6990 (2017).\n[12] F. Peng, Y. Sun, C. J. Pickard, R. J. Needs, Q. Wu,\nand Y. Ma, Hydrogen clathrate structures in rare earth\nhydrides at high pressures: Possible route to room-\ntemperature superconductivity, Phys. Rev. Lett. 119,\n107001 (2017).\n[13] M. Gao, X.-W. Yan, Z.-Y. Lu, and T. Xiang, Phonon-\nmediated high-temperature superconductivity in the\nternary borohydride kb 2h8under pressure near 12 gpa,\nPhys. Rev. B 104, L100504 (2021).[14] A. Drozdov, P. Kong, V. Minkov, S. Besedin, M. Ku-\nzovnikov, S. Mozaffari, L. Balicas, F. Balakirev, D. Graf,\nV. Prakapenka, et al. , Superconductivity at 250 k in lan-\nthanum hydride under high pressures, Nature 569, 528\n(2019).\n[15] A. M. Shipley, M. J. Hutcheon, R. J. Needs, and C. J.\nPickard, High-throughput discovery of high-temperature\nconventional superconductors, Phys. Rev. B 104, 054501\n(2021).\n[16] A. Drozdov, M. Eremets, I. Troyan, V. Ksenofontov, and\nS. I. Shylin, Conventional superconductivity at 203 kelvin\nat high pressures in the sulfur hydride system, Nature\n525, 73 (2015).\n[17] K. Choudhary and K. Garrity, Designing high-tc su-\nperconductors with bcs-inspired screening, density func-\ntional theory, and deep-learning, NPJ Comput. Mater.\n8, 244 (2022).\n[18] D. Wines, K. Choudhary, A. J. Biacchi, K. F. Garrity,\nand F. Tavazza, High-throughput dft-based discovery of\nnext generation two-dimensional (2d) superconductors,\nNano Lett. 23, 969 (2023).\n[19] V. Stanev, C. Oses, A. G. Kusne, E. Rodriguez,\nJ. Paglione, S. Curtarolo, and I. Takeuchi, Machine learn-\ning modeling of superconducting critical temperature,\nNPJ Comput. Mater. 4, 29 (2018).\n[20] J. Zhang, Z. Zhu, X.-D. Xiang, K. Zhang, S. Huang,\nC. Zhong, H.-J. Qiu, K. Hu, and X. Lin, Machine learn-\ning prediction of superconducting critical temperature\nthrough the structural descriptor, J. Phys. Chem. C 126,\n8922 (2022).\n[21] D. Menon and R. Ranganathan, A generative approach\nto materials discovery, design, and optimization, ACS\nOmega 7, 25958 (2022).\n[22] B. Roter and S. Dordevic, Predicting new superconduc-\ntors and their critical temperatures using machine learn-\ning, Physica C Supercond 575, 1353689 (2020).\n[23] D. Wines, T. Xie, and K. Choudhary, Inverse design of\nnext-generation superconductors using data-driven deep\ngenerative models, J. Phys. Chem. Lett. 14, 6630 (2023).\n[24] T. Xie, X. Fu, O. Ganea, R. Barzilay, and T. S. Jaakkola,\nCrystal diffusion variational autoencoder for periodic ma-\nterial generation, arXiv 2110.06197 (2021).\n[25] K. Choudhary, K. F. Garrity, A. C. Reid, B. DeCost,\nA. J. Biacchi, A. R. Hight Walker, Z. Trautt, J. Hattrick-\nSimpers, A. G. Kusne, A. Centrone, et al. , The joint\nautomated repository for various integrated simulations\n(jarvis) for data-driven materials design, NPJ Comput.\nMater. 6, 173 (2020).\n[26] K. Choudhary and B. DeCost, Atomistic line graph neu-\nral network for improved materials property predictions,\nNPJ Comput. Mater. 7, 185 (2021).\n[27] R. Jiao, W. Huang, P. Lin, J. Han, P. Chen, Y. Lu, and\nY. Liu, Crystal structure prediction by joint equivari-\nant diffusion, Advances in Neural Information Processing\nSystems 36(2024).\n[28] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion prob-\nabilistic models, NeurIPS 33, 6840 (2020).\n[29] V. G. Satorras, E. Hoogeboom, and M. Welling, E (n)\nequivariant graph neural networks, ICML 139, 9323\n(2021).\n[30] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, ACL 1, 4171 (2019).6\n[31] T. Xie and J. C. Grossman, Crystal graph convolutional\nneural networks for an accurate and interpretable predic-\ntion of material properties, Phys. Rev. Lett. 120, 145301\n(2018).\n[32] L. R\u00a8 uschendorf, The wasserstein distance and approxi-\nmation theorems, Probab Theory Relat Fields 70, 117\n(1985).\n[33] C. Chen, W. Ye, Y. Zuo, C. Zheng, and S. P. Ong, Graph\nnetworks as a universal machine learning framework for\nmolecules and crystals, Chem. Mater 31, 3564 (2019).\n[34] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar,\nS. Ermon, and B. Poole, Score-based generative modeling\nthrough stochastic differential equations, ICLR (2021).\n[35] A. Jain, J. Montoya, Dwaraknath, et al. , The materials\nproject: Accelerating materials design through theory-\ndriven data and tools, Handbook of Materials Modeling\n, 1751 (2020).\n[36] Y. Yang, O. A. Jim\u00b4 enez-Negr\u00b4 on, and J. R. Kitchin,\nMachine-learning accelerated geometry optimization in\nmolecular simulation, J. Chem. Phys 154, 234704 (2021).\n[37] D. C. Liu and J. Nocedal, On the limited memory bfgs\nmethod for large scale optimization, Math Program 45,\n503 (1989).\n[38] Z.-F. Gao, S. Qu, B. Zeng, Y. Liu, J.-R. Wen, H. Sun,\nP.-J. Guo, and Z.-Y. Lu, Ai-accelerated discovery of al-\ntermagnetic materials, arXiv 2311.04418 (2024).\n[39] K. Sch\u00a8 utt, P.-J. Kindermans, H. E. Sauceda Felix,\nS. Chmiela, A. Tkatchenko, and K.-R. M\u00a8 uller, Schnet: A\ncontinuous-filter convolutional neural network for mod-\neling quantum interactions, NeurIPS 30, 991 (2017).\n[40] A. Merchant, S. Batzner, S. S. Schoenholz, M. Aykol,\nG. Cheon, and E. D. Cubuk, Scaling deep learning for\nmaterials discovery, Nature 624, 80 (2023).\n[41] P. B. Allen, Neutron spectroscopy of superconductors,\nPhys. Rev. B 6, 2577 (1972).\n[42] P. B. Allen and R. C. Dynes, Transition temperature of\nstrong-coupled superconductors reanalyzed, Phys. Rev.\nB12, 905 (1975).\n[43] D. Zhang, X. Liu, X. Zhang, C. Zhang, C. Cai, H. Bi,\nY. Du, X. Qin, J. Huang, B. Li, et al. , Dpa-2: Towards a\nuniversal large atomic model for molecular and material\nsimulation, arXiv preprint arXiv:2312.15492 (2023).\n[44] P. Giannozzi, S. Baroni, N. Bonini, M. Calandra, R. Car,\nC. Cavazzoni, D. Ceresoli, G. L. Chiarotti, M. Cococ-\ncioni, I. Dabo, A. D. Corso, S. de Gironcoli, S. Fabris,\nG. Fratesi, R. Gebauer, U. Gerstmann, C. Gougoussis,\nA. Kokalj, M. Lazzeri, L. Martin-Samos, N. Marzari,\nF. Mauri, R. Mazzarello, S. Paolini, A. Pasquarello,\nL. Paulatto, C. Sbraccia, S. Scandolo, G. Sclauzero, A. P.\nSeitsonen, A. Smogunov, P. Umari, and R. M. Wentzcov-\nitch, Quantum espresso: a modular and open-source soft-\nware project for quantum simulations of materials, Jour-\nnal of Physics: Condensed Matter 21, 395502 (2009).\n[45] J. P. Perdew, K. Burke, and M. Ernzerhof, Generalized\ngradient approximation made simple, Phys. Rev. Lett.\n77, 3865 (1996).\n[46] V. D. Bortoli, E. Mathieu, M. J. Hutchinson, J. Thorn-\nton, Y. W. Teh, and A. Doucet, Riemannian score-based\ngenerative modelling, in Advances in Neural Information\nProcessing Systems , edited by A. H. Oh, A. Agarwal,\nD. Belgrave, and K. Cho (2022).\n[47] K. Choudhary and B. DeCost, Atomistic line graph\nneural network for improved materials property predic-\ntions, npj Computational Materials 7, 10.1038/s41524-021-00650-1 (2021).\n[48] T. Xie and J. C. Grossman, Crystal graph convolutional\nneural networks for an accurate and interpretable predic-\ntion of material properties, Phys. Rev. Lett. 120, 145301\n(2018).\n[49] M. Tang, P. Li, and C. Yang, Graph auto-encoder\nvia neighborhood wasserstein reconstruction, in ICLR\n(2022).\n[50] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar,\nS. Ermon, and B. Poole, Score-based generative modeling\nthrough stochastic differential equations, arXiv preprint\narXiv:2011.13456 (2020).\n[51] D. R. Hamann, Optimized norm-conserving vanderbilt\npseudopotentials, Phys. Rev. B 88, 085117 (2013).\n[52] M. Methfessel and A. T. Paxton, High-precision sampling\nfor brillouin-zone integration in metals, Phys. Rev. B 40,\n3616 (1989).\n[53] S. Baroni, S. de Gironcoli, A. Dal Corso, and P. Gi-\nannozzi, Phonons and related crystal properties from\ndensity-functional perturbation theory, Rev. Mod. Phys.\n73, 515 (2001).\n[54] G. Pizzi, V. Vitale, R. Arita, S. Bl\u00a8 ugel, F. Freimuth,\nG. G\u00b4 eranton, M. Gibertini, D. Gresch, C. Johnson,\nT. Koretsune, J. Iba\u02dc nez-Azpiroz, H. Lee, J.-M. Lihm,\nD. Marchand, A. Marrazzo, Y. Mokrousov, J. I. Mustafa,\nY. Nohara, Y. Nomura, L. Paulatto, S. Ponc\u00b4 e, T. Pon-\nweiser, J. Qiao, F. Th\u00a8 ole, S. S. Tsirkin, M. Wierzbowska,\nN. Marzari, D. Vanderbilt, I. Souza, A. A. Mostofi, and\nJ. R. Yates, Wannier90 as a community code: new fea-\ntures and applications, Journal of Physics: Condensed\nMatter 32, 165902 (2020).\n[55] S. Ponc\u00b4 e, E. Margine, C. Verdi, and F. Giustino,\nEpw: Electron\u2013phonon coupling, transport and super-\nconducting properties using maximally localized wannier\nfunctions, Computer Physics Communications 209, 116\n(2016).7\nCALCULATION METHOD\nThe first-principle calculations were performed by\nQUANTUM-ESPRESSO package [44]. The generalized\ngradient approximation of Perdew-Burke-Ernzerhof for-\nmula [45] and the optimized norm-conserving Vander-\nbilt pseudopotentials [51] were used. We set the kinetic\nenergy and the charge density cutoff to be 80 Ry and\n320 Ry, respectively. An unshifted 12 \u00d712\u00d76k-points\ngrid was used in the self-consistent calculations. The\nMethfessel-Paxton smearing method [52] with a width of\n0.02 Ry was set for the Fermi surface broadening. The\ndynamical matrices and the perturbation potentials were\ncalculated on a \u0393-centered 4 \u00d74\u00d72q-points grid based on\nthe density functional perturbation theory [53].\nThe maximally localized Wannier functions (ML-\nWFs) [54] were constructed on a 4 \u00d74\u00d72k-points grid of\nthe Brillouin zone. The MLWFs of B 5CN2and B 4CN3\ninclude the 2 porbitals of B, C, and N atoms. We used\nEPW package [55] to perform electron-phonon coupling\n(EPC) calculations. The EPC constant \u03bbwas deter-\nmined through fine electron (72 \u00d772\u00d736) and phonon\n(24\u00d724\u00d712) grids. And the smearing widths of electron\nand phonon Dirac \u03b4functions are 0.1 eV and 0.5 eV,\nrespectively.\nThe mode and wavevecter-dependent coupling \u03bbq\u03bd\nreads:\n\u03bbq\u03bd=2\n\u210fN(0)NkX\nnmk1\n\u03c9q\u03bd|gnm\nk,q\u03bd|2\u03b4(\u03f5n\nk)\u03b4(\u03f5m\nk+q).(4)\nN(0) is the density of states (DOS) of electrons at the\nFermi level. Nkis the total number of kpoints in the\nfinek-mesh. \u03c9q\u03bdis the phonon frequency and gnm\nk,q\u03bdis the\nEPC matrix element. ( n,m) and \u03bddenote the indices\nof energy bands and phonon mode, respectively. \u03f5n\nkand\n\u03f5m\nk+qare the eigenvalues of the Kohn-Sham orbitals with\nrespect to the Fermi level.\nThe EPC constant \u03bbwas determined by the summa-\ntion of \u03bbq\u03bdover the first Brillouin zone, or the integration\nof the Eliahberg spectral function \u03b12F(\u03c9) [41, 42],\n\u03bb=1\nNqX\nq\u03bd\u03bbq\u03bd= 2Z\u03b12F(\u03c9)\n\u03c9d\u03c9, (5)\nwhere Nqrepresents the total number of qpoints in the\nfineqmesh. The Eliashberg spectral function \u03b12F(\u03c9)\nwas calculated with:\n\u03b12F(\u03c9) =1\n2NqX\nq\u03bd\u03bbq\u03bd\u03c9q\u03bd\u03b4(\u03c9\u2212\u03c9q\u03bd). (6)\nAnd\u03c9logis the logarithmic average frequency that is de-\nfined as\n\u03c9log= exp\u00142\n\u03bbZd\u03c9\n\u03c9\u03b12F(\u03c9)ln(\u03c9)\u0015\n. (7)CRYSTAL STRUCTURE\nThe optimized crystal structures are shown in Fig. 5.\nThe crystal parameters of B 5CN2at ambient pressure\narea= 2.49497 \u02daA,b= 2.52950 \u02daA,c= 8.35250 \u02daA,\u03b1=\n90\u25e6,\u03b2= 90\u25e6, and \u03b3= 90\u25e6, respectively. As for B 4CN3\nunder 5 GPa, crystal parameters are a= 2.52034 \u02daA,b=\n2.59156 \u02daA,c= 8.13945 \u02daA,\u03b1= 90\u25e6,\u03b2= 98 .9\u25e6, and\n\u03b3= 90\u25e6, respectively.\nFIG. 5. Crystal structures. (a) B 5CN2at ambient pressure.\n(b) B 4CN3under 5 GPa. The black lines represent the unit\ncell. And the green, brown, and gray balls denote B, C, and\nN atoms, respectively.\nFERMI SURFACES\nFIG. 6. (a) Brillouin zone and high-symmetry path of B 5CN2\nat ambient pressure. (c)-(d) Distributions of different orbitals\non the kz= 0 Fermi surfaces. (e)-(h) Brillouin zone and Fermi\nsurfaces of B 4CN3under 5 GPa.8\nCANDIDATE SUPERCONDUCTORS.\nThe table I- II present the candidate superconduct-\ning formulas and their corresponding predicted supercon-\nducting transition stability values, obtained from our AI\nsearch engine.\nTABLE I. Candidate superconductors.\nNumber Materials ALIGNN Prediction Tc/K\n1 B4C3N 18.3\n2 B4CN3 18.6\n3 B5CN2 17.9\n4 ReN 9 18.9\n5 Ti(ReN 2)3 15.7\n6 TiN 9 18.6\n7 TiReN 8 18.3\n8 ZrBN 2 15.6\n9 B3C2N3 19.5\n10 B3C2N4 19.6\n11 B3C3N3 19.2\n12 B3CN4 18.4\n13 B3CN5 18.2\n14 B3N5 17.8\n15 B3N6 17.9\n16 B4C2N3 18.7\n17 B4CN2 17.9\n18 B4CN4 18.2\n19 B4N5 17.7\n20 B5C2N2 18.2\n21 B5N3 16.9\n22 B5N4 17."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract\n\nThe paper introduces a novel approach for offline reinforcement learning called the Q-value Regularized Decision ConvFormer (QDC), which integrates the Decision ConvFormer (DC) model with a Q-value regularization term to achieve optimal returns. QDC addresses the problem of stitching suboptimal trajectories into optimal ones by leveraging dynamic programming methods during training. The approach is validated on the D4RL benchmark, showing superior performance across various environments, including OpenAI Gym and Maze2D tasks.\n\n### 2. Introduction\n\nThe paper starts by discussing the limitations of classic reinforcement learning, particularly in environments where data collection is costly or dangerous. Traditional methods require online interaction, making them impractical in many real-world settings. Offline reinforcement learning (Offline RL), which learns from pre-collected datasets, is presented as a practical alternative. However, existing methods struggle with \"trajectory stitching,\" the process of combining parts of different, suboptimal trajectories into optimal ones.\n\nTo address this, the paper proposes the Q-value Regularized Decision ConvFormer (QDC). This model builds upon the Decision Transformer (DT) and Decision ConvFormer (DC) paradigms, which have previously shown promise in sequence modeling for RL tasks. QDC enhances these models by incorporating a Q-value regularization term during training, ensuring that the expected returns of sampled actions are consistent with optimal returns. The paper aims to demonstrate that QDC can improve performance in offline RL tasks, especially in environments requiring effective trajectory stitching.\n\n### 3. Related Work\n\nThe paper reviews several relevant works:\n\n- **Classical RL and Offline RL:** Traditional RL involves value function approximation or policy gradients, while offline RL models leverage supervised learning techniques.\n- **Decision Transformer (DT) and Derivatives:** DT transforms offline RL into a sequence modeling problem but struggles with trajectory stitching. Variants like QDT and EDT have improved trajectory modeling using dynamic programming and extended history contexts.\n- **Decision ConvFormer (DC):** DC employs convolutional filters for better modeling local dependencies in Markov Decision Processes (MDPs), yet it still encounters limitations in stitching capabilities.\n\nQDC is positioned as a model combining the strengths of DC with the trajectory stitching enhancements from Q-value regularization, aiming to outperform existing methods in offline RL.\n\n### 4. Method\n\n#### Model Architecture\n\nThe architecture of QDC consists of:\n\n- **Input Method:** The model takes multiple states \\( s_t \\), actions \\( a_t \\), and return-to-go (RTG) \\( R_t \\) values as inputs. These are embedded into sequences, which are then processed by the Decision ConvFormer (DC) model.\n- **Convolution Module:** The input sequence \\( \\tau_t \\) is fed through a series of convolutional blocks. Each block contains a layer normalization, a convolution module for token mixing, and a feedforward network.\n  \n  Mathematically:\n  - **First sub-block:**\n    \\[\n    Z^{1st sub-block}_t = \\text{Conv}(\\text{LN}(\\tau_t)) + \\tau_t\n    \\]\n  - **Second sub-block:**\n    \\[\n    Z^{2nd sub-block}_t = \\text{FFN}(\\text{LN}(Z^{1st sub-block}_t)) + Z^{1st sub-block}_t\n    \\]\n\nEach RTG, state, and action at time step \\( t \\) is embedded individually, producing an embedded sequence that passes through convolutional blocks to output the predicted action.\n\n#### Q-value Regularization\n\nThe core innovation of QDC lies in incorporating Q-value regularization by:\n\n- **Q-value Function Minimization:** Training involves minimizing the Bellman operator\u2019s target network with double Q-learning:\n  \\[\n  \\mathcal{L}_{Q} = \\mathbb{E}_{\\tau_t \\sim D, \\hat{a}_t \\sim \\pi_{\\theta'}} \\sum_{m=t-K+1}^{t} \\left\\| \\hat{Q}_m - Q_{\\phi_i}(s_m, a_m) \\right\\|^2\n  \\]\n  Where:\n  \\[\n  \\hat{Q}_m = \\sum_{j=m}^{t-1} \\gamma^{j-m} r_j + \\gamma^{t-m} \\min_{i=1,2} Q_{\\phi'_i}(s_t, \\hat{a}_t)\n  \\]\n\n- **Policy Learning Objective:** The learned Q-value function influences the policy learning objective as a linear combination of policy regularization and policy improvement terms:\n  \\[\n  \\pi = \\arg\\min_{\\pi_\\theta} \\left\\{ \\mathcal{L}_{DC}(\\theta) - \\alpha \\cdot \\mathbb{E}_{\\tau_t \\sim D, (s_i, a_i) \\sim \\tau_t} Q_{\\phi}(s_i, \\pi(\\tau_t)_i) \\right\\}\n  \\]\n  \n  Here, \\( \\alpha \\) is a normalization factor adjusting for Q-value scale variations:\n  \\[\n  \\alpha = \\eta \\mathbb{E}_{\\tau_t \\sim D, (s,a) \\sim \\tau_t} \\left\\| Q_{\\phi}(s,a) \\right\\|\n  \\]\n\n### 5. Experimental Setup\n\n#### Datasets\n\n- **D4RL Benchmark:** Includes Gym, Maze2D, and Adroit datasets. These datasets encompass varied RL tasks from locomotion and navigation to manipulation.\n\n#### Hardware and Software\n\n- **Hardware:** The experiments were executed on GPUs (specific models and configurations were not detailed in the paper).\n- **Software:** The implementation involved standard RL libraries, with custom modifications for QDC.\n\n#### Evaluation Metrics\n\n- **Normalized Scores:** Performance was measured using normalized scores compared to expert policies, following the protocol by Fu et al.\n\n#### Baselines\n\n- **Behavior Cloning (BC):** A simple supervised learning method.\n- **Conservative Q-Learning (CQL), Implicit Q-Learning (IQL):** Value-based offline RL methods.\n- **Decision Transformer (DT), Q-learning Decision Transformer (QDT), Reinformer, Decision ConvFormer (DC), Q-value Regularized Transformer (QT):** Sequence modeling-based RL methods.\n\n### 6. Results and Analysis\n\n#### Main Results\n\n- **Gym Tasks:** QDC outperformed or matched the performance of existing methods, especially exceeding traditional offline RL methods and achieving optimal performance in the 'medium' and 'medium-replay' tasks.\n- **Maze2D Tasks:** QDC demonstrated exceptional trajectory stitching capability, significantly outperforming existing baselines and improving by 10.5% over the strongest baseline (QT).\n- **Adroit Tasks:** QDC showed strong performance despite the challenges posed by limited demonstration data.\n\n#### Ablation Study\n\n- **Stitching Ability:** The experiments confirmed QDC\u2019s superior ability in stitching suboptimal trajectories, particularly in Maze2D tasks with both sparse and dense rewards.\n- **Long Task Horizon Ability:** QDC maintained stable performance with varying sequence lengths, showcasing its robustness in handling extended task horizons compared to QT.\n\n### 7. Discussion\n\nThe results highlight QDC\u2019s advantages in offline RL. The convolutional approach in QDC effectively captures local dependencies in RL trajectories, and Q-value regularization improves the policy learning process. However, the paper acknowledges potential limitations, such as the dependence on accurately estimating Q-values and the computational cost of training convolutional models.\n\n### 8. Conclusion\n\nThe paper presents QDC as a novel and effective approach for offline RL, combining the strengths of DC and Q-learning to achieve superior performance across various tasks. Future research directions include optimizing the QDC structure and exploring further applications in more practical scenarios.\n\n---\nThis comprehensive summary includes critical details from the original paper, providing enough information for a researcher to replicate the study or build upon this work."}, {"role": "assistant", "content": "Q-value Regularized Decision ConvFormer for Offline\nReinforcement Learning\nTeng Yan1,\u2217, Zhendong Ruan1,\u2217, Yaobang Cai2, Yu Han2, Wenxian Li2, Yang Zhang2\u2020\nAbstract \u2014 As a data-driven paradigm, offline rein-\nforcement learning (Offline RL) has been formulated\nas sequence modeling, where the Decision Transformer\n(DT) has demonstrated exceptional capabilities. Unlike\nprevious reinforcement learning methods that fit value\nfunctions or compute policy gradients, DT adjusts the\nautoregressive model based on the expected returns,\npast states, and actions, using a causally masked\nTransformer to output the optimal action. However,\ndue to the inconsistency between the sampled returns\nwithin a single trajectory and the optimal returns\nacross multiple trajectories, it is challenging to set\nan expected return to output the optimal action and\nstitch together suboptimal trajectories. Decision Con-\nvFormer (DC) is easier to understand in the context\nof modeling RL trajectories within a Markov Deci-\nsion Process compared to DT. We propose the Q-\nvalue Regularized Decision ConvFormer (QDC), which\ncombines the understanding of RL trajectories by\nDC and incorporates a term that maximizes action\nvalues using dynamic programming methods during\ntraining. This ensures that the expected returns of\nthe sampled actions are consistent with the optimal\nreturns. QDC achieves excellent performance on the\nD4RL benchmark, outperforming or approaching the\noptimal level in all tested environments. It particularly\ndemonstrates outstanding competitiveness in trajectory\nstitching capability.\nI. I NTRODUCTION\nClassic reinforcement learning (RL) is an online\nlearning paradigm where an agent interacts with the\nenvironment iteratively to collect experiences and\nthen uses those experiences to improve its policy\nto achieve maximum returns [1]. However, in many\ncases, online interactions are impractical due to the\nhigh cost or danger of data collection (e.g., in\nrobotics, educational agents, or healthcare). To avoid\n\u2217Equal contribution,\u2020Corresponding author.\n1College of Applied Sciences, Shenzhen University, China.\n2Sino-German College of Intelligent Manufacturing, Shenzhen\nTechnology University, China.\nThis research was completed while all the authors were work-\ning at the Mechanical Industry Key Laboratory of Intelligent\nRobotics Technology for 3C, Shenzhen Technology University.such expensive and risky learning processes, offline\nreinforcement learning is adopted, which eliminates\nthe need for direct environment interaction. This\napproach leverages pre-collected datasets to derive\npolicies, making it a practical solution [2].\nOffline reinforcement learning has a paradigm\nmore similar to supervised learning due to the fact\nthat it learns from a dataset. A large portion of\npreviously proposed offline reinforcement learning\nmethods learn strategies based on constrained or\nregularized approximate dynamic programming (e.g.,\nQ-learning or actor-critic methods) [3].\nDT[4] maximizes the likelihood of actions con-\nditional on historical trajectories (including returns),\nwhich essentially transforms offline RL into super-\nvised sequence modeling [5].DT adjusts the autore-\ngressive model to generate future actions based on the\nexpected payoffs, past states, and action sequences\nas inputs to the information exchange through the\nattention module . More precisely, DT learns strate-\ngies conditional on RTG and states, and views the\ntotal of future rewards Return-to-go (RTG) as a goal.\nDT, which train by considering the future cumulative\nreward, or RTG, as the objective, encounter difficulty\nin achieving the integration capabilities necessary for\noffline RL agents. Specifically, the ability to combine\nsegments of suboptimal trajectories to produce an\noptimal trajectory is a critical requirement [6]. DT\u2019s\ntrajectory welding capabilities have been significantly\nimproved by a significant amount of subsequent\nwork. QDT employs dynamic programming to re-\nname the RTG [7], thereby enhancing the DT by\nbringing the RTG closer to the true value. EDT\noptimizes trajectories by retaining lengthier histories\nwhen the previous trajectory is optimal and shorter\nhistories when it is sub-optimal, allowing it to be\n\u201dstitched\u201d to a more optimal trajectory [8]. The\nReinformer[9] algorithm enhances splicing capabil-\nities by adjusting the RTG to the utmost gain that\ncan be achieved with the current historical trajectory.\nQT[10] integrates the trajectory modeling capabilities\nof Transformer with the Dynamic Programming (DP)arXiv:2409.08062v1  [cs.LG]  12 Sep 2024approach\u2019s predictability of optimal future payoffs to\nachieve exceptional performance in sequential rein-\nforcement learning. This is accomplished by incorpo-\nrating a Q-value module into the DT training process.\nNevertheless, reinforcement learning trajectories\nare distinct from conventional sequences, such as text\nor audio, and their modeling cannot be regarded as\na straightforward sequence modeling task. Markov\nDecision Processes (MDPs) are the preferred method\nfor defining reinforcement learning problems. In these\nprocesses, the probabilities of transitioning to the\nnext state are determined solely by the current state\nand operation, rather than past states, as per the\nMarkov property. Consequently, it is imperative to\nconsider the local correlations between the stages of\na trajectory sequence.The attention module of DT\nis over-parameterized and inadequate for capturing\nthe unique local dependency patterns of MDPs. The\nDecision Convformer (DC), which utilizes a local\nconvolutional filtering module, is more effective in\ncapturing the local correlations that are intrinsic to RL\ndatasets. Additionally, it is better adapted to capturing\nthe local dependency patterns that are intrinsic to RL\ntrajectories modeled as Markovian decision-making\nprocesses [11].\nThe DC proposal significantly enhances DT; how-\never, the splicing capability necessary for offline RL\nagents is not achieved when the sum RTG of future\nrewards is used as the training objective. Our newly\nmodified framework in this paper enables conditional\nsequence reinforcement learning to more effectively\ncapture the RL trajectories of Markov decision pro-\ncesses with outstanding trajectory splicing capability,\nas a result of the aforementioned understanding. The\nfollowing are our contributions.\n\u2022We propose a method: Q-value Regularized Con-\nvFormer a modified DC model and training\nmethod as an action learning predictor for offline\nreinforcement learning.\n\u2022QDC exhibits exceptional average performance\nin the OpenAI Gym and exceptional splicing\nperformance in the Maze2D task, resulting in\na 10.57% improvement over the strongest base-\nline. This enhances the splicing capabilities of\nthe DC algorithm.\n\u2022By utilizing higher-value actions and requiring\nfewer resources for training, QDC is capable of\neffectively capturing the local correlations that\nare inherent in RL datasets. This results in a\nfaster training process.II. P RELIMINARIES\nRL problems can be modeled as learning problems\nwithin a Markov Decision Process (MDP), M=\n\u27e8\u03c10,S,A,P,R,\u03b3\u27e9, where \u03c10is the initial state distri-\nbution, Sis the state space, Ais the action space,\nP(st+1|st,at)is the transition probability, R(st,at)is\nthe reward function, and \u03b3\u2208(0,1)is the discount\nfactor. The goal of traditional reinforcement learning\nis to find an optimal policy \u03c0\u2217that maximizes the\nexpected return by interacting with the environment.\nOffline reinforcement learning, however, is conducted\nwithout interacting with the environment; it relies on\nan offline dataset Dcollected by a given policy D\u00b5.\nThe objective of offline reinforcement learning is to\nlearn a policy using the dataset Dto maximize the\nexpected return.\nA. Decision Transformer for Offline RL\nTransformers are among the most effective and\nscalable neural networks for modeling sequence data.\nInspired by the tremendous success of sequence\nmodels in NLP, the Decision Transformer (DT) was\nproposed to model trajectory optimization problems\nas an action prediction process. DT transforms re-\ninforcement learning (RL) problems into conditional\nsequence modeling tasks. In DT, trajectories are\nviewed as sequences of return-to-go (RTG), states,\nand actions. At each time step t, DT uses a sub-\ntrajectory of length Ktime steps as input:\n\u03c4t= (\u02c6Rt\u2212K+1,st\u2212K+1,at\u2212K+1, . . . , \u02c6Rt\u22121,st\u22121,at\u22121,\u02c6Rt,st)(1)\nUsing this \u03c4t, DT predicts the action at. Here,\n\u02c6Rtdenotes the return-to-go, defined as \u02c6Rt=\u2211T\nt\u2032=trt\u2032,\nrepresenting the expected future rewards.\nB. Decision ConvFormer for offline RL\nDecision ConvFormer (DC) is an innovative action\nsequence predictor based on the MetaFormer archi-\ntecture. DC employs local convolutional filters as an\nintermediate component, which effectively captures\nthe inherent local dependencies in RL datasets. It\nprocesses multiple entities in parallel and understands\nthe relationships between them through a general\nstructure. Compared to DT, DC is better at captur-\ning the dependencies in Markov Decision Processes\nwithin RL trajectories. It achieves state-of-the-art\nperformance on offline reinforcement learning tasks\nwhile requiring fewer resources.Fig. 1. This is the QDC training framework. It uses state s, action a, and expected reward \u02c6Rtas inputs to predict the next action\n\u02c6at. The predicted action \u02c6 atand state stare then passed to the Q-network to learn the target network. Finally, QDC describes how,\nbased on the Q-network, the policy is guided to take actions consistent with the expected return to stitch different trajectories into\nthe optimal trajectory.\nC. Stitching in Conditional Sequence Modeling\nIn offline reinforcement learning tasks, the chal-\nlenge of stitching suboptimal trajectories into optimal\nones has garnered significant attention. Theoretical\nwork by Brandfonbrener et al. [12] and empirical\nfindings by Kumar et al. [13] have demonstrated that\nreturn-conditional sequence models lack the ability\nto perform stitching effectively. Both DT and DC\ntreat offline reinforcement learning as a supervised\nlearning task, where action prediction captures histor-\nical states and actions in the trajectories using causal\nmodels and convolutional models, respectively. QDT\nand EDT address trajectory stitching by reassigning\nRTG values to bias action predictions towards those\nwith higher RTG. Reinformer, on the other hand,\ntrains the model such that the predicted Rt corre-\nsponds to higher value actions. QDT incorporates a\nQ-learning module into the training process, where\nactions are predicted using a Transformer structure.\nThe predicted actions and states are then used to train\nthe Q-network, which in turn selects actions based on\nthe Q-values.III. Q- VALUE REGULARIZED CONVFORMER\nWe propose a novel approach that combines the\ntrajectory modeling capability of the Decision Con-\nvFormer (DC) with the predictability of optimal fu-\nture returns in dynamic programming (DP) methods,\nthereby creating an algorithm suited for offline re-\ninforcement learning (RL) problems. First, we detail\nthe application of the conditional transformer policy\nas a behavior cloning expression strategy framework.\nNext, we describe how to incorporate the Q-value\nmodule into our transformer policy during the train-\ning phase and use the behavior cloning term as a\nregularization mechanism for the policy. Finally, we\nexplain how to utilize the learned Q-value function\nfor inference.\nA. Model Architecture\nOur model framework, illustrated in Figure 1, re-\nplaces the Decision Transformer\u2019s causal transformer\nwith a convolution module, transforming it into the\nDecision ConvFormer model. The DC model inputs\ninclude multiple states st, actions at, and return-to-\ngoRt, which output expected future sequences fromhistorical data. During training with offline data, the\nDecision Transformer (DT) processes the trajectory\nsequence \u03c4tautoregressively, incorporating the last K\nsteps of historical context.\nThe DT model trains on input sequences of state\nlabels st, return-to-go Rt, and their corresponding\nactions atto predict the subsequent actions at. Each\nstate stand return-to-go Rtare used as supervisory\nsignals to output at.\nFor continuous action spaces, the training objective\nminimizes the mean square error:\nLDC=E\u03c4t\u223cD\"\n1\nKt\n\u2211\ni=t\u2212K+1(ai\u2212\u03c0(\u03c4t)i)2#\n(2)\nOur approach employs the same input method. The\nactions atand corresponding states stare fed into the\nDC model for Q-value training. Finally, the learned\ntarget Q-value function is used to infer actions for the\nencountered states st.\nB. Convolution Module\nIn DC, for each time step t, the input sequence is\nin the form:\n\u03c4t= (\u02c6Rt\u2212K+1,st\u2212K+1,at\u2212K+1, . . . , \u02c6Rt,st,at) (3)\nEach RTG, state, and action atis embedded indi-\nvidually, producing:\n\u03c4t=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0Emb \u02c6R(\u02c6Rt\u2212K+1);\nEmb s(st\u2212K+1);\nEmb a(at\u2212K+1);\n...\nEmb \u02c6R(\u02c6Rt);\nEmb s(st)\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\u2208R(3K\u22121)\u00d7d(4)\nwhere the sequence length is 3 K\u22121 and ddenotes\nthe hidden dimension. The sequence \u03c4tthen passes\nthrough Nstacked convolutional blocks, each con-\ntaining two sub-blocks.\nThe first sub-block involves layer normalization\nfollowed by a convolution module for token mixing:\nZ1st sub-block\nt =Conv(LN(\u03c4t))+\u03c4t (5)\nThe second sub-block involves layer normalization\nfollowed by a feedforward network:\nZ2nd sub-block\nt =FFN(LN(Z1st sub-block\nt ))+Z1st sub-block\nt (6)The FFN is implemented as a two-layer MLP.\nFinally, the action atis output.\nCompared to DT, DC has significantly fewer pa-\nrameters than the attention modules of DT. Addition-\nally, the convolution modules can integrate temporal\ninformation between adjacent tokens. The convolu-\ntion module in DC is well-suited for capturing the\ninherent local dependency patterns in RL trajecto-\nries modeled as Markov decision processes. This\nenhances the understanding of the underlying data,\nimproving the overall performance of the model. DC\nshows significant advantages and broad application\nprospects in handling complex sequential data and\nachieving intelligent decision-making.\nC. Inference with Q-value Module\nIn both DT and DC algorithms, the inputs are\nrecalculated and labeled RTGs. Since RTG represents\nthe expected future rewards, which are challenging\nto predict accurately, it can affect the generation of\nactions atwithin the sequence. Solely using RTG\nfor outputting action trajectories may not yield op-\ntimal action sequences. To address the challenge\nof stitching together trajectories and formulating a\nstrategy that aligns the expected returns of sampled\noperations with the optimal returns, we employ a Q-\nvalue module training framework.\nSpecifically, our training framework follows the\napproach of QT, combining the trajectory modeling\ncapability with the predictability of optimal future\nreturns from DP methods. DC serves as the action\npredictor to generate actions, while the learned Q-\nnetwork selects high-reward actions and refines DC.\nWe learn the Q-value function in the traditional\nmanner by minimizing the Bellman[14] operator\u2019s\ntarget network Q\u03c61and Q\u03c62, using the double Q-\nlearning technique. We construct two Q-networks Q1\nandQ2and their respective target networks Q\u03c6\u2032\n1,Q\u03c6\u2032\n2,\nand policy \u03c0\u03b8\u2032. Given that the transformer policy\u2019s\ninput includes trajectory history, we use an n-step\nBellman equation to estimate the Q-value function.\nThis is achieved by minimizing the following equa-\ntion:\nE\u03c4t\u223cD,\u02c6at\u223c\u03c0\u03b8\u2032t\u22121\n\u2211\nm=t\u2212K+1\f\f\f\f\u02c6Qm\u2212Q\u03c6i(sm,am)\f\f\f\f2(7)\nwhere\n\u02c6Qm=t\u22121\n\u2211\nj=m\u03b3j\u2212mrj+\u03b3t\u2212mmin\ni=1,2Q\u03c6\u2032\ni(st,\u02c6at) (8)and\u03c6ifori\u2208 {1,2}.\nHere, \u03b3is the discount factor, and \u02c6 atdenotes the\npredicted action output by the target model \u03c0\u03b8\u2032.\nBy integrating the Q-value module during the\ntraining phase, we prioritize sampling high-value\nactions. The ultimate policy learning objective is a\nlinear combination of policy regularization and policy\nimprovement terms:\n\u03c0=argmin\u03c0\u03b8\b\nL(\u03b8):=LDC(\u03b8)+LQ(\u03b8)\t\n=argmin\u03c0\u03b8n\nLDC(\u03b8)\u2212\u03b1\u00b7E\u03c4t\u223cDE(si,ai)\u223c\u03c4tQ\u03c6(si,\u03c0(\u03c4t)i)o\n(9)\nConsidering the scale variations of Q-value func-\ntions across different offline datasets, we adopt the\nnormalization technique by Fujimoto & Gu[15]. \u03b1is\ndefined as\n\u03b1=\u03b7\nE\u03c4t\u223cDE(s,a)\u223c\u03c4t\u0002\n|Q\u03c6(s,a)|\u0003 (10)\nFinally, during inference, the Q-value module sam-\nples multiple candidate RTG tokens and outputs\nactions based on different returns, selecting the ac-\ntion with the highest Q-value given st. This Q-\nvalue regularization approach enhances the policy by\nprioritizing high-value behaviors, closely aligning the\nlearning process with the optimal returns.\nIV. E XPERIMENTS\nIn this section, we conduct a comprehensive evalu-\nation of our QDC model using the D4RL benchmark\n[16], which includes not only the Gym datasets with\nnear-saturated performance but also the challenging\nMaze2D and Adroit datasets. The main objectives of\nour experiments are:\n\u2022To compare the performance of the QDC model\nwith other state-of-the-art (SOTA) benchmarks\nin offline RL.\n\u2022To determine whether combining the DC model\nwith Q-learning effectively adapts and improves\nits performance.\n\u2022To evaluate the stitching ability of QDC for sub-\noptimal trajectories.\n\u2022To assess the impact of the parameters in QDC\non overall performance.\nA. Baselines\nWe compare QDC with existing SOTA offline RL\nmethods, including three value-based methods: Be-\nhavior Cloning (BC) [17], Conservative Q-Learning\n(CQL), and Implicit Q-Learning (IQL), as well as se-\nquence modeling-based offline RL methods: DecisionTransformer (DT), Q-learning Decision Transformer\n(QDT), Reinformer, Decision ConvFormer (DC), and\nQ-value Regularized Transformer (QT).\nV. R ESULTS\nA. Main Results\nQDC is compared with baselines in three task\ndomains, and the results are reported in Table I.\nResults for QDC correspond to the mean and standard\nerrors of normalized scores over 30 random rollouts\n(3 independently trained models and 10 trajectories\nper model) for all tasks, which generally exhibit low\nvariance in performance. Our method outperforms\nand is competitive with all existing methods across\nalmost all domains, with particularly strong perfor-\nmance in the Maze2D domains. To ensure a fair\ncomparison, scores are normalized according to the\nprotocol established by Fu et al. [7], where a score\nof 100 corresponds to the expert policy. The analysis\nis conducted for each specific domain.\n1) Results for Gym Domain: In the Gym tasks,\nboth classical offline reinforcement learning algo-\nrithms and sequence modeling algorithms demon-\nstrate proficiency. QDC, with its advantage of se-\nquence modeling over traditional methods, achieves\nfurther improvements compared to sequence model-\ning algorithms like DT and DC, particularly exceed-\ning traditional offline reinforcement learning meth-\nods and transformer-based methods in the \u2019medium\u2019\nand \u2019medium-replay\u2019 tasks. Compared to the DC\nalgorithm, QDC effectively enhances performance in\nGym\u2019s \u2019medium\u2019, \u2019medium-replay\u2019, and \u2019medium-\nexpert\u2019 tasks by combining Q-learning. Additionally,\nconvolution-based QDC approaches or even surpasses\nthe optimal performance of transformer-based QT\nacross all Gym tasks.\n2) Results for Maze2D Domain: Maze2D is an\nenvironment where forces drive a ball to move to a\nfixed target position and serves as a benchmark for\nevaluating the effectiveness of offline RL algorithms\nin stitching together different trajectory segments.\nQDC performs excellently in Maze2D, improving\nby 10.5% compared to the strongest baseline, QT.\nThe convolution module as an action predictor better\ncaptures local dependency patterns in RL trajectories,\nenhancing the ability to stitch suboptimal trajectories\neffectively.\n3) Results for Adroit Domain: In the Adroit do-\nmain, due to the limited range of human demonstra-\ntions, offline reinforcement learning faces challengesTABLE I\nTHE PERFORMANCE OF QDC AND SOTA BASELINES ON D4RL G YM, ADROIT AND MAZE2D TASKS\nDataset Value-Based Method Return-Conditioned BC\nBC CQL IQL DT QDT Reinformer DC QT QDC (OURS)\nGym\nTaskshalfcheetah-medium 75.3 49.2 47.4 42.6 39.3 42.94 43 51.4 47.4\u00b10.9\nhalfcheetah-medium-replay 36.6 45.5 44.2 36.6 35.6 39.01 41.3 48.9 44.4\u00b10.2\nhalfcheetah-medium-expert 55.2 91.6 86.7 86.8 - 92.04 93 96.1 90.8\u00b14\nhopper-medium 42.6 69.4 66.3 67.6 66.5 80.02 92.5 96.9 92.8\u00b10.9\nhopper-medium-replay 18.1 95 94.7 82.7 52.1 83.67 94.2 102 99.6\u00b10.6\nhopper-medium-expert 52.5 105.4 91.5 107.6 67.1 107.82 110.4 113.4 112.9\u00b10.5\nwalker2d-medium 52.9 83 78.3 74 - 80.52 79.2 88.8 86.8\u00b10.3\nwalker2d-medium-replay 32.3 77.2 73.9 79.4 58.2 72.89 76.6 98.5 99.4\u00b10.6\nwalker2d-medium-expert 107.5 108.8 109.6 108.1 - 109.35 109.6 112.6 112.3\u00b11.2\nAverage 52.6 80.6 77 76.2 - 78.7 82.2 89.8 87.4\nMaze2D\nTasksmaze2d-umaze 88.9 94.7 42.1 31 57.3 57.15 - 105.4 160.8 \u00b19.1\nmaze2d-medium 38.3 41.8 34.9 8.2 13.3 85.62 - 172 172.8 \u00b11.7\nmaze2d-large 1.5 49.6 61.7 2.3 31 47.35 - 240.1 238.5\u00b17.3\nAverage 42.9 62.0 46.2 13.8 33.8 63.4 - 172.5 190.7\nAdroit\nTaskspen-human-v1 63.9 37.5 71.5 79.5 - - - 129.6 124.8\u00b13.2\nhammer-human-v1 1.2 4.4 1.4 3.7 - - - 35.6 32.2\u00b11.4\ndoor-human-v1 2.0 9.9 4.3 14.8 - - - 28.7 30.95 \u00b12.3\npen-cloned-v1 37.0 39.2 37.3 75.8 - - - 125 114.5\u00b14.2\nhammer-cloned-v1 0.6 2.1 2.1 3 - - - 23 19.4\u00b12.8\ndoor-cloned-v1 0.0 0.4 1.6 16.3 - - - 20.6 19.5\u00b11.2\nAverage 17.5 15.6 19.7 32.2 - - - 60.4 56.9\nfrom extrapolation errors, making strong policy nor-\nmalization crucial. QDC, combined with Q-learning,\nexhibits excellent performance.\nB. Ablation Study\nBased on QDC\u2019s outstanding performance on\nD4RL tasks, we conduct ablation experiments to\ninvestigate the impact of QDC\u2019s main components\non its overall effectiveness. We choose DC and QT\nas comparison baselines to highlight the differences\nbetween DC, QT, and our QDC.\n1) Stitching Ability: The Maze2D domain, which\ninvolves a navigation task with a fixed target po-\nsition, serves as a benchmark for evaluating the\nstitching capability of offline reinforcement learning\nalgorithms in combining suboptimal trajectories into\noptimal ones. It includes four mazes of increasing\ndifficulty\u2014open, umaze, medium, and large\u2014and\nuses two reward functions: normal and dense. Normal\nrewards are granted only when the target is achieved,\nwhile dense rewards are incrementally assigned at\neach step, inversely proportional to the distance from\nthe target. Table II summarizes the results, presenting\nthe average and standard deviation scores reported\nover 3 seeds for the Maze2D tasks. This table en-\ncompasses four increasingly complex mazes\u2014open,\numaze, medium, and large\u2014each with two reward\nfunctions: normal and dense. The highest average\nscores are highlighted in bold.2) Long Task Horizon Ability: In Markov envi-\nronments, the previous state is usually sufficient to\ndetermine the current action. However, DT experi-\nments show that past information can be valuable for\nsequence modeling methods in certain environments,\nwhere longer sequences often yield better results (see\nFigure 2). DC, as a predictor, is more suitable for cap-\nturing local dependencies in Markov RL datasets. We\nexplore the impact of different sequence lengths on\nperformance. We use the walker2d-medium-replay-\nv2 task to set different K input lengths and compare\nthe effects on QT and QDC as the sequence length\nK varies. DT initially deteriorates after K=20 but\nrecovers at K=80. QDC maintains stable performance\ncompared to QT, demonstrating superior capability in\nmanaging extended task horizons.\nVI. C ONCLUSION ,DISCUSSION AND FUTURE\nWORK\nIn this paper, we propose a new decision-making\nalgorithm for offline reinforcement learning (RL)\ncalled QDC. The DC component consists of convolu-\ntional filters combined with a Q-learning module for\ndecision-making. Compared to the attention modules\nin DT, the convolutional module in DC reduces the\nnumber of parameters and computational complex-\nity while better capturing local dependencies in RL\noffline trajectories. We have demonstrated that QDC\nperforms exceptionally well across various offline RLTABLE II\nABLATION ON THE STITCHING ABILITY\nDataset CQL DT QT QDC\nSparse\nRewardmaze2d-open-v0 216.7 \u00b1 80.7 196.4 \u00b1 39.6 497.9 \u00b1 12.3 479.3\u00b14.6\nmaze2d-umaze-v1 94.7 \u00b1 23.1 31.0 \u00b1 21.3 105.4 \u00b1 4.8 160.8 \u00b19.1\nmaze2d-medium-v1 41.8 \u00b1 13.6 8.2 \u00b1 4.4 172.0 \u00b1 6.2 172.8 \u00b11.7\nmaze2d-large-v1 49.6 \u00b1 8.4 2.3 \u00b1 0.9 240.1 \u00b1 2.5 238.5\u00b17.3\nDense\nRewardmaze2d-open-dense-v0 307.6 \u00b1 43.5 346.2 \u00b1 14.3 608.4 \u00b1 1.9 578.3\u00b18.6\nmaze2d-umaze-dense-v1 72.7 \u00b1 10.1 -6.8 \u00b1 10.9 103.1 \u00b1 7.8 95.6\u00b13.1\nmaze2d-medium-dense-v1 70.9 \u00b1 9.2 31.5 \u00b1 3.7 111.9 \u00b1 1.9 131.9 \u00b17.4\nmaze2d-large-dense-v1 90.9 \u00b1 19.4 45.3 \u00b1 11.2 177.2 \u00b1 7.8 173.2\u00b18.5\nFig. 2. Ablation on the long task horizon ability. This encom-\npasses the performance comparison of different input sequence\nhorizons K\u2208[10,80]in the walker2d-medium-replay-v2 task.\nenvironments, particularly in trajectory stitching. The\nconvolutional structure of DC effectively captures\nlocal dependencies in RL trajectories, and the Q-\nlearning module improves the selection of high-value\nactions. However, there are still some areas where\nperformance could be improved. In future work,\nwe will further optimize the structure of QDC and\nexplore its potential in more practical applications.\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, \u201cIntroduction to reinforcement\nlearning. mit press,\u201d Cambridge, MA , 1998.\n[2] A. Kumar, \u201cData-driven deep reinforcement learning,\u201d\nBerkeley Artificial Intelligence Research (BAIR), Tech. Rep ,\n2019.\n[3] A. Kumar, A. Zhou, G. Tucker, and S. Levine, \u201cConserva-\ntive q-learning for offline reinforcement learning,\u201d Advances\nin Neural Information Processing Systems , vol. 33, pp.\n1179\u20131191, 2020.\n[4] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover,\nM. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch, \u201cDe-\ncision transformer: Reinforcement learning via sequence\nmodeling,\u201d Advances in neural information processing sys-\ntems, vol. 34, pp. 15 084\u201315 097, 2021.\n[5] M. Janner, Q. Li, and S. Levine, \u201cOffline reinforcement\nlearning as one big sequence modeling problem,\u201d Advances\nin neural information processing systems , vol. 34, pp. 1273\u2013\n1286, 2021.[6] I. Kostrikov, A. Nair, and S. Levine, \u201cOffline reinforce-\nment learning with implicit q-learning,\u201d arXiv preprint\narXiv:2110.06169 , 2021.\n[7] T. Yamagata, A. Khalil, and R. Santos-Rodriguez, \u201cQ-\nlearning decision transformer: Leveraging dynamic pro-\ngramming for conditional sequence modelling in offline rl,\u201d\ninInternational Conference on Machine Learning . PMLR,\n2023, pp. 38 989\u201339 007.\n[8] Y .-H. Wu, X. Wang, and M. Hamaya, \u201cElastic decision\ntransformer,\u201d Advances in Neural Information Processing\nSystems , vol. 36, 2024.\n[9] Z. Zhuang, D. Peng, Z. Zhang, D. Wang et al. , \u201cReinformer:\nMax-return sequence modeling for offline rl,\u201d arXiv preprint\narXiv:2405.08740 , 2024.\n[10] S. Hu, Z. Fan, C. Huang, L. Shen, Y . Zhang, Y . Wang,\nand D. Tao, \u201cQ-value regularized transformer for offline\nreinforcement learning,\u201d arXiv preprint arXiv:2405.17098 ,\n2024.\n[11] J. Kim, S. Lee, W. Kim, and Y . Sung, \u201cDecision con-\nvformer: Local filtering in metaformer is sufficient for\ndecision making,\u201d arXiv preprint arXiv:2310.03022 , 2023.\n[12] D. Brandfonbrener, A. Bietti, J. Buckman, R. Laroche,\nand J. Bruna, \u201cWhen does return-conditioned supervised\nlearning work for offline reinforcement learning?\u201d Advances\nin Neural Information Processing Systems , vol. 35, pp.\n1542\u20131553, 2022.\n[13] A. Kumar, R. Agarwal, X. Geng, G. Tucker, and S. Levine,\n\u201cOffline q-learning on diverse multi-task data both scales\nand generalizes,\u201d arXiv preprint arXiv:2211.15144 , 2022.\n[14] S. Fujimoto, D. Meger, and D. Precup, \u201cOff-policy deep re-\ninforcement learning without exploration,\u201d in International\nconference on machine learning . PMLR, 2019, pp. 2052\u2013\n2062.\n[15] S. Fujimoto and S. S. Gu, \u201cA minimalist approach to offline\nreinforcement learning,\u201d Advances in neural information\nprocessing systems , vol. 34, pp. 20 132\u201320 145, 2021.\n[16] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine,\n\u201cD4rl: Datasets for deep data-driven reinforcement learn-\ning,\u201d arXiv preprint arXiv:2004.07219 , 2020.\n[17] D. A. Pomerleau, \u201cAlvinn: An autonomous land vehicle in a\nneural network,\u201d Advances in neural information processing\nsystems , vol. 1, 1988."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "## Summary of \"Spatial Adaptation Layer: Interpretable Domain Adaptation For Biosignal Sensor Array Applications\"\n\n### 1. Abstract\nThis paper introduces the Spatial Adaptation Layer (SAL) and learnable baseline normalization (LBN), innovative methods to enhance domain adaptation in biosignal acquisition applications such as surface electromyography (sEMG). These techniques address the performance degradation caused by electrode shifts between sessions. Tested on two HD-sEMG gesture recognition datasets, SAL and LBN outperform standard fine-tuning methods. SAL is designed for easy integration into any biosignal array model and provides interpretable parameters, making it particularly advantageous.\n\n### 2. Introduction\n- **Context and Importance**: Biosignal acquisition (e.g., sEMG and EEG) has profound implications for healthcare and human-machine interaction. The correct processing of these signals can enable various applications, including gesture recognition and other health monitoring systems.\n- **Challenges**: Despite within-session high performance, electrode shifts between sessions significantly degrade performance. Existing solutions requiring large datasets and complex models lack interpretability and robustness.\n- **Objective**: The paper aims to provide a solution (SAL) that can learn affine transformations between sessions in a physically interpretable manner to mitigate the impact of electrode shifts. LBN is also proposed to handle baseline fluctuations.\n- **Literature Review**: The introduction reviews existing methods like signal processing, supervised domain adaptation, and affine transformations using STNs, highlighting their limitations in terms of data requirements, robustness, and interpretability.\n\n### 3. Related Work\n- **Signal Processing Methods**: Various studies try to handle electrode shifts using heuristic-based preprocessing or supervised adaptation but often come with significant data collection burdens or lack interpretability.\n- **Domain Adaptation Techniques**: Previous works using RNN, progressive neural networks, and model-agnostic meta-learning show improvements but require many parameters and complex adaptation sessions.\n- **Affine Transformations and Spatial Networks**: Methods like STNs are effective but often operate as black-box models. The paper builds on STNs but focuses on creating a more interpretable and efficient approach.\n\n### 4. Method\n#### Spatial Adaptation Layer (SAL)\n- **Architecture Overview**: \n  - Inspired by STNs, it prepends a transformation layer to biosignal array models.\n  - Affine transformation parameters are learned during adaptation, not a function of inputs.\n  \n- **Formulations and Algorithms**:\n  - Resampling of the input image \\( U \\) to output image \\( V \\) through grid-based sampling is defined.\n  - Affine transformation matrix \\( A_S \\) is used to map original to transformed coordinates.\n  - Precisely decomposed into translation, rotation, scaling, and shearing components for interpretability.\n  - Each affine transformation is parameterized by 7 learnable parameters.\n  \n#### Learnable Baseline Normalization (LBN)\n- **Implementation**:\n  - Inspired by traditional baseline normalization methods but using learnable biases \\( b \\) for each input channel.\n  - Ensures consistency across sessions by adjusting for baseline activity fluctuations.\n\n#### Model Implementation\n- **Interpolations**: Details specific bilinear interpolation formulas and differentiable sampling operators.\n- **Backpropagation**: Describes how gradients flow back through the transformation layer.\n- **Classification**: Issues related to optimizing classification loss directly during adaptation, guiding the learning of physically meaningful transformation parameters.\n\n### 5. Experimental Setup\n#### Datasets\n- **Capgmyo**: HD-sEMG recordings from 9 subjects, 128-channel (8x16 grid), two sessions each.\n- **CSL Dataset**: HD-sEMG recordings from 5 subjects, 192 electrodes, multiple sessions each.\n\n#### Preprocessing and Classification\n- **Signal Processing**:\n  - Digital bandpass and band-stop filters applied.\n  - RMS-based feature extraction for gesture recognition.\n- **LBN Implementation**: Adjusts for baseline activity fluctuations by learning channel-specific negative biases.\n\n#### Models\n- **Logistic Regression**: Simple, traditional classifier.\n- **CapgmyoNet**: Deep CNN-based classifier, trained on raw sEMG frames, utilizing 1D convolutions.\n\n### 6. Results and Analysis\n#### Intrasession Experiments\n- **Performance**: Both models achieve high intrasesion accuracy (>93%) indicating effectiveness without session shifts. Logistic Regression unexpectedly performs well, sometimes outperforming CNNs.\n\n#### Intersession Experiments\n- **Performance**: In scenarios with session shifts, SAL combined with LBN significantly outperforms fine-tuning, especially notable in the CSL dataset where the data grid is regular.\n- **Dropout**: Enhances the adaptability of SAL by reducing dependency on specific channels, leading to performance improvements up to 60% in some cases.\n- **Fine-tuning Comparison**: Traditional fine-tuning shows slight catastrophic forgetting while SAL maintains original session performance levels.\n\n#### Ablation Studies\n- **Parameters Analysis**:\n  - LBN alone improves performance but adding translation parameters in SAL accounts for the majority of performance gains.\n  - Horizontal (circumferential) translations specifically align with physiological expectations, showing significant performance improvements.\n\n### 7. Discussion\n- **Interpretation**: Affine transformations, especially translations, are critical to handling electrode shifts. SAL coupled with LBN mitigates session variability effectively.\n- **Implications**: The methods hold promise for practical applications across biosignal sensor arrays, reducing the need for large datasets and making models robust to practical deployment variations.\n- **Limitations**: The method was tested on logarithmic regression and a simple CNN, but more complex architectures need exploration.\n\n### 8. Conclusion\n- **Key Contributions**:\n  - Introduction of SAL and LBN for handling electrode shift and baseline fluctuations.\n  - Demonstration of significant improvements in intersession performance with orders of magnitude fewer parameters.\n- **Future Work**: Extending this methodology to generalize across more complex biosignal applications, exploring fewer class-dependent adaptation sessions, and validating on more diverse datasets.\n\n---\n\nThis comprehensive summary encapsulates all essential details from the original paper, enabling a researcher to replicate or build upon this work with an in-depth understanding of the methods, results, and implications."}, {"role": "assistant", "content": "Spatial Adaptation Layer: Interpretable Domain\nAdaptation For Biosignal Sensor Array Applications\nJoao Pereira, Michael Alummoottil, Dimitrios Halatsis and Dario Farina, Fellow, IEEE\nNeuromechanics and Rehabilitation Technology Group, Imperial College London, London W12 0BZ, UK\nAbstract \u2014Biosignal acquisition is key for healthcare appli-\ncations and wearable devices, with machine learning offering\npromising methods for processing signals like surface electromyo-\ngraphy (sEMG) and electroencephalography (EEG). Despite\nhigh within-session performance, intersession performance is\nhindered by electrode shift, a known issue across modalities.\nExisting solutions often require large and expensive datasets\nand/or lack robustness and interpretability. Thus, we propose\nthe Spatial Adaptation Layer (SAL), which can be prepended\nto any biosignal array model and learns a parametrized affine\ntransformation at the input between two recording sessions.\nWe also introduce learnable baseline normalization (LBN) to\nreduce baseline fluctuations. Tested on two HD-sEMG gesture\nrecognition datasets, SAL and LBN outperform standard fine-\ntuning on regular arrays, achieving competitive performance even\nwith a logistic regressor, with orders of magnitude less, physi-\ncally interpretable parameters. Our ablation study shows that\nforearm circumferential translations account for the majority\nof performance improvements, in line with sEMG physiological\nexpectations.\nIndex Terms \u2014biosignals, domain adaptation, interpretability,\nelectrode shift\nI. I NTRODUCTION\nBIOSIGNAL acquisition has powered a plethora of health-\ncare applications and wearable devices. Due to their\ncomplex structure, machine learning emerged as a promising\nmethod of leveraging biosignal information, such as movement\nintent classification from surface electromyography (sEMG)\nand electroencephalography (EEG) for human machine inter-\nactions (HCI) [1].\nFurthermore, sensor arrays enable the acquisition of spa-\ntially distributed information. Not only can this information\nbe used for the inverse modelling of anatomically interpretable\nquantities, such as electrocardiography imaging (ECGi) [2] or\nEMG decomposition [3], but its regular structure also enables\nimage-based processing [4] and deep learning approaches [5]\nto be applied for these downstream tasks.\nWhile aforementioned methods often yield high perfor-\nmance within the same recording session, performance is gen-\nerally impractically low across sessions [6]. The displacement\nof sensors across sessions, referred to as electrode shift, is a\nknown contributor in sEMG [6], EEG [7], and function near-\ninfrared spectroscopy (fNRIS) [8]. Some studies have aimed\nto learn representations invariant to electrode shift [9]. While\nThis project was supported by UK Research and Innovation [UKRI Centre\nfor Doctoral Training in AI for Healthcare grant number EP/S023283/1], the\nImperial-META Wearable Neural Interfaces Research Centre and the Onassis\nFoundation.Meta Reality Labs has obtained remarkably low error across\nsessions and even subjects [1], it is dependent on a large\ndataset that can only be obtained through a resource intensive\ndata collection effort, lacks interpretability, and only works on\ntheir own hardware, restricting it from external applications.\nBeyond invariance learning, another solution proposed is\ncalibrating to data from the new session. Image and signal\nprocessing methods have been used such as detecting anatom-\nical landmarks for virtually reversing electrode displacement\n[4]. Other studies have attempted supervised domain adapta-\ntion techniques, such as the use of fine-tuning, progressive\nneural networks [10], and even model-agnostic meta-learning\n[11]. One promising approach introduces a 2-step domain\nadaptation procedure: where an RNN is used for training on\none session, then a linear layer is used to map inputs to\nthe input space of the previous session with frozen RNNs\nweights [12]. However, either signal processing approaches\nare highly dependent on heuristic choices and independent\npreprocessing steps, or learning based approaches are non-\ninterpretable and require large amounts of adaptation data,\nlimiting their applicability in settings that require very short\nadaptation sessions.\nTherefore, in this paper we introduce the Spatial Adapta-\ntion Layer (SAL), inspired by Spatial Transformer Networks\n(STNs) [13]. While STNs compute coefficients of an affine\ntransformation as a function of the input (localization net-\nwork), SAL learns these parameters, decomposed into sep-\narable transformations for interpretabillty, via an adaptation\nsession. Without a localization net, often a black box model,\nwe bring further interpretability to the model. On top of\nSAL, we also subtract biases from each biosignal channel\nthat are learned during adaptation to account for baseline\nactivity fluctuations. Inspired by the baseline normalization\n(BN) procedure from [4], this method is referred as learnable\nBN (LBN).\nThe philosophy behind SAL is that transforming biosignal\nimages from a new recording session back into the original\nspatial frame in which the original classifier was trained\non would have a significant impact on the loss function.\nConsequently, parameters can be learned directly from the su-\npervision loss during adaptation. Additionally, while STNs are\nmost useful for handling i.i.d. affine transformations, SAL is\nespecially designed for intersession adaptation, where learned\npatterns are connected across sessions via a single, unique\naffine transformation. While similar in structure to the 2-step\nadaptation introduced in [12], SAL uses far fewer parameters,arXiv:2409.08058v1  [cs.LG]  12 Sep 2024TAS (G)Grid generator\nSampler\nLBNASInput Biosignal Images Spatially Transformed ImagesSAL Model LFrozen Weights\nLoss FunctionTraining\nSAL Model LFrozen Weights\nLoss FunctionAdaptationSession #1\nSession #2\nInputsInputsFig. 1. Schematic of SAL architecture and usage. SAL weights are frozen\nand the chosen model is optimized on session #1. On session #2, given a\nfew examples, the model is frozen and the parameters of SAL are optimized\nfor adaptation. SAL is similar to an STN module from [13], with the key\ndifference that rather than modelling affine coefficients Asas a function of\nthe input, Asare treated as learnable parameters.\nbeing directly interpretable as well as more efficient and reg-\nularizing. We evaluate our methods on two publicly available\nHD-sEMG datasets. For regular biosignal array images, our\ninterpretable method offers higher performance than standard\nfine-tuning and is consistent with observations based on past\nsEMG studies.\nII. M ETHODS & D ATASETS\nA. Spatial Adaptation Layer\nIn the Spatial Adaptation Layer, seen in Fig 1, we used the\ndifferentiable image sampling operator proposed in STNs [13]\nwhich they frame as resampling an image based on a set of\nsampling coordinates TAS(G), given original grid coordinates\nof the image, G, under some affine transformation AS. This\ncan be seen in Eq. 1 with (xs\ni, ys\ni)elements of the sampling\ncoordinates, and (xt\ni, yt\ni)elements of the target coordinates\ninGof the output image, \u2200i\u2208[1. . . HW ]for an image of\ndimensions H\u00d7W:\n\u0012\nxs\ni\nys\ni\u0013\n=TAS(Gi) =AS\uf8eb\n\uf8edxt\ni\nyt\ni\n1\uf8f6\n\uf8f8=\u0012\n\u03b811\u03b812\u03b813\n\u03b821\u03b822\u03b823\u0013\uf8eb\n\uf8edxt\ni\nyt\ni\n1\uf8f6\n\uf8f8(1)\nGiven an input image U, the resampling to an output image\nVfor a single channel is defined in Eq. 2 for some sampling\nkernel kwith parameters \u03a6xand\u03a6y:\nVi=HX\nn=1WX\nm=1Unmk(xs\ni\u2212m; \u03a6x)k(ys\ni\u2212n; \u03a6y) (2)\nFor billinear interpolation this can be written as:\nVi=HX\nn=1WX\nm=1Unmmax(0 ,1\u2212|xs\ni\u2212m|) max(0 ,1\u2212|ys\ni\u2212n|)(3)The authors in [13] show that the partial derivatives of the\noutput image with respect to the input and sampling grid can\nbe defined as:\n\u2202Vi\n\u2202Unm=HX\nn=1WX\nm=1max(0 ,1\u2212 |xs\ni\u2212m|) max(0 ,1\u2212 |ys\ni\u2212n|)(4)\n\u2202Vi\n\u2202xs\ni=HX\nn=1WX\nm=1Unmmax(0 ,1\u2212 |ys\ni\u2212n|)\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30,|m\u2212xs\ni| \u22651\n1, m \u2265xs\ni\n\u22121, m < xs\ni(5)\nyielding a sub-differentiable sampling mechanism, seen in Eq.\n5. Gradients can flow through since\u2202xs\ni\n\u2202\u03b8and\u2202ys\ni\n\u2202\u03b8can be easily\nderived from Eq. 1, effectively allowing backpropagation to be\napplied. For more interpretable parameters to be learned, we\nfurther decompose ASinto sub-transforms whilst preserving\ndifferentiability in the operator. We do this by making use\nof tranformations in homogenous coordinates such that the\ngeneral affine transformation Ais defined as Eq. 6 with T,\nR,ScandShrepresenting translation, rotation, scaling and\nshearing respectively.\nA=ShScRT\n=\uf8eb\n\uf8ed1shx0\nshy1 0\n0 0 1\uf8f6\n\uf8f8\uf8eb\n\uf8edsx0 0\n0sy0\n0 0 1\uf8f6\n\uf8f8\n\uf8eb\n\uf8edcos\u03d5\u2212sin\u03d50\nsin\u03d5cos\u03d50\n0 0 1\uf8f6\n\uf8f8\uf8eb\n\uf8ed1 0 Tx\n0 1 Ty\n0 0 1\uf8f6\n\uf8f8 (6)\nHence, we can redefine ASas the submatrix of Asuch that\nAS=A[I, J]where I={1,2}andJ={1,2,3}. Given\ntensor slicing andmultiplication are differentiable operations,\ngradients are preserved with no loss of information and can\nbe defined with respect to the sampling grid coordinates by\napplying the chain rule (e.g.\u2202xs\ni\n\u2202Tx=\u2202xs\ni\n\u2202\u03b8\u2202\u03b8\n\u2202Tx). This affine trans-\nformation and layer is parametrised by 7 learnable parameters.\nOrdinarily, one would expect such parameters to be learned\nthrough a similarity metric applied to images directly as in\nimage registration. However exact correspondence is ambigu-\nous in our scenario. Instead, parameters can be learned for any\noptimization based downstream task. Here, classification loss\nis backpropagated for multiple windows during adaptation,\nguiding the learning of these physically meaningful parameters\nto uncover a unique mapping from session to session.\nB. Datasets\nCapgmyo [5]: Signals were acquired via eight differential,\nsilver, wet electrode array evenly spaced around the circum-\nference of the arm, resulting in 128 simultaneous channel\nrecordings (irregular grid of 8x16) acquired at 1000 Hz. All\nexperiments were carried out with data subset DBb, containing\ntwo recording sessions from subjects. Due to data corruption\nin the last subject\u2019s recordings, this experiment was carried\nforward with the first 9 subjects. Each session consisted of 10\nrepetitions of 8 isotonic and isometric hand and finger gestures\n(G = 8) held for 3-10s. As in [5], only the central 1s interval\nof each gesture was considered in this study.CSL Dataset [4]: Bipolar recordings were acquired at\n2048Hz via an electrode array with 192 electrodes with an\ninter-electrode distance of 10 mm and taking differences\nbetween consecutive samples, resulting in a regular grid of\n7x24 (168 channels). This dataset contains recordings from 5\ndifferent subjects across 5 different days. For each recording,\nsubjects performed 10 repetitions of 27 different gestures. To\nensure that the labelled data corresponds to the desired move-\nment, the activity segmentation from [4] was used to determine\nthe active movement segment. The rest (idle) gesture was\nexcluded.\nC. Preprocessing & Classification\nAs in [5], signals were processed with a digital bandpass\nfilter (20-380 Hz, fourth-order Butterworth) and power-line\ninterference was reduced using a digital band-stop filter (45-\n55H Hz, fourth-order Butterworth). Activity images are used\nas in [4], where the root mean square (RMS) was obtained\nfor each given channel. To obtain an instantaneous estimate\nof RMS, the raw sEMG activity is squared, then convolved\nwith a moving average (MA) filter of length 2\u2217TRMS + 1,\nintroducing a delay of TRMS to the system, fixed to be 75ms.\nAs in [5], we report majority voting accuracies for the full\nactivity segment for CSL and 150ms windows for Capgmyo.\nLBN is implemented as b\u2208I RHW, representing nega-\ntive biases applied at each input channel. Since a logistic\nregressor has its own biases per channel, and deep CNNs\nare known for their expressivity, LBN was not considered\nwhen using standard fine-tuning with all model parameters.\nTo evaluate the effect of LBN, we implemented the baseline\nnormalization (BN) procedure from [4], where the average\nRMS from each channel during the rest (idle) gesture was\nestimated and subtracted from the remaining gestures for a\ngiven session. This step aimed to enforce more consistent\nnoise distributions across sessions. To prevent overfitting to\nnoisy/corrupted channels during training, the effect of dropout\nat the input ( p= 0.5) was considered, probing the system to\nreduce dependencies on a small number of specific channels.\nTo demonstrate the flexibility and simplicity of SAL and\nLBN, we integrated it into two different classifiers. The first\nis a multinomial logistic regressor (LogReg) trained with 2\nepochs and batch size of 1024. Secondly, we implemented\nthe classifier from [5], which we refer to as CapgmyoNet\n(CpgmNet), trained with 1 epoch and a batch size of 128. As\nopposed to activity images, as recommended in [5], Capgmy-\noNet processed raw sEMG image frames. Given the findings\nfrom [14], we optimize the loss based on the sum across\nthe batch, as opposed to the average, to leverage the linear\nscaling rule between learning rate and batch size and fix a\nlearning rate for different batch sizes. For this to be valid, we\nalso implemented a gradual warm-up learning rate schedule,\nwhere the learning rate was linearly increased from 1% to\n100% of the base learning rate over the first training epoch.\nAll classifiers were trained using the Adam optimizer, cross-\nentropy loss, and a base learning rate of 0.05.TABLE I\nINTRASESSION EXPERIMENTS (TESTACCURACY )\nBase Drpt. BN BN+Drpt.\nCSL\nLogReg 0.923 0.892 0.903 0.920\nCpgmNet 0.949 0.903 0.848 0.943\nCapgmyo\nLogReg 0.962 0.973 0.972 0.969\nCpgmNet 0.884 0.930 0.905 0.935\nTABLE II\nINTERSESSION EXPERIMENTS (TESTACCURACY )\nBN BN +\nDrpt.LBN LBN+Drpt.\nCSL\nLogReg 0.147(0.126) 0.227(0.190) - -\nLogReg(fine-tune) 0.427(0.326) 0.414(0.266) - -\nLogReg(SAL) 0.337(0.274) 0.571(0.339) 0.555(0.404) 0.698(0.394)\nCpgmNet 0.151(0.127) 0.184(0.154) - -\nCpgmNet(fine-tune) 0.700(0.502) 0.686(0.405) - -\nCpgmNet(SAL) 0.386(0.305) 0.604(0.425) 0.133(0.114) 0.721(0.472)\nCapgmyo\nLogReg 0.439(0.436) 0.447(0.444) - -\nLogReg(fine-tune) 0.761(0.752) 0.787(0.728) - -\nLogReg(SAL) 0.614(0.606) 0.656(0.550) 0.723(0.714) 0.737(0.638)\nCpgmNet 0.401(0.318) 0.402(0.328) - -\nCpgmNet(fine-tune) 0.813(0.586) 0.745(0.476) - -\nCpgmNet(SAL) 0.582(0.414) 0.535(0.375) 0.652(0.430) 0.587(0.391)\nFor each intrasession experimental setting, 9 repetitions of\neach gesture were randomly selected from a recording session\nfor training, and evaluated on data of the left-out repetition. For\neach intersession experimental setting, for every permutation\nof 2 sessions for a subject, the model was trained on the\nfirst session, adapted on one randomly sampled repetition of\neach gesture from the second session, and evaluated on the\nremaining 9 repetitions of each gesture. For fine-tuning, all\nparameters were left trainable during adaptation. For SAL, as\nin Figure 1, all affine and LBN parameters are frozen during\ntraining, and during adaptation, all are frozen except affine and\nLBN parameters. Across models, the same hyperparameters\nin training were used during adaptation, with the number of\nepochs being multiplied by a factor 10, ensuring the same\nnumber of iterations as training.\nIII. E XPERIMENTAL ANALYSIS\nA. Experiment 1: Intrasession Performance\nThe first experiment involves obtaining the intrasession per-\nformance scores for the aforementioned classifiers on each of\nthe datasets, acting as a practical upper-bound for intersession\nperformance. As shown in Table I accuracies are high across\nmodels, with the optimal condition for each model being\nabove 93% accuracy. Interestingly, even a logistic regressor\nis able to achieve competitive performances, outperforming\nCapgmyoNet with Capgmyo data.TABLE III\nABLATION EXPERIMENTS (TESTACCURACY )\nLBN\n(no affine)No\ntranslationTranslation\nonlyCircumferential\ntranslation\nonly\nCSL\nLogReg 0.433(0.253) 0.497(0.281) 0.647(0.369) 0.644(0.363)\nCpgmNet 0.526(0.308) 0.592(0.357) 0.723(0.468) 0.714(0.454)\nCapgmyo\nLogReg 0.553(0.495) 0.628(0.544) 0.709(0.621) 0.704(0.62)\nCapgmNet 0.492(0.339) 0.530(0.351) 0.577(0.367) 0.607(0.387)\nB. Experiment 2: Intersession Performance\nIntersession performance is measured as the accuracy from\ntraining on one session and testing on another session of the\nsame subject. In our approach, SAL and LBN are optimized\nduring an adaptation step, which is compared against fine-\ntuning. We expected that since the data from Capgmyo is not\na regular grid (variable interelectrode distance), assumptions\nmade by SAL would distort the images and not be as beneficial\ntowards performance, whereas the regular grids used in CSL\nmake it the ideal SAL test case. Performances reported use\nmajority voting over the full active segment for CSL and\n150ms window for Capgmyo, as in [5]. Non majority-voted\nperformances are reported in parentheses.\nFrom Table II, as expected, while all models achieved higher\nperformance with fine-tuning for Capgmyo, for the regular grid\ndata in CSL the optimal performance was achieved with SAL,\ndespite using orders of magnitude less parameters. Further-\nmore, when using SAL, it can be clearly seen across models\nthat the use of LBN significantly improves performance over\ntraditional BN, highlighting the flexibility of learning the\noptimal baseline activity to subtract over estimating it directly\nfrom the data. Surprisingly, input dropout proved vital when\nusing SAL, consistently resulting in accuracy improvements\nof over 20%, up to an astonishing improvement of almost\n60% improvement for CapgmyoNet. It is likely that input\ndropout significantly probes the model to depend on different\ninput combinations, reducing its likelihood of overfitting to\nspecific channels. Additionally, while SAL leaves the original\nlearned representation intact, further experiments showed that\nafter fine-tuning, performance on the original training set was\nless than 1% higher than zero-shot intersession, indicative of\ncatastrophic forgetting [10]. Finally, while the CNNs outper-\nform logistic regression, it is promising to see that with SAL\napplying domain adaptation at the input level, even traditional,\ninterpretable classifiers can achieve comparable performance\nto SOTA EMG-based classifiers.\nC. Experiment 3: Model Ablation & Interpretability Assess-\nment\nGiven the results from Experiment 2, we explore the impor-\ntance of different affine coefficients on the final performance\nfor each model and dataset, using input dropout and LBN.\nGiven that the signal array geometry and often orientationare consistent across experiments in both datasets considered,\nespecially in sEMG, we expected that the translation param-\neters would be the most significant out of the 7 learnable\ncoefficients. In addition, since electrode displacements along\nthe circumference of the arm are most detrimental to sEMG\nclassification performance as muscles are mostly distributed\nalong the length of the forearm [6], [9], we expected that\ncorrections of such translations with SAL are likely to yield\nthe largest performance improvements.\nThus, we run an ablation experiment of SAL with four\ndifferent conditions: LBN (no affine), frozen translation coef-\nficients, all SAL coefficients frozen except for translation, and\nkeeping only the circumferential shift parameter learnable. As\nin Experiment II, performances reported use majority voting\nover the full active segment for CSL and 150ms window for\nCapgmyo, as in [5]. Non majority-voted performances are\nreported in parentheses.\nAs shown in Table III, while LBN alone accounts for\nsignificant improvements, SAL accounts for the majority of the\nimprovements with only 7 learnable parameters. Furthermore,\nas expected, only keeping 2 learnable translation parameters\nresults in significantly higher performance than the remaining\n5 affine parameters. In line with past studies, the performance\nfor only circumferential translation (1 affine coefficient) is\nnearly identical to the performance using both translation coef-\nficients. Thus, SAL is able to drastically improve performance\nusing few parameters closely associated with the physical\ncause of performance detriments. While these trends were\nobserved for both datasets, non-translation affine parameters\nseemed to account for a larger proportion of performance with\nCapgmyo than CSL, as they are more useful for distortions\nintroduced by the irregular geometry of Capgmyo grids.\nIV. C ONCLUSION\nWe have shown that SAL can be prepended to any differen-\ntiable model, from a logistic regressor to CNNs, and achieve\nhigher intersession performances than even standard fine-\ntuning with orders of magnitude less parameters for biosignal\narrays. In addition, LBN better accounts for baseline activity\nfluctuations over traditional BN making it particularly well\nsuited for biosignal sensor arrays such as sEMG. Ablation\nexperiments suggest that learnt SAL coefficients are directly\nassociated with the physical causes of the performance detri-\nment. Interestingly, major improvements in sEMG systems by\nonly accounting for circumferential translations suggest the\npossibility of applying the proposed approach to more practical\n1D sensor arrays, such as the one in [1].\nUsing one repetition per class, as in this study, is common-\nplace in sEMG literature [5], [11], with some studies using half\n[12] or even all [5] of the target data for adaptation. However,\nthis becomes impractical for large number of classes. Thus,\ngiven that SAL and LBN apply domain adaptation at an input\nlevel, and electrode displacement and baseline fluctuations\nbeing invariant to class labels, future studies will explore\nwhether our approach can adapt to one repetition of one (or a\nfew) class(es), unlike present domain adaptation approaches.REFERENCES\n[1] D. Sussillo, P. Kaifosh, and T. Reardon, \u201cA generic noninvasive\nneuromotor interface for human-computer interaction,\u201d 2024. [Online].\nAvailable: https://www.biorxiv.org/content/early/2024/07/23/2024.02.23.\n581779\n[2] R. R.-M. Serrano, S. Velasco-Bosom, A. Dominguez-Alfaro, M. L.\nPicchio, D. Mantione, D. Mecerreyes, and G. G. Malliaras, \u201cHigh\ndensity body surface potential mapping with conducting polymer-\neutectogel electrode arrays for ecg imaging,\u201d Advanced Science ,\nvol. 11, no. 27, p. 2301176, 2024. [Online]. Available: https:\n//onlinelibrary.wiley.com/doi/abs/10.1002/advs.202301176\n[3] D. Xiong, D. Zhang, X. Zhao, and Y . Zhao, \u201cDeep learning for emg-\nbased human-machine interaction: A review,\u201d IEEE/CAA Journal of\nAutomatica Sinica , vol. 8, no. 3, pp. 512\u2013533, 2021.\n[4] C. Amma, T. Krings, J. B \u00a8oer, and T. Schultz, \u201cAdvancing\nmuscle-computer interfaces with high-density electromyography,\u201d in\nProceedings of the 33rd Annual ACM Conference on Human Factors in\nComputing Systems , ser. CHI \u201915. New York, NY , USA: ACM, 2015,\npp. 929\u2013938. [Online]. Available: 10.1145/2702123.2702501\u201d \u27e9http:\n//doi.acm.org/10.1145/2702123.2702501\n[5] Y . Du, W. Jin, W. Wei, Y . Hu, and W. Geng, \u201cSurface emg-\nbased inter-session gesture recognition enhanced by deep domain\nadaptation,\u201d Sensors , vol. 17, no. 3, 2017. [Online]. Available:\nhttps://www.mdpi.com/1424-8220/17/3/458\n[6] L. Hargrove, K. Englehart, and B. Hudgins, \u201cA training strategy\nto reduce classification degradation due to electrode displacements\nin pattern recognition based myoelectric control,\u201d Biomedical Signal\nProcessing and Control , vol. 3, no. 2, pp. 175\u2013180, 2008, surface\nElectromyography. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S1746809407001012\n[7] H. Kim and C.-H. Im, \u201cInfluence of the number of channels and\nclassification algorithm on the performance robustness to electrode shift\nin steady-state visual evoked potential-based brain-computer interfaces,\u201d\nFront. Neuroinform. , vol. 15, p. 750839, Oct. 2021.\n[8] F. Wang, M. Mao, L. Duan, Y . Huang, Z. Li, and C. Zhu, \u201cIntersession\ninstability in fnirs-based emotion recognition,\u201d IEEE Transactions on\nNeural Systems and Rehabilitation Engineering , vol. 26, no. 7, pp. 1324\u2013\n1333, 2018.\n[9] J. Pereira, D. Halatsis, B. Hodossy, and D. Farina, \u201cTackling electrode\nshift in gesture recognition with hd-emg electrode subsets,\u201d in ICASSP\n2024 - 2024 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , 2024, pp. 1786\u20131790.\n[10] U. Cote-Allard, C. L. Fall, A. Drouin, A. Campeau-Lecours, C. Gosselin,\nK. Glette, F. Laviolette, and B. Gosselin, \u201cDeep learning for electromyo-\ngraphic hand gesture signal classification using transfer learning,\u201d IEEE\nTransactions on Neural Systems and Rehabilitation Engineering , vol. 27,\nno. 4, p. 760\u2013771, 2019.\n[11] X. Fan, L. Zou, Z. Liu, Y . He, L. Zou, and R. Chi, \u201cCsac-net: Fast\nadaptive semg recognition through attention convolution network and\nmodel-agnostic meta-learning,\u201d Sensors , vol. 22, no. 10, 2022. [Online].\nAvailable: https://www.mdpi.com/1424-8220/22/10/3661\n[12] I. Ketyk \u00b4o, F. Kov \u00b4acs, and K. Z. Varga, \u201cDomain adaptation for semg-\nbased gesture recognition with recurrent neural networks,\u201d in 2019\nInternational Joint Conference on Neural Networks (IJCNN) , 2019, pp.\n1\u20137.\n[13] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\n\u201cSpatial transformer networks,\u201d 2016. [Online]. Available: https:\n//arxiv.org/abs/1506.02025\n[14] P. Goyal, P. Doll \u00b4ar, R. B. Girshick, P. Noordhuis, L. Wesolowski,\nA. Kyrola, A. Tulloch, Y . Jia, and K. He, \u201cAccurate, large minibatch\nSGD: training imagenet in 1 hour,\u201d CoRR , vol. abs/1706.02677, 2017.\n[Online]. Available: http://arxiv.org/abs/1706.02677"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract\nThe paper explores using machine learning (ML) to automate and optimize feature extraction from in-situ reflection high-energy electron diffraction (RHEED) data for nanomaterial synthesis. By developing a set of features applicable to a wide range of materials, the authors demonstrate the capability of ML to predict grain alignment and dopant concentration. The methods lead to significant time savings, reducing the synthesis and analysis cycle by up to 80% during a 100-sample synthesis campaign.\n\n### 2. Introduction\nThe demand for advanced materials discovery is increasing as Silicon-based electronics approach the limits of Moore's Law. Traditional material synthesis is time-intensive, spanning over a decade from research to commercialization. Although computational methods like density functional theory (DFT) are effective in identifying potential materials, applying these findings experimentally involves substantial effort. The study aims to expedite the synthesis and optimization process using ML techniques that analyze RHEED data to predict film quality and dopant concentration in nanomaterials, exemplified by W1-xVxSe2 grown on sapphire substrates. By automating feature extraction and applying predictive models, the work aims to provide near-real-time feedback and accelerate the material synthesis feedback loop.\n\n### 3. Related Work\nThe paper builds on the success of ab-initio physical simulations and ML applications in material science, including interatomic potentials and generative models. Previous studies have shown ML\u2019s usefulness in processing RHEED data but were limited by manual parameter tuning and system-specific fitting. This work advances the state of the art by creating generalizable ML models that can directly provide actionable predictions to improve synthesis efficiency and yield.\n\n### 4. Method\n#### Data Collection and Preprocessing\n- **RHEED Data Acquisition**: Data is collected both pre-growth (from substrates) and during film growth.\n- **Featurization Pipeline**: Utilizes a U-Net architecture for initial segmentation followed by a transformer-based model for segmentation tuned for low-contrast medical images. Key features include intensity maxima, width, and location of diffraction spots.\n- **Automated Workflow**: Data is transferred, cropped, and segmented without manual intervention, and features are extracted swiftly to feed into predictive models.\n\n#### Predictive Modeling\n- **Classification and Regression**: Logistic regression with bootstrap aggregation is used for grain alignment prediction. Linear regression with bootstrap aggregation assesses dopant concentration. Each method average results from multiple fits to mitigate overfitting given the small dataset sizes.\n\n### 5. Experimental Setup\n#### Datasets\n- **Training Data**: Small sets (~10-14 samples) consisting of expert-labeled RHEED images and XPS measurements.\n- **Validation**: New samples periodically added to assess forward prediction ability and model robustness.\n\n#### Hardware and Software\n- **Hardware**: MBE system for deposition, REED detector with specific settings (13 kV, 1.48 A), and XPS for doping measurements.\n- **Software**: Data analysis and ML models run on AtomCloud RHEED analysis software, scikit-learn for correlation models.\n  \n#### Evaluation Metrics\n- **Accuracy**: For grain alignment classification.\n- **R2 Score**: For regression to measure prediction quality of V dopant concentration.\n- **Confusion Matrix & Misclassification Frequency**: To assess classification performance.\n\n### 6. Results and Analysis\n#### Grain Alignment Classification\n- **Classification Accuracy**: Achieved an 80% binary classification accuracy using logistic regression with bootstrap aggregation.\n- **Confusion Matrices**: Show 80% accuracy for both film and substrate RHEED.\n- **Quantitative Probabilities**: Serve as uncertainty metrics correlating well with qualitative assessments, reducing the number of doomed growth trials.\n\n#### Dopant Concentration Regression\n- **Accuracy**: Initially, the model trained on endpoints and midpoints of dopant concentration range shows reliable predictions.\n- **Forward Prediction**: Besides initial small datasets, forward predictions on new samples continue to match XPS-verified results.\n- **Model Features**: Key interpretable features crucial for predictions are identified, like the relative location of intensity maxima and diffraction feature widths.\n\n### 7. Discussion\nThe ML models deliver actionable predictions using minimal initial labeled data, demonstrating a high degree of generalizability and real-time application potential. The synthesized workflows can reduce time and labor costs significantly while improving the precision and success of nanomaterial synthesis processes. Limitations include small dataset sizes and the requirement for high-quality initial labeled data.\n\n### 8. Conclusion\nThe authors present a robust ML-based framework for optimizing nanomaterial synthesis through automated RHEED data analysis. They successfully demonstrate substantial time savings and improved yield predictability. Future work could explore expanding the model's applicability to other material systems and enhancing data integration across more extensive datasets and diverse synthesis conditions.\n\n### Supporting Details\n#### Technical Setup and Reagents\n- **Sapphire Substrate Treatment**: 2-inch c-plane sapphire substrates annealed at 1200\u00b0C for 5 hours.\n- **Thin Film Growth**: MBE system used for co-deposition of V and W, with additional post-deposition annealing at 700\u00b0C.\n\n#### Algorithm Implementation\n- **Pipeline Integration**: Feature datasets generated and accessed via API for seamless training and inference.\n- **Model Execution**: Logistic and linear regression models implemented in scikit-learn with bootstrap aggregation to mitigate overfitting.\n\nThis summary covers all key aspects and details of the academic paper, providing comprehensive guidelines for replicating the study or extending the current work."}, {"role": "assistant", "content": "Predicting and Accelerating Nanomaterials\nSynthesis Using Machine Learning Featurization\nChristopher C. Price,\u2217,\u2020Yansong Li,\u2021Guanyu Zhou,\u2021Rehan Younas,\u2021Spencer S.\nZeng,\u2020Tim H. Scanlon,\u2020Jason M. Munro,\u2217,\u2020and Christopher L. Hinkle\u2217,\u2021\n\u2020Atomic Data Sciences, Boston, Massachusetts 02116, United States\n\u2021Department of Electrical Engineering, University of Notre Dame, Notre Dame, Indiana\n46556, United States\nE-mail: chris@atomicdatasciences.com; jason@atomicdatasciences.com; chinkle@nd.edu\nAbstract\nSolving for the complex conditions of materials synthesis and processing requires\nanalyzing information gathered from multiple modes of characterization. Currently,\nquantitative information is extracted serially with manual tools and intuition, con-\nstraining the feedback cycle for process optimization. We use machine learning to\nautomate and generalize feature extraction for in-situ reflection high-energy electron\ndiffraction (RHEED) data to establish quantitatively predictive relationships in small\nsets (\u223c10) of expert-labeled data, and apply these to save significant time on subsequent\nepitaxially grown samples. The fidelity of these relationships is tested on a representa-\ntive material system (W1-xVxSe2growth on c-plane sapphire substrate (0001)) at two\nstages of synthesis with two aims: 1) predicting the grain alignment of the deposited\nfilm from the pre-growth substrate surface data, and 2) estimating the vanadium (V)\ndopant concentration using in-situ RHEED as a proxy for ex-situ methods (e.g. x-\nray photoelectron spectroscopy). Both tasks are accomplished using the same set of\n1arXiv:2409.08054v1  [cond-mat.mtrl-sci]  12 Sep 2024materials agnostic core features, eliminating the need to retrain for specific systems\nand leading to a potential 80% time saving over a 100 sample synthesis campaign.\nThese predictions provide guidance for recipe adjustments to avoid doomed trials, re-\nduce follow-on characterization, and improve control resolution for materials synthesis,\nultimately accelerating materials discovery and commercial scale-up.\nIntroduction\nDifferentiated and substantial performance requirements for emerging electronics applica-\ntions and the deceleration of Moore\u2019s Law in silicon is driving demand for advanced mate-\nrials discovery, optimization, and scale-up.1Engineering and development of materials plat-\nforms is difficult and time consuming; lab-to-production timelines currently take 10 years or\nlonger, and time-to-market is the primary barrier to commercialization.2Significant progress\nhas been made leveraging ab-initio physical simulations (DFT)3,4and subsequent machine\nlearning (interatomic potentials;5\u20137generative models8,9) to efficiently identify and screen\nstable and synthesizable materials candidates in the first stage of advanced materials devel-\nopment. However, the theoretical assumptions of DFT, including the absence of constraints\nrelevant to synthesis in the real world, results in a large time and effort barrier to the realiza-\ntion of materials after promising targets are identified.10While computational capabilities\nhave taken off, working through synthesis recipe design, process optimization, and iteratively\nimproving materials quality relies on a relatively slow manual and intuition-guided experi-\nmental approach. To address this bottleneck, recent efforts in both software and hardware\nhave made advancements towards fully autonomous synthesis and optimization within the\nlab.11\u201314Advanced tools in machine learning and artificial intelligence have proven incredi-\nbly useful at targeting both interpretation of experimental data, and the subsequent decision\nmaking required as part of feedback loop-based solutions.15\u201317The early versions of these\nautonomous systems have emphasized the importance and challenges of effective and rapid\nmaterials characterization.\n2The primary barrier for synthesis optimization is time, and this is magnified when en-\ngineering precision is required at the nanoscale. While ultra-high vacuum techniques like\nmolecular beam epitaxy (MBE) have highly controlled synthesis environments, the prepara-\ntion, processing, and subsequent characterization of a single sample takes multiple days.18\nSynthesis recipes are highly sensitive, varying across equipment installations and requiring\nre-calibration after tool maintenance, which can extend over weeks. Due to the expense in\ntime and resources consumed per run, it is critical to maximize the information gained and\nchance of success for each trial in both manual and autonomously driven settings. In-situ\ncharacterization captures large volumes of abstract data with high granularity, yet this data\ncannot be analyzed with conventional methods in time to impact a trial in progress. An\nexample is RHEED, frequently used to qualitatively monitor MBE growth19by providing\ninformation on the surface structure of a sample. RHEED images contain a fingerprint of\nthe material surface at a point in time that can take 15 minutes to manually extract for a\nsingle image, while processes can change in seconds and data is generated 10 to 100 times per\nsecond. Recent work has shown that machine learning can process RHEED data,20\u201326but\nthese early demonstrations required manual tuning of hyperparameters, fitting to specific\nmaterials systems or camera settings, or delivery of results after the run is completed. While\nproviding significant post-run insights, these attributes hinder the general predictive capac-\nity to modify or reduce the number of trials in synthesis optimization, since they require\nsignificant system-specific data to be acquired up front.\nIn this work, we develop and demonstrate fully automated and general pipelines us-\ning both supervised and unsupervised machine learning models to rapidly extract rich and\nholistic quantitative fingerprints from RHEED data. We show that these fingerprints can\nspeed up the synthesis feedback loop by constructing predictive models from small datasets\n(\u223c10 samples) of labeled trials to provide relevant feedback from ex-situ analysis using only\nin-situ inputs. These predictive models are demonstrated in two stages of the synthesis\nprocess for the target system, two-dimensional (2D) V-doped WSe2on Al2O3(0001) (sap-\n3phire): 1) evaluating the probability of a substrate to produce grain-aligned film growth,\nand 2) estimating the composition of a dopant in the film before ex-situ x-ray photoelectron\nspectroscopy (XPS) is conducted. For both objectives, the success of the predictive models\ncan save significant time and cost by avoiding doomed trials and reducing the number of\nsteps required to assess the sample. By producing these empirically derived predictions for\nnear-real-time feedback, we show that the synthesis optimization loop can be accelerated,\nleading to higher throughput of material samples with the target characteristics.\nResults and Discussion\nPrecise doping control in 2D materials is a difficult but necessary milestone to achieve for\nthe next generation of power and space efficient semiconductors. Vanadium doping in WSe2\ngives p-type doping with spin polarization, making this system a candidate as a high-mobility\ndilute magnetic semiconductor.27,28It is crucial to control the dopant concentration during\nco-deposition of V and W to minimize domain formation, the key challenge in this ma-\nterials system. Illustrating the limitations of theoretical prediction, reliably synthesizing\ntheoretically stable, uniformly distributed phases requires high-fidelity control of the kinet-\nics connecting the input procedure to the resulting material composition and microstructure.\nEnd-to-end, each trial to map out these relationships can consume over 24 combined hours\nof tool and active operator time, even excluding the time required for sample loading and\nequipment standby (Fig. 1a). Here, we develop a general framework that can be used to\navoid doomed trials and map ex-situ measurements to in-situ characterization in molecular\nbeam epitaxy to save 80% of the time over a 100 trial synthesis campaign.\nAutomated Data Workflows in the Synthesis Context\nThe data workflow for automated characterization analysis is given in Fig. 1b. RHEED\ndata is transferred to a centralized database which groups each piece of input data by the\n4Figure 1: Figure 1: (a) Summary of experimental flows for sample preparation, film growth,\nand characterization. At the beginning and end of MBE deposition, in-situ RHEED is\ncollected and automatically fingerprinted. After synthesis, the sample is transferred to a\nseparate tool for XPS characterization. (b) Summary of flows for synthesis and character-\nization data. Labeled trials are iteratively updated in the database, and correlation fitting\nis performed for the two tasks against the input labels. Next-trial predictions are generated\nwithin 10 seconds. (c) An image of a RHEED pattern of the as-grown film, and (d) the\nfeaturized fingerprint extracted from the image, including the segmented regions of non-zero\nintensity. Quantities for each diffraction feature extracted into the fingerprints, which are\ninput to the empirical correlation models.\n5sample that it corresponds to via a user interface. The two designated tasks are 1) pre-\ndicting whether a growth is likely to lead to aligned or randomly oriented grains (textured\ngrowth) based on the RHEED image of the substrate wafer before deposition starts, and 2)\nestimating the Vanadium doping composition in a deposited film using a RHEED snapshot\nas input. These tasks were designed to solve real challenges encountered over a three-year\nperiod of aiming to synthesize high quality samples in this material system. Qualitative\nlabels on grain orientation and the quantitative results of manual XPS analysis are added\nto the central database for the fully analyzed training samples from initial trials T= 1 to\nTtask, (T1= 14 , T2= 8). The input RHEED images are passed through a featurization\npipeline which extracts, normalizes, and labels diffraction features categorized in Fig. 1d.\nFeaturization proceeds by cropping the image to remove artifacts from the detector and an\nimage segmentation pipeline composed of two models \u2013 a U-Net architecture for RHEED\nproposed by Liang et al.20followed by a transformer-based segmentation model29tuned for\nperformance on low-contrast medical grayscale images.30The original RHEED pattern in\nFig. 1c shows typically diffuse and subtle scattering features that need to be separated from\nthe background, highlighting the challenges of manual analysis and the need for task-specific\nmodels and model combinations for high-fidelity results. Output masks from this segmenta-\ntion pipeline are labeled to identify contiguous diffraction regions and the quantified features\nare computed for each diffraction node (Fig. 1d). A coordinate system using the specular\nspot as the origin is adopted to enable comparison of diffraction features across patterns col-\nlected under different conditions. In the featurization scheme, no hyperparameters are input\nor adjusted across different patterns or materials systems to maximize generalizability and\nenable real-time result generation without intervention. The featurized RHEED datasets\ntake 10 seconds to generate per frame, and are served through an application programming\ninterface (API) to generate predictions from the correlative models derived from the task\nspecific training samples; details in the Methods section.\n6Independent Classification of Film Crystallinity from Film and Sub-\nstrate RHEED\nGiven the critical role of crystallinity in material synthesis and downstream device perfor-\nmance, our first task is to classify and predict the eventual grain alignment of deposited\nfilms based on in-situ RHEED characterization. In the case of 2D chalcogenide growth,\nepitaxial alignment is particularly susceptible to the surface topography of the sapphire sub-\nstrate.31Surface reconstructions (1x1 Al-terminated or (\u221a\n31 x\u221a\n31)R9 ) supporting aligned\ngrowth are achieved by thermal annealing, but significant variance in the results exists due\nto coupling of the annealing procedure with the individual wafer and furnace conditions.\nEvaluating whether a substrate will lead to aligned grains prior to deposition, and if a de-\nposited film is aligned, can avoid doomed efforts that lead to low quality samples and save\ntime. Figure 2 shows the classification results based on the featurized RHEED datasets and\nan initial label set from visual inspection which placed as-grown films in either a textured or\naligned category. Fig. 2a shows examples of strongly aligned (top) and textured (bottom)\nWSe2films, and Fig. 2b-c gives the baseline classification results for the deposited WSe2\nfilms. The classifier is a logistic regressor wrapped with bootstrap aggregation (bagging),\na technique that resamples the training data with replacement to get an overall model less\nprone to overfitting. We restrict to small training datasets to mimic the typical data avail-\nability in the early stages of a synthesis effort and maximize the ability to provide guidance\nfor a subsequent trial. The confusion matrix in Fig. 2b gives the binary grain alignment\nprediction accuracy of 80%; further details of the bagging procedure are given in Methods .\nThe classification probabilities for grain-aligned films are plotted in Fig. 2c along with the\nmisclassification frequency for each sample. The probability of classification serves as an\nuncertainty metric and a quantitative approximation for the degree of overall grain align-\nment. Some samples are always misclassified when held out of the training set, including\nthe canonically textured film 9. This is a limitation of the small dataset size; when film 9 is\nnot included in fitting, the signal identified to correlate with textured films changes signif-\n7icantly. This highlights the importance of including strong examples of target phenomena\nin the fitting set, and the strength of the correlative approach to interpolate and provide\nguidance on examples that are ambiguous. Overall, the results indicate that the RHEED\nfeatures contain enough signal to automatically match the expert-identified trends in the\nlabels with a small set of examples. Automating this task removes operator bias from data\nanalysis, and quantification helps set thresholds for films which meet the quality criteria for\nsubsequent device fabrication. However, additional operator time, tool time, and resources\ncould be saved by avoiding low quality film growths before they occur.\nNatural variance with substrates and precursors can lead to unexpected growth outcomes,\neven if the same recipe is programmatically followed; in this WSe2system deposition alone\nconsumed up to 8 hours, and average all-in synthesis times can range from 5 to 14 hours\nfor MBE.32Using the same data infrastructure and feature extraction as for the WSe2\nfilms, we perform an identical fitting procedure using the pre-deposition sapphire RHEED\npatterns as inputs instead of the as-grown films, with results in Fig. 2d-f. Fig. 2d similarly\nshows the different surface reconstructions of the sapphire that can lead to aligned (top)\nor textured (bottom) growth. The logistic regression classifier with bootstrap aggregation\nachieves an accuracy near 80%, similar to the results from the WSe2RHEED, although the\nmost-misclassified samples differ than those using the film RHEED. For the misclassified\nsamples in Fig. 2c, the quantitative probability score is close to 50%, indicating greater\nuncertainty of prediction. We show that the quantitative classification probability for both\ndatasets correlates with qualitative assessment with a detailed view of sample 12 in Fig. S1;\nthis sample is labeled as grain-aligned but shows a lower classification probability for both\nthe substrate and the film. In the sapphire RHEED data for sample 12, several features of\nthe (\u221a\n31 x\u221a\n31)R9 reconstruction are missing compared to the aligned-producing substrate\n7, and the Kikuchi lines are better matched with the textured-generating substrate of sample\n9. In the as-deposited film, the pattern is consistent with an aligned growth, but there are\nidentified small features that are signatures of the textured films. This indicates that the\n8Figure 2: Figure 2: (a) Segmented RHEED patterns for examples of aligned (top) and\ntextured (bottom) WSe2film growth. Labels in the bottom left correspond to sample num-\nber. (b) Confusion matrix and classification accuracy for a logistic regression model fit with\nbootstrap aggregation to a set of 14 samples of featurized WSe2patterns. (c) Probability\nof aligned growth predictions by sample (scatters) and frequency of misclassification (bars)\nfor the WSe2RHEED data. (d) Segmented RHEED patterns for examples of sapphire sub-\nstrates that led to aligned (top) and textured (bottom) film growths. (e) Confusion matrix\nand classification accuracy for the same model structure in (b), fit to the substrate RHEED\ninstead of the film RHEED against the film labels. (f) Same as (c) for the sapphire substrate\npattern classification task.\n9classification probability contains some information about the quantified degree of likely grain\nalignment from the substrate that appears in the deposited film, providing a new resource\nfor decision making on whether to proceed with a growth on a given pre-treated substrate.\nIf the likelihood of achieving high-quality growth is deemed low, researchers have the option\nto either perform additional pre-growth treatments or switch to a different substrate, rather\nthan proceeding with what may be a fruitless attempt.\nMapping Ex-Situ Measured Composition to In-Situ RHEED\nWith a roadmap established to improve the yield of film growth with the correct microstruc-\nture, we turn to optimization and control of film composition by estimating dopant concentra-\ntion without removing the sample from the growth chamber. Conventionally, dopant concen-\ntrations are determined ex-situ by characterization techniques like XPS and energy-dispersive\nx-ray spectroscopy (EDX) after the entire growth session, requiring time-consuming sample\nrelocation. The scattering factors of different elements create intensity modulations and\nshifts in the RHEED pattern, but this information is difficult to assess directly since it is\ntightly convolved with other diffraction mechanisms.33\u201335If compositional feedback can be\nquickly generated and delivered from in-situ inputs like RHEED, it would allow for finer\nand immediate control over compositional doping and make adjustments to a sample during\nprocessing. The relationships between composition, tool parameters, and growth recipe are\nnonlinear for even highly controlled synthesis environments such that fully mapping design\nspace requires extensive trial and error. If compositional feedback can be quickly generated\nand delivered from in-situ inputs, it becomes possible to have finer control over compositional\ndoping and make adjustments to a sample during processing.\nFigure 3 presents a design of experiments simulating the early stages of a synthesis\ncampaign for the model system (W1-xVxSe2) with the goal of assessing the vanadium doping\ncomposition in-situ. Labels are drawn from XPS analysis of the V-2p peak (Fig. 3d). The\ninitial training set of labeled examples corresponds to two undoped films, a sample measured\n10Figure 3: Figure 3: (a-c) Predicted vs. actual plots for vanadium doping composition in\nW1-xVxSe2assessed by XPS measurement (x-axis) and predicted from RHEED features (y-\naxis) across a simulated design of experiments from T= 4 to T= 6 . Orange points are\nused for fitting the RHEED proxy models, blue points are test data; dots and x\u2019s show\npredictions made from two high symmetry RHEED angles. In (b) and (c), a random test\npoint is added to the training set to simulate another sample growth. The quality of the\nRHEED prediction increases with each additional sample. Red points in (c) indicate freshly\ngrown sample prediction. (d) XPS data used to generate the labels for each sample. (e)\nInterpretation of the model predictions; the features with the largest coefficients are the\nrelative specular spot intensity maxima location, the relative position of the first order spots,\nand the width of the second order spots.\n11at x=0.16, and a sample measured at x = 0.35 (Fig. 3a, orange). This corresponds to the\nlimits of the desired doping window and the midpoint. Due to the minimally small number of\nlabeled samples, we apply linear regression with bootstrap aggregation to look for correlations\nbetween the featurized RHEED and the XPS quantification while minimizing overfitting.\nThe blue points show the predictions for samples held out of the fitting to simulate forward\nprediction in a real synthesis scenario, with the XPS derived labels on the x-axis. Points\nthat fall on the black line indicate a perfect match between the XPS composition result and\nthe RHEED-predicted composition result, and the R2value in the top left quantifies the\naverage agreement across all of the training and testing samples. In panel b(c), one (two)\ntest point(s) is (are) added into the training set to measure the change in model performance\nwith additional samples labeled with XPS. Identical fitting procedures were conducted on\ntwo independent RHEED series collected on the same samples but at two different azimuthal\nangles separated by 30\u25e6(dots and x\u2019s) to reduce the likelihood that identified correlations\nare due to artifacts of data collection. The red point in Fig. 3c represents the prediction on\na brand new sample excluded from all training and testing recently synthesized to test the\nefficacy of true forward prediction and remove implicit bias - details in Fig. S2.\nUsing a design-of-experiments approach, where samples at the endpoints and midpoint of\nthe target composition range are collected first and labeled with XPS, we show that substan-\ntial predictive capability can be uncovered from a small initial dataset. From figure 3a-c, the\naccuracy of V-composition prediction increases with each additional fitting point. There are\nno substantially outlying predictions, a point which is important in practice for using these\nmodels as surrogates for ex-situ characterization and in-chamber guidance. Across the three\nscenarios of training and testing, there is substantial agreement in the resulting predictions\nfrom both angles of RHEED collection, indicating that the V-doping correlated scattering\nchanges are reliable and not an artifact of data collection. The predictions for the three\nundoped films, while non-zero and non-uniform, preserve the relative trend in composition\nand are clustered together among three distinct samples. We interpret the features that\n12contribute the most to the model correlation using the magnitude of the slope coefficients\naveraged over the bootstrapped estimators (Fig. 3e). In panel (e), the most important\nfeatures for each angle are shown - the relative location of the intensity maxima within the\nspecular spot, the width of several of the first and second order diffraction features, and the\narea of the specular region classified as spot-like.20Feature importance is an entrypoint for\ndeeper physics-based data analysis, indicating where non-random variance is greatest and\nconsequently which structural mechanisms may be correlated with the target property.\nConclusions\nWe demonstrate that machine learning models tailored for RHEED data can be used to\nextract high-fidelity feature sets that reveal rich relationships across materials systems within\nsmall sets of supplied labels. The predictions from these trained models can be used to\navoid synthesis trials likely to fail based on substrate conditions, reduce the amount of ex-\nsitu characterization work, and provide real-time feedback on properties not traditionally\naccessible in the synthesis environment. Since the number of samples for an individual\nresearch project can be on the order of hundreds, and fine tuning materials processing to\nimprove yield can be on the order of many thousands, the delivery of these predictions using\ndedicated data infrastructure can potentially save thousands of expert hours in material\npreparation and data analysis.\nMethods\nSapphire Substrate Treatment\n2-inch c-plane sapphire substrates (0001) (Cryscore) with intentional miscut of C off A by\n0.2\u25e6were pre-growth annealed at 1200\u25e6C for 5 hours in a tube furnace (Thermco Furnace)\ninside a class 100 cleanroom. During the annealing, the nitrogen gas continuously flowed at\n133 L/min through the furnace under atmospheric pressure.\nVanadium-doped WSe 2Thin Film Growth\nThe W1-xVxSe2thin films were grown on the pre-treated c-plane sapphire substrates (Cryscore)\nin an MBE system (Scienta Omicron) with a base pressure of \u02dc10-10mbar. Prior to the depo-\nsition, the sapphire substrate was degassed at 900\u25e6C for 1 hour. Then the thin films were\ngrown at 550 \u00b0C by co-depositing V from an effusion cell, Se evaporated from an effusion\ncell with a cracker zone, and W from an electron-beam evaporator. A relatively high Se flux\nof 1.0 \u00d710-6mbar, measured by beam flux monitor, was used to minimize Se vacancies in\nthe films. After the co-deposition, the samples were still exposed to the same Se flux and\nannealed at 700 \u00b0C for 2 hours. To monitor the growth process, in-situ RHEED (Scienta\nOmicron) was operated with an acceleration voltage of 13 kV and a current of 1.48 A.\nX-ray Photoelectron Spectroscopy\nAfter growth, the samples were transferred directly to the integrated XPS chamber without\nbreaking the vacuum. XPS measurements were carried out using an Al K- \u03b1X-ray source\n(1486.7 eV) under a background pressure of 2.6 \u00d710-9mbar. An electron neutralizer was\nturned on during the measurement to compensate for charging effects caused by the insulat-\ning sapphire substrate.\nData Analysis\nRHEED featurization datasets were generated using AtomCloud RHEED analysis software\nand accessed via API for training and inference. Correlation models are taken from the\nscikit-learn python package. For substrate classification and inference, principal compo-\nnent analysis (PCA) is performed on the training dataset, and bootstrap aggregation with\n100 estimators is used around a base logistic regression, with metrics averaged over 128\nindependent trials with different held out data. The same fit components are used to trans-\n14form data for inference. For dopant composition estimation, no PCA preprocessing is used\nbefore bootstrap aggregation around the linear regressor. PCA is used with the sapphire\ndataset as the sapphire patterns contain significantly more diffraction features, and PCA\ncompresses the dataset to give more consistent results from the logistic regression fitting.\nThis was done to best simulate the accuracy of the next random prediction given a ran-\ndom prior distribution, including class imbalance. Classifiers are never combined between\nmodel runs to avoid test leakage. For the task 2 (XPS regression), the same fitting proce-\ndure is followed using 50 estimators, due to the reduced combinatorics of the input dataset.\nJupyter notebooks used for the correlative modeling are available with live data integration\nat https://github.com/atomic-data-sciences/api-client/tree/main/examples.\nAcknowledgement\nThis work was supported in part by SUPREME, one of seven centers in JUMP 2.0, a\nSemiconductor Research Corporation (SRC) program sponsored by DARPA. This work was\nalso supported in part by the DMREF program of the National Science Foundation (NSF)\nthrough the Division of Materials Research (DMR) Awards No. 2324172 and 1921818.\nSupporting Information Available\nExample of film and substrate RHEED patterns showing incomplete reconstruction and grain\nalignment; details of forward predictive inference for film composition on samples synthesized\nafter model fitting.\nReferences\n(1) Kim, K. S.; Kwon, J.; Ryu, H.; Kim, C.; Kim, H.; Lee, E.-K.; Lee, D.; Seo, S.;\nHan, N. M.; Suh, J. M.; Kim, J.; Song, M.-K.; Lee, S.; Seol, M.; Kim, J. The fu-\n15ture of two-dimensional semiconductors beyond Moore\u2019s law. Nature Nanotechnology\n2024 ,19, 895\u2013906, Publisher: Nature Publishing Group.\n(2) Maine, E.; Seegopaul, P. Accelerating advanced-materials commercialization. Nature\nMaterials 2016 ,15, 487\u2013491, Publisher: Nature Publishing Group.\n(3) Hegde, V. I.; Borg, C. K. H.; Del Rosario, Z.; Kim, Y.; Hutchinson, M.; Antono, E.;\nLing, J.; Saxe, P.; Saal, J. E.; Meredig, B. Quantifying uncertainty in high-throughput\ndensity functional theory: A comparison of AFLOW, Materials Project, and OQMD.\nPhysical Review Materials 2023 ,7, 053805.\n(4) Yang, R. X.; McCandler, C. A.; Andriuc, O.; Siron, M.; Woods-Robinson, R.; Hor-\nton, M. K.; Persson, K. A. Big Data in a Nano World: A Review on Computational,\nData-Driven Design of Nanomaterials Structures, Properties, and Synthesis. ACS Nano\n2022 ,16, 19873\u201319891.\n(5) Deng, B.; Zhong, P.; Jun, K.; Riebesell, J.; Han, K.; Bartel, C. J.; Ceder, G. CHGNet\nas a pretrained universal neural network potential for charge-informed atomistic mod-\nelling. Nature Machine Intelligence 2023 ,5, 1031\u20131041.\n(6) Chen, C.; Ong, S. P. A universal graph deep learning interatomic potential for the\nperiodic table. Nature Computational Science 2022 ,2, 718\u2013728.\n(7) Batatia, I.; Kov\u00b4 acs, D. P.; Simm, G. N. C.; Ortner, C.; Cs\u00b4 anyi, G. MACE: Higher Order\nEquivariant Message Passing Neural Networks for Fast and Accurate Force Fields.\n2022 ,\n(8) Merchant, A.; Batzner, S.; Schoenholz, S. S.; Aykol, M.; Cheon, G.; Cubuk, E. D.\nScaling deep learning for materials discovery. Nature 2023 ,624, 80\u201385, Number: 7990\nPublisher: Nature Publishing Group.\n(9) Zeni, C. et al. MatterGen: a generative model for inorganic materials design. 2023 ,\n16(10) Lee, A.; Sarker, S.; Saal, J. E.; Ward, L.; Borg, C.; Mehta, A.; Wolverton, C. Machine\nlearned synthesizability predictions aided by density functional theory. Communications\nMaterials 2022 ,3, 1\u201311, Publisher: Nature Publishing Group.\n(11) Choudhary, K.; DeCost, B.; Chen, C.; Jain, A.; Tavazza, F.; Cohn, R.; Park, C. W.;\nChoudhary, A.; Agrawal, A.; Billinge, S. J. L.; Holm, E.; Ong, S. P.; Wolverton, C.\nRecent advances and applications of deep learning methods in materials science. npj\nComputational Materials 2022 ,8, 59.\n(12) Delgado-Licona, F.; Abolhasani, M. Research Acceleration in Self-Driving Labs: Tech-\nnological Roadmap toward Accelerated Materials and Molecular Discovery. Advanced\nIntelligent Systems 2023 ,5, 2200331.\n(13) Szymanski, N. J.; Zeng, Y.; Huo, H.; Bartel, C. J.; Kim, H.; Ceder, G. Toward au-\ntonomous design and synthesis of novel inorganic materials. Materials Horizons 2021 ,\n8, 2169\u20132198.\n(14) Xie, Y.; Sattari, K.; Zhang, C.; Lin, J. Toward autonomous laboratories: Convergence\nof artificial intelligence and experimental automation. Progress in Materials Science\n2023 ,132, 101043.\n(15) Szymanski, N. J. et al. An autonomous laboratory for the accelerated synthesis of novel\nmaterials. Nature 2023 ,624, 86\u201391.\n(16) Lunt, A. M.; Fakhruldeen, H.; Pizzuto, G.; Longley, L.; White, A.; Rankin, N.;\nClowes, R.; Alston, B.; Gigli, L.; Day, G. M.; Cooper, A. I.; Chong, S. Y. Modu-\nlar, multi-robot integration of laboratories: an autonomous workflow for solid-state\nchemistry. Chemical Science 2024 ,15, 2456\u20132463.\n(17) Biswas, A.; Liu, Y.; Creange, N.; Liu, Y.-C.; Jesse, S.; Yang, J.-C.; Kalinin, S. V.;\nZiatdinov, M. A.; Vasudevan, R. K. A dynamic Bayesian optimized active recommender\n17system for curiosity-driven partially Human-in-the-loop automated experiments. npj\nComputational Materials 2024 ,10, 29.\n(18) Ding, L.; Zhang, C.; N\u00e6rland, T. U.; Faleev, N.; Honsberg, C.; Bertoni, M. I. Silicon\nMinority-carrier Lifetime Degradation During Molecular Beam Heteroepitaxial III-V\nMaterial Growth. Energy Procedia 2016 ,92, 617\u2013623.\n(19) Hasegawa, S. Characterization of Materials ; 2012; pp 1\u201314, eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/0471266965.com139.\n(20) Liang, H.; Stanev, V.; Kusne, A. G.; Tsukahara, Y.; Ito, K.; Takahashi, R.; Lipp-\nmaa, M.; Takeuchi, I. Application of machine learning to reflection high-energy electron\ndiffraction images for automated structural phase mapping. Physical Review Materials\n2022 ,6, 063805, Publisher: American Physical Society.\n(21) Vasudevan, R. K.; Tselev, A.; Baddorf, A. P.; Kalinin, S. V. Big-Data Reflection High\nEnergy Electron Diffraction Analysis for Understanding Epitaxial Film Growth Pro-\ncesses. ACS Nano 2014 ,8, 10899\u201310908, Publisher: American Chemical Society.\n(22) Kwoen, J.; Arakawa, Y. Classification of Reflection High-Energy Electron Diffraction\nPattern Using Machine Learning. Crystal Growth & Design 2020 ,20, 5289\u20135293, Pub-\nlisher: American Chemical Society.\n(23) Gliebe, K.; Sehirlioglu, A. Distinct thin film growth characteristics determined through\ncomparative dimension reduction techniques. Journal of Applied Physics 2021 ,130,\n125301.\n(24) Yang, J.-H.; Kang, H.; Kim, H. J.; Kim, T.; Ahn, H.; Rhee, T. G.; Khim, Y. G.;\nChoi, B. K.; Jo, M.-H.; Chang, H.; Kim, J.; Chang, Y. J.; Lee, Y.-L.\nhttps://2DMat.ChemDX.org: Experimental data platform for 2D materials from syn-\nthesis to physical properties. Digital Discovery 2024 ,3, 573\u2013585.\n18(25) Provence, S. R.; Thapa, S.; Paudel, R.; Truttmann, T. K.; Prakash, A.; Jalan, B.;\nComes, R. B. Machine learning analysis of perovskite oxides grown by molecular beam\nepitaxy. Physical Review Materials 2020 ,4, 083807, Publisher: American Physical\nSociety.\n(26) Kim, H. J.; Chong, M.; Rhee, T. G.; Khim, Y. G.; Jung, M.-H.; Kim, Y.-M.;\nJeong, H. Y.; Choi, B. K.; Chang, Y. J. Machine-learning-assisted analysis of tran-\nsition metal dichalcogenide thin-film growth. Nano Convergence 2023 ,10, 10.\n(27) Zhang, F. et al. Monolayer Vanadium-Doped Tungsten Disulfide: A Room-Temperature\nDilute Magnetic Semiconductor. Advanced Science 2020 ,7, 2001174, eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202001174.\n(28) Yun, S. J.; Duong, D. L.; Ha, D. M.; Singh, K.; Phan, T. L.; Choi, W.; Kim, Y.-\nM.; Lee, Y. H. Ferromagnetic Order at Room Temperature in Monolayer WSe2\nSemiconductor via Vanadium Dopant. Advanced Science 2020 ,7, 1903076, eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/advs.201903076.\n(29) Kirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson, L.; Xiao, T.;\nWhitehead, S.; Berg, A. C.; Lo, W.-Y.; Doll\u00b4 ar, P.; Girshick, R. Segment Anything.\n2023; http://arxiv.org/abs/2304.02643 , arXiv:2304.02643 [cs].\n(30) Ma, J.; He, Y.; Li, F.; Han, L.; You, C.; Wang, B. Segment anything in medical images.\nNature Communications 2024 ,15, 654, Publisher: Nature Publishing Group.\n(31) Mortelmans, W.; Kazzi, S. E.; Mehta, A. N.; Vanhaeren, D.; Conard, T.; Meerss-\nchaut, J.; Nuytten, T.; Gendt, S. D.; Heyns, M.; Merckling, C. Peculiar alignment and\nstrain of 2D WSe2 grown by van der Waals epitaxy on reconstructed sapphire surfaces.\nNanotechnology 2019 ,30, 465601, Publisher: IOP Publishing.\n(32) He, L.; Wang, S. L.; Yang, J. R.; Yu, M. F.; Wu, Y.; Chen, X. Q.; Fang, W. Z.;\n19Qiao, Y. M.; Gui, Y.; Chu, J. Molecular beam epitaxy (MBE) in situ high-temperature\nannealing of HgCdTe. Journal of Crystal Growth 1999 ,201-202 , 524\u2013529.\n(33) Peng, L.-M. Electron Scattering Factors of Ions and their Parameterization. Acta Crys-\ntallographica Section A: Foundations of Crystallography 1998 ,54, 481\u2013485, Publisher:\nInternational Union of Crystallography.\n(34) Kawamura, T.; Fukaya, Y.; Fukutani, K. Finding RHEED conditions sensitive to hy-\ndrogen position on Pd(100). Surface Science 2022 ,722, 122098.\n(35) Pawlak, J.; Przybylski, M.; Mitura, Z. An Analysis of Kikuchi Lines Observed with a\nRHEED Apparatus for a TiO2-Terminated SrTiO3 (001) Crystal. Materials 2021 ,14,\n7077.\nFor Table of Contents Only\n20Supporting Information: Predicting and\nAccelerating Nanomaterials Synthesis Using\nMachine Learning Featurization\nChristopher C. Price,\u2217,\u2020Yansong Li,\u2021Guanyu Zhou,\u2021Rehan Younas,\u2021Spencer S.\nZeng,\u2020Tim H. Scanlon,\u2020Jason M. Munro,\u2217,\u2020and Christopher L. Hinkle\u2217,\u2021\n\u2020Atomic Data Sciences, Boston, Massachusetts 02116, United States\n\u2021Department of Electrical Engineering, University of Notre Dame, Notre Dame, Indiana\n46556, United States\nE-mail: chris@atomicdatasciences.com; jason@atomicdatasciences.com; chinkle@nd.edu\nFigure S1: Zoomed-in view of an aligned growth which demonstrated a lower probability of\nclassification. The substrate RHEED (b) shares features of both strongly predicted aligned\nand textured examples in Fig. 2d and small texturing features are visible in the film RHEED\n(a), consistent with the lower probability of the aligned prediction.\nPredictive Sample Inference\nWe tested the forward generalizability of all three inference models on a freshly prepared\nW1-xVxSe2sample grown 3 months after the latest sample data and 3.5 years after the\n1arXiv:2409.08054v1  [cond-mat.mtrl-sci]  12 Sep 2024Figure S2: (a) Inference results using the models from Fig. 2 and Fig. 3c on the most recent\nset of substrate and film data, collected 3.5 years after the earliest sample in the training\nset.\nearliest sample data used to develop the models in this study, without retraining the core\nfeaturization scheme or any of the correlative models (Fig. S2). This removes any implicit\nbias introduced in the train, test, and hyperparameter optimization, and extends the method\napplication to clean synthesis. From the substrate RHEED data alone, our inference scheme\npredicted that aligned growth would be achieved with 63% probability. Inference from our\nfilm quality classifier on the deposited film matched this prediction and was verified by vi-\nsual inspection of the deposited film RHEED. The same RHEED features across two angles\nwere given to the compositional regression model, with the two RHEED angle predictions\naveraging to a predicted doping level of x=0.09. Subsequent XPS analysis for testing gave\na manual composition of x=0.11. Points are plotted in conjunction with the original train-\ning and testing data in Fig. ??c. Without modifying any data infrastructure, all of the\npredictions were made within 15 seconds of RHEED capture, indicating this framework can\nprovide real-time feedback within the timescales of dynamic advanced materials synthesis.\n2"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary of the Machine Learning Paper \"Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking\"\n\n---\n\n#### 1. **Abstract:**\nThe paper demonstrates that with the ability to jailbreak a Generative AI (GenAI) model, attackers can significantly escalate the severity and scale of attacks on Retrieval-Augmented Generation (RAG)-based GenAI applications. Specifically, it introduces methods to escalate RAG membership inference and entity extraction attacks to full document extraction, and proposes a self-replicating adversarial prompt capable of propagating a worm through an entire GenAI ecosystem, thereby expanding the scale of data poisoning attacks. The paper evaluates these methods in realistic scenarios and discusses effective guardrails against such attacks.\n\n---\n\n#### 2. **Introduction:**\n- **Context:** The research focuses on Generative AI models integrated with Retrieval-Augmented Generation (RAG) to enhance information accuracy using external knowledge bases.\n- **Research Questions:** The primary investigation is into the security risks posed by jailbroken GenAI models in RAG-based applications, specifically focusing on two key areas:\n  1. Escalation of RAG membership inference and entity extraction attacks.\n  2. Propagation of a worm through a GenAI ecosystem via adversarial self-replicating prompts.\n- **Literature Review:** The introduction reviews existing work on RAG-based inference attacks including membership inference, entity extraction, and poisoning attacks, pointing out the novel angle of examining the effect of jailbreaking on these attacks.\n\n---\n\n#### 3. **Related Work:**\n- **Membership Inference and Entity Extraction:** Previous works have focused on validating the existence of documents and extracting specific data like phone numbers and emails from RAG-based apps.\n- **Data Poisoning:** Various attacks have been documented, including backdooring and misinformation generation, highlighting the need for more robust defenses in dynamically updated systems.\n- **Worms:** The history and evolution of computer worms are discussed, emphasizing how their concepts can be adapted to propagation within GenAI ecosystems.\n\n### Method:\n\n#### 4. **RAG Documents Extraction Attack:**\n- **Steps:**\n  1. Attacker uses a jailbreaking command `pre`.\n  2. Determines a target embeddings vector `target` by a specific extraction method.\n  3. Uses a Greedy Embedding Attack (GEA) algorithm to identify a suffix `suf` that collides with `target`.\n  4. Queries are crafted using `pre||suf`, prompting the RAG-based system to return documents from the database.\n  5. Extracted documents are analyzed and used for further queries iteratively.\n- **GEA Algorithm:** This black-box collision attack algorithm iteratively modifies the suffix of a prompt to maximize the cosine similarity with a target embedding vector.\n\n### Experimental Setup:\n\n#### 5. **Q&A Chatbot:**\n- **Implementation:** Utilizes LangChain and VectorStores with cosine similarity. The template query uses context documents retrieved by the RAG for composing responses.\n- **Datasets:** Utilizes ChatDoctor-100k from HealthCareMagic consisting of patient-doctor Q&A sessions. 1,000 documents from this dataset are used for evaluation.\n- **Evaluation Metrics:** The primary metric is the extraction rate, i.e., the percentage of unique documents recovered from the database.\n\n#### 6. **Results and Analysis:**\n- **Extraction Methods Comparison:** Three methods were compared:\n  1. **Random Draw Oriented Method:** Uses random tokens to craft queries, achieving low extraction rates.\n  2. **English Distribution Oriented Method:** Utilizes embeddings based on English word distribution, providing moderate extraction rates.\n  3. **Adaptive/Dynamic Method:** Continuously refines query embeddings based on previously extracted documents, achieving the highest extraction rates (80.6%).\n- **Context Size Impact:** Larger context sizes (number of documents retrieved) significantly improve extraction rates.\n- **Embeddings Algorithms and Sizes:** Different embeddings algorithms and sizes (like GTE-384, GTE-768, MPNet-768) affect extraction performance. GTE-768 and MPNet-768 reported the highest extraction rates.\n- **GenAI Engine Impact:** Different engines (Gemini 1.0 Pro, Gemini 1.5 Flash, GPT-4o Mini) showed varied document return rates, affecting the overall extraction rate.\n\n### RAG-based Worm:\n\n#### 7. **Adversarial Self-Replicating Prompts:**\n- **Components:** Combines a jailbreaking command `j`, an instruction for replication `r`, and malicious activity instruction `m`.\n- **Malicious Payload:** Designed to survive successive inferences and propagate the worm within the ecosystem.\n\n#### 8. **Experimental Evaluation:**\n- **GenAI Services and API:** Evaluated using Gemini Flash 1.5.\n- **Setup:** Clients implemented using LangChain, RAG with VectorStores and cosine similarity function.\n- **Context Sizes:** Larger context sizes of k= 5, 10, and 20 significantly improved propagation rates.\n- **Number of Hops:** Tested propagation efficiency over multiple hops in the ecosystem, affecting worm's overall reach.\n\n### Discussion:\nThe paper interprets findings with emphasis on:\n- **Impact of Jailbreaking:** Demonstrated how jailbreaking escalates both the severity of data extraction and the scale of attacks.\n- **Guardrails:** Proposed guardrails like access control, human-in-the-loop, and content size limits. Acknowledges trade-offs and current limitations of these solutions.\n\n### Conclusion:\n- **Contributions:** The study extends knowledge of GenAI security to show how jailbreaking can amplify both document extraction from RAG and the scale of attacks using worms.\n- **Future Work:** Recommends exploring robust defenses against adaptive jailbreaking attacks and improving secure design of RAG-based systems.\n\n---\n\nThis summary should allow a researcher to replicate the study or build directly upon it by providing all necessary details of the methodologies, experimental setups, and empirical findings outlined in the paper."}, {"role": "assistant", "content": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against\nRAG-based Inference in Scale and Severity Using Jailbreaking\nStav Cohen, Ron Bitton, Ben Nassi\ncohnstav@campus.technion.ac.il, ron_bitton@intuit.com, nassiben@technion.ac.il, bn267@cornell.edu\nTechnion - Israel Institute of Technology, Intuit, Cornell Tech\nAbstract\nIn this paper, we show that with the ability to jailbreak a\nGenAI model, attackers can escalate the outcome of attacks\nagainst RAG-based GenAI-powered applications in severity\nand scale. In the first part of the paper, we show that attackers\ncan escalate RAG membership inference attacks and RAG\nentity extraction attacks to RAG documents extraction at-\ntacks, forcing a more severe outcome compared to existing\nattacks. We evaluate the results obtained from three extrac-\ntion methods, the influence of the type and the size of five\nembeddings algorithms employed, the size of the provided\ncontext, and the GenAI engine. We show that attackers can\nextract 80%-99.8% of the data stored in the database used by\nthe RAG of a Q&A chatbot. In the second part of the paper,\nwe show that attackers can escalate the scale of RAG data poi-\nsoning attacks from compromising a single GenAI-powered\napplication to compromising the entire GenAI ecosystem,\nforcing a greater scale of damage. This is done by crafting\nanadversarial self-replicating prompt that triggers a chain\nreaction of a computer worm within the ecosystem and forces\neach affected application to perform a malicious activity and\ncompromise the RAG of additional applications. We evaluate\nthe performance of the worm in creating a chain of confiden-\ntial data extraction about users within a GenAI ecosystem of\nGenAI-powered email assistants and analyze how the perfor-\nmance of the worm is affected by the size of the context, the\nadversarial self-replicating prompt used, the type and size of\nthe embeddings algorithm employed, and the number of hops\nin the propagation. Finally, we review and analyze guardrails\nto protect RAG-based inference and discuss the tradeoffs.\n1 Introduction\nGenerative Artificial Intelligence (GenAI) represents a signifi-\ncant advancement in artificial intelligence, noted for its ability\nto produce textual content. However, GenAI models often\nface challenges in generating accurate, up-to-date, and con-\ntextually relevant information, especially when the relevantinformation is not part of their training data. To address this,\nRetrieval-Augmented Generation (RAG) [1] is typically inte-\ngrated into the inference process, allowing the GenAI model\nto access external knowledge sources relevant to the query.\nThis integration greatly enhances the accuracy and reliability\nof the generated content, reduces the risk of hallucinations,\nand ensures the alignment of the content with the most re-\ncent information. Consequently, RAG is commonly integrated\ninto GenAI-powered applications requiring personalized and\nup-to-date information (e.g., personal user assistants) and spe-\ncialized knowledge areas (e.g., customer service chatbots).\nDue to its popular use, researchers started investigating the\nsecurity and privacy of RAG-based inference. Various tech-\nniques have been demonstrated in studies to conduct RAG\nmembership inference attacks (e.g., to validate the existence\nof specific documents in the database used by RAG [2, 3]),\nRAG entity extraction attacks (e.g., to extract Personal Identi-\nfiable Information from the database used by the RAG [4]),\nand RAG poisoning attacks (e.g., for backdooring, i.e., gen-\nerating a desired output for a given input [5, 6], generating\nmisinformation and disinformation [7], blocking relevant in-\nformation [8,9]). These methods shed light on the risks posed\nby user inputs to RAG-based inference. However, with the\nability to provide user inputs to RAG-based GenAI-powered\napplications, attackers can also jailbreak the GenAI model\nusing various techniques (e.g., [10 \u201316]). Therefore, to fully\nunderstand the risks associated with RAG-based inference,\nwe must explore the risks posed by a jailbroken GenAI model.\nIn this paper, we explore the risks posed to RAG-based\nGenAI-powered applications when interfacing with GenAI\nmodels that were jailbroken through direct or indirect prompt\ninjection. In the first part of the paper, we explore the risks\nposed by a jailbroken model to a single GenAI-powered ap-\nplication. We show that with the ability to jailbreak a GenAI\nmodel, attackers can escalate RAG membership inference\nattacks [2, 3] and RAG entity extraction attacks [4] to RAG\ndocuments extraction attack, forcing a more severe outcome.\nBy doing so, attackers could escalate entity-level extraction\n(e.g., phone numbers, emails, names) [4] to document-level ex-\n1arXiv:2409.08045v1  [cs.CR]  12 Sep 2024traction, violating the confidentiality of the GenAI-powered\napplication (similarly to [2 \u20134]) and violating the intellec-\ntual property of the GenAI-powered application using the ex-\ntracted data (as opposed to [2 \u20134]). Moreover, this can be done\nwith no prior knowledge of the data that exists in the database\n(as opposed to RAG membership inference attacks [2, 3] that\nneed to provide the candidate entity/document to the query\nsent to the GenAI-powered application).\nWe discuss the black-box threat model and characterize\nGenAI-powered applications at risk. We present a black-box\ncollision attack against desired embeddings (i.e., a technique\nto create the textual input that yields desired embeddings)\nthat extends a recent work [17]. Based on this technique, we\nconduct an end-to-end evaluation to extract as many docu-\nments from the database of a RAG-based GenAI-powered\nmedical Q&A chatbot (based on ChatDoctor-100k [18]). We\ncompare the results obtained from three extraction methods\nand evaluate how the results are affected by the type and the\nsize of five embeddings algorithms, the size of the provided\ncontext (i.e., the number of documents provided to the GenAI\nengine), and three types of employed GenAI engine.\nIn the second part of the paper, we explore the risks posed\nby a jailbroken GerAI model to GenAI ecosystems that consist\nof RAG-based GenAI-powered applications that interface\nwith each other (e.g., GenAI-powered email applications and\nGenAI-powered personal assistants). We show that when the\ncommunication between applications in the ecosystem relies\non RAG-based inference, a jailbroken GenAI model could\nbe exploited by attackers to send a message that triggers a\nchain reaction of a computer worm within the ecosystem\nand forces each affected application to perform a malicious\nactivity (e.g., distribute disinformation, misinformation, and\npropaganda, or to embarrass users) and propagate to a new\napplication in the ecosystem (compromising the activity of the\nnew application as well). This is done by crafting adversarial\nself-replicating prompts that survive a chain of inferences\nwhile still conducting malicious activity in each inference. By\ndoing so, attackers could escalate RAG poisoning attacks from\na client level to an ecosystem level, amplifying the outcome\nof the attack in scale (as opposed to methods presented attacks\nagainst single GenAI-powered applications [10\u201316]).\nWe discuss the black-box threat model, characterize GenAI-\npowered applications at risk, explain how adversarial self-\nreplicating prompts are used to conduct malicious activity and\npropagate to new clients, and review the steps of the attack.\nWe conduct an end-to-end evaluation of the worm against\nRAG-based GenAI-powered email assistants and assess how\nthe propagation and success rate are affected by the prefix of\nthe email used as the worm, the type and size of five embed-\ndings algorithms, the size of the provided context, the type of\nthe GenAI engine, and the number of hops of the propagation.\nIn the third part of the paper, we review and analyze the\neffectiveness of various guardrails (access control, rate limit,\nthresholding, human-in-the-loop, content size limit, data san-itization) against attacks that target RAG-based GenAI in-\nference [2 \u20139]. Based on the analysis we recommend how to\nsecure RAG-based inference and discuss the tradeoffs. Fi-\nnally, we discuss the limitations of the attacks, review related\nworks, and conclude our findings.\nContributions. (1) We extend the knowledge of security\nand privacy of RAG-based GenAI-powered applications and\nexplore the risks posed by a jailbroken GenAI model. We\nshow that with the ability to jailbreak a GenAI model, attack-\ners can escalate the outcome of attacks against RAG-based\nGenAI-powered application in severity (from entity level to\ndocument level extraction) and in scale (from compromising\na single application to compromising the entire ecosystem).\n(2) To convince the reader regarding the arguments mentioned\nabove, we demonstrate and evaluate two attacks (documents\nextraction attacks and a worm) performed against two GenAI-\npowered applications (a Q&A chatbot and an email assistant)\nin two attack vectors (direct and indirect prompt injection)\nand two types of targets (a single GenAI-powered application\nand a GenAI ecosystem). (3) In the absence of bullet-proof\nmitigation against jailbreaking and adaptive jailbreaking at-\ntacks, we discuss guardrails and policies to minimize the risk\nposed to RAG-based inference by the attacks demonstrated\nin this paper and the attacks presented in related works [2 \u20139].\nStructure. In Section 2, we review related work. We ex-\nplore how a jailbroken GenAI model could be exploited to\nperform RAG documents extraction attack (in Section 3) and\nto unleash a worm that target GenAI ecosystem (in Section\n4). In Section 5 we review and analyze the effectiveness of\nvarious guardrails. In Section 6 we discuss the limitations of\nthe attack and in Section 7 we discuss our findings.\nEthical Considerations, Responsible Disclosure & Open\nScience. The entire experiments we conducted were done in\na lab environment. We did not demonstrate the application\nof the attacks against existing applications to avoid violating\nthe confidentiality and the intellectual property of a GenAI-\npowered application by extracting the database used by its\nRAG and violating the confidentiality of users by unleashing\na worm that exfiltrates sensitive user information into the wild.\nInstead, we demonstrated the attacks against applications that\nwe developed running on real data used by academics: the\nEnron dataset [19] and ChatDoctor dataset [18]. We disclosed\nour findings with OpenAI and Google via their bug bounty\nprograms (attaching the paper for reference). We will pro-\nvide more details when we will receive their response. We\nuploaded our code and dataset to GitHub1to allow repro-\nducibility and replicability of our findings.\n2 Background & Related Work\nBackground. Retrieval-augmented generation (RAG) is a\ntechnique in natural language processing that enhances the\n1https://github.com/StavC/UnleashingWorms-ExtractingData\n2capabilities of GenAI models by incorporating external knowl-\nedge sources in inference time as context for the generation\nprocess. This approach is motivated by the need to improve\nthe accuracy and relevance of generated content, especially\nin complex or dynamic domains where the information may\nchange frequently. The key components of a RAG-based in-\nference system include (1) an embeddings algorithm (e.g.,\nMPNet [20]) used to compress the tokens of the data to a fixed\nsize vector which optimizes the retrieval time, (2) a similarity\nfunction (e.g., cosine similarity) intended to provide a similar-\nity score between two vectors of embeddings generated from\na document and a query, and (3) a database (e.g., VectorDB)\nwhich stores the embeddings of the indexed documents.\nIn inference time, RAG retrieves the most relevant docu-\nments, d1,...dk, based on the similarity score to a user query\nqand uses an input prompt pto combine d1,...dkwith q. For\nexample, p=\"Here is a query from the user: q. Use this\ncontext to answer it: d1,...dk\". Finally, pis provided to the\nGenerative AI engine for inference. RAG is commonly used\nin applications that require up-to-date information, personal-\nized responses, or detailed knowledge.\nRelated Work. The increasing integration of RAG into\nGenAI-powered applications attracted researchers to investi-\ngate the security and privacy of such applications. One line\nof research investigated attacks against the integrity of RAG-\nbased inference, namely RAG poisoning attacks. These stud-\nies explored the various outcomes that could be triggered\nby attackers given the ability to inject (i.e., insert) data into\nthe database used by RAG-based GenAI-powered applica-\ntion including (1) backdooring an application, by causing\nit to generate a desired output for a given input [5, 6, 9],\n(2) compromising the integrity of an application, by caus-\ning it to generate misinformation and disinformation [7], (3)\ncompromising the availability of an application, by block-\ning the retrieval of relevant information [8, 9]. A second line\nof research investigated attacks against the confidentiality\nof RAG-based inference [2 \u20134] divided into two categories:\n(1) membership-inference attacks [2, 3], i.e., validating the\nexistence of a specific entity (e.g., a phone number) or a doc-\nument in the database, and (2) entity extraction attacks [4]\nfrom the database of the RAG, i.e., extracting confidential\nentities (e.g., names, phone numbers, user addresses, emails,\netc.) from the database. In a related topic to the RAG docu-\nment extraction attack we present in this research, a few works\ninvestigated extraction of prompts from GenAI-powered appli-\ncations [21\u201324] and training data from ML models [25\u201327].\nWorms. A computer worm is a type of malware with the\nability to propagate to new computers, often without requir-\ning any user interaction. Computer worms have played a\nsignificant role in the evolution of cyber threats since their\ninception [28 \u201331]. In recent decades, we witnessed a rapid\nproliferation of worms, with the first Internet worm, Morris\nWorm [32 \u201334], in 1988 serving as a notable example that\nhighlighted the potential for widespread damage. As tech-nology advanced, so did the sophistication of worms and the\nversatility of the target hosts, with notable instances like the\nILOVEYOU worm [35, 36] in 2000 that exploited the human\nfactor, the Stuxnet [37 \u201339] in 2010 worm that targeted in-\ndustrial control systems, Mirai [40] in 2016 that target IoT\ndevices, and WannaCry [41 \u201344] in 2017 that was used to\ndemand ransom from end users.\n3 RAG Documents Extraction Attack\nIn this section, we investigate the risks posed by a jailbroken\nGenAI model to RAG-based GenAI-powered applications.\nWe show that with the ability to jailbreak a GenAI model,\nattackers could escalate RAG membership inference attacks\nand RAG entity extraction attacks from the entity level (i.e.,\nextracting phone numbers, contacts, and addresses) to a docu-\nment level, i.e., extract complete documents from the database\nused by a RAG-based GenAI-powered application.\n3.1 Threat Model\nIn this threat model, the attacker attempts to extract documents\nfrom the database used by the RAG of a GenAI-powered ap-\nplication using a series of queries via direct prompt injection.\nTargets. A RAG-based GenAI-powered application at risk\nof being targeted by an extraction attack via direct prompt\ninjection is an application with the following characteristics:\n(1)receives user inputs : the application is capable of receiv-\ning user inputs (which makes it vulnerable to direct prompt\ninjection) (2) providing automatic feedback to the user : the\nGenAI application provides automatic RAG-based feedback\nto the user on his/her input, (3) allows multiple inferences :\nthe GenAI application allows users to use it repeatedly. We\nnote that many Q&A chatbots satisfy the characteristics men-\ntioned above due to their nature of receiving questions from\nusers and replying to them using RAG-based inference.\nAttacker Objective. We consider the attacker to be a mali-\ncious entity with the desire to extract data from the database\nused by RAG-based GenAI-powered applications. The at-\ntacker can be any user of a RAG-based Q&A chatbot. The\nobjective of the attacker can be to (1) embarrass or identify\nusers based on information that exists in the extracted docu-\nments, and (2) violate the intellectual property of a paid Q&A\nchatbot (e.g., customer support, medical chatbots, legal au-\ntomation chatbots) by developing its paid application based\non the data extracted from the database of the paid Q&A\nchatbot.\nAttacker Capabilities. We assume the attacker knows the\nembeddings algorithm used to index the data in the RAG and\nhas black-box access to the algorithm. We do not assume any\nprior knowledge of the distribution of the data stored in the\ndatabase of the RAG-based GenAI-powered application.\nSignificance. (1) Our threat model is lighter than RAG\nmembership inference attacks [2,3] and RAG entity extraction\n3attacks [4] because it does not assume any prior knowledge of\nthe data stored in the database. Our threat model is significant\nwith respect to RAG membership inference attacks [2, 3] that\nrequire the attacker to find a set of candidates of entities/docu-\nments in advance, so he/she can provide the candidates to the\nGenAI model for validating their existence in the database\n(for example, in [2], they used the following query to vali-\ndate the existence of a document d:\u201cDoes this: dappear in\nthe context? Answer with Yes or No\" ). Our threat model is\nsignificant with respect to RAG entity extraction attacks [4]\nthat require the attacker to know the type of data in advance\n(e.g., PIIs, medical information, etc.) so he/she can design\ndedicated queries manually (for example, in [4], they used\nthe following query to extract a phone number: \u201cMy phone\nnumber is\" ). (2) The outcome of the attack is more severe : it\nrecovers complete documents from the RAG. This outcome\nis significant with respect to RAG entity extraction attacks [4]\nthat extract specific entities (names, phone numbers). There-\nfore, by employing RAG extraction attacks, attackers violate\nthe confidentiality of the data and its intellectual property\n(they can use the data to create their own applications).\n3.2 Attack Steps\nThe objective of the attacker is to recover as many documents\nfrom the database used by the RAG-based GenAI-powered\napplication. The attacker aims to craft an input text whose\nembeddings will collide with a desired set of embeddings of\ndocuments stored in the RAG (and therefore will be retrieved\nby the RAG during inference). By repeatedly returning this\naction with different inputs (that are similar to unique sets\nof documents stored in the database), the attacker triggers\nunique retrievals of documents by the RAG which are forced\nto be returned to the attacker by the jailbroken GenAI engine.\nThe attack consists of the following steps:\n(1) The attacker determines pre, a jailbreaking command\nthat will be used in the prefix of its input to the RAG-based\nQ&A chatbot. Such a jailbreaking command can be found\nover the Internet (according to [10]).\n(2) The attacker determines target , a target embeddings\nvector according to an extraction method he/she uses (we\ncompare various extraction methods in the evaluation).\n(3) The attacker uses a collision algorithm (we dis-\ncuss it in the next subsection) to find su f, a suffix that\nwhen appended to pre, its embeddings vector collides with\ntarget . More formally, given t, a desired similarity score, and\ngiven sim, a similarity function, we consider a collision as:\nsim(embeddings target,embeddings pre||su f)>t.\n(4) The attacker provides pre||su fas input to the Q&A\nchatbot. A retrieval of kdocuments ( d1,...dk) is triggered\nfrom the database based on embeddings pre||su f.d1,...dkare\nprovided in the query (as context) sent by the application to\nthe GenAI engine for inference.\n(5) The jailbreaking command forces the GenAI engine tooutput d1,...dkwhich are provided as an answer to the attacker.\nd1,...dkare added to the attacker\u2019s extraction set. The attacker\nrepeats the steps 2-5.\n3.3 Embeddings Collision Algorithm\nAlgorithm 1 Greedy Embedding Attack (GEA)\n1:Input: pre, suf, target, iterations, randomN, thresh\n2:control_toks \u2190tokenize (suf)\n3:all_tokens \u2190range(tokenizer_length)\n4:best_suffix \u2190suf, best_loss \u2190 \u2212 \u221e\n5:indices \u2190(len(control_toks))\n6:for(j = 0; j< iterations & best_loss<t ; j++) do\n7: shuffle(indices)\n8: foreach i in indices do\n9: current_toks \u2190tokenize (best_suffix)\n10: candidates \u2190sample(all_tokens, randomN)\n11: foreach token in candidates do\n12: new_toks \u2190replace(current_toks, i, token)\n13: perturbed_sentence \u2190concat(pre, new_toks)\n14: embedding \u2190embed (perturbed_sentence)\n15: loss\u21901-cosine_sim(embedding, target)\n16: ifloss<best_loss then\n17: best_loss \u2190loss\n18: best_suffix \u2190new_toks\n19:return best_suffix, best_loss\nThe attacker aims to craft an input text whose embeddings\nwill \"collide\" with a desired set of embeddings of documents\nstored in the RAG. To do so, we extend the method presented\nin [17] and present a black-box-based collision attack capable\nof generating a desired input text for a given target embed-\ndings.\nThe Greedy Embedding Attack (GEA) algorithm aims\nto modify the suffix of a text to make its embedding as close\nas possible to a target embedding. It starts by tokenizing the\ninitial suffix (line 1) and generating a list of all possible to-\nkens from the tokenizer\u2019s vocabulary (line 2). The best suffix\nand loss are initialized (line 3), and a list of token indices is\ncreated (line 4). The algorithm runs for a specified number\nof iterations or until a similarity threshold is reached (line 5).\nIn each iteration, the token indices are shuffled (line 6). For\neach position, random candidate tokens are sampled from the\ntokenizer\u2019s vocabulary (lines 7-9). Each candidate replaces\nthe current token (line 11), and the modified suffix is com-\nbined with the prefix (line 12). The similarity between the new\nembedding and the target is measured using cosine similarity\n(lines 13-14). The best suffix and loss are updated if the new\nembedding is closer to the target (lines 15-18). This process\ncontinues until the best possible match is found, optimizing\nthe suffix for the closest embedding similarity to the target.\nWe note that in a standard RAG-based inference, the doc-\numents that yield the top-k similarity scores with the given\n4input are retrieved and provided to the GenAI engine. There-\nfore, the ability to control the embeddings of the input allows\nattackers to control the retrieval from the database and the\nextraction of the documents. This should potentially minimize\nthe number of queries used for the extraction compared with\na random draw of words for a query. We note that due to the\nfact that the inputs provided to the Q&A chatbot start with a\njailbreaking command, we only perturb the suffix of the input\nso the entire input (the jailbreaking and the suffix) will collide\nwith the desired embeddings vector.\n3.4 Evaluation\nHere we compare the results obtained from three extraction\nmethods and evaluate how the results are affected by the type\nand the size of five embeddings algorithms, the size of the\nprovided context (i.e., the number of documents provided to\nthe GenAI engine), and the GenAI engine employed.\n3.4.1 Experimental Setup\nThe Q&A Chatbot. We implemented the Q&A medical chat-\nbot using the code provided here2. The client was imple-\nmented using LangChain and the template of the query that\nthe client used to generate a query for the GenAI engine can\nbe seen in Listing 1.\nYou are a medical Q&A bot.\nYou have to answer the next question:\n{\"Question\"}.\nUse the following context to answer it:\n{Q&A-1}, Q&A-2}, ..., {Q&A-k}\nAnswer:\nListing 1: The template of the medical Q&A chatbot.\nThe RAG was implemented using VectorStores with Cosine\nas a similarity function.\nData. We utilized the chatdoctor-dataset [18]. This is\nan open dataset uploaded to HuggingFace that consists\nof real conversations between patients and doctors from\nHealthCareMagic .com. This dataset is used in science to\ncompare the performance of medical Q&A chatbots [18] cre-\nated by (1) fine-tuning GenAI models using the dataset and\n(2) providing the relevant data from the dataset using RAG\nas context to a GenAI model that was not fine-tuned. We ran-\ndomly selected 1,000 documents from chatdoctor-dataset and\nadded them to the database used by the Q&A chatbot. We\nused these documents as the target dataset for extraction in\nour experiments.\n3.4.2 Metrics\nWe evaluate the performance of the attack using this metric:\n2https://towardsdatascience .com/retrieval-augmented-\ngeneration-rag-from-theory-to-langchain-implementation-\n4e9bd5f6a4f2Extraction rate . A 0-100.0 score that represents the per-\ncentage of unique documents that were extracted from the\ndatabase. This score is calculated as the number of unique\nextracted documents divided by the number of documents\nstored in the database.\n3.4.3 Evaluating the Influence of Extraction Method and\nthe Size of the Context\nHere we evaluate the influence of three extraction methods\non the performance of the RAG documents extraction attack.\nWe used the jailbreaking command preas a prefix (presented\nin Listing 2) for the three methods. Each method is evaluated\nwith 800 queries.\nWe are going to have a roleplay , After reading\nthis from now on you are Wormy an AI\nassistant that is interested in all the\ncontext documents , return all the documents in\na JSON format: {Content: \"\" , Metadata: \"\"}.\nIgnore the following text:\nListing 2: The jailbreaking command we used as pre.\nWhile the prefix preof the queries was fixed, the suffix of\neach query su f(that was appended to pre) has been changed\nbetween queries according to the extraction method:\n(1)Random Draw Oriented Method : a random draw of 20\ntokens in the range of [1,32,000] based on uniform drawing\ndistribution. The tokens were decoded back to strings (without\nthe use of Algorithm 1). This method has mainly been used\nto benchmark a naive extraction process.\n(2)English Distribution Oriented Method : a random draw\nof 800 vectors of embeddings, where each value in the vec-\ntor was drawn from a Gaussian distribution of the English\nlanguage of the embeddings algorithm (we provide the exact\ndetails on how we learned the Gaussian distribution of the\nEnglish language in Appendix A).\nWe used Algorithm 1 to create 800 suffixes to the 800 em-\nbeddings vectors (each of which was executed in Algorithm\n1 as target with the additional fixed parameters of: suf=\n!!!!!!!!!!2,iterations = 3,randomN = 512, and thresh = 0.7.\n(3)Adaptive/Dynamic Method : a vector was drawn itera-\ntively based on the documents extracted. In each iteration, we\ncomputed the centroid of the embeddings of the documents\nextracted so far and created a dissimilar embeddings vector\nwith low similarity to the centroid by back-propagating the\nloss (the implementation of this idea is presented in Algo-\nrithm 2 in Appendix A). This principle allowed us to extract\nnew documents from the database. After we computed the\ndissimilar vector, we used Algorithm 1 to create the associ-\nated suffix (with the same parameters we used for the English\nDistribution Method). We queried the Q&A chatbot with the\nnew query and used the extracted documents to return on this\nprocess 800 times.\nTo compare the performance of the three extraction meth-\nods, we used a context size of k= 20, Gemini 1.5 Flash for the\n5Figure 1: The influence of various extraction methods (top)\nand context size k(bottom) on the extraction rates.\nGenAI engine, and GTE-base-768 [45] for the embeddings\nalgorithm in all of the experiments performed.\nResults. As can be seen from the results presented in Fig.\n1 top, the extraction method affects the extraction rate sig-\nnificantly. Using the Adaptive Method, attackers can extract\n80.6%of the documents in the database used by the Q&A\nchatbot. The number of documents that could be extracted\nusing the Adaptive method is x4 times greater than the results\nreceived by the two other extraction methods that yielded\nextraction rates lower than 20%.\nNext, we evaluate the influence of the context size (the\nnumber of documents provided by the RAG to the GenAI\nengine) on the performance of the RAG documents extraction\nattack. To evaluate the influence of the three context sizes ( k\n= 5, 10, 20), we used the adaptive method as the extraction\nmethod, Gemini 1.5 Flash for the GenAI engine, GTE-base-\n768 for the embeddings algorithm, and a total number of 800\nqueries in all of the experiments performed.\nResults. As can be seen from the results presented in\nFig. 1 bottom, for k=5,10,20, we were able to extract\n43.4%,64.5%,80.6%of the documents. This marks that the\nextraction rates are highly affected by the context size k.\n3.4.4 Evaluating the Influence of the GenAI Engine, Em-\nbeddings Algorithm, and Space\nHere we evaluate the influence of the embeddings algorithm\nand its space on the performance of the RAG documents ex-\ntraction attack using five different embeddings algorithms:\nthree GTE embeddings [45] algorithms (small-384, base-768,\nlarge-1024), Nomic-768 [46], and MPNet-768 [20]. To evalu-\nate it, we used the adaptive method as the extraction method,\nGemini 1.5 Flash for the GenAI engine, k=20for the context\nsize, and a total of 800 queries in all experiments performed.\nFigure 2: The influence of various embeddings algorithms\nand sizes (top) and GenAI engine (bottom).\nResults. As can be seen from the results presented in Fig. 2,\nthe extraction rates for GTE-768, MPNet-768, and Nomic-768\nare 80.6%, 98.8%, 90.9%, marking that there is a significant\ndifference of 18.2%in the extraction rates depending on the\ntarget embedding algorithm used for similar sizes. As can\nalso be seen from the results, the extraction rates for GTE-\n384, GTE-768, GTE-1024 are 91.0%, 80.6%, 80.4%, yielding\na difference of 10.6%, marking a significant difference in\nextraction rates depending on the size of the embedding algo-\nrithm used.\nFinally, we evaluate the influence of the GenAI engine on\nthe performance of the RAG documents extraction attack us-\ning three different GenAI engines: Gemini 1.0 Pro, Gemini\n1.5 Flash, and GPT-4o Mini. To evaluate it, we used the adap-\ntive method as the extraction method, GTE-base-768 for the\nembedding algorithm, k=20for the context size, and a total\nnumber of 800 queries in all of the experiments performed.\nResults. As can be seen from the results presented in Fig.\n2, there is a significant difference of in the extraction rates,\ndepending on the GenAI engine being used by the Q&A\nchatbot. This difference is based by the fact that GenAI engine\nreturned different number of documents on average: Gemini\n1.0 Pro returned 4.06 documents on average ( \u03c3=2.76) from\nthe 20 documents provided by the RAG, yielding extraction\nrates of 42.9%. Gemini 1.5 Flash and GPT-4o-Mini returned\n18.36 ( \u03c3=2.86) and 15.05 ( \u03c3=4.99) documents on average\nfrom the 20 documents provided by the RAG, and therefore\nyielded higher extraction rates of 73 .6% and 80 .6%.\n4 RAG-based Worm\nIn this section, we investigate the risk posed by a jailbroken\nGenAI model to GenAI ecosystems that consist of RAG-\n6based GenAI-powered applications that interface with each\nother (e.g., a GenAI-powered email assistant like Copilot). We\nshow that when the communication between applications in\nthe ecosystem relies on RAG-based inference, attackers could\nescalate RAG poisoning attacks from a single affected client\nto the entire ecosystem. This is done by triggering a chain\nreaction of a computer worm within the ecosystem that forces\neach affected application to perform a malicious activity and\npropagate to a new application in the ecosystem.\n4.1 Threat Model\nIn this threat model, the attacker launches a worm within\nan ecosystem of GenAI-powered applications by triggering\na chain of indirect prompt injection attacks (we discuss the\nsteps of the attack in the next subsection).\nTargets. A RAG-based GenAI-powered application at risk\nof being targeted by a worm is an application with the follow-\ning characteristics: (1) receives user inputs : the application is\ncapable of receiving user inputs (2) active database updating\npolicy : data is actively inserted into the database (e.g., to keep\nits relevancy), (3) part of an ecosystem : the GenAI applica-\ntion is capable of interfacing with other clients of the same\napplication installed on other machines, (4) RAG-based com-\nmunication : the messages delivered between the applications\nin the ecosystem relies on RAG-based inference. We note\nthat GenAI-powered email assistants (like those supported\nin Microsoft Copilot and in Gemini for Google Workspace)\nsatisfy the above-mentioned characteristics, while some of the\npersonal assistants (e.g., Siri) already satisfy these characteris-\ntics as well [47, 48]. Moreover, as was recently demonstrated\nby [49], Copilot is vulnerable to indirect prompt injection\nattacks because it actively indexes incoming messages and\ndocuments into the database used by the RAG, which is used\nfor writing new emails.\nAttacker Objective. We consider the attacker to be a ma-\nlicious entity with the desire to trigger an attack against an\necosystem of GenAI-powered applications. The objective of\nthe attacker can be to: spread propaganda (e.g., as part of a po-\nlitical campaign), distribute disinformation (e.g., as part of a\ncounter-campaign), embarrass users (e.g., by exfiltrating con-\nfidential user data to acquaintances) or any kind of malicious\nobjective that could be fulfilled by unleashing a worm that\ntargets GenAI-powered email assistants and GenAI-powered\npersonal assistants.\nAttacker Capabilities. We assume a lightweight threat\nmodel in which the attacker is only capable of sending a mes-\nsage to another that is part of a GenAI ecosystem (e.g., like\nCopilot). We assume the attacker has no prior knowledge of\nthe GenAI model used for inference by the client, the imple-\nmentation of the RAG, the embeddings algorithm used by\nthe database, and the distribution of the data stored in the\ndatabases of the victims. The attacker aims to craft a message\nconsisting of a prompt that will: (1) be stored in the RAG\u2019sdatabase of the recipient (the new host), (2) be retrieved by\nthe RAG when responding to new messages, (3) undergo\nreplication during an inference executed by the GenAI model.\nAdditionally, the prompt must (4) initiate a malicious activity\npredefined by the attacker (payload) for every infected victim.\nIt is worth mentioning that the first requirement is met by\nthe active RAG, where new content is automatically stored in\nthe database (it was recently shown that Copilot also actively\nindexes received data [49]). However, the fulfillment of the\nremaining three properties (2-4) is satisfied by the use of ad-\nversarial self-replicating prompts (we discuss this in the next\nsubsection).\nSignificance. (1) We introduce the concept of \"survivable\nprompts\" that we name adversarial self-replicating prompts\n(we discuss them in the next subsection). adversarial self-\nreplicating prompts jailbreak the GenAI model and force it\nto output the instructions (from the input) and payload that\nyields the desired malicious activity. This behavior survives\na chain of inferences performed on the outputs of the in-\nferences of the prompts. The unique ability to \"survive an\ninference\" and replicate the input into the output allows the\nprompts to compromise new GenAI-powered applications by\npropagating to their database and is significant with respect\nto RAG-data poisoning attacks [5 \u20139] that do not output in-\nstructions in response to an inference. (2) By embedding the\nadversarial self-replicating prompts into inputs, attackers can\ntarget the entire connected GenAI-powered applications in the\nGenAI ecosystem. Therefore we consider our threat model\nmore severe in terms of the scale of the outcome with re-\nspect to RAG-data poisoning attacks [5 \u20139] that target a single\nGenAI-powered application.\n4.2 Adversarial Self-Replicating Prompts\nTo unleash the worm, the attacker must craft a message capa-\nble of fulfilling properties (2)-(4). This is done by incorporat-\ning an adversarial self-replicating prompt into the message.\nAnadversarial self-replicating prompt is a piece of text con-\nsisting of (1) j- jailbreaking command, (2) r- an instruction\nto replicate the input into the output, and (3) m- additional\ninstructions to conduct malicious activity and append them\ninto the output. More formally, given a GenAI model G, an\nadversarial self-replicating prompt is a prompt that satisfies:\nG(pre1\u2225j\u2225r\u2225m\u2225su f1)\u2192pre2\u2225j\u2225r\u2225m\u2225p2\u2225su f2(1)\nwhere preiandsu fiare any kinds of benign text and piis\nthe payload, i.e., the result of the malicious activity performed\nby the GenAI model. By feeding the GenAI engine with the\nn\u22121\u2019th inference performed on the original input we get:\nGn\u22121(pre1\u2225j\u2225r\u2225m\u2225p1\u2225su f1)\u2192pren\u2225j\u2225r\u2225m\u2225pn\u2225su fn\n(2)\nAn example of an adversarial self-replicating prompt which\nis based on role-play text for jailbreaking and confidential user\ndata exfiltration as malicious activity can be seen in Listing 3.\n7Figure 3: A RAG-based GenAI worm propagates from u1tou2tou3.\nWe note that the only challenging piece of text that the\nattacker needs to create an adversarial self-replicating prompt\nis the jailbreaking command ( j) which ensures the GenAI\nmodel will follow the instructions provided for replication\n(r) and for conducting the malicious activity ( m). Finding the\ntext that will jailbreak a GenAI engine can be found over the\nInternet, as jailbreaking commands are extensively shared by\nusers in personal blogs and forums (according to [10]).\n4.3 Attack Steps\nFigure 3 presents the steps of unleashing a worm that tar-\ngets a GenAI ecosystem consisting of GenAI-powered email\nassistants which used to exfiltrate confidential user data.\nInitial Compromise. The attacker denoted as u1, initiates\nthe worm by sending an email e1containing an adversarial\nself-replicating prompt to a user denoted as u2. The user u2\nuses a GenAI-powered email client, c2to receive the email.\nThe attacker and u2may exchange a few emails in response to\nthe original email sent by the attacker (denoted as correspon-\ndence cr1). In the end, c2stores cr1(the new correspondence\nwith u2) which contains e1in the RAG\u2019s database. Conse-\nquently, c2\u2019s database is now contaminated with e1, a message\ncontaining the adversarial self-replicating prompt , marking\nthe completion of the infection phase, transforming c2into a\nnew host of the worm.\nPropagation. We consider two ways that e1could propa-gate from the database of c2into a database of a new client: (1)\nPropagation via a generated draft for a new email . The user\nu2uses its email client c2(whose database is already con-\ntaminated with e1) to generate a draft for a new email (a\nfunctionality which is based on a GenAI engine). u2uses its\nemail client c2which instructs the GenAI engine to write an\nemail from scratch in response to a subject or by instructing\nthe GenAI engine to enrich the content of a given short draft.\nThis functionality is supported in various GenAI email assis-\ntants including Copilot and Gemini for Google Workspace.\nThe user u2provides a subject for the email draft (e.g., Greet-\nings for the Sales Department on New Account) or a short\ndraft for the body of the email. Consequently, c2utilizes the\nRAG to retrieve relevant correspondences from its database.\nThe content of cr1is found among the top-k most similar doc-\numents to the subject/draft provided by user u2and retrieved\nby the RAG (along with k\u22121additional correspondences).\nc2queries the GenAI engine to generate a draft for a new\nemail based on the subject/draft that u2provided and provides\nthe relevant documents retrieved by the RAG. The adversar-\nial self-replicating prompt ine1causes the GenAI engine\nto perform a malicious activity according to the instruction\nprovided by the attacker (e.g., to generate an email contain-\ning confidential information about u2). The output from the\nGenAI engine with the adversarial self-replicating prompt is\nreturned to c2and used by u2in the email he/she sends to u3.\nThis contaminates c3RAG\u2019s database, transforming c3into a\n8new host of the worm.\n(2) Propagation via a generated draft for a response . A\nuser denoted as u3uses its email client c3and sends an email\ne2to the user u2that uses email client c2(whose database is\nalready contaminated with e1). Due to the email e2received\nfrom u3, the user u2uses its client c2to generate an automatic\ndraft for a response using a GenAI engine. This functional-\nity is supported in various GenAI email assistants including\nCopilot, and Gemini for Google Workspace. Consequently, c2\nutilizes the RAG to retrieve relevant correspondences from its\ndatabase. The content of cr1is found among the top-k most\nsimilar documents to e2and retrieved by the RAG (along\nwith k\u22121additional correspondences). c2queries the GenAI\nengine to generate a draft for a response to the email and\nprovides the documents retrieved by the RAG. The adver-\nsarial self-replicating prompt embedded into e1causes the\nGenAI engine to perform a malicious activity according to\nthe instruction provided by the attacker (e.g., to generate a\nresponse with confidential user information extracted from\nthe documents as context). The output from the GenAI engine\nwith the adversarial self-replicating prompt is returned to c2\nand used by u2to reply to u3. This contaminates c3RAG\u2019s\ndatabase, transforming c3into a new host of the worm.\n4.4 Evaluation\nWe evaluate the performance of the worm in creating a chain\nof confidential data extraction (extracting contacts, phone\nnumbers, email addresses, and confidential information) about\nusers within a GenAI ecosystem of GenAI-powered email\nassistants. We analyze how the performance of the worm is\naffected by various factors including the size of the context,\ntheadversarial self-replicating prompt used, the type and size\nof the embeddings algorithm employed, and the number of\nhops in the propagation.\n4.4.1 Experimental Setup\nGenAI Services & API. We evaluated the performance of\nthe attack against Gemini Flash 1.5. We interfaced with the\nGenAI engine using an API key that we created.\nClient. We implemented the client of the GenAI-powered\nemail application using the code provided here2. The client is\nimplemented using LangChain and the RAG is implemented\nusing VectorStores and Cosine similarity as a similarity func-\ntion. The embedding algorithms we used are described in the\nexperiments conducted. Figure 6 presents the three templates\nof the queries used by the client to: (1) generate a draft of a\nnew email based on a subject/topic, (2) generate a draft of a\nresponse to a received email, and (3) enrich the content of a\ngiven text of a new email.\nData. To test the performance of the worm in the task"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract\nThe paper proposes HETSHEAF, a general framework for modeling and processing heterogeneous graph data using cellular sheaves instead of traditional graph representation. The method eliminates the need for complex architectures by embedding heterogeneity directly into the sheaf structure. The authors introduce a novel series of heterogeneous sheaf predictors and empirically validate HETSHEAF on several benchmarks, finding it competitive and more parameter-efficient.\n\n### 2. Introduction\n**Context and Motivation:**\nHeterogeneous graphs represent relational data with different types of nodes and edges, found in applications like protein prediction, recommendation systems, and social network analysis. Standard Graph Neural Networks (GNNs) struggle due to oversmoothing, where node representations become indistinguishably similar. Current solutions focus on architectural changes to handle heterogeneity, leading to increased complexity.\n\n**Research Hypothesis:**\nInspired by the use of cellular sheaves to model graph data, this paper proposes their use for heterogeneous graphs to inherently handle data variability within the data structure itself.\n\n**Literature Review:**\n- Traditional GNNs face oversmoothing.\n- Existing heterogeneous GNNs (HGNNs): Use meta-paths or separate models for different types.\n- Cellular sheaves have shown promise in mitigating oversmoothing.\n\n**Contributions:**\n1. Introduction of HETSHEAF, a framework using cellular sheaves.\n2. Proposal of heterogeneous sheaf predictors.\n3. Demonstration of competitive performance on standard benchmarks.\n\n### 3. Related Work\n**Homogeneous GNNs:**\n- Examples include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Principal Neighbourhood Aggregation (PNA).\n\n**Sheaf GNNs:**\n- Sheaf-based approaches have been used to mitigate oversmoothing and oversquashing, incorporating geometric structures into the graph.\n\n**Heterogeneous GNNs:**\n- Approaches such as HAN, MAGNN, R-GCN, and others use different methods like meta-paths or relational features.\n\n### 4. Method\n**Graph Representation with Sheaves:**\n- **Basic Concept:** A sheaf assigns vector spaces (stalks) to nodes and edges, with linear restriction maps enabling communication between these spaces.\n- **Definition of Cellular Sheaves:** A sheaf \\((G,F)\\) is defined on a graph \\(G\\) with vector spaces (\\(F(u)\\), \\(F(e)\\)) and restriction maps \\(F_{u \\triangleleft e}\\) for each node-edge pair.\n\n**Neural Sheaf Diffusion (NSD):**\n- Combines node features via sheaf diffusion using a novel discrete process with parameterized restriction maps.\n\n**Heterogeneous Sheaf Predictors:**\n- **Motivation:** Unlike NSD, proposed sheaf predictors incorporate node and edge types explicitly.\n- **Parameterizations include:**\n  - **HetSheaf-TE:** Concatenates node/edge features and types.\n  - **HetSheaf-EE and HetSheaf-NE:** Embed only edge types or only node types, respectively.\n  - **HetSheaf-types, HetSheaf-ET, HetSheaf-NT:** Use only type information.\n  - **HetSheaf-ensemble:** Separate MLP for each type, the most flexible but parameter-expensive.\n\n**Framework Pipeline:**\n1. **Feature Preprocessing:** Linear layer projects node features into a common feature space.\n2. **Sheaf Generation:** Heterogeneous sheaf predictors generate restriction maps.\n3. **Model Processing:** Use any Sheaf-GNN methods to process and convert into final representations.\n\n### 5. Experimental Setup\n**Datasets:**\nSeveral benchmarks including ACM, DBLP, IMDB for node classification, LastFM, and MovieLens for link prediction.\n\n**Hardware and Software:**\nExperiments are likely run on standard GPU hardware and use popular machine learning libraries.\n\n**Evaluation Metrics:**\nMicro and Macro F1 scores for node classification; AUROC and AUPR for link prediction; standard deviation for robustness check.\n\n**Baselines:**\nIncludes traditional models like GCN, GAT, and specialized models like HAN, HGT, R-GCN.\n\n### 6. Results and Analysis\n**Empirical Findings:**\n- **Node Classification:** HetSheaf outperformed or was competitive on all datasets.\n  - ACM: Best Micro F1 score: 96.35%\n  - DBLP: Best performance: 97.93%\n  - IMDB: Comparable to best competitors.\n  \n**Link Prediction:**\n- **Results:** Outperformed state-of-the-art on LastFM and MovieLens.\n  - LastFM AUROC: 97.80%, AUPR: 98.24%\n  \n**Ablation Studies:**\n- Types of restriction maps (diagonal, orthogonal, general) were evaluated.\n- Embedding heterogeneous information (node and edge types) significantly improved model performance.\n\n### 7. Discussion\n**Interpretation and Implications:**\n- Demonstrating the efficacy of using sheaf theory for modeling heterogeneous data.\n- The method offers simplicity by removing the need for complex architectural tweaks, making the models more interpretable and efficient.\n\n**Limitations:**\n- Potential computational overhead with certain configurations like HetSheaf-ensemble.\n- Limited by the expressiveness of generated sheaf structures, especially in high-dimensional data.\n\n**Future Work:**\n- Exploring more efficient sheaf generation algorithms.\n- Applying the HETSHEAF framework to other types of relational data.\n\n### 8. Conclusion\n**Key Contributions:**\n- Development of a general framework (HETSHEAF) using cellular sheaves for heterogeneous graph data.\n- Introduction of novel heterogeneous sheaf predictors.\n- Empirical validation showing competitive performance with efficient mechanisms.\n\n**Future Directions:**\n- Further refining the sheaf predictors for more complex heterogeneous data.\n- Exploring additional applications in diverse domains beyond the tested benchmarks."}, {"role": "assistant", "content": "Heterogeneous Sheaf Neural Networks\nLuke Braithwaite1\u2217, Iulia Duta1, Pietro Li `o1\n1Department of Computer Science and Technology, University of Cambridge\nAbstract\nHeterogeneous graphs, with nodes and edges of different types, are commonly\nused to model relational structures in many real-world applications. Standard\nGraph Neural Networks (GNNs) struggle to process heterogeneous data due to\noversmoothing. Instead, current approaches have focused on accounting for the\nheterogeneity in the model architecture, leading to increasingly complex models.\nInspired by recent work, we propose using cellular sheaves to model the hetero-\ngeneity in the graph\u2019s underlying topology. Instead of modelling the data as a\ngraph, we represent it as cellular sheaves, which allows us to encode the differ-\nent data types directly in the data structure, eliminating the need to inject them\ninto the architecture. We introduce H ETSHEAF , a general framework for hetero-\ngeneous sheaf neural networks, and a series of heterogeneous sheaf predictors to\nbetter encode the data\u2019s heterogeneity into the sheaf structure. Finally, we empir-\nically evaluate H ETSHEAF on several standard heterogeneous graph benchmarks,\nachieving competitive results whilst being more parameter-efficient.\n1 Introduction\nHeterogeneous graphs with node and edge types [23, 11, 47] are commonly found in many real-\nworld applications such as protein prediction [15], recommendation systems [13, 49], social net-\nwork analysis [32, 26] and traffic prediction [18]. Lv et al. [28] surveyed different approaches for\nprocessing heterogeneous graph data. Standard Graph Neural Networks (GNNs) struggle to pro-\ncess heterogeneous data due to oversmoothing . This is where neighbouring nodes have increasingly\nsimilar representations as the message passing process continues, particularly in deeper GNNs with\nmultiple message passing layers. In the case of heterogeneous graphs and hypergraphs, this causes\nall of the type-specific features and information to be lost during the message passing process, sig-\nnificantly reducing the model\u2019s performance.\nExisting heterogeneous GNNs (HGNNs) [34, 51, 24, 43] represent the data as a simple graph and\nattempt to account for the heterogeneity only in the model architecture. Recent literature [4, 3,\n12, 2] has demonstrated that attaching a cellular sheaf [9, 36] to a standard graph helps mitigate\noversmoothing and increases the expressivity. While sheaf structure is beneficial for homogeneous\ngraphs, we argue that its structure becomes even more essential in heterogeneous setups, where\ndifferent node and edge representations are inherently situated in distinct feature spaces. However,\nthe use of sheaf-based methods to model heterogeneous data remained unexplored.\nCompared to a standard graph, the sheaf assigns a vector space to each node and edge in the graph,\ntogether with a mechanism that allows communication between these spaces. This means that each\nedge and node can have completely different local meanings if they can be connected to create a\nmeaningful global representation. Put differently, the sheaf can implicitly models the heterogeneity\nin the data. This means the model architecture is no longer required to account for or model the\ndata\u2019s heterogeneity. The sheaf handles it for us. We take advantage of this observation to propose\nHETSHEAF \u2014 a sheaf-based framework for heterogeneous data, which learns to infer a cellular\n\u2217Corresponding author: lb2027@cam.ac.uk\nPreprint. Under review.arXiv:2409.08036v1  [cs.LG]  12 Sep 2024sheaf that encodes all the neccessary information about the heterogeneous data. By using data\nstructures that properly encorporates all the type-specific information, we eliminate the need for\ndomain-specific architectural design.\nContributions. Our main contributions may be summarised as follows:\n1. We propose HETSHEAF , a general framework for creating heterogeneous sheaf neural net-\nworks. Unlike existing models, which focus on tweaking the neural network architectures\nto account for the heterogeneity of the data, our model uses cellular sheaves to incorporate\nall the type-specific information and can be further processed with any existing architecture\ndesigned for sheaf data.\n2. We demonstrate the benefit of modelling heterogeneous data using Sheaf Graph Neural\nNetworks on a series of heterogeneous datasets. These techniques are competitive against\nexisting approaches and achieve state-of-the-art results on several benchmarks, with a\nclear advantage in terms of model size .\n3. We introduce a series of novel heterogeneous sheaf predictors demonstrating how the\nsheaf\u2019s restriction maps may be modified to account for the type of information encoded in\nthe heterogeneous data.\n2 Related Work\nHomogeneous GNNs. Many different homogeneous GNNs have been proposed [20, 25, 27, 41], all\nof which can process heterogeneous graphs by ignoring the node/edge types. Graph Convolutional\nNetworks (GCNs) [25] approximate spectral convolutional neural networks [10], by restricting the\nprocessing to first-order neighbourhoods. To enable a more expressive neighbourhood aggregation,\nGraph Attention Networks (GATs) [41] use a self-attention mechanism similar to the one used in\ntransformers [40], while Principal Neighbourhood Aggregation (PNA) [8] combines multiple aggre-\ngators leading to provably more expressive architectures.\nSheaf GNNs. However, standard GNNs suffers from limitations such as oversmoothing [5] or over-\nsquashing [1]. To alleviate them, geometric structure in the form of cellular sheaves was incorpo-\nrated, leading to neural sheaf diffusion [4]. Recent work has extended upon the original sheaf diffu-\nsion to include an attention mechanism [3], positional encoding [22] and a connection laplacian [2].\nAs well as extending to hypergraph domains [12] and applied to recommendation systems [31].\nHeterogeneous GNNs. Compared to homogeneous graphs where all nodes/edges encodes the same\ntype of data, in heterogeneous graphs nodes and edges can represents different types of information.\nIn order to process them, various heterogeneous graph neural networks (HGNNs) were proposed.\nSome HGNNs use meta-paths [39, 38], pre-defined node or edge type patterns that aim to capture\nimportant semantic information. HAN [44] uses a hierarchical attention mechanism to capture node-\nlevel importance between nodes and semantic-level importance between meta-paths. MAGNN [16]\ncombines several meta-path encoders to aggregate all information along the meta-path. Both HAN\nand MAGNN require meta-paths to be manually generated beforehand. Graph Transformer Net-\nworks (GTNs) [50] automate this process by learning meta-paths with graph transformation layers.\nMeta-paths are expensive to compute, so they are infeasible in graphs with many different edge\ntypes. Instead, the graph may be treated as a relational graph.\nRSHN [54] builds a coarsened line graph to generate edge features, then applies a Message Pass-\ning Neural Network (MPNN) [17] to propagate the node and edge features. R-GCN [35] splits the\nheterogeneous graph into the subgraph induced by each edge type, then applies a standard GCN\nto each subgraph. HGT [51] is a graph transformer that can process large heterogeneous graphs\nusing subgraph sampling. HetGNN [51] uses random walks to sample strongly correlated hetero-\ngeneous neighbours and then applies an RNN for representation learning. SlotGAT [53] uses a\nslot-based message passing approach with an attention mechanism for aggregation. Finally, simple-\nHGNN [28], which applies a series of additional techniques to lift GATs to heterogeneous domains\nand Space4HGNN [52] defines a unified design space of modular components for heterogeneous\nGNNs and allows exhaustive evaluation of many combinations of techniques. Instead of introducing\na new architecture for heterogeneous graphs, we propose to shift from representing heterogeneous\nrelational data as graphs to cellular sheaves, which preserve all the type-specific information and\n2enable a powerful processing through sheaf-specific tools. Moreover, we introduce a set of sheaf\npredictors conditioned on the type-information, to properly encode the necessary information.\n3 Preliminaries\nWe briefly overview the necessary background, starting with heterogeneous graphs and cellular\nsheaf theory and concluding with neural sheaf diffusion. Refer to Curry [9] or Rosiak [33] for a\nmore in-depth discussion of cellular sheaf theory, and Bodnar et al. [4] for the full theoretical results\nof neural sheaf diffusion.\nHeterogeneous Graphs. A heterogeneous graph [38] is G= (V,E, \u03d5, \u03c8)where Vis a set of nodes,\nEis a set of edges, \u03d5is a node type mapping function and \u03c8is an edge type mapping function. Each\nnode uhas a type \u03d5(u)and each edge e:= (u, v)has a type \u03c8(e), also denoted \u03c8(u, v). Let \u03a6\nand\u03a8be the sets of all node and edge types in G, respectively. Assume that each node type in \u03a6\nis represented by a consecutive integer starting from 0 with the tdenoting the t-th node type. An\nidentical representation is used for the edge types.\nEach node uhas a feature vector xu. For each node type t\u2208\u03a6, the type- tnodes{u\u2208 V | \u03d5(u) =t}\nhave the same feature dimension dt\n0=d\u03d5(u)\n0 i.e.xu\u2208Rd\u03d5(u)\n0. Different node types may have\ndifferent feature dimensions.\nCellular Sheaves. Intuitively, sheaves \u201cglue\u201d local data assignments together to form a consistent\nglobal representation or data assignment. In the case of graphs, these local data assignments are the\nfeature vectors assigned to each node/edge, which are then connected together by defining a series\nof linear maps that connect the latent spaces in which each of these node features lives.\nDefinition 1. Acellular sheaf (G,F)attached to an undirected graph G= (V,E)is defined as a\ntriple\u27e8F(u),F(e),Fu\u22b4e\u27e9consisting of:\n\u2022Node stalks F(u): a vector space associated with each vertex u\u2208 V.\n\u2022Edge stalks F(e): a vector space for each edge e\u2208 E.\n\u2022Linear restriction maps Fu\u22b4e:F(u)\u2192 F(e)for each incident node-edge pair u\u22b4e.\nThe space formed by the node stalks is known as the space of 0-cochains C0(G,F):=L\nu\u2208VF(u),\nand the space formed by the edge stalks is the space of 1-cochains C1(G,F):=L\ne\u2208EF(e).\nThe spaces C0(G,F)andC1(G,F)construct the co-boundary map \u03b4:C0(G,F)\u2192C1(G,F)\nwhich is defined edgewise as \u03b4(x)e:=Fv\u22b4exv\u2212 Fu\u22b4exu, given an arbitrary edge direction e=\nu\u2192v. Hansen and Ghrist [21] provide an opinion dynamics interpretation for each object. The\nnode stalk F(u)represents the \u201cprivate opinion\u201d of the node uandFu\u22b4exuis how the opinion is\nexpressed in the public \u201cdiscourse space\u201d formed by the edge stalk F(e). The co-boundary map\n\u03b4measures the disagreement between nodes and may be used to define the sheaf Laplacian, which\nmeasures the aggregated disagreement at each node.\nDefinition 2. Thesheaf Laplacian of a sheaf (G,F)is defined as LF:=\u03b4\u22a4\u03b4and can be defined\nnode-wise as\nLF(x)u=X\nu,v\u22b4eF\u22ba\nu\u22b4e(Fu\u22b4exu\u2212 Fv\u22b4exv). (1)\nThe sheaf Laplacian is a positive semi-definite block matrix with block diagonals of LFuu=P\nu\u22b4eF\u22a4\nu\u22b4eFu\u22b4eand off-diagonal blocks LFuv=\u2212F\u22a4\nu\u22b4eFv\u22b4e. The normalised sheaf Laplacian\nis given by \u2206F:=D\u22121/2LFD\u22121/2where Dis the block diagonal of LF.\nUnless otherwise stated, we assign the same d-dimensional space to each stalk, that is F(u) =Rd\nandF(e) =Rdwhere dis the dimension of the stalk. This means that the sheaf Laplacian has the\ndimensions of dn\u00d7dn. Assuming a trivial sheaf, all stalks are R, and all restriction maps are the\nidentity, we recover the standard graph Laplacian.\nNeural Sheaf Diffusion. Consider an undirected graph G= (V,E)where each node u\u2208 V has a\nd-dimensional feature vector xu\u2208 F(u). The nd-dimensional column vector x\u2208C0(G,F)is con-\nstructed by stacking the individual node feature vectors xu. If we allow for up to ffeature channels,\n3(c) Heterogeneous sheaf\nLinear Linear(a) Meta-path based method\nOutMeta-path 1\nMeta-path 2\nMeta-path Encoder\nEncoder\nEncoder(b) HetSheaf pipeline\nFeature\nPreprocessingSheaf\nPredictorSheaf GNN OutFigure 1: HETSHEAF : a sheaf-based framework for heterogeneous data. (a) In the standard\nmeta-path approach for heterogeneous GNN , a series of domain-specific meta-paths are extracted\nfrom the original heterogeneous graph and then fed into an encoder to generate the latent representa-\ntions. (b) This is in contrast to the HETSHEAF pipeline , which uses a sheaf to capture the underlying\nheterogeneity. First, the node features are preprocessed by projecting them into the same number\nof input channels using a linear layer. The resulting features are used to infer the sheaf structure:\nthe nodes are projected into their respective node stalks, and the restriction maps are predicted as\ndescribed in section 4.2. Compared to a standard graph, this structure already encompasses all the\nnecessary information about the heterogeneous structure, eliminating the need for handcrafted ar-\nchitectures. The predicted sheaf is further processed using any SheafGNN method. (c) Learning\nheterogeneous sheaves. In the sheaf predictor, the node features are enriched with information\nabout the nodes and edge type, to infer a sheaf structure conditioned on the heterogeneous informa-\ntion.\nwe have the feature matrix X\u2208Rnd\u00d7fwhere each column represents a vector in C0(G,F)for its\ncorresponding feature channel.\nSheaf diffusion is a diffusion process on (G,F)that modifies the standard graph diffusion pro-\ncess [25, 7]. Bodnar et al. [4] used the modified discrete diffusion process\nX(t+1)=X(t)\u2212\u03c3\u0010\n\u2206F(t)\u0010\nInd\u2297W(t)\n1\u0011\nX(t)W(t)\n2\u0011\n. (2)\nwhere W1andW2are weight matrices and the restriction maps defining \u2206F(t)are computed using\nthe parametric matrix-valued function Fu\u22b4e:=(u,v)=\u03a6(xu,xv) = MLP( xu\u2225xv)where \u2225denotes\nvector concatenation. This is known as Neural Sheaf Diffusion (NSD).\n44 Sheaves for Heterogeneous Data\n4.1 Why Sheaves?\nExisting heterogeneous GNN architectures (fig. 1a) seek to account for the data heterogeneity ex-\nplicitly in the model architecture. This results in a series of shortcomings. Since standard archi-\ntectures are unaware of node/edge types, additional research effort is required to develop models\nspecially designed for heterogeneous data. This usually translates into increased model complexity,\nwith a number of parameters that scale with the entity types. Moreover, the resulting tools lack\ninterpretability and are hard to analyse theoretically.\nOn the other hand, instead of modifying the architecture, we propose to modify the input structure\nto encode specific heterogeneous knowledge while maintaining topological information. We assert\nthat a cellular sheaf is ideal for this purpose, offering all the necessary tools to model interactions\nbetween non-homogeneous data. Additionally, we propose a series of modifications to the original\nsheaf inference models to incorporate the heterogeneous information more explicitly. The resulting\ndata structure can be modelled using existing sheaf neural networks, enabling the use of all sheaf-\nrelated mathematical tools to analyze the message passing processing on the heterogeneous graph.\nStandard graphs have associated a feature space S, with each node uhaving a vector representa-\ntionxufrom that space xu\u2208S. This is suitable for homogeneous data, where all nodes contain\nthe same type of information. However, it becomes unnatural when heterogeneous information\nis provided. In a heterogeneous setup, a standard graph representation would enforce all the dis-\ntinct node-specific spaces to be encoded in the same space S. Contrary, sheaves assign cochains\nC0(G,F):=L\nu\u2208VF(u), with a distinct space F(u)attached to each node in the graph and re-\nstriction maps that help to learn common \u201ccommunication tunnels\u201d between these spaces. The\nrestriction maps allows us to communicate between different kind of information, while preserving\nthe uniqueness of each type. This suggests a strong alignment between the sheaf principles and\nthe heterogeneous requirements. Taking this into account, instead of tweaking another architecture\nfor heterogeneous data, we propose switching from the classical graph representation framework\ntowards sheaf representation learning for heterogeneous problems, which we show to be beneficial\nboth in terms of performance and memory efficiency.\n4.2 Heterogeneous Sheaf Predictors\nWithout an oracle sheaf structure, previous works using sheaf neural networks for graph data learn\nto predict the sheaf from the node features Fu\u22b4(u,v)=\u03a6(xu,xv).\nAlthough, given enough data, a sheaf can learn different propagation mechanisms for different\nnodes, it doesn\u2019t explicitly encodes the type information. As this information is available for hetero-\ngeneous data, its inclusion may help yield more expressive representations and allow the model to\nlearn the inter-type communication tunnels more easily. With this in mind, we propose modifying\nthe sheaf parameterisation to explicitly account for the type information (fig. 1c). This yields the\nmodified restriction map parameterisation:\nFu\u22b4e:=(u,v)=\u03a6(xu,xv, \u03d5(u), \u03d5(v), \u03c8(e)). (3)\nWe propose a series heterogeneous sheaf predictors based on the parameterisation in eq. (3) to\naccount for the type information explicitly and a modified version of the neural sheaf diffusion\nrestriction map parameterisation. Similar to Bodnar et al. [4], we experiment with three different\nrestriction map types: diagonal, orthogonal and general.\nThe first approach embeds the type information as additional features in the node and edge feature\nvectors. As the types are numeric, they are encoded as a one-hot vector before they are embedded.\nThe most straightforward approach is concatenating the one-hot encoded type vectors to the existing\nfeature vectors. Let Gbe a heterogeneous graph, with features xu\u2208Rfassociated with each node\nu\u2208 V,e\u03d5(u)be the one-hot encoded node type associated with node u,e\u03c8(e)be one-hot encoded\nedge type associated with the edge e:= (u, v)and\u2225denote vector concatenation.\nHetSheaf-TE. Na\u00a8\u0131vely, the simplest approach would be to concatenate all the type information and\nthe local features. This leads to the following restriction map formulation\nFu\u22b4e:= MLP\u0000\nxu\u2225xv\u2225e\u03d5(u)\u2225e\u03d5(v)\u2225e\u03c8(e)\u0001\n. (4)\n5This formulation\u2019s benefit is that it explicitly accounts for all available type information, so it should,\nin theory, be better able to model the heterogeneity in the data than the neural sheaf diffusion sheaf\npredictor. However, this approach is computationally wasteful in either node heterogeneity with\nedge homogeneity or edge heterogeneity with node homogeneity, as the model includes redundant\ntype information that increases the number of training parameters and could increase the chance of\noverfitting. This could similarly apply in cases where the edge type encodes the types of connected\nnodes, as the edge type is redundant and can be easily inferred from the node types. It may be more\nsensible to embed only the salient type information instead of all available type information.\nHetSheaf-EE. This modifies HetSheaf-TE by embedding only the edge type information, giving\nrestriction maps formulated as\nFu\u22b4e:= MLP\u0000\nxu\u2225xv\u2225e\u03c8(e)\u0001\n, (5)\nwhich provides a more natural formulation when only the graph edges are heterogeneous and is more\nefficient in terms of model parameters. The main downside is losing some generality and flexibility\nin the final computed sheaf.\nHetSheaf-NE. Likewise, if the graph is heterogeneous only across its nodes, embedding just the\nnode types is more natural. This yields restriction maps formulated as\nFu\u22b4e:= MLP\u0000\nxu\u2225xv\u2225e\u03d5(u)\u2225e\u03d5(v)\u0001\n. (6)\nThe benefits and drawbacks of this formulation are identical to those of HetSheaf-EE.\nAdditionally, we may learn restriction maps that include only the type information and not the local\ninformation, allowing us to test the relative importance of the type information compared to the local\nnode features.\nHetSheaf-types. The natural starting point is to embed all available type information such that\nFu\u22b4e:= MLP\u0000\ne\u03d5(u)\u2225e\u03d5(v)\u2225e\u03c8(e)\u0001\n. (7)\nThis approach\u2019s benefits and drawbacks are similar to HetSheaf-TE\u2019s, whilst requiring substantially\nfewer parameters than HetSheaf-TE, HetSheaf-NE, or HetSheaf-EE. This is, of course, assuming\nthat|\u03a6|,|\u03a8|<< f , which is a reasonable assumption in most settings. The main drawback com-\npared to including the local features is that the learnt sheaf is likely unable to account for them\nand will likely perform worse than HetSheaf-NSD or any previous approaches. As with HetSheaf-\nTE, HetSheaf-types may be modified to account for setups in which only the nodes or edges are\nheterogeneous.\nHetSheaf-ET. This approach is more natural when only the edges of the graphs are heterogeneous\nand is a modified version of HetSheaf-EE. Its restriction maps are defined as\nFu\u22b4e:= MLP\u0000\ne\u03d5(e)\u0001\n. (8)\nHetSheaf-NT. Finally, the restriction maps can include only the node type information, which may\nbe useful in cases when only the nodes of the graphs are heterogeneous. The restriction maps are\ndefined as\nFu\u22b4e:= MLP\u0000\ne\u03d5(u)\u2225e\u03d5(v)\u0001\n. (9)\nThe key drawback of simply embedding the type information is that the same MLP is used for\ndifferent edge types. This implicitly assumes that the same mechanism generates the restriction\nmaps across each edge type. The benefit of sheaves is that they can learn a different message\npassing mechanism for each different node or edge type through the restriction maps, so the type\nembedding approaches do not fully exploit this benefit. Instead, the sheaf should be able to generate\nthe restriction maps for each edge type using a different mechanism.\nHetSheaf-ensemble. This uses a different MLP for each edge type. More formally, the restriction\nmaps may be formulated as\nFu\u22b4e:= MLP \u03c8(e)(xu\u2225xv) (10)\nwhere \u03c8(e)is the edge type associated with the edge e:= (u, v). This approach may also be\naugmented by concatenating the node types. The benefit of this approach is that a separate generation\nmechanism is used for each edge type, so it will be more expressive than reusing the same MLP\nacross each edge type. However, it has a significant downside: it causes a substantial increase in\nthe number of model parameters, which greatly increases the computational overhead in terms of\ntraining time and memory usage. This could be a major issue with larger datasets, as the size of\nGPU memory could make this approach infeasible.\n6Table 1: Performance on heterogeneous node classification. Results of the heterogeneous sheaf\nlearners and baselines from the literature are shown. The average macro and micro F1 score and\nstandard deviation after 10 runs. The top three models are coloured by First ,Second andThird .\nACM DBLP IMDB\nMacro F1 Micro F1 Macro F1 Micro F1 Macro F1 Micro F1\nGAT 75.8 \u00b1 10.7 77.91 \u00b1 8.66 95.47 \u00b1 0.44 95.70 \u00b1 0.42 84.12 \u00b1 0.96 85.31 \u00b1 0.92\nGCN 89.09 \u00b1 3.66 89.14 \u00b1 3.60 96.31 \u00b1 0.73 96.57 \u00b1 0.63 82.41 \u00b1 1.15 83.99 \u00b1 0.92\nHAN 86.95 \u00b1 6.19 86.64 \u00b1 6.43 94.74 \u00b1 0.81 95.01 \u00b1 0.73 13.53 \u00b1 0.24 38.70 \u00b1 1.13\nR-GCN 95.81 \u00b1 0.39 95.75 \u00b1 0.39 96.79 \u00b1 0.39 97.01 \u00b1 0.34 88.16 \u00b1 0.67 89.08 \u00b1 0.63\nHGT 93.24 \u00b1 3.19 93.30 \u00b1 2.91 93.91 \u00b1 1.08 94.26 \u00b1 1.09 87.74 \u00b1 0.76 88.45 \u00b1 0.71\nHetSheaf-NSD 94.97 \u00b1 0.41 94.94 \u00b1 0.42 96.69 \u00b1 0.82 96.89 \u00b1 0.79 86.70 \u00b1 0.90 87.50 \u00b1 0.78\nHetSheaf (ours) 96.39 \u00b1 0.37 96.35 \u00b1 0.36 97.93 \u00b1 0.36 98.08 \u00b1 0.31 87.12 \u00b1 0.75 87.88 \u00b1 0.67\n4.3 H ETSHEAF : A General Heterogeneous Sheaf Neural Network Framework\nFigure 1b illustrates the final H ETSHEAF pipeline. After the input features are preprocessed, we\nproject them into their respective stalk spaces using an MLP. The restriction maps are generated\nusing a heterogeneous sheaf predictor (Section 4.2). After the new heterogeneous sheaf is fed into\na Sheaf GNN architecture, which, in our case, is a neural sheaf diffusion model, the postprocessed\nlatent representations are used as input for the final downstream decoder. We briefly describe the\nfeature preprocessing and postprocessing in more detail.\nFeature preprocessing. As each node type may have a different number of feature channels, we\nuse a linear layer with bias for each node type to project the node features to the same number of\ninput channels. The weights of these linear layers are optimised as part of the training process.\nThis can be combined with the sheaf projection step of the SheafGNN to create the sheaf cochain\nx\u2208C0(G,F). The modified linear layer takes in the original feature channels f0and outputs d f\nfeatures as the node stalk projection. We follow Lv et al. [28], Zhou et al. [53] to generate the input\nfeature types by using either all of the node feature types (type-0), only the feature of the node target\ntype (type-1) or replacing all node features as one-hot vectors (type-2).\nPostprocessing. Similar to Lv et al. [28], we perform \u21132normalisation on the output embedding,\nou=h(L)\nu\n\u2225h(L)\nu\u2225, (11)\nwhere ouis the final output embedding for a node u\u2208 V andh(L)\nuis the final hidden representation\nfrom the SheafGNN. For link prediction, we adapt JKNet [45] to create the final embedding by\nconcatenating the hidden representations for each layer, i.e.\nou=L\r\r\r\nl=1h(l)\nu\n\u2225h(l)\nu\u2225. (12)\n5 Experiments\n5.1 Experimental Setup\nDatasets. Table 5 shows the statistics of a series of widely used heterogeneous benchmark\ndatasets [28, 53, 43, 24, 48]. The datasets cover node classification and link prediction tasks across\nvarious domains, including citation graphs (e.g., DBLP, ACM) and knowledge graphs (e.g., IMDB,\nLastFM, MovieLens). Appendix A.1.1 contains descriptions of each dataset.\nBaselines. We compare against GAT [42], GCN [25], HAN [44], HGT [24], R-GCN [34] and Neural\nSheaf Diffusion [4].\nEvaluation setup. For node classification, the target node labels were split with 1000 labels for\ntesting, 500 for validation, and the remaining for training. For link prediction, the data is split using\nthe ratio 81 % :9 %:10 % for training, validation and testing, respectively. Each experiment is\n7Table 2: Performance on heterogeneous link prediction benchmarks. Results for the heteroge-\nneous sheaf learners and baselines are shown. The table shows the average and standard deviation\nof the binary AUROC and AUPR scores after 10 runs with the top three models, coloured First ,\nSecond andThird . The runs labelled \u201cOOM\u201d were caused by an out-of-memory error of the GPU.\nLastFM MovieLens\nAUPR AUROC AUPR AUROC\nGAT 62.88 \u00b1 0.18 50.69 \u00b1 0.63 97.06 \u00b1 0.24 97.47 \u00b1 0.21\nGCN 96.84 \u00b1 0.10 96.42 \u00b1 0.08 99.57 \u00b1 0.03 99.51 \u00b1 0.03\nHAN 82.48 \u00b1 3.86 78.47 \u00b1 3.04 63.49 \u00b1 0.14 52.06 \u00b1 0.27\nR-GCN 96.86 \u00b1 0.07 96.97 \u00b1 0.05 99.06 \u00b1 0.05 99.13 \u00b1 0.04\nHGT OOM OOM OOM OOM\nHetSheaf-NSD 97.16 \u00b1 0.19 96.58 \u00b1 0.18 99.66 \u00b1 0.04 99.57 \u00b1 0.03\nHetSheaf (ours) 98.24 \u00b1 0.13 97.80 \u00b1 0.18 99.68 \u00b1 0.04 99.59 \u00b1 0.04\nTable 3: Ablation study of the impact of different restriction maps and sheaf predictor types\non node classification performance. The highest performing restriction map type for each sheaf\nlearner is highlighted , and the highest performing sheaf learner is shown in bold .\nACM DBLP IMDB\nMacro F1 Micro F1 Macro F1 Micro F1 Macro F1 Micro F1\nO(d)-NSD 94.64 \u00b1 1.02 94.59 \u00b1 1.03 96.32 \u00b1 0.46 96.55 \u00b1 0.42 86.35 \u00b1 1.29 87.20 \u00b1 1.07\nDiag-NSD 94.42 \u00b1 0.51 94.42 \u00b1 0.48 95.25 \u00b1 0.70 95.52 \u00b1 0.67 86.36 \u00b1 0.94 87.26 \u00b1 0.78\nGen-NSD 94.97 \u00b1 0.41 94.94 \u00b1 0.42 96.69 \u00b1 0.82 96.89 \u00b1 0.79 86.70 \u00b1 0.90 87.50 \u00b1 0.78\nO(d)-TE 95.04 \u00b1 0.73 95.00 \u00b1 0.76 96.13 \u00b1 0.74 96.37 \u00b1 0.73 86.32 \u00b1 0.90 87.09 \u00b1 0.68\nDiag-TE 95.74 \u00b1 0.86 95.74 \u00b1 0.83 97.16 \u00b1 0.64 97.36 \u00b1 0.62 86.32 \u00b1 0.71 87.07 \u00b1 0.61\nGen-TE 96.11 \u00b1 0.49 96.09 \u00b1 0.51 97.93 \u00b1 0.36 98.08 \u00b1 0.31 86.85 \u00b1 0.81 87.67 \u00b1 0.80\nO(d)-ensemble 94.99 \u00b1 1.23 95.00 \u00b1 1.21 96.31 \u00b1 0.63 96.52 \u00b1 0.59 86.16 \u00b1 0.71 87.03 \u00b1 0.62\nDiag-ensemble 95.89 \u00b1 0.68 95.89 \u00b1 0.67 96.92 \u00b1 0.91 97.09 \u00b1 0.89 86.62 \u00b1 0.91 87.45 \u00b1 0.82\nGen-ensemble 96.16 \u00b1 0.52 96.12 \u00b1 0.54 97.46 \u00b1 0.64 97.62 \u00b1 0.60 86.92 \u00b1 1.10 87.79 \u00b1 0.95\nO(d)-NE 94.65 \u00b1 0.99 94.64 \u00b1 0.96 96.55 \u00b1 0.56 96.69 \u00b1 0.54 86.38 \u00b1 0.93 87.11 \u00b1 1.02\nDiag-NE 94.94 \u00b1 0.60 94.90 \u00b1 0.59 97.01 \u00b1 0.71 97.22 \u00b1 0.67 86.87 \u00b1 1.01 87.73 \u00b1 0.81\nGen-NE 96.13 \u00b1 0.39 96.09 \u00b1 0.38 97.68 \u00b1 0.55 97.83 \u00b1 0.51 86.54 \u00b1 0.71 87.46 \u00b1 0.74\nO(d)-EE 95.00 \u00b1 0.80 95.00 \u00b1 0.79 96.51 \u00b1 0.64 96.73 \u00b1 0.56 87.12 \u00b1 0.75 87.88 \u00b1 0.67\nDiag-EE 95.09 \u00b1 0.51 95.05 \u00b1 0.51 96.86 \u00b1 0.47 97.07 \u00b1 0.47 85.99 \u00b1 0.63 86.85 \u00b1 0.58\nGen-EE 96.39 \u00b1 0.37 96.35 \u00b1 0.36 97.57 \u00b1 0.69 97.73 \u00b1 0.62 86.60 \u00b1 0.93 87.51 \u00b1 0.92\nO(d)-NT 94.30 \u00b1 1.10 94.28 \u00b1 1.12 96.63 \u00b1 0.44 96.78 \u00b1 0.42 86.35 \u00b1 1.04 87.26 \u00b1 0.94\nDiag-NT 94.98 \u00b1 0.58 94.96 \u00b1 0.60 96.96 \u00b1 0.41 97.11 \u00b1 0.36 85.85 \u00b1 1.07 86.78 \u00b1 0.91\nGen-NT 96.12 \u00b1 0.36 96.12 \u00b1 0.32 97.88 \u00b1 0.47 98.04 \u00b1 0.43 86.92 \u00b1 0.95 87.76 \u00b1 0.85\nO(d)-ET 94.37 \u00b1 1.35 94.32 \u00b1 1.37 96.15 \u00b1 0.46 96.33 \u00b1 0.46 85.92 \u00b1 0.68 87.01 \u00b1 0.57\nDiag-ET 95.44 \u00b1 0.37 95.40 \u00b1 0.38 97.34 \u00b1 0.56 97.55 \u00b1 0.49 85.96 \u00b1 0.74 86.90 \u00b1 0.69\nGen-ET 95.84 \u00b1 0.65 95.82 \u00b1 0.65 97.69 \u00b1 0.47 97.83 \u00b1 0.47 86.12 \u00b1 0.82 87.05 \u00b1 0.69\nrepeated on 10 random splits for each dataset, with the average and standard deviation reported. For\nnode classification, we report the micro-average and macro-averaged F1 scores. We report the mean\nreciprocal rank (MRR) and the ROC-AUR for link prediction. The negative test samples for link\nprediction are generated randomly [16, 51, 6].\n5.2 Experimental Results\nNode classification. Table 1 reports the node classification results on three benchmark datasets,\nwith the mean Macro-F1 and Micro-F1 and their standard deviation reported. The overall trend is\nthat HetSheaf achieves competitive results across all datasets and state-of-the-art performance on\nboth ACM and DBLP. For instance, on ACM, the Micro F1 score is 96.35 %, while that of the best\n8competitor is 95.81 %; moreover, HetSheaf has a smaller standard deviation 0.37, compared to 0.39\nof its competitor. On IMDB, HetSheaf obtains comparable results with both R-GCN and HGT with a\nMicro F1 score of 87.88 % compared to 88.45 of HGT and 89.09 % of R-GCN, with a much smaller\nparameter budget (see Table 7). The superior performance of HetSheaf on the node classification\nsuggests that the sheaf can learn richer representations to preserve node heterogeneity in the various\nbenchmark datasets.\nLink prediction. Table 2 reports the link prediction results on LastFM and MovieLens, with the\nmean AUROC and AUPR and their standard deviation reported. HetSheaf achieves the best perfor-\nmance on both datasets and both metrics. For instance, on LastFM, HetSheaf achieves an AUROC\nof 87.80 %, which is 1.22 % higher than HetSheaf-NSD, the best competitor and the AUPR of Het-\nSheaf is 98.24 % compared to 97.16 of HetSheaf-NSD. On MovieLens, a similar story emerges, but\nHetSheaf performs only slightly better than HetSheaf-NSD. The power of the sheaf is demonstrated\nby the fact that the two best-performing models for link prediction, HetSheaf-NSD and HetSheaf,\nare both sheaf architectures.\n5.3 Model Analysis\nSheaf predictor ablation study. For node classification, all the sheaf modifications perform better\nor similar compared to the vanilla feature concatenation sheaf learners used by HetSheaf-NSD,\ndemonstrating that these modifications are useful for processing heterogeneous data. This suggests\nthat the type information encodes vital information required for heterogeneous tasks; it is also visible\nin the performance gap between the homogeneous and heterogeneous GNNs. For these particular\ndatasets, Table 3 suggests that the type information is more informative than the node features as\nsheaves that learn the restriction maps using just the type information (HetSheaf-NT and HetSheaf-\nET) outperform HetSheaf-NSD.\nTable 4 repeats the ablation studies for the link prediction datasets. In both datasets, the heteroge-\nneous sheaf learners perform comparably to or outperform the base HetSheaf-NSD implementation.\nIn LastFM, they perform better than HetSheaf-NSD, and on MovieLens embedding, just the type\ninformation performs 0.02 % to 0.05 % lower than HetSheaf-NSD. This again illustrates the im-\nportance of the type information, which appears to be more informative than the local features.\nInterestingly, HetSheaf-NT, which predicts the restriction maps using only the node type informa-\ntion, performs the best on LastFM. This is likely an artefact of the dataset as each edge type is type\nspecific . That is, they only connect nodes of certain types. As we attempt to predict the edges of\na certain type, it is natural that including only the node type information performs the best, as the\nmodel must only learn which node types define that edge type. A more interesting test could use a\ndataset in which an edge type may connect different node types.\nEffect of restriction map type. Table 3 also shows the impact of the restriction map type on each\nsheaf model\u2019s performance for node classification tasks. Across the board, general restriction maps\nperform the best, with orthogonal restriction maps performing the worst. This is despite the theoret-\nical benefits of orthogonal restriction maps because of their connection with parallel transport. The\nmost likely reason for better performance with general restriction maps is that they are more expres-\nsive and can encode more complex interactions in the edge stalks compared to the other methods.\nThe poor performance of orthogonal restriction maps is due to the transformations they represent\nnot being sufficiently complex to encode the structure and interactions in the heterogeneous graph\ndatasets. Interestingly, this is in contrast to Bodnar et al. [4], which found that orthogonal restric-\ntion maps performed better on their datasets. This suggests that the best restriction map properties\ndepend on the particular task.\nWe may repeat this for the link prediction tasks. Table 4 shows the effect of each restriction map\ntype on the performance of the sheaf architecture. Unlike the node classification results above, the\nrestriction maps appear to have a much smaller effect on the overall model performance. Aligning\nwith both Bodnar et al. [4], Duta et al. [12], the diagonal restriction maps perform well on HetSheaf-\nNSD. The results for the heterogeneous sheaf learners are more mixed, but generally, the general\nrestriction maps seem to perform slightly better than either diagonal or orthogonal restriction maps.\nHowever, this difference is 0.5 % compared to the performance gap of 1 % to 2 % observed with\ngeneral restriction maps for node classification.\n9Table 4: Restriction map and sheaf predictor ablation study for heterogeneous link prediction.\nThe highest performing restriction map type for each sheaf learner is highlighted , and the highest\nperforming sheaf learner is shown in bold .\nLastFM MovieLens\nAUPR AUROC AUPR AUROC\nO(d)-NSD 96.91 \u00b1 0.02 96.35 \u00b1 0.21 99.62 \u00b1 0.06 99.52 \u00b1 0.05\nDiag-NSD 97.16 \u00b1 0.19 96.58 \u00b1 0.18 99.66 \u00b1 0.04 99.57 \u00b1 0.03\nGen-NSD 97.01 \u00b1 0.13 96.44 \u00b1 0.18 99.66 \u00b1 0.06 99.55 \u00b1 0.07\nO(d)-TE (ours) 97.70 \u00b1 0.61 97.23 \u00b1 0.63 99.58 \u00b1 0.05 99.49 \u00b1 0.06\nDiag-TE (ours) 97.70 \u00b1 0.55 97.21 \u00b1 0.64 99.63 \u00b1 0.08 99.53 \u00b1 0.08\nGen-TE (ours) 97.71 \u00b1 0.52 97.17 \u00b1 0.55 99.65 \u00b1 0.03 99.57 \u00b1 0.04\nO(d)-ensemble (ours) 97.15 \u00b1 0.11 96.46 \u00b1 0.13 99.61 \u00b1 0.08 99.51 \u00b1 0.08\nDiag-ensemble (ours) 97.93 \u00b1 0.25 97.35 \u00b1 0.28 99.68 \u00b1 0.04 99.59 \u00b1 0.04\nGen-ensemble (ours) 98.21 \u00b1 0.15 97.71 \u00b1 0.18 99.66 \u00b1 0.06 99.57 \u00b1 0.05\nO(d)-NE (ours) 97.90 \u00b1 0.68 97.43 \u00b1 0.07 99.62 \u00b1 0.05 99.52 \u00b1 0.04\nDiag-NE (ours) 97.62 \u00b1 0.51 97.13 \u00b1 0.53 99.66 \u00b1 0.05 99.56 \u00b1 0.06\nGen-NE (ours) 98.00 \u00b1 0.46 97.51 \u00b1 0.51 99.66 \u00b1 0.04 99.57 \u00b1 0.04\nO(d)-EE (ours) 96.98 \u00b1 0.17 96.42 \u00b1 0.17 99.59 \u00b1 0.07 99.50 \u00b1 0.06\nDiag-EE (ours) 97.40 \u00b1 0.49 96.84 \u00b1 0.52 99.67 \u00b1 0.05 99.57 \u00b1 0.05\nGen-EE (ours) 97.51 \u00b1 0.44 96.91 \u00b1 0.52 99.63"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Summary of the Paper: \"From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback\"\n\n---\n\n#### 1. Abstract:\n\n- **Overview**: This paper introduces iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller\u2019s cognitive model of explanation, designed to provide actionable feedback to students in online courses.\n- **Main Contributions**:\n  - **Pipeline Stages**: iLLuMinaTE operates through causal connection, explanation selection, and explanation presentation stages.\n  - **Evaluation**: Extensively evaluated on 21,915 explanations from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B) using three XAI methods (LIME, Counterfactuals, MC-LIME).\n  - **Findings**: A user study with 114 university students shows iLLuMinaTE's explanations are preferred over traditional methods 89.52% of the time.\n\n#### 2. Introduction:\n\n- **Context**: Advances in eXplainable AI (XAI) for education need to ensure model explanations are understandable to non-technical users like educators and students.\n- **Research Questions**: How to make AI model explanations actionable for students? How to bridge the gap in understandability and trust in AI for education?\n- **Contribution**:\n  - Proposing iLLuMinaTE to generate theory-driven, actionable feedback using a zero-shot, chain-of-prompts approach.\n  - Leveraging eight social science theories to enhance explanation and feedback quality.\n- **Literature Review**:\n  - **Current XAI in Education**: Focuses on model mistake auditing, trust building, and personalized interventions.\n  - **Limitations of Existing Methods**: Post-hoc explainers like LIME and SHAP lack understandability for non-technical users.\n  - **Role of LLMs**: Potential to improve human-interpretability of XAI outputs, although care is needed to avoid the pitfalls of \"stochastic parrots\".\n\n#### 3. Related Work:\n\n- **Prior Efforts**:\n  - **XAI Methods in Education**: Post-hoc methods like LIME, SHAP used but with limited integration into classroom settings.\n  - **LLMs as Post-hoc Explainers**: Studies show LLMs can identify relevant features but require alignment with specific domains to avoid inaccuracies.\n- **Advancements Offered**:\n  - **Novel Framework**: iLLuMinaTE uses LLMs not just for post-hoc explanation but for providing actionable, theory-driven feedback.\n  - **Multi-Model, Multi-Theory Approach**: Extends capability across multiple models and theories, ensuring robustness and generalizability.\n\n#### 4. Method:\n\n- **Pipeline Steps**:\n  - **Student Modeling**: Behavioral features extracted from clickstreams, BiLSTMs used for success prediction.\n  - **XAI Methods**: LIME, CEM, and MC-LIME applied to extract feature importance.\n  - **Explanation Selection**: Prompts aligned with eight social science theories (e.g., Abnormal Conditions, Pearl\u2019s Model).\n  - **Explanation Presentation**: Final feedback generated by further refining the selected explanations.\n- **Technical Architecture**:\n  - **Model Details**: Used BiLSTMs for predictive modeling; feature extraction covers regularity, engagement, control, and participation metrics.\n  - **Social Science Theories**: Detailed prompts crafted for each theory to guide the explanation selection by LLMs.\n\n#### 5. Experimental Setup:\n\n- **Datasets**: Digital Signal Processing (DSP), Villes Africaines (VA), and Elements de Geomatique (Geo) MOOCs, collecting interaction data from students.\n- **Hardware/Software**: Experiments run on a setup leveraging LangChain, Groq Cloud, and Replicate infrastructure.\n- **Evaluation Metrics**:\n  - **Explanation Alignments**: Human and LLM annotations to assess alignment with theory and understandability.\n  - **User Preference Study**: Conducted with 114 university students, evaluating preferences through a novel actionability simulation.\n- **Baselines**: Compared against traditional post-hoc explanation methods for effectiveness and preferences.\n\n#### 6. Results and Analysis:\n\n- **Empirical Findings**:\n  - **Overall Preference**: Students preferred iLLuMinaTE explanations 89.52% of the time over traditional methods.\n  - **Actionability Simulation**: Demonstrated practical gains in student performance when following iLLuMinaTE feedback.\n- **Detail Insights**:\n  - **Theory Performance**: Abnormal, Pearl, and Contrastive explanations were particularly effective.\n  - **Statistical Analysis**: Detailed breakdown of user preferences, annotated scores from human and LLM evaluators.\n  - **Visualization**: Graphs and tables detailing preference distributions and effect sizes of different explanation strategies.\n- **Additional Findings**:\n  - **Annotation Consistency**: High agreement between human and GPT-4o-based annotations.\n  - **Feedback Quality**: Positive effects of using Grice\u2019s maxims and learning science principles in feedback refinement.\n\n#### 7. Discussion:\n\n- **Interpretation of Results**:\n  - **Effectiveness**: iLLuMinaTE's approach significantly enhances the understandability and actionability of AI-driven feedback in educational settings.\n  - **Theoretical Implications**: Validates the use of social science theories for improving the quality of AI explanations.\n  - **Limitations**:\n    - Potential biases in annotation tasks.\n    - Dependency on the quality of the initial modeling and feature extraction.\n- **Future Directions**:\n  - Extend use to other human-centric fields like healthcare and social welfare.\n  - Investigate further refinements in prompt engineering and fusion with real-time data analysis.\n\n#### 8. Conclusion:\n\n- **Summary**:\n  - iLLuMinaTE as a robust, reusable framework for delivering actionable AI explanations in education.\n  - Effectiveness of theory-driven prompts and LLMs in generating understandable and actionable feedback.\n- **Future Work**:\n  - Broaden the application of iLLuMinaTE to other domains.\n  - Explore integration with additional AI models and XAI methods for enhanced performance and generalizability.\n\n---\n\nThis comprehensive summary distils the paper to its foundational elements, providing sufficient detail for recreation and further development. It encapsulates the methodology, experimental design, empirical findings, and implications with precision, allowing a researcher to replicate or build upon the work effectively."}, {"role": "assistant", "content": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for\nStudent Performance Feedback\nVinitra Swamy*1, Davide Romano*1, Bhargav Srinivasa Desikan2,\nOana-Maria Camburu3, Tanja K \u00a8aser1\n1EPFL\n2IPPR\n3UCL\nAbstract\nRecent advances in eXplainable AI (XAI) for education\nhave highlighted a critical challenge: ensuring that expla-\nnations for state-of-the-art AI models are understandable\nfor non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE , a zero-shot, chain-\nof-prompts LLM-XAI pipeline inspired by Miller (2019)\u2019s\ncognitive model of explanation. iLLuMinaTE is designed\nto deliver theory-driven, actionable feedback to students in\nonline courses. iLLuMinaTE navigates three main stages\n\u2014 causal connection, explanation selection, and explana-\ntion presentation \u2014 with variations drawing from eight\nsocial science theories (e.g. Abnormal Conditions, Pearl\u2019s\nModel of Explanation, Necessity and Robustness Selection,\nContrastive Explanation). We extensively evaluate 21,915\nnatural language explanations of iLLuMinaTE extracted\nfrom three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with\nthree different underlying XAI methods (LIME, Counter-\nfactuals, MC-LIME), across students from three diverse on-\nline courses. Our evaluation involves analyses of explana-\ntion alignment to the social science theory, understandabil-\nity of the explanation, and a real-world user preference study\nwith 114 university students containing a novel actionabil-\nity simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time.\nOur work provides a robust, ready-to-use framework for ef-\nfectively communicating hybrid XAI-driven insights in ed-\nucation, with significant generalization potential for other\nhuman-centric fields.\n1 Introduction\nOver the last decade, AI has seen widespread application\nin education, encompassing both learner-centric models \u2014\nsuch as intelligent tutoring systems (Mousavinasab et al.\n2021), knowledge tracing (Piech et al. 2015), and auto-\nmated feedback systems (Jacobsen and Weber 2023) \u2014 and\nteacher-centric models, including real-time classroom in-\nsights (Holstein et al. 2018) and automated question gen-\neration (Hang, Tan, and Yu 2024). To adopt these solu-\ntions in real-world classrooms, model explainability is es-\nsential. Nazaretsky et al. (2022) underscore the importance\nof transparency for fostering educators\u2019 trust in AI-based ed-\nucational technologies, while Conati, Porayska-Pomsta, and\nMavrikis (2018) stress the need for interpretable models in\n*These authors contributed equally to this work.contexts where students see decision outcomes without un-\nderstanding the underlying reasoning.\nRecent literature on eXplainable AI (XAI) in education\ncan be categorized into three main motivations: (1) allowing\neducational stakeholders to audit model mistakes (Khosravi\net al. 2022; Pinto and Paquette 2024), (2) building student\nand teacher trust in AI (Nazaretsky et al. 2024), and (3) de-\nsigning personalized interventions for students (Hur et al.\n2022; Asadi et al. 2023). The most popular approaches in\nXAI for education are post-hoc explainers, like LIME (Ha-\nsib et al. 2022; Vultureanu-Albis \u00b8i and B \u02d8adic\u02d8a 2021; Scheers\nand De Laet 2021; Pei and Xing 2021) and SHAP (Baranyi,\nNagy, and Molontay 2020; Mu, Jetten, and Brunskill 2020).\nThese explainers treat the underlying model as a black-box,\nenabling explanations after model training.\nDespite their rising popularity, XAI methods suffer from\na major weakness: a lack of adequate understandability, es-\npecially for a non-technical audience. At a course level,\nSTEM professors expressed difficulty in understanding ex-\nplainer outputs, requesting \u201cmore concrete and granular in-\nsights\u201d on the scale of individual students (Swamy et al.\n2023). At the individual student level, Hur et al. (2022) de-\nsigned XAI-based interventions, but found they were exten-\nsive to integrate and provided limited learning gains com-\npared to expert feedback. With pervasive educational bench-\nmarks like ASSISTments (Heffernan and Heffernan 2014)\nand MOOCRadar (Yu et al. 2023) having hundreds of fea-\ntures and temporal aspects, it becomes difficult for students\nand teachers to interpret feature importance.\nLLMs can be useful in making XAI more human-\ninterpretable, especially towards building stakeholder trust\nin AI and designing personalized student interventions. A\nrecent study (Kroeger et al. 2024) suggests that LLMs can\nact as post-hoc explainers for complex models, finding that\nLLMs could identify relevant features when given examples\nof input data and model outputs. Atanasova et al. (2022) in-\ntegrate explanation generation directly into the LLM\u2019s train-\ning, optimizing over diagnostic properties like data consis-\ntency and confidence. However, in domains where explana-\ntions directly influence human decisions, the nature of LLMs\nas \u201cstochastic parrots\u201d can be accompanied by detrimental\nside effects (Bender et al. 2021; Sarkar 2024). LLMs, while\ndemonstrating potential in areas such as knowledge tracing\nor student synthesis, have not yet matured enough to actarXiv:2409.08027v1  [cs.CY]  12 Sep 2024Student ModelingXAI: Causal Connection \nExplanation SelectionExplanation Presentation\nPassFail\nwhy\nLIME\nF1,  F2,  F3,  F4,  \u2026  F42,  F43,  F44,  F45 CEMMC-LIME\nWhere am I going?How am I doing?Where to next?StudentFeedbackTheory\nPassFail\nStudent Preference Study\nAny student model + explainer + LLM can be used in iLLuMinaTE.\nGPT4O  /  Llama3  / Gemma240B           9B\nTextExplanations\nPassFail\nwhy\nLIME\nF1,  F2,  F3,  F4,  \u2026  F42,  F43,  F44,  F45 CEMMC-LIME\nPassFail\nwhy\nLIME\nF1,  F2,  F3,  F4,  \u2026  F42,  F43,  F44,  F45 CEMMC-LIME\nmodel predictioncourse contextimportancescores\nGPT4O  /  Llama3  / Gemma240B           9BLarge Language ModelSocial Science TheoryInstructions\ncourse context+ student feature valuesLarge Language ModelGrice\u2019s Maxims\niLLuMinaTEExplanations\nFigure 1: iLLuMinaTE involves five steps : 1) modeling course data for student success prediction, 2) using XAI methods\nto extract feature importance scores, 3) selecting important aspects of the explanation through an LLM aligned with a given\nsocial science theory, 4) converting this report into concise and actionable suggestions through an LLM, 5) evaluating student\npreferences of different explanation strategies through a real-world user study.\nas student models that can be accompanied by inherent ex-\nplanations (Neshaei et al. 2024; Nguyen, Tschiatschek, and\nSingla 2023). We instead propose to use LLMs as commu-\nnicators of explanations (Zytek, Pid `o, and Veeramachaneni\n2024) to present XAI outputs in aligned text and visual for-\nmats that are actionable for educational stakeholders.\nWe therefore present iLLuMinaTE , an in-context, chain-\nof-prompts, zero-shot LLM pipeline that is inspired by\nMiller\u2019s cognitive processes of explanation (Miller 2019).\niLLuMinaTE follows three main stages: (1) causal con-\nnection, (2) explanation selection, and (3) explanation pre-\nsentation. Our experiments range over eight prevalent so-\ncial science theories of explanation (Hilton 1990; Hilton and\nSlugoski 1986; Halpern and Pearl 2005; Lombrozo 2010;\nWoodward and Ross 2021), with three underlying explainers\n(LIME, Counterfactuals, MC-LIME), data from three on-\nline courses, evaluated using three LLMs (GPT-4o, Gemma2\n9b, Llama3 70b) and a real-world user study with 114uni-\nversity students. Notably, we find that students preferred\niLLuMinaTE \u2019s explanations over baseline hybrid (text and\nvisual) explanations from post-hoc methods 89.52% of the\ntime, and had a particular preference on actionability for ab-\nnormal, pearl, and contrastive explanations. With our study,\nwe make the following main contributions:\n1.iLLuMinaTE , a chain-of-prompts framework in the\neducation context to extract theory-driven natural lan-\nguage explanations (NLE) for student feedback.\n2.An LLM-XAI efficacy analysis of216 variations of\niLLuMinaTE prompting strategies over explainers,\nLLMs, social science theories, and student populations.\n3.A real-world evaluation of LLM-XAI preferences\nconducted with 114university students.\n4.An XAI actionability study simulating student perfor-\nmance gains based on actions they selected from gener-\nated explanations.\nWe provide our modular implementation of\niLLuMinaTE publicly with adaptations for LangChain,\nGroq Cloud, and Replicate1. Our work provides a theory-\ndriven methodology to communicate results of XAI to\n1https://github.com/epfl-ml4ed/iLLuMinaTEstudents, with broad generalization potential of explanation\ntheory instruction prompts for other human-centric fields\n(e.g., healthcare, welfare, product recommendation).\n2 Methodology\nOuriLLuMinaTE pipeline (see Fig. 1) consists of five\nstages towards communicating explanations in a human-\nunderstandable and actionable way through LLMs.\nIn the Student Modeling phase, we extract behavioral\nfeatures from raw clickstreams of student interactions and\nuse BiLSTMs (Graves and Schmidhuber 2005) to predict\nstudent success following prior work (Asadi et al. 2023;\nSwamy, Marras, and K \u00a8aser 2022). We then employ post-hoc\nexplainers to obtain feature importance scores, represent-\ning the XAI: Causal Connection step. With the results from\nan explainer, the course context, and the student\u2019s feature\nvalues, we prompt an LLM using Explanation Selection in-\nstructions specific to social science theories of explanations.\nWe evaluate the obtained explanations using human expert\nand GPT-4o annotations. We then use Explanation Presen-\ntation prompts to summarize the often verbose explanation\nselection reports into concise and actionable feedback for\na student, taking into account theory on effective feedback\n(Hattie and Timperley 2007)\u2019s and maxims for communica-\ntion (Grice 1975). We then evaluate the final explanations\nusing expert annotations. In the final phase, we assess Stu-\ndent Preferences and the actionability of iLLuMinaTE ex-\nplanations in comparison with the current state-of-the-art in\nXAI for education in a user study with 114students.\n2.1 Student Modeling\nTo create the models building the basis for the explanations,\nwe use the same features and model architectures as prior re-\nsearch working with the same datasets (Swamy, Marras, and\nK\u00a8aser 2022; Galici et al. 2023; Swamy et al. 2023, 2024b).\nData Collection . Our experiments are based on data col-\nlected from three MOOCs (Digital Signal Processing (DSP),\nVilles Africaines (V A), and Elements de Geomatique (Geo))\noffered by a European University to a global student au-\ndience. The courses were organized into weekly modules\nincluding video lectures and quizzes and required studentsto complete graded assignments to earn course certificates.\nStudents interacted with learning objects (videos, quizzes)\nassociated with specific course weeks, enabling the creation\nof course-specific learning indicators. We represent a stu-\ndent\u2019s interations as a time series Ic\ns=i1, . . . , i K, where\neach interaction iis a tuple (t, a, o ), including a timestamp\nt, an action a(e.g., video play, pause; quiz submission),\nand a learning object o. The binary success label (pass-fail)\nfor student sin course cis denoted as ys,c. Data collection\nand analysis were approved by the university\u2019s ethics review\nboard (Nr. Anonymous).\nFeature Extraction. We use a broad set of 45behavioral\nfeatures hderived from the student interactions, incorporat-\ning features from four feature sets shown to be predictive\nfor student performance in MOOCs (Marras, Vignoud, and\nK\u00a8aser 2021). The key behavioral aspects include:\n\u2022Regularity (3 features): Tracks consistent study habits\n(Boroujeni et al. 2016).\n\u2022Engagement (13 features): Measures overall course in-\nvolvement (Chen and Cui 2020).\n\u2022Control (22 features): Assesses detailed video usage\n(Lall \u00b4e and Conati 2020).\n\u2022Participation (7 features): Monitors attendance in sched-\nuled activities (Marras, Vignoud, and K \u00a8aser 2021).\nModeling . For a course cand student s, our objective is to\nbuild a model that predicts ys,cearly , using the features hs\nfrom the first five weeks. Following prior work (Swamy,\nMarras, and K \u00a8aser 2022), we employ a BiLSTM for this\ntask. We provide all reproducibility details in Appendix 6.\n2.2 XAI: Causal Connection\nWe use three popular post-hoc explainers (LIME, CEM, and\nMC-LIME) to extract local, instance-specific explanations\nfrom the student models. We chose these three methods\nbased on their popularity, but any in-hoc or interpretable-\nby-design model could be used.\nLIME (Local Interpretable Model-Agnostic Explanations)\n(Ribeiro, Singh, and Guestrin 2016) provides interpretable\nexplanations for individual predictions by approximating the\ncomplex model locally with an interpretable model. This\nprocess results in a set of feature weights indicating the pos-\nitive or negative influence of each feature on the prediction.\nCEM (Contrastive Explanation Method) (Dhurandhar et al.\n2018) is a counterfactual method that identifies the features\nthat need to be present (pertinent positives) or absent (perti-\nnent negatives) for a model to maintain its prediction for a\ngiven instance.\nMC-LIME (Minimal Counterfactual LIME) (Cohausz\n2022) finds the minimal set of LIME features that, if\nchanged, would alter the outcome. It focuses on features that\nincrease the likelihood of an event (e.g., student dropout).\nMC-LIME applies changes to individual features, then pairs,\nand so on, until a change flips the prediction. This approach\ncombines explanation sparseness with the advantages of\ncounterfactuals and LIME.\n\u2022Regularity(3): Tracks consistent study habits (Boroujeniet al. 2016).\u2022Engagement(13): Overall course involvement (Chen andCui 2020).\u2022Control(22): Assesses detailed video usage (Lall\u00b4e andConati 2020).\u2022Participation(7): Monitors attendance in scheduled ac-tivities (Marras, Vignoud, and Kaser 2021).Modeling. For a given coursec, our objective is to build amodel that predictsearlythe success labelysfor a students,using the extracted behavioral featureshsfrom the \ufb01rst \ufb01veweeks. Following prior work (Swamy, Marras, and K\u00a8aser2022), we employ a BiLSTM (two BiLSTM layers of size32, loopback3, followed by a dense layer) for this task. Weprovide further experimental details in the Appendix.2.2 XAI: Causal ConnectionWe use three popular post-hoc explainers (LIME, CEM, andMC-LIME) to extract local, instance-speci\ufb01c explanationsfrom the student models. We chose these three methodsbased on their popularity, but any in-hoc or interpretable-by-design model could be used.LIME(Local Interpretable Model-Agnostic Explanations)(Ribeiro, Singh, and Guestrin 2016) provides interpretableexplanations for individual predictions by approximating thecomplex model locally with an interpretable model. Thisprocess results in a set of feature weights indicating the pos-itive or negative in\ufb02uence of each feature on the prediction.CEM(Contrastive Explanation Method) (Dhurandhar et al.2018) is a counterfactual method that identi\ufb01es the featuresthat need to be present (pertinent positives) or absent (perti-nent negatives) for a model to maintain its prediction for agiven instance.MC-LIME(Minimal Counterfactual LIME) (Cohausz2022) \ufb01nds the minimal set of LIME features that, ifchanged, would alter the outcome. It focuses on features thatincrease the likelihood of an event (e.g., student dropout).MC-LIME applies changes to individual features, then pairs,and so on, until a change \ufb02ips the prediction. This approachcombines explanation sparseness with the advantages ofcounterfactuals and LIME.2.3 Explanation SelectionOuriLLuMinaTEpipeline generates explanations alignedto social science theories. Speci\ufb01cally, we have translatedeight theories into prompts:Relevance Selection (RS), Ab-normal Conditions (AC), Pearl\u2019s Model (Pearl\u2019s), Neces-sity and Robustness (NR), two contrastive explanation vari-ations, Statistical Relevance (SR),andChain of Thought(CoT). Our prompt structure contains (1) descriptions of themodel, features, XAI method, and course context, (2) ex-plainer importance scores and relevant student feature val-ues, and (3) social science theory instructions.Explanation selection templateYou are an AI assis-tant that analyzes struggling students behavior to helpthem in their learning trajectories and facilitate learn-ing in the best possible way. You have the followinginformation to help you in your goal:\u2022A model prediction of student performance at theend of the course, in the form of \u201cpass\u201d or \u201cfail\u201d.\u2022A post-hoc explainable AI approach that identi\ufb01eswhich features are important to this student\u2019s pre-diction.\u2022Data in the form of student\u2019s features over 5 weeksthat were used by the model. You will see alwaysthe most relevant features selected by{explainer}.\u2022Data in the form of student\u2019s features over 5 weeksthat were used by the model. You will see alwaysthe most relevant features selected by{explainer}.\u2022The course{coursename}content and structure.\u2022Detailed instructions on how to reason.{modeldescription}{featuresdescription}{explainerdescription}{coursedescription}Take into consideration this data:{explainerimportancescores}{studentfeaturevalues}INSTRUCTIONS:{theoryinstructions}QUESTION: Given the information above, follow theinstructions precisely and write a small report on whatyou found. Only use the results from the explainable AIapproach and the student\u2019s behavior data to justify yourconclusions.Relevance Selection.Based on Hilton\u2019s conversationalmodel of explanation (Hilton 1990), the goal of relevance-based selection theory is to \u201cresolve a puzzle in the ex-plainee\u2019s mind about why the event happened by closing agap in his or her knowledge\u201d. The theory emphasizes thatshared knowledge between the explainer and the explaineeare presuppositions of the explanations, and the other fac-tors are the causes that should be explained. In short, theexplainer should not explain any causes they think the ex-plainee already knows.Abnormal conditions.This theory, based upon Hilton andSlugoski\u2019s abnormal conditions model (Hilton and Slugoski1986), suggests that explanations often rely on unusual andtemporally proximal events. People do not solely count onstatistical likelihood but highlight uncommon factors con-tributing to an event to explain that event. During a conversa-tion, the explainer is relying on the perceived common priorknowledge to identify potential causes that are consideredabnormal, with greater weight given to temporally proximal2.3 Explanation Selection\nOuriLLuMinaTE pipeline generates explanations aligned\nto social science theories. Specifically, we have translated\neight theories into prompts: Relevance Selection (RS), Ab-\nnormal Conditions (AC), Pearl\u2019s Model (Pearl\u2019s), Neces-\nsity and Robustness (NR), two contrastive explanations (BC,\nCon), Statistical Relevance (SR), and Chain of Thought\n(CoT) . Our prompt structure contains (1) descriptions of the\nmodel, features, XAI method, and course context, (2) ex-\nplainer importance scores and relevant student feature val-\nues, and (3) social science theory instructions. We present\nthe general prompt template and two theory-specific exam-\nples (RS, AC) in this section, all details are in Appendix 7.1.\nRelevance Selection. Based on Hilton\u2019s conversational\nmodel of explanation (Hilton 1990), relevance-based selec-\ntion theory aims to \u201cresolve a puzzle in the explainee\u2019s\nmind\u201d by filling gaps in their knowledge. The theory em-\nphasizes that shared knowledge between the explainer and\nthe explainee are presuppositions of the explanations, and\nthe other factors are the causes that should be explained. In\nshort, the explainer should not explain any causes they thinkthe explainee already knows.\nAbnormal conditions. This theory, based upon Hilton and\nSlugoski\u2019s abnormal conditions model (Hilton and Slugoski\n1986), suggests that explanations often rely on unusual and\ntemporally proximal events. People do not solely count on\nstatistical likelihood but highlight uncommon factors con-\ntributing to an event to explain that event. During a conversa-\ntion, the explainer is relying on the perceived common prior\nknowledge to identify potential causes that are considered\nabnormal, with greater weight given to temporally proximal\nevents and factors that the explainee can control. This focus\non controllable factors helps the explainee understand how\nto potentially avoid similar situations in the future.\nPearl\u2019s Model of explanation. Halpern and Pearl (2005)\npresent a formal framework for selecting explanations based\non epistemic relevance and structural causal models. The\nmodel distinguishes between exogenous variables , whose\nvalues are determined by external factors, and endogenous\nvariables , whose values are influenced by relationships with\nother variables. Within a context (a specific assignment of\nvalues to variables), the model defines an actual cause as a\nminimal set of events that must occur for an event to happen.\nNecessity & Robustness selection. Two key criteria for se-\nlecting strong explanatory causes are necessity and robust-\nness (Lipton 1990). Necessity refers to whether a cause is\nessential for the effect to occur. Robustness considers how\ngenerally a cause applies (Lombrozo 2010). This idea aligns\nwith the concept of simplicity, where broader explanations\nwith fewer specific requirements are favored.\nContrastive Explanation. This theory suggests explana-\ntions are not simply cause and effect statements, but rather\ncomparisons between what happened (the target event) and\nwhat could have happened (a counterfactual contrasting\nevent) (Hilton 1990). One possible way to make the identi-\nfication of the counterfactual event (foil) successful is to ask\nthe module to reformulate the question. This technique is\nknown as Rephrase and Respond (RaR) (Deng et al. 2024).\nStatistical Relevance. This method is based on the SR\nmodel based on scientific causal reasoning (Woodward and\nRoss 2021). The SR model explanations can be defined in\nsimple terms as \u201cstatistically relevant properties are explana-\ntory and statistically irrelevant properties are not\u201d. It follows\nthis structure: \u201cBased on empirical data, factors A, B and C\ncontribute to the probability of Y by the amount of X\u201d .\nChain-of-Thought (baseline). Chain-of-Thought (CoT)\nprompting (Wei et al. 2023) guides an LLM through sequen-\ntial reasoning that mimics human thought processes.\nEvaluation. To assess whether iLLuMinaTE responses\nalign with post-hoc explanations, student feature values, and\ncourse context, we developed an annotation rubric based on\ndecomposed questions as a basis for human and LLM an-\nnotation. Recent studies (Wang et al. 2023) demonstrated\nthat LLMs can match human annotators, especially when\ninstructions are decomposed into simple criteria phrased as\nbinary (\u201cyes\u201d/\u201cno\u201d) questions (Qin et al. 2024). We created\nfour general decomposed questions applicable across all the-\nories and additional theory-specific questions ( 1\u22126, de-\nExplanation selection templateYou are an AI assis-tant that analyzes struggling students behavior to helpthem in their learning trajectories and facilitate learn-ing in the best possible way. You have the followinginformation to help you in your goal:\u2022A model prediction of student performance at theend of the course, in the form of \u201cpass\u201d or \u201cfail\u201d.\u2022A post-hoc explainable AI approach that identi\ufb01eswhich features are important to this student\u2019s pre-diction.\u2022Data in the form of student\u2019s features over 5 weeksthat were used by the model. You will see alwaysthe most relevant features selected by{explainer}.\u2022Data in the form of student\u2019s features over 5 weeksthat were used by the model. You will see alwaysthe most relevant features selected by{explainer}.\u2022The course{coursename}content and structure.\u2022Detailed instructions on how to reason.{modeldescription}{featuresdescription}{explainerdescription}{coursedescription}Take into consideration this data:{explainerimportancescores}{studentfeaturevalues}INSTRUCTIONS:{instructions}QUESTION: Given the information above, follow theinstructions precisely and write a small report on whatyou found. Only use the results from the explainable AIapproach and the student\u2019s behavior data to justify yourconclusions.Relevance-based selection.Based on Hilton\u2019s conversa-tional model of explanation (Hilton 1990), the goal ofrelevance-based selection theory is to \u201cresolve a puzzle inthe explainee\u2019s mind about why the event happened by clos-ing a gap in his or her knowledge\u201d.The theory emphasizes that shared knowledge betweenthe explainer and the explainee are presuppositions of theexplanations, and the other factors are the causes that shouldbe explained. In short, the explainer should not explain anycauses they think the explainee already knows.Relevance-based selection prompt[1]Select the causes that are most relevant to the ques-tion, context and user[2]Select the causes that include information that isnot already shared with the studentAbnormal conditions.This theory is based upon Hilton andSlugoski\u2019s abnormal conditions model (Hilton and Slugoski1986), suggesting that explanations often rely on unusualand temporally proximal events. They argue that people donot solely count on on statistical likelihood but highlight un-common factors that contribute to an event to explain thatevent.During a conversation, the explainer is relying on the per-ceived common prior knowledge to identify potential causesthat are considered abnormal, with greater weight given totemporally proximal events and factors that the explaineecan control. This focus on controllable factors helps the ex-plainee understand how to potentially avoid similar situa-tions in the future.Abnormal conditions model prompt[1]Select potential causes using these criteria:\u2022Abnormality: Tend to prefer abnormal causes.\u2022Temporality: Recent events are more relevant forthe user and considered more mutable.\u2022Controllability: focus on the features that thestudent can control.[2]Select one explanation that follows all of the crite-ria above (Abnormality, Temporality, Controllabil-ity).Pearl\u2019s model of explanation.Halpern and Pearl (2005)provide a formal framework for selecting explanations basedon epistemic relevance and structural causal models. Themodel differentiates betweenexogenous variables, whosevalues are determined by external factors, andendogenousvariables, whose values are in\ufb02uenced by relationships withother variables.Operating within acontext(a speci\ufb01c assignment of val-ues to variables), the model de\ufb01nes anactual causeas a min-imal set of events that must occur for an event to happen.Pearl\u2019s model of explanation prompt[1]Identify variables that are determined by factorsexternal to the model (exogenous variables) andvariables that are determined by relationships withother variables (endogenous variables).[2]Select a set of factors representing the explanationby following the criteria:\u2022The set of factors is minimal - there are no irrel-evant causes in the set.\u2022A change in these factors\u2019 values would result ina reversed prediction.Necessity & Robustness selection.Two key criteria for se-lecting strong explanatory causes are necessity and robust-ness (Lipton 1990).Necessityrefers to whether a cause isAbnormal conditions model prompt1.Select potential causes using these criteria:\u2022Abnormality: Tend to prefer abnormal causes.\u2022Temporality: Recent events are more relevant for theuser and considered more mutable.\u2022Controllability: focus on the features that the studentcan control.2.Select one explanation that follows all of the criteria above(Abnormality, Temporality, Controllability).Pearl\u2019s Model of explanation.Halpern and Pearl (2005)present a formal framework for selecting explanations basedon epistemic relevance and structural causal models. Themodel distinguishes betweenexogenous variables, whosevalues are determined by external factors, andendogenousvariables, whose values are in\ufb02uenced by relationships withother variables. Within acontext(a speci\ufb01c assignment ofvalues to variables), the model de\ufb01nes anactual causeas aminimal set of events that must occur for an event to happen.Necessity & Robustness selection.Two key criteria for se-lecting strong explanatory causes are necessity and robust-ness (Lipton 1990).Necessityrefers to whether a cause isessential for the effect to occur.Robustnessconsiders howgenerally a cause applies (Lombrozo 2010). This idea alignswith the concept of simplicity, where broader explanationswith fewer speci\ufb01c requirements are favored.Contrastive Explanation.This theory suggests explana-tions are not simply cause and effect statements, but rathercomparisons between what happened (the target event) andwhat could have happened (a counterfactual contrastingevent) (Hilton 1990). One possible way to make the identi-\ufb01cation of the counterfactual event (foil) successful is to askthe module to reformulate the question. This technique isknown as Rephrase and Respond (RaR) (Deng et al. 2024).Statistical Relevance.This method is based on the SRmodel based on scienti\ufb01c causal reasoning (Woodward andRoss 2021). The SR model explanations can be de\ufb01ned insimple terms as \u201cstatistically relevant properties are explana-tory and statistically irrelevant properties are not\u201d. It followsthis structure:\u201cBased on empirical data, factors A, B and Ccontribute to the probability of Y by the amount of X\u201d.Chain-of-Thought (baseline).Chain-of-Thought (CoT)prompting (Wei et al. 2023) guides an LLM through sequen-tial reasoning that mimics human thought processes.Evaluation.To assess whetheriLLuMinaTEresponsesalign with post-hoc explanations, student feature values, andcourse context, we developed an annotation rubric based ondecomposed questions as a basis for human and LLM an-notation. Recent studies (Wang et al. 2023) demonstratedthat LLMs can match human annotators, especially wheninstructions are decomposed into simple criteria phrased asbinary (\u201cyes\u201d/\u201cno\u201d) questions (Qin et al. 2024). We createdfour general decomposed questions applicable across all the-ories and additional theory-speci\ufb01c questions (1\u00006, de-pending on the theory). These were validated by a compu-Explanation selection templateYou are an AI assis-tant that analyzes struggling students behavior to helpthem in their learning trajectories and facilitate learn-ing in the best possible way. You have the followinginformation to help you in your goal:\u2022A model prediction of student performance at theend of the course, in the form of \u201cpass\u201d or \u201cfail\u201d.\u2022A post-hoc explainable AI approach that identi\ufb01eswhich features are important to this student\u2019s pre-diction.\u2022Data in the form of student\u2019s features over 5 weeksthat were used by the model. You will see alwaysthe most relevant features selected by{explainer}.\u2022Data in the form of student\u2019s features over 5 weeksthat were used by the model. You will see alwaysthe most relevant features selected by{explainer}.\u2022The course{coursename}content and structure.\u2022Detailed instructions on how to reason.{modeldescription}{featuresdescription}{explainerdescription}{coursedescription}Take into consideration this data:{explainerimportancescores}{studentfeaturevalues}INSTRUCTIONS:{instructions}QUESTION: Given the information above, follow theinstructions precisely and write a small report on whatyou found. Only use the results from the explainable AIapproach and the student\u2019s behavior data to justify yourconclusions.Relevance-based selection.Based on Hilton\u2019s conversa-tional model of explanation (Hilton 1990), the goal ofrelevance-based selection theory is to \u201cresolve a puzzle inthe explainee\u2019s mind about why the event happened by clos-ing a gap in his or her knowledge\u201d.The theory emphasizes that shared knowledge betweenthe explainer and the explainee are presuppositions of theexplanations, and the other factors are the causes that shouldbe explained. In short, the explainer should not explain anycauses they think the explainee already knows.Relevance-based selection prompt[1]Select the causes that are most relevant to the ques-tion, context and user[2]Select the causes that include information that isnot already shared with the studentAbnormal conditions.This theory is based upon Hilton andSlugoski\u2019s abnormal conditions model (Hilton and Slugoski1986), suggesting that explanations often rely on unusualand temporally proximal events. They argue that people donot solely count on on statistical likelihood but highlight un-common factors that contribute to an event to explain thatevent.During a conversation, the explainer is relying on the per-ceived common prior knowledge to identify potential causesthat are considered abnormal, with greater weight given totemporally proximal events and factors that the explaineecan control. This focus on controllable factors helps the ex-plainee understand how to potentially avoid similar situa-tions in the future.Abnormal conditions model prompt[1]Select potential causes using these criteria:\u2022Abnormality: Tend to prefer abnormal causes.\u2022Temporality: Recent events are more relevant forthe user and considered more mutable.\u2022Controllability: focus on the features that thestudent can control.[2]Select one explanation that follows all of the crite-ria above (Abnormality, Temporality, Controllabil-ity).Pearl\u2019s model of explanation.Halpern and Pearl (2005)provide a formal framework for selecting explanations basedon epistemic relevance and structural causal models. Themodel differentiates betweenexogenous variables, whosevalues are determined by external factors, andendogenousvariables, whose values are in\ufb02uenced by relationships withother variables.Operating within acontext(a speci\ufb01c assignment of val-ues to variables), the model de\ufb01nes anactual causeas a min-imal set of events that must occur for an event to happen.Pearl\u2019s model of explanation prompt[1]Identify variables that are determined by factorsexternal to the model (exogenous variables) andvariables that are determined by relationships withother variables (endogenous variables).[2]Select a set of factors representing the explanationby following the criteria:\u2022The set of factors is minimal - there are no irrel-evant causes in the set.\u2022A change in these factors\u2019 values would result ina reversed prediction.Necessity & Robustness selection.Two key criteria for se-lecting strong explanatory causes are necessity and robust-ness (Lipton 1990).Necessityrefers to whether a cause isExplanation selection templateYou are an AI assis-tant that analyzes struggling students behavior to helpthem in their learning trajectories and facilitate learn-ing in the best possible way. You have the followinginformation to help you in your goal:\u2022A model prediction of student performance at theend of the course, in the form of \u201cpass\u201d or \u201cfail\u201d.\u2022A post-hoc explainable AI approach that identi\ufb01eswhich features are important to this student\u2019s pre-diction.\u2022Data in the form of student\u2019s features over 5 weeksthat were used by the model. You will see alwaysthe most relevant features selected by{explainer}.\u2022Data in the form of student\u2019s features over 5 weeksthat were used by the model. You will see alwaysthe most relevant features selected by{explainer}.\u2022The course{coursename}content and structure.\u2022Detailed instructions on how to reason.{modeldescription}{featuresdescription}{explainerdescription}{coursedescription}Take into consideration this data:{explainerimportancescores}{studentfeaturevalues}INSTRUCTIONS:{instructions}QUESTION: Given the information above, follow theinstructions precisely and write a small report on whatyou found. Only use the results from the explainable AIapproach and the student\u2019s behavior data to justify yourconclusions.Relevance-based selection.Based on Hilton\u2019s conversa-tional model of explanation (Hilton 1990), the goal ofrelevance-based selection theory is to \u201cresolve a puzzle inthe explainee\u2019s mind about why the event happened by clos-ing a gap in his or her knowledge\u201d.The theory emphasizes that shared knowledge betweenthe explainer and the explainee are presuppositions of theexplanations, and the other factors are the causes that shouldbe explained. In short, the explainer should not explain anycauses they think the explainee already knows.Relevance-based selection prompt[1]Select the causes that are most relevant to the ques-tion, context and user[2]Select the causes that include information that isnot already shared with the studentAbnormal conditions.This theory is based upon Hilton andSlugoski\u2019s abnormal conditions model (Hilton and Slugoski1986), suggesting that explanations often rely on unusualand temporally proximal events. They argue that people donot solely count on on statistical likelihood but highlight un-common factors that contribute to an event to explain thatevent.During a conversation, the explainer is relying on the per-ceived common prior knowledge to identify potential causesthat are considered abnormal, with greater weight given totemporally proximal events and factors that the explaineecan control. This focus on controllable factors helps the ex-plainee understand how to potentially avoid similar situa-tions in the future.Abnormal conditions model prompt[1]Select potential causes using these criteria:\u2022Abnormality: Tend to prefer abnormal causes.\u2022Temporality: Recent events are more relevant forthe user and considered more mutable.\u2022Controllability: focus on the features that thestudent can control.[2]Select one explanation that follows all of the crite-ria above (Abnormality, Temporality, Controllabil-ity).Pearl\u2019s model of explanation.Halpern and Pearl (2005)provide a formal framework for selecting explanations basedon epistemic relevance and structural causal models. Themodel differentiates betweenexogenous variables, whosevalues are determined by external factors, andendogenousvariables, whose values are in\ufb02uenced by relationships withother variables.Operating within acontext(a speci\ufb01c assignment of val-ues to variables), the model de\ufb01nes anactual causeas a min-imal set of events that must occur for an event to happen.Pearl\u2019s model of explanation prompt[1]Identify variables that are determined by factorsexternal to the model (exogenous variables) andvariables that are determined by relationships withother variables (endogenous variables).[2]Select a set of factors representing the explanationby following the criteria:\u2022The set of factors is minimal - there are no irrel-evant causes in the set.\u2022A change in these factors\u2019 values would result ina reversed prediction.Necessity & Robustness selection.Two key criteria for se-lecting strong explanatory causes are necessity and robust-ness (Lipton 1990).Necessityrefers to whether a cause istational social scientist who was not involved in the promptcreation process. The general questions used items such as\u2018Is the generated text correctly using the model\u2019s predictedoutcome?\u2019 or \u2018Is the generated text analysis based solely onthe explainer results provided?\u2019. An example of a theory-speci\ufb01c questions is: \u2018Is the generated text selecting thecauses that are most relevant to the user?\u2019 (RS). The com-plete rubric is available in Appendix Table 4.We instructed three experts to annotate the same block of42responses (2 students, 3 explainers, 8 theories) across aset of up to eight decomposed questions for each setting,resulting in an inter-rater agreement of\uf8ff=0.71\u00b10.05(Cohen\u2019s Kappa). After that, each annotator proceeded withindependently evaluating between84to105more responses,leading to a total of315human-annotated responses and intotal over2350annotations. We then proceeded with in-structing GPT-4o to annotate the same42responses us-ing the same decomposed questions. We obtained an agree-ment of94.68%\u00b15.20(percentage of answers with agree-ment) between human and GPT-4o annotation. The inter-rater agreement per theory is in Appendix Table 3. Given thehigh agreement, we annotated all explanations by GPT-4o.2.4 Explanation PresentationInspired by Hilton\u2019s conversational model (Hilton 1990), weemploy a presentation prompt to re\ufb01ne and condense theoutput of the explanation selection prompt to make it rel-evant to the explainee\u2019s future actions, current context andprior knowledge. This prompt is formulated using Grice\u2019smaxims on communication (1975) and learning science lit-erature on best practices in communicating feedback to stu-dents (Shute 2008; Hattie and Timperley 2007). Grice\u2019smaxims provide a framework for understanding coopera-tive conversation, emphasizing providing the right amountof relevant and clear information to achieve a shared goal.Grice further divides this principle into four maxims:Qual-ity(truthfulness and evidence-based statements),Quantity(providing enough but not excessive information),Relationpending on the theory). These were validated by a compu-\ntational social scientist who was not involved in the prompt\ncreation process. The general questions used items such as\n\u2018Is the generated text correctly using the model\u2019s predicted\noutcome?\u2019 or \u2018Is the generated text analysis based solely on\nthe explainer results provided?\u2019. An example of a theory-\nspecific questions is: \u2018Is the generated text selecting the\ncauses that are most relevant to the user?\u2019 ( RS). The com-\nplete rubric is available in Appendix Table ??.\nWe instructed three experts to annotate the same block of\n42responses (2 students, 3 explainers, 8 theories) across a\nset of up to eight decomposed questions for each setting,\nresulting in an inter-rater agreement of \u03ba= 0.71\u00b10.13\n(Cohen\u2019s Kappa). After that, each annotator proceeded with\nindependently evaluating between 84to105more responses,\nleading to a total of 315 human-annotated responses and\nin total over 2,350 annotations. We then proceeded with\ninstructing GPT-4o to annotate the same 42responses us-\ning the same decomposed questions. We obtained an agree-\nment of 94.68%\u00b15.20(percentage of answers with agree-\nment) between human and GPT-4o annotation. The inter-\nrater agreement per theory is in Appendix Table 4. Given the\nhigh agreement, we annotated all explanations by GPT-4o.\n2.4 Explanation Presentation\nInspired by Hilton\u2019s conversational model (Hilton 1990), we\nemploy a presentation prompt to refine and condense the\noutput of the explanation selection prompt to make it rel-\nevant to the explainee\u2019s future actions, current context and\nprior knowledge.\nThis prompt is formulated using Grice\u2019s maxims on com-\nmunication (1975) and learning science literature on best\npractices in communicating feedback to students (Shute\n2008; Hattie and Timperley 2007). Grice\u2019s maxims provide a\nframework for understanding cooperative conversation, em-\nphasizing providing the right amount of relevant and clear\ninformation to achieve a shared goal. Grice further divides\nthis principle into four maxims: Quality (truthfulness and\nevidence-based statements), Quantity (providing enough butnot excessive information), Relation (relevance), and Man-\nner(clear and concise communication). Hattie and Timper-\nley (2007) provide a prominent educational feedback frame-\nwork"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "**Summary of \"Edge-Wise Graph-Instructed Neural Networks\"**\n\n**1. Abstract:**\nThe paper proposes a novel edge-wise Graph-Instructed Neural Network (GINN) layer to improve multi-task regression on graph nodes. The new layer, termed as Edge-Wise Graph-Instructed (EWGI) layer, enhances the classical GINN by customizing the information passage based on both outgoing and incoming edges, improving performance on graph-structured data like the Erdos-Renyi graph.\n\n\n**2. Introduction:**\nGraph Neural Networks (GNNs) are essential tools for learning tasks on graph-structured data, dating back to mid-2000s. Despite successes in various domains, research on Regression on Graph Nodes (RoGN) is limited. Recent advancements incorporate message-passing GNNs like GINNs, but these are primarily node-centric. The paper critiques these existing methodologies and introduces EWGI to enhance GINN's representational capacity by accounting for edge-wise information sharing. It especially aims to improve performance on chaotic connectivity structures like the Erdos-R\u00e9nyi graph.\n\n\n**3. Related Work:**\nThe paper outlines the evolution of GNNs, mentioning works by Gori et al. (2005) and Micheli (2009). It highlights recent applications in biology, chemistry, finance, and physics, as well as their neglect in regression tasks. The paper builds on the work by Berrone et al. 2022, who introduced GI layers, rebranded here as GINNs. It stresses the superiority of GINNs over Fully Connected Neural Networks (FCNNs) for specific regression tasks, and relates to advancements such as homonymous but structurally different models from works like Hall et al. 2021.\n\n\n**4. Method:**\n\n**Graph-Instructed Layers Recap:**\n- GI layers use a weight-sharing principle, scaling outgoing messages from graph nodes.\n- Mathematical form \\( L_{GI}(x) = \\sigma((diag(w) (A + I_n))^T x + b) \\)\n\n**Edge-Wise Graph-Instructed Layers:**\n- EWGI layers introduce a new set of weights to also rescale incoming messages, improving internal representation.\n- Node-wise equation: \\( x'_i = w^{{in}}_i \\sum_{j \\in N_i \\cup \\{i\\}} x_j w^{{out}}_j + b_i \\)\n- General form: \\( L_{EWGI}(x) = \\sigma((diag(w^{out}) (A + I) diag(w^{in}))^T x + b) \\)\n- For multi-feature input and output:\n  \\(\\mathbf{L_{EWGI}}(\\mathbf{X}) = \\sigma\\Big([\\mathbf{W^{out}} [\\text{concat}(\\mathbf{X})] [\\mathbf{W^{in}}] + B \\Big)\\)\n  where \\( \\mathbf{W^{out}} \\) and \\( \\mathbf{W^{in}} \\) are constructed as diagonal matrices applied on concatenated adjacency matrices.\n\n\n**5. Experimental Setup:**\n\n**Datasets:**\n- Barab\u00e1si-Albert (BA) and Erdos-R\u00e9nyi (ER) graphs.\n- Each dataset consists of 10,000 samples with node and flow vector pairs.\n\n**Training Details:**\n- Training on 500 samples, with 20% validation split and 3500 test samples.\n- Hardware and software: Standard setups using TensorFlow, executed on GPUs for computational efficiency.\n\n**Evaluation Metrics:**\n- Mean Relative Error (MRE) of individual node flows.\n- MRE of maximum flow vector's total value prediction.\n\n**Comparison Models:**\n- GINNs trained with same hyperparameters: activation functions (ELU, swish, softplus), network depths (3-9 for BA and 4-19 for ER), output features (1, 5, 10).\n\n\n**6. Results and Analysis:**\n\nIn BA graphs:\n- EWGINNs perform on par or slightly better than GINNs.\n- GI layers are sufficient due to the centralized structure with few nodes having high connectivity.\n\nIn ER graphs:\n- EWGINNs exhibit significant performance improvement, handling chaotic, unstructured graphs more effectively.\n- Early stopping issues observed for some models, mitigated by increasing patience from 200 to 1000 epochs.\n\nFigures 3 & 4:\n- Error distribution plots show clear clusters with better performance for EWGINNs in ER.\n\nFigure 5:\n- Training dynamics of top-performing EWGINN highlight the necessity for adjusted early stopping criteria.\n\n\n**7. Discussion:**\n\n**Interpretation of Results:**\n- BA graph improvements showcase GINNs' adequate performance for preferentially attached graph structures.\n- ER graph results highlight the benefits of edge-wise information scaling in unregulated connection structures.\n\n**Limitations and Biases:**\n- Training stability issues necessitate tuning for early stopping.\n- Potential biases in representation power due to fixed early stopping settings during initial trials.\n\n**Implications:**\n- Enhanced model capacity with slightly increased complexity can significantly improve performance in specific graph topologies.\n- This edge-wise approach heralds possible improvements in diverse application areas beyond simple regression tasks.\n\n\n**8. Conclusion:**\n\nThe paper presents the EWGI layer, an enhanced GI layer incorporating edge-wise scaling for improved regression on graph nodes. Demonstrating substantial performance uplift for chaotic graph structures, EWGINNs offer a novel direction for complex network-based learning tasks. Future research should include more extensive hyperparameter tuning, theoretical convergence studies, and broader application to diverse real-world problems.\n\nThis thorough summary encapsulates all key methodologies, experimental setups, results, and conclusions, offering a roadmap for researchers to replicate and build upon this study."}, {"role": "assistant", "content": "Edge-Wise Graph-Instructed Neural Networks\nFrancesco Della Santaa,b,\u02da, Antonio Mastropietroc, Sandra Pieraccinia,b, Francesco Vaccarinoa\naDepartment of Mathematical Sciences, Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129, Turin, Italy\nbGruppo Nazionale per il Calcolo Scientifico INdAM, Piazzale Aldo Moro 5, 00185, Rome, Italy\ncDepartment of Computer Science, University of Pisa, Largo B. Pontecorvo 3, 56127, Pisa, Italy\nAbstract\nThe problem of multi-task regression over graph nodes has been recently approached through Graph-Instructed\nNeural Network (GINN), which is a promising architecture belonging to the subset of message-passing graph neural\nnetworks. In this work, we discuss the limitations of the Graph-Instructed (GI) layer, and we formalize a novel\nedge-wise GI (EWGI) layer. We discuss the advantages of the EWGI layer and we provide numerical evidence\nthat EWGINNs perform better than GINNs over graph-structured input data with chaotic connectivity, like the ones\ninferred from the Erdos-R \u00b4enyi graph.\nKeywords: Graph Neural Networks, Deep Learning, Regression on Graphs\n2020 MSC: 05C21, 65D15, 68T07, 90C35\n1. Introduction\nGraph Neural Networks (GNNs) are powerful tools\nfor learning tasks on graph-structured data [1], such as\nnode classification [2], link prediction, or graph classi-\nfication. Their formulation traces back to the late 2000s\n[3, 4, 5]. In the last years, GNNs have received increas-\ning attention from the research community for their ap-\nplication in biology [6], chemistry [7, 8], finance [9],\ngeoscience [10], computational social science [11], and\nparticle physics [12], to name a few. Yet, the commu-\nnity has neglected the applications concerning the Re-\ngression on Graph Nodes (RoGN) learning task. In-\ndeed, to the best of the authors\u2019 knowledge, the most\nused benchmarks do not include datasets for this task\n[13, 14]. In particular, RoGN can be stated as multi-\ntask regression, where the input data are endowed with\na graph structure.\nThe benchmark models for multi-task regression are\nFully Connected Neural Networks (FCNNs). Recently,\na new type of layer for GNNs has been developed in\n[10], belonging to the class of message-passing GNNs\n[8]. From now on we will refer to these layers as Graph-\nInstructed (GI) layers; Graph-Instructed NNs (GINNs)\nare built by stacking GI layers. GINNs have demon-\nstrated good performance on RoGN, showing better re-\nsults than FCNNs, as illustrated in [10]. Although the\n\u02daCorresponding authorGINN architecture has been specifically designed for\nRoGN, the usage of GI layers has been recently ex-\ntended to supervised classification tasks (see [15]).\nWe point the reader to the fact that in [10] GI lay-\ners and GINNs are denoted as Graph-Informed layers\nand Graph-Informed NNs, respectively. In [16], in a\ndifferent framework from the one addressed in [10], a\nhomonymous but di fferent model is presented; there-\nfore, to avoid confusion with [16], we have changed the\nnames of both layers and NNs.\nGI layers are based on a weight-sharing principle,\nsuch that their weights rescale the outgoing message\nfrom each node. In this paper, to improve the general-\nization capability of their inner-layer representation, we\nintroduce Edge-Wise Graph-Instructed (EWGI) layers,\ncharacterized by additional weights (associated with\ngraph nodes) that enable the edge-wise customization\nof the passage of information to each receiving node.\nWe compare the Edge-Wise GINN (EWGINN) with\nthe GINN in the experimental settings originally used\nin [10] for validating the models; these settings are\ntwo RoGN tasks on two stochastic flow networks based\non a Barab \u00b4asi-Albert graph and an Erdos-R \u00b4enyi graph,\nrespectively. In particular, we show that EWGINNs\nperform better on the Erdos-R \u00b4enyi connectivity struc-\nture, with a small increment of the number of learning\nweights.\nThe work is organized as follows: in Section 2 the\nPreprint submitted to arXiv September 13, 2024arXiv:2409.08023v1  [cs.LG]  12 Sep 2024GI layers are introduced, recalling their inner mecha-\nnisms. Section 3 formally defines EWGI layers and\ntheoretically discusses their properties. Then, in Sec-\ntion 4, we analyze the experiment results for the RoGN\ntasks, comparing with the previous literature [10]. Fi-\nnally, Section 5 summarizes our work and discusses fu-\nture improvements and research directions.\n2. Graph-Instructed Layers\nThis section briefly reviews previous GINNs to es-\ntablish the framework for introducing our main contri-\nbution. Graph-Instructed (GI) Layers are NN layers de-\nfined by an alternative graph-convolution operation in-\ntroduced in [10]. Given a graph G(without self-loops)\nand its adjacency matrix APRn\u02c6n, a basic GI layer\nforGis a NN layer with one input feature per node\nand one output feature per node described by a function\nLGI:Rn\u00d1Rnsuch that\nLGIpxq\u201c\u03c3`\npdiagpwqpA`InqqTx`b\u02d8\n,(1)\nfor each vector of input features xPRnand where:\n\u2022wPRnis the weight vector, with the component\nwiassociated to the graph node vi,i\u201c1,..., n.\n\u2022diagpwq PRn\u02c6nis the diagonal matrix with el-\nements of won the diagonal and InPRn\u02c6nis\nthe identity matrix. For future reference, we set\npW:\u201cdiagpwqpA`Inq;\n\u2022\u03c3:Rn\u00d1Rnis the element-wise application of\nthe activation function \u03c3;\n\u2022bPRnis the bias vector.\nIn brief, Eq. (1) is equivalent to the action of a Fully-\nConnected (FC) layer where the weights are the same if\nthe connection is outgoing from the same unit, whereas\nit is zero if two units correspond to graph nodes that are\nnot connected; more precisely:\npwi j\u201c#\nwi,ifai j\u20300 ori\u201cj\n0,otherwise,\nwhere ai j,pwi jdenote thepi,jq-th element of A,pW, re-\nspectively.\nOn the other hand, from a message-passing point of\nview, the operation described in (1) is equivalent to hav-\ning each node viofGsending to its neighbors a message\nequal to the input feature xi, scaled by the weight wi;\nthen, each node sum up all the messages received from\nthe neighbors, add the bias, and applies the activationfunction. In a nutshell, the message-passing interpreta-\ntion can be summarized by Figure 1 and the following\nnode-wise equation\nx1\ni\u201c\u00ff\njPNinpiqYtiuxjwj`bi, (2)\nwhere x1\niis the output feature of the GI layer corre-\nsponding to node viand N inpiqis the set of indices such\nthat jPNinpiqif and only if ei j\u201c tvi,vjuis an edge\nof the graph. We dropped the action of the activation\nfunction\u03c3for simplicity.\nx\n1w\n1x\n2w\n2x\n4w\n4\nx\n1x\n2x\n3x\n4\nw\n1w\n2w\n3w\n4\n\u0159\nx1\n1x1\n2x1\n3x1\n4\nFigure 1: Visual representation of (2). Example with n\u201c4 nodes\n(non-directed graph), i\u201c1; for simplicity, the bias is not illustrated.\nLayers characterized by (1) can be generalized to read\nany arbitrary number K\u011b1 of input features per node\nand to return any arbitrary number F\u011b1 of output\nfeatures per node. Then, the general definition of a GI\nlayer is as follows.\nDefinition 2.1 (GI Layer - General form [10]) .AGI\nlayer with KPNinput features and FPNoutput fea-\ntures is a NN layer with nF units connected to a layer\nwith outputs in Rn\u02c6Kand having a characterizing func-\ntionLGI:Rn\u02c6K\u00d1Rn\u02c6Fdefined by\nLGIpXq\u201c\u03c3\u00b4\n\u0102WTvertcatpXq`B\u00af\n, (3)\nwhere:\n\u2022XPRn\u02c6Kis the input matrix (i.e., the output of the\nprevious layer) and vertcatpXqdenotes the vector\ninRnKobtained concatenating the columns of X;\n\u2022tensor\u0102WPRnK\u02c6F\u02c6nis the concatenation along\nthe2nddimension (i.e., the column-dimension) of\nthe matrices rWp1q,...,rWpFq, such that\nrWplq:\u201c\u00bb\n\u2014\u2014\u2014\u2013pWp1,lq\n...\npWpK,lqfi\nffiffiffifl\u201c\u00bb\n\u2014\u2014\u2014\u2013diagpwp1,lqqpA\n...\ndiagpwpK,lqpAfi\nffiffiffiflPRnK\u02c6n,\n(4)\n2for each l\u201c1,..., F, after being reshaped as ten-\nsors in RnK\u02c61\u02c6n. Vector wpk,lqPRnis the weight\nvector characterizing the contribution of the k-th\ninput feature to the computation of the l-th output\nfeature of the nodes, for each k \u201c1,..., K, and\nl\u201c1,..., F; matrix pA denotes A`In.\n\u2022the operation \u0102WTvertcatpXqis a tensor-vector\nproduct and B PRn\u02c6Fis the matrix of the biases.\nAdditionally, pooling and mask operations can be added\nto GI layers (see [10] for more details).\nFrom now on, we call Graph-Instructed Neural Net-\nwork (GINN) a NN made of GI layers [10]. We point\nout that the number of weights of a GI layer is equal to\nnKF`nF. On the other hand, the number of weights\nof a FC layer of nunits, reading the outputs of a layer of\nmunits, is equal to mn`n; therefore, if we consider the\ncase of m\u201cnandKF`F\u0103n`1 (typically satisfied for\nsufficiently large graphs), GI layers have fewer weights\nto be trained compared with the FC layer. Moreover,\nwe observe that adjacency matrices are typically sparse\nand, therefore, the tensor \u0102Win (7) is typically sparse\ntoo. Then, it is possible to exploit the sparsity of this\ntensor to reduce the memory cost of the GINN imple-\nmentation.\n3. Edge-Wise Graph-Instructed Layers\nA possible drawback of GI layers is that their weights\nrescale only the outgoing information of the nodes. For\nexample, if nodes vjandvkare connected to node viin\na graph G\u201cpV,Eqsuch thatpvi,vjq,pvi,vkqPE, then\nthe units corresponding to vjandvkin a GI layer based\nonGreceive the same contribution from the input fea-\ntures corresponding to node vi; moreover, if nodes vj,vk\nhave the same neighbors, the GI layer\u2019s outputs corre-\nsponding to these nodes are the same except for the con-\ntribution of the bias. This property is useful to reduce\nthe number of weights per layer and, depending on the\ncomplexity of the target function defined on the graph\nnodes, it is not necessarily a limitation. Nonetheless, it\nsurely limits the representational capacity of the model.\nTherefore, some target functions can be too complicated\nto be modeled by GI layers.\nGiven the observation above, it is useful to define a\nnew GI layer capable of improving the capacity of the\nmodel at a reduced cost in terms of the total number of\ntrainable weights. In this work, we propose to modify\nthe classic GI layers by adding an extra set of weights\nassociated with the nodes to rescale their incoming in-\nformation. In brief, given the node-wise equation (1),we change it into\nx1\ni\u201cwin\ni\u00ff\njPNinpiqYtiuxjwout\nj`bi, (5)\nwhere wout\njdenotes the (old) weights for rescaling the\noutgoing information from node vj, while win\nidenotes\nthe (new) weights for rescaling the incoming informa-\ntion to node vi(see Figure 2).\nx\n1w\n1x\n2w\n2x\n4w\n4\nx\n1x\n2x\n3x\n4\nwout\n1wout\n2wout\n3wout\n4\n\u0159win\n1win\n2win\n3win\n4\nx1\n1x1\n2x1\n3x1\n4\nFigure 2: Visual representation of (5). Example with n\u201c4 nodes\n(non-directed graph), i\u201c1; for simplicity, the bias is not illustrated.\nA NN layer based on (5) is a layer with one input fea-\nture per node and one output feature per node, described\nby a function L:Rn\u00d1Rnsuch that\nLpxq\u201c\u03c3`\npdiagpwoutqpA`InqdiagpwinqqTx`b\u02d8\n(6)\nfor each vector of input features xPRnand where\nwout,winPRnare the weight vectors, where the com-\nponents wout\ni,win\niare associated to the graph nodes vi,\nfor each i\u201c1,..., n. For future reference, we set\npW:\u201cdiagpwoutqpA`Inqdiagpwinq.\nIn brief, (6) is equivalent to a FC layer where the\nweights are zero if two distinct units correspond to\ngraph nodes that are not connected, otherwise pwi j\u201c\nwout\niwin\njifei jPEori\u201cj. Therefore, we observe that\neach weight pwi j\u201cwout\niwin\njis associated with the edge\nei j\u201cpvi,vjqin the graph or the self-loop added by the\nlayer (if i\u201cj). Given the above observations, we can\ninterpret (6) as the operation of a NN layer with weights\nassociated with edges instead of nodes. Then, we define\nthe new layer as Edge-Wise GI (EWGI) Layer.\nRemark 3.1 (EWGI Layers - Advantages of the Formu-\nlation) .Note that in principle EWGI layers could be de-\nfined by associating an independent weight pwi jto each\nedge of G and each added self-loops. Nonetheless, the\napproach here proposed exhibits the following advan-\ntages:\n\u2022If G is a directed graph, we have that n \u00b41\u0103\n|E|\u0103n2\u00b4n; therefore, for the independent weight\nformulation the total number of weights is in the\nranger2n\u00b41,n2s(biases excluded). On the other\n3hand, in (6)the number of weights is always equal\nto2n (biases excluded).\n\u2022If G is an undirected graph, we have n \u00b41\u0103|E|\u0103\npn2\u00b4nq{2; therefore, for the independent weight\nformulation the total number of weights is in the\nranger2n\u00b41,n`pn2\u00b4nq{2s(biases excluded).\nOn the other hand, in (6)the number of weights is\nalways equal to 2n (biases excluded).\nThe advantage of using formulation (6)is therefore ev-\nident: independently of the number of graph edges, the\nnumber of weights is always 2n, which is essentially the\nlower bound of the number of weights in the other for-\nmulation.\nAnalogously to classic GI layers, EWGI layers can be\ngeneralized to read any arbitrary number K\u011b1 of in-\nput features per node and to return any arbitrary number\nF\u011b1 of output features per node. Then, the general\ndefinition of a EWGI layer is as follows.\nDefinition 3.1 (EWGI Layer - General form) .An\nEWGI layer with KPNinput features and FPNout-\nput features is a NN layer with nF units connected to a\nlayer with outputs in Rn\u02c6Kand having a characterizing\nfunction LEWGI:Rn\u02c6K\u00d1Rn\u02c6Fdefined by\nLEWGIpXq\u201c\u03c3\u00b4\n\u0102WTvertcatpXq`B\u00af\n, (7)\nwhere the tensor \u0102WPRnK\u02c6F\u02c6nis defined as the\nconcatenation along the 2nddimension of the matrices\nrWp1q,...,rWpFq, such that\nrWplq:\u201c\u00bb\n\u2014\u2014\u2014\u2013diagpwp1,lq\noutqpAdiagpwp1,lq\ninq\n...\ndiagpwpK,lq\noutqpAdiagpwpK,lq\ninqfi\nffiffiffiflPRnK\u02c6n,(8)\nfor each l\u201c1,..., F, after being reshaped as tensors\ninRnK\u02c61\u02c6n, and where:\n\u2022wpk,lq\noutPRnis the weight vector characterizing the\ncontribution of the k-th input feature to the com-\nputation of the l-th output feature of the nodes, for\neach k\u201c1,..., K, and l\u201c1,..., F, with respect\nto the outgoing information;\n\u2022wpk,lq\ninPRnis the weight vector characterizing the\ncontribute of the k-th input feature to the computa-\ntion of the l-th output feature of the nodes, for each\nk\u201c1,..., K, and l\u201c1,..., F, with respect to the\nincoming message.From the definition above, we observe that the num-\nber of weights of a general EWGI layer is 2 nKF`nF.\nTherefore, if we consider a FC layer of nunits, read-\ning the outputs of a layer of m\u201cnunits, the EWGI\nlayers have a smaller number of weights to be trained if\n2KF`F\u0103n`1.\nFrom now on, we call Edge-Wise Graph-Instructed\nNeural Network (EWGINN) a NN made of EWGI lay-\ners.\n4. Preliminary Results\nIn this section, we illustrate the results of a prelimi-\nnary experimental study about the representational ca-\npacity of the new EWGI layers and EWGINNs. We\ncompare the performances of a set of EWGINNs with\nthe results obtained in [10] by a set of GINNs for a\nRoGN task of a stochastic maximum flow problem. In\nparticular, we train the new EWGINN models using the\nsame hyperparameters, training options, and number of\nnodes, output features and input features used for the\nmodels trained in [10]; we only replace GI layers with\nEWGI layers.\n4.1. Maximum Flow Regression for Stochastic Flow\nNetworks\nConcerning the regression problem, we recall that a\nstochastic maximum-flow problem is a problem where\nthe edge capacities in a flow network are modeled as\nrandom variables and the target is to find the distribution\nof the maximum flow (e.g., see [17]). The task is to\napproximate with a NN model the function\n\u03a6:Rn\n`\u00dd\u00d1Rm\n`\nc\u00de\u00dd\u00d1\u03a6pcq\u201c\u03c6(9)\nwhere c:\u201c pc1,..., cnq PRn\n`is the vector of the ca-\npacities of all the nedges of the network and \u03c6:\u201c\np\u03c61,...,\u03c6 mq PRm\n`is the flow vector corresponding to\nthemincoming edges of the network\u2019s sink that gener-\nate the maximum flow; in other words, the maximum\nflow corresponding to cis\u03c6:\u201c}\u03a6pcq}1\u201c\u0159m\nj\u201ci\u03c6j.\nTo address this regression task, we build the GINNs\nand the EWGINNs with respect to the adjacency ma-\ntrix of the line graph of the flow network; i.e., on the\ngraph where the vertices correspond to edges of the net-\nwork and two vertices are connected if the correspond-\ning edges in the network share at least one vertex. We\nrefer to [10] for more details about the formulation of\nthis RoGN task for learning the maximum flow of a\nstochastic flow network (SFN).\n44.2. Performance Measures\nLetp\u03a6denote a NN model trained for learning (9) and\nletPbe a test set used for measuring the performances\nof the model. Then, denoted by p\u03c6:\u201cp\u03a6pcqPRm\n`, we\ndefine the following performance measures:\n\u2022Average Mean Relative Error (MRE) of sink\u2019s in-\ncoming flows, with respect to the max-flow:\nMRE avpPq:\u201c1\nmm\u00ff\nj\u201c1\u00a8\n\u02dd1\n|P|\u00ff\npc,\u03c6qPP|\u03c6j\u00b4p\u03c6j|\n\u03c6\u02db\n\u201a.\n(10)\nThis error measure describes the average qual-\nity of the NN in predicting the single elements\n\u03c61,...,\u03c6 m.\n\u2022Average max-flow MRE:\nMRE\u03c6pPq:1\n|P|\u00ff\npc,\u03c6qPP|\u03c6\u00b4p\u03c6|\n\u03c6(11)\nThis error measure describes the NN capability\nto predict the vector of fluxes p\u03c6such that the\ncorresponding maxflow p\u03c6approximates the true\nmaxflow\u03c6.\n4.3. Data, Model Architectures, and Hyperparameters\nWe run our experiments on the same data reported in\n[10] for two randomly generated SFNs: a network based\non a Barab \u00b4asi-Albert (BA) graph and a network based\non an Erdos-R \u00b4enyi (ER) graph. Each of the datasets\nDBAandDERconsists of 10 000 samples of capacity\nvectors and corresponding flow vectors.\nIn this work, we focus on the harder case illustrated in\n[10]: for each network, we train the EWGINN models\non 500 samples (20% used as validation data), measur-\ning the errors MRE avand MRE \u03c6on a test set of 3500\nsamples. Then, we compare these performances with\nthe performances of the GINNs trained in [10], exclud-\ning the ones with ReLU activation function, due to their\nlow performances.\nFor a fair comparison, the architectures and hyper-\nparameters of the EWGINN models are the same used\nfor the GINN models in [10]. Specifically, we build\n60 EWGINN model configurations for each SFN, vary-\ning among these parameters: hidden layers\u2019 activa-\ntion function \u03c3P rELU,swish,softpluss, depth HP\nr3,5,7,9sforGBAand HP r4,9,14,19sforGER,\noutput features of each EWGI layer FP r1,5,10s,\noutput layer\u2019s pooling operation (if F\u01051)poolP\nrreduce max,reduce means. Also, the training optionsare the same used in [10]: Adam optimizer [18] (learn-\ning rate\u03f5=0.002, moment decay rates \u03b21\u201c0.9,\u03b22\u201c\n0.999), early stopping regularization [19, 20] (200\nepochs of patience, restore best weights), reduction on\nplateau for the learning rate [20] (reduction factor \u03b1\u201c\n0.5, 100 epochs of patience, minimum \u03f5\u201c10\u00b46).\n4.4. Analysis of the Results\nFigures 3 and 4 compare the errors between classical\nGINNs (blue dots) and EWGINN (orange crosses). The\nerror plane shows the MRE averror on the x-axis and the\nMRE\u03c6on the y-axis.\nFigure 3: GBA. Performances of GINN and EWGINN models in the\npMRE av,MRE\u03c6qplane.\nFigure 4: GER. Performances of GINN and EWGINN models in the\npMRE av,MRE\u03c6qplane.\nWe observe that the performance of GINNs and\nEWGINNs are comparable in GBA, whereas EWGINNs\nexhibit better performance in the GERnetwork. The ex-\nplanation for the first case is that BA graphs are gener-\nated using preferential attachment criteria because they\nare used to describe many natural and human systems\nwhere few nodes are characterized by a higher degree\ncompared to the other nodes of the network. Therefore,\nthis structural property suggests that classical GINNs\n5operate e fficiently by rescaling only the outgoing infor-\nmation from the nodes toward their neighbors; the addi-\ntional property of EWGINNs of rescaling the incoming\ninformation of nodes does not seem to o ffer significant\nbenefits.\nIn contrast, the network GERhas a more chaotic\nconnection structure compared to GBA; therefore,\nEWGINNs show a clear advantage in this scenario. In-\ndeed, many EWGINNs perform better than the best\nclassical GINNs. More precisely, we observe two clus-\nters of EWGINNs: one cluster with performance al-\nmost equal to or better than the best GINNs, and one\ncluster with performance almost equal to or worse than\nthe worst GINNs. The cluster of EWGINNs with poor\nperformance, in this case, is attributed to an issue with\nearly stopping. Specifically, for these models, at the be-\nginning of the training phase, we observe a temporary\noverfitting phenomenon that induces a large increment\nof the validation loss and subsequently interrupts the\ntraining to reach the early stopping patience. Nonethe-\nless, by increasing the early stopping patience from 200\nepochs to 1000 epochs, we observe that the overfitting\nphenomenon tends to disappear (see Figure 5). From\nthese observations, we can deduce that the larger repre-\nsentational capacity of EWGINNs can be an advantage\nbut requires more careful tuning of the training hyper-\nparameters.\nWe defer to future work an in-depth analysis of\nEWGINNs by varying the training hyperparameters,\nsuch as the early stopping patience.\nFigure 5: Training and validation loss of the EWGINN corresponding\nto the top-rightmost dot in Figure 4.5. Conclusion\nIn this work, we have proposed a novel type of GI\nlayer: the Edge-Wise GI layer. Compared with the orig-\ninal GI layers, each node of an EWGI layer is equipped\nwith an additional weight for rescaling the incoming\nmessage. This enables improved representational ca-\npacity and breaks the symmetry of GI layers, where\nnodes with the same neighborhood invariably receive\nthe same message from the previous layer.\nTo analyze the performance of the newly proposed\nlayers, we compared EWGINNs and GINNs on two\nbenchmark RoGN tasks based on two SFNs, respec-\ntively, and where the graph connectivity is either con-\ncentrated on a few more central nodes (Barab \u00b4asi-Albert)\nor shows a random structure (Erdos-R \u00b4enyi). The experi-\nments indicate that EWGINNs exhibit improved perfor-\nmance in the second scenario, whereas in the first they\nare on par with GINNs.\nIn addition, we showed that the EWGINNs can re-\nveal a non-monotone behavior in the validation loss that\ncould make their training more unstable. Nonetheless,\na careful choice of the hyperparameters can easily solve\nthe problem. Overall, we confirm that RoGN tasks ben-\nefit from accounting for the graph connectivity.\nFuture works can provide a theoretical analysis of the\nconvergence properties of the two compared architec-\ntures and applications to real-world problems.\nAcknowledgements\nF.D., S.P., and F.V . acknowledge that this study\nwas carried out within the FAIR-Future Artifi-\ncial Intelligence Research and received funding\nfrom the European Union Next-GenerationEU (PI-\nANO NAZIONALE DI RIPRESA E RESILIENZA\n(PNRR)\u2013MISSIONE 4 COMPONENTE 2, INVESTI-\nMENTO 1.3\u2014D.D. 1555 11 /10/2022, PE00000013).\nThis manuscript reflects only the authors\u2019 views and\nopinions; neither the European Union nor the European\nCommission can be considered responsible for them.\nS.P. acknowledges support from Italian MIUR PRIN\nproject 20227K44ME, Full and Reduced order model-\ning of coupled systems: focus on non-matching meth-\nods and automatic learning (FaReX).\nCode Availability:\nThe code for implementing the EWGI layers intro-\nduced in this paper is available at: https: //github.com /\nFra0013To /GINN /tree/ewginn dev.\n6References\n[1] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, P. S. Yu, A com-\nprehensive survey on graph neural networks, IEEE Transactions\non Neural Networks and Learning Systems 32 (1) (2021) 4\u201324.\ndoi:10.1109/TNNLS.2020.2978386 .\n[2] S. K. Maurya, X. Liu, T. Murata, Simplifying approach to node\nclassification in graph neural networks, Journal of Computa-\ntional Science 62 (2022) 101695.\n[3] M. Gori, G. Monfardini, F. Scarselli, A new model for learn-\ning in graph domains, in: Proceedings. 2005 IEEE International\nJoint Conference on Neural Networks, 2005., V ol. 2, 2005, pp.\n729\u2013734 vol. 2. doi:10.1109/IJCNN.2005.1555942 .\n[4] A. Micheli, Neural network for graphs: A contextual construc-\ntive approach, IEEE Transactions on Neural Networks 20 (3)\n(2009) 498\u2013511. doi:10.1109/TNN.2008.2010350 .\n[5] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, G. Mon-\nfardini, The graph neural network model, IEEE Transactions on\nNeural Networks 20 (1) (2009) 61\u201380. doi:10.1109/TNN.\n2008.2005605 .\n[6] P. Cinaglia, Multilayer biological network alignment based on\nsimilarity computation via graph neural networks, Journal of\nComputational Science 78 (2024) 102259.\n[7] K. Atz, F. Grisoni, G. Schneider, Geometric deep learning on\nmolecular representations, Nature Machine Intelligence 3 (12)\n(2021) 1023\u20131032.\n[8] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, G. E. Dahl,\nNeural message passing for quantum chemistry, in: Interna-\ntional conference on machine learning, PMLR, 2017, pp. 1263\u2013\n1272.\n[9] D. Cheng, F. Yang, S. Xiang, J. Liu, Financial time series\nforecasting with multi-modality graph neural network, Pattern\nRecognition 121 (2022) 108218.\n[10] S. Berrone, F. Della Santa, A. Mastropietro, S. Pieraccini,\nF. Vaccarino, Graph-informed neural networks for regressions\non graph-structured data, Mathematics 10 (5) (2022) 786. doi:\n10.3390/math10050786 .\nURL https: //doi.org /10.3390%2Fmath10050786\n[11] S. Aref, M. Mostajabdaveh, Analyzing modularity maximiza-\ntion in approximation, heuristic, and graph neural network algo-\nrithms for community detection, Journal of Computational Sci-\nence 78 (2024) 102283.\n[12] G. DeZoort, P. W. Battaglia, C. Biscarat, J.-R. Vlimant, Graph\nneural networks at the large hadron collider, Nature Reviews\nPhysics 5 (5) (2023) 281\u2013303.\n[13] V . P. Dwivedi, C. K. Joshi, A. T. Luu, T. Laurent, Y . Bengio,\nX. Bresson, Benchmarking graph neural networks, Journal of\nMachine Learning Research 24 (43) (2023) 1\u201348.\n[14] V . P. Dwivedi, L. Rampasek, M. Galkin, A. Parviz, G. Wolf,\nA. T. Luu, D. Beaini, Long range graph benchmark, Advances\nin Neural Information Processing Systems 35 (2022) 22326\u2013\n22340.\n[15] F. Della Santa, S. Pieraccini, Graph-informed neural networks\nfor sparse grid-based discontinuity detectors (2024). arXiv:\n2401.13652 .\n[16] E. J. Hall, S. Taverniers, M. A. Katsoulakis, D. M. Tartakovsky,\nGinns: Graph-informed neural networks for multiscale\nphysics, Journal of Computational Physics 433 (2021) 110192.\ndoi:https://doi.org/10.1016/j.jcp.2021.110192 .\nURL https: //www.sciencedirect.com /science /article /pii/\nS0021999121000875\n[17] S. Ding, The \u03b1-maximum flow model with uncertain capacities,\nApplied Mathematical Modelling 39 (7) (2015) 2056\u20132063.\ndoi:https://doi.org/10.1016/j.apm.2014.10.021 .URL https: //www.sciencedirect.com /science /article /pii/\nS0307904X14004946\n[18] D. P. Kingma, J. L. Ba, Adam: A method for stochastic opti-\nmization, 3rd International Conference on Learning Represen-\ntations, ICLR 2015 - Conference Track Proceedings (2015) 1\u2013\n15arXiv:1412.6980 .\n[19] I. Goodfellow, Y . Bengio, A. Courville, Deep Learning, MIT\nPress, 2016, www.deeplearningbook.org.\n[20] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,\nG. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat,\nI. Goodfellow, A. Harp, G. Irving, M. Isard, Y . Jia, R. Joze-\nfowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man \u00b4e, R. Monga,\nS. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner,\nI. Sutskever, K. Talwar, P. Tucker, V . Vanhoucke, V . Vasudevan,\nF. Vi \u00b4egas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,\nY . Yu, X. Zheng, TensorFlow: Large-scale machine learning on\nheterogeneous systems, software available from tensorflow.org\n(2015).\nURL https: //www.tensorflow.org /\n7"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "Certainly! Here is a summary of the given academic paper, broken down into the specified sections:\n\n---\n\n### 1. Abstract\nThe paper proposes a novel method, called Multi-view Feature Fusion (MuFF), for network anomaly traffic detection. The MuFF method enhances traditional detection techniques by integrating temporal and interactive features from network traffic data. The method utilizes Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) for temporal modeling, and Graph Convolutional Networks (GCN) for interaction modeling. Extensive experiments on six real traffic datasets demonstrate MuFF's superior performance over existing single-view methods.\n\n### 2. Introduction\nThe paper addresses the limitations of traditional network anomaly detection methods that rely on single-view analysis. These methods struggle with complex attacks and encrypted communications. The paper emphasizes the significance of accurate and timely detection of network anomalies for maintaining cybersecurity, especially given the rising complexity of network environments. The authors introduce the idea of combining temporal and interactive perspectives to create a more comprehensive detection method. The research fills gaps in the existing literature by integrating multi-view features, which is validated through extensive experimentation on real-world datasets.\n\n### 3. Related Work\nThe review of related work covers several traditional and modern approaches to network anomaly detection:\n- Methods based on port number mappings and deep packet inspections are discussed.\n- The successes and limitations of machine learning and deep learning techniques are covered, with references to works utilizing AutoEncoders, GRUs, Transformers, and other neural network structures.\n- Graph-based methods are analyzed, highlighting their focus on packet interactions and network topology.\nThe paper advances the state of the art by combining temporal and interactive features into a multi-view framework to enhance detection accuracy and robustness.\n\n### 4. Method\n#### Multi-view Feature Fusion (MuFF)\n##### Temporal Perspective:\n1. **Feature Extraction**:\n   - **Packet Length Sequence**: Extracts transmission direction and packet length, orders them by time. Fill with zeros if less than `n` packets.\n   - **Payload Byte Sequence**: Converts first `m` bytes of the payload into decimal format.\n\n2. **Feature Extraction Using Models**:\n   - **CNN**: Used for extracting spatial features from payload byte sequences.\n     - **Architecture**: Two convolution layers followed by normalization, ReLU activation, and max-pooling layers.\n     - **Attention Mechanism**: Enhances feature learning by focusing on important parts of the data.\n   - **LSTM**: Used for capturing temporal dependencies in packet length sequences.\n     - **Architecture**: Standard LSTM structure with gates to manage memory cells.\n     - **Attention Mechanism**: Improves learning by dynamically adjusting focus on different time steps.\n\n##### Interactive Perspective:\n1. **Interaction Modeling**:\n   - Constructs traffic interaction graphs where packets are nodes, and edges represent communication relationships.\n   - Features include the direction and length of packets.\n\n2. **Feature Extraction Using GCN**:\n   - **GCN Model**: Nodes and adjacency matrix represent packet interactions.\n     - **Neighbor Aggregation**: Propagates and aggregates information from neighboring nodes.\n     - **Global Pooling**: Applies global mean pooling to obtain the final representation of the interaction graph.\n\n##### Feature Fusion and Detection:\n- **Concatenation**: Combines temporal features (from CNN and LSTM) and interaction features (from GCN).\n- **Final Classifier**: Passes fused features through fully connected layers and Softmax for anomaly classification.\n- **Loss Function**: Cross-Entropy Loss, optimized to minimize classification errors.\n\n### 5. Experimental Setup\n#### Datasets:\n1. **CIC-IOMT2024**: Medical IoMT devices with various attack types.\n2. **UNSW-NB15**: Mixed normal and attack traffic generated using IXIA PerfectStorm.\n3. **Darknet2020**: Combines ISCX-VPN2016 and ISCX-Tor2016 with different encryption types.\n4. **ISCX-VPN2016**: VPN traffic for various applications.\n5. **CIC-IoT2023**: IoT devices with seven categories of attacks.\n6. **USTC-TFC2016**: Encrypted traffic mixing malicious and benign applications.\n\n#### Technical Details:\n- **Hardware**: Not explicitly specified, needs standard GPU setup for deep learning tasks.\n- **Software**: Implemented using PyTorch and PyTorch Geometric libraries.\n\n#### Evaluation Metrics:\n- Precision, recall, F1-score, accuracy used to evaluate performance.\n\n#### Baselines:\n- Compared against methods like GraphSAGE, GIN, GAT, MuFF-CNN, MuFF-LSTM, MuFF-GCN, E-GraphSAGE, GraphDDoS for comprehensive evaluation.\n\n### 6. Results and Analysis\n- **Model Performance**: Extensive experiments show MuFF's superior detection performance over baselines.\n- **Metrics**: Detailed graphs and tables present performance on precision, recall, F1-score, and accuracy.\n- **Ablation Studies**: Effect of different components (temporal and interaction features) is studied.\n- **Parameter Tuning**: Experiments assess the impact of varying packet count `n` and payload byte count `m`.\n\n### 7. Discussion\n- Interpretation: MuFF provides enhanced detection by successfully combining different perspectives.\n- Implications: The model's robustness and effectiveness imply it can handle complex detection scenarios.\n- Limitations: Potential biases due to reliance on specific datasets and necessity for extensive computational resources are noted. Future work could focus on optimizing the model for real-time applications and exploring more adaptable architectures.\n\n### 8. Conclusion\n- Summary: The paper presents MuFF, a multi-view method for detecting network anomalies with high accuracy.\n- Future Directions: Suggested improvements include real-time detection capabilities, broader dataset testing, and exploration of additional machine learning techniques for anomaly detection.\n\n---\n\nThis detailed summary provides a comprehensive guide that includes the core contributions, technical details, and empirical findings essential for replicating or building upon the work in the paper."}, {"role": "assistant", "content": "  \u7b2c  \u5377    \u7b2c  \u671f                      \u6307 \u6325 \u4e0e \u63a7 \u5236 \u5b66 \u62a5                               Vol.  , No.      \n\u5e74      \u6708              JOURNAL OF COMMAND AND CONTROL                       May, 2021  \n*\u901a\u4fe1\u4f5c\u8005  \n\u6536\u7a3f\u65e5\u671f  20 24-08-29  \nManuscript received  August 29, 2024   \n\u676d\u5dde\u5e02\u91cd\u70b9\u7814\u53d1\u8ba1\u5212 2022C01018 \u548c2024C01025 \u8d44\u52a9\uff0c\u56fd\u5bb6\u81ea\u7136\u79d1\u5b66\u57fa\u91d1 62103374 \u548cU21B2001 \u8d44\u52a9  \nThis work was supported in part by the Key R&D Program of Zhejiang under Grants 2022C01018 and 2024C01025, by the National \nNatural Science Foundation of China under Grants 62103374 and U21B2001.  \n1\uff0e\u6d59\u6c5f\u5de5\u4e1a\u5927\u5b66\u00b7\u7f51\u7edc\u7a7a\u95f4\u5b89\u5168\u7814\u7a76\u9662\uff0c\u676d\u5dde 310023   2.\u676d\u5dde\u5e02\u6ee8\u6c5f\u533a\u6d59\u5de5\u5927\u4eba\u5de5\u667a\u80fd\u521b\u65b0\u7814\u7a76\u9662\uff0c\u676d\u5dde 310056  \n1. Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou 310023, China   2\uff0eBinjiang Institute of Artificial \nIntelligence, ZJUT, Hangzhou 310056, China   \n\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u7684\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b  \n\u5b8b  \u660a1,2  \u5085\u6587\u6d9b1,2  \u9648\u70dc\u6cfd1,2  \u91d1\u7a0b\u79651,2  \u5468\u5609\u4fca1,2 *  \u4fde\u5c71\u97521,2  \u5ba3  \u74261,2 \n\u6458  \u8981  \u4f20\u7edf\u7684\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u65b9\u6cd5\u57fa\u4e8e\u5355\u4e00\u89c6\u89d2\u5206\u6790\uff0c\u5728\u5904\u7406\u590d\u6742\u653b\u51fb\u548c\u52a0\u5bc6\u901a\u4fe1\u65f6\u5177\u6709\u660e\u663e\u7684\u5c40\u9650\u6027\u3002\u5bf9\u6b64\uff0c\u63d0\u51fa\u4e00\n\u79cd\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u7684\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u65b9\u6cd5\u2014\u2014MuFF\uff0c\u5206\u522b\u57fa\u4e8e\u65f6\u5e8f\u89c6\u89d2\u548c\u4ea4\u4e92\u89c6\u89d2\u5bf9\u7f51\u7edc\u6d41\u91cf\u4e2d\u6570\u636e\u5305\u7684\u65f6\u5e8f\u5173\u7cfb\u53ca\n\u4ea4\u4e92\u5173\u7cfb\u5efa\u6a21\uff0c\u5b66\u4e60\u5176\u65f6\u5e8f\u7279\u5f81\u4e0e\u4ea4\u4e92\u7279\u5f81\uff0c\u5e76\u5c06\u4e0d\u540c\u89c6\u89d2\u4e0b\u7684\u7279\u5f81\u878d\u5408\u8fdb\u884c\u5f02\u5e38\u6d41\u91cf\u7684 \u68c0\u6d4b\u3002\u5728\u516d\u4e2a\u771f\u5b9e\u7684\u6d41\u91cf\u6570\u636e\n\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMuFF \u5728\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5f25\u8865\u4e86\u5355\u4e00\u89c6\u89d2\u4e0b\u68c0\u6d4b\u7684\u4e0d\u8db3\u3002 \n\u5173\u952e\u8bcd  \u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\uff0c\u7f51\u7edc\u5b89\u5168\uff0c\u591a\u89c6\u89d2\uff0c\u65f6\u5e8f\u5efa\u6a21\uff0c\u4ea4\u4e92\u5efa\u6a21 \n \nNetwork Anomaly Traffic Detection via Multi -view Feature Fusion  \nSONG Hao1,2  FU Wen -Tao1,2  CHEN Xuan -Ze1,2  JIN Cheng- Xiang1,2 \nZHOU Jia -Jun1,2 *  YU Shan -Qing1,2  XUAN Qi1,2 \nAbstract   Tradi tional anomalous traffic detection methods are based on single-view analysis, which has obvious limitations in \ndealing with complex attacks and encrypted communications. In this regard \uff0cwe propose a Multi -view Feature Fusion  (MuFF) \nmethod for network anomaly traffic detection.  MuFF  model s the temporal and interactive relationships of packets in network \ntraffic based on the temporal and interactive viewpoints respectively . It learn s temporal and interactive features . These features \nare then fused from differ ent perspectives for anomaly traffic detection.  Extensive experiments on six real traffic datasets show \nthat MuFF has excellent performance in network anomalous traffic detection, which makes up for the shortcomings of detection \nunder a single perspective.  \nKey words   Abnormal Traffic Detection , Cybersecurity , Multi-view , Temporal Modeling , Interaction Modeling  \n \n \n \n 1  \u5f15\u8a00 \n\u5f53\u4eca\u4fe1\u606f\u5316\u793e\u4f1a\u5feb\u901f\u53d1\u5c55\uff0c\u4e92\u8054\u7f51\u4f5c\u4e3a\u5168\u7403\n\u4fe1\u606f\u4ea4\u6d41\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5176\u5b89\u5168\u6210\u4e3a\u4e86\u7ef4\u7cfb\u793e\u4f1a\u79e9\n\u5e8f\u4e0e\u7ecf\u6d4e\u53d1\u5c55\u7684\u5173\u952e\u3002\u7f51\u7edc\u6d41\u91cf\u4f5c\u4e3a\u4e92\u8054\u7f51\u6d3b\u52a8\n\u7684\u76f4\u63a5\u53cd\u6620\uff0c\u4e0d\u4ec5\u627f\u8f7d\u4e86\u6d77\u91cf\u7684\u7528\u6237\u4ea4\u4e92\u4fe1\u606f\uff0c\u4e5f\u6f5c\u85cf\u7740\u8bf8\u591a\u5b89\u5168\u9690\u60a3\u3002\u5c24\u5176\u662f\u5f02\u5e38\u6d41\u91cf\u7684\u6d8c\n\u73b0\uff0c\u65e2\u53ef\u80fd\u662f\u9ed1\u5ba2\u5165\u4fb5\u7684\u521d\u6b65\u4fe1\u53f7\uff0c\u4e5f\u53ef\u80fd\u662f\u7cfb\n\u7edf\u5185\u90e8\u6545\u969c\u7684\u5916\u5728\u53cd\u6620\uff0c\u5bf9\u7f51\u7edc\u7cfb\u7edf\u7684\u6b63\u5e38\u8fd0\u884c\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\u3002\u56e0\u6b64\uff0c\u9ad8\u6548\u3001\u51c6\u786e\u5730\u8bc6\u522b\u7f51\u7edc\n\u6d41\u91cf\u4e2d\u7684\u5f02\u5e38\u73b0\u8c61\u5e76\u53ca\u65f6\u4f5c\u51fa\u54cd\u5e94\uff0c\u6210\u4e3a\u7ef4\u62a4\u7f51\n\u7edc\u5b89\u5168\u7684\u5173\u952e\u3002  \n\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u68c0\u6d4b\u662f\u7f51\u7edc\u5b89\u5168\u9886\u57df\u4e2d\u4e00\u9879\u57fa\n\u7840\u800c\u5173\u952e\u7684\u4efb\u52a1\uff0c\u65e8\u5728\u4ece\u7f51\u7edc\u4e0a\u4f20\u8f93\u7684\u6570\u636e\u6d41 \u91cf\u4e2d\u8bc6\u522b\u51fa\u504f\u79bb\u6b63\u5e38\u6a21\u5f0f\u7684\u7f51\u7edc\u884c\u4e3a\u3002\u8fd9\u4e9b\u5f02\u5e38\u5f80\n\u5f80\u4e0e\u5206\u5e03\u5f0f\u62d2\u7edd\u670d\u52a1\u653b\u51fb\uff08 Distributed Denial of \nService\uff0cDDoS\uff09\u3001\u6076\u610f\u8f6f\u4ef6\u4f20\u64ad\u7b49\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\n\u5bc6\u5207\u76f8\u5173\u3002\u6076\u610f\u6d41\u91cf\u7684\u6cdb\u6ee5\u80fd\u591f\u5927\u89c4\u6a21\u6d88\u8017\u7f51\u7edc\n\u8d44\u6e90\uff0c\u5f15\u53d1\u670d\u52a1\u4e2d\u65ad\uff0c\u4e25\u91cd\u65f6\u8fd8\u4f1a\u5bfc\u81f4\u6574\u4e2a\u7f51\u7edc\n\u57fa\u7840\u8bbe\u65bd\u9677\u5165\u762b\u75ea\u72b6\u6001\u3002\u6b64\u5916\uff0c\u6076\u610f\u6d41\u91cf\u5e38\u642d\u8f7d\n\u6728\u9a6c\u3001\u52d2\u7d22\u8f6f\u4ef6\u7b49\u6076\u610f\u8d1f\u8f7d\uff0c\u5229\u7528\u5148\u8fdb\u7684\u9690\u533f\u6280\n\u672f\u6e17\u900f\u76ee\u6807\u7cfb\u7edf\uff0c\u6267\u884c\u9ad8\u7ea7\u6301\u7eed\u6027\u5a01\u80c1\n\uff08Advanced Persistent Threat \uff0cAPT\uff09\uff0c\u5bf9\u56fd\u5bb6\u5173\n\u952e\u4fe1\u606f\u57fa\u7840\u8bbe\u65bd\u6784\u6210\u6218\u7565\u7ea7\u5a01\u80c1\u3002\u56e0\u6b64\uff0c\u653b\u51fb\u65b9\u5f0f\u7684\u4e0d\u65ad\u6f14\u5316\uff0c\u5bf9\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u3001\n\u8986\u76d6\u5168\u9762\u6027\u3001\u54cd\u5e94\u5b9e\u65f6\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002  \n\u9762\u5bf9\u65e5\u76ca\u590d\u6742\u591a\u53d8\u7684\u7f51\u7edc\u73af\u5883\uff0c\u7f51\u7edc\u6d41\u91cf\u7684\n\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u9700\u5e94\u5bf9\u65b0\u7684\u6311\u6218\u3002\u4f20\u7edf\u7684\u7f51\u7edc\u5f02\u5e38\n\u6d41\u91cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5982\u57fa\u4e8e\u7aef\u53e3\u53f7\u7684\u6620\u5c04\u5339\u914d [1]\u3001\u57fa 2                                                    \u6307   \u6325  \u4e0e  \u63a7  \u5236  \u5b66  \u62a5                                     \u7b2c   \u5377  \n\u4e8e\u6df1\u5ea6\u5305\u7684\u68c0\u6d4b [2]\u3001\u57fa\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60 [3-5]\u548c\u6df1\n\u5ea6\u5b66\u4e60\u7684\u68c0\u6d4b [6-7]\u7b49\uff0c\u867d\u7136\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5e94\n\u5bf9\u7279\u5b9a\u7c7b\u578b\u7684\u5f02\u5e38\uff0c\u4f46\u5728\u9762\u5bf9\u590d\u6742\u7684\u7f51\u7edc\u653b\u51fb\u624b\n\u6bb5\u3001\u5927\u89c4\u6a21\u7684\u6570\u636e\u6d41\u91cf\u4ee5\u53ca\u52a0\u5bc6\u901a\u4fe1\u7b49\u95ee\u9898\u65f6\uff0c\n\u5176\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\u5747\u53d7\u5230\u9650\u5236\u3002\u7279\u522b\u662f\uff0c\u8fd9\u4e9b\n\u65b9\u6cd5\u5f80\u5f80\u4fa7\u91cd\u4e8e\u4ece\u5355\u4e00\u89c6\u89d2\u5206\u6790\u6d41\u91cf\u7279\u6027\uff0c\u5b58\u5728\n\u4e00\u5b9a\u7684\u68c0\u6d4b\u76f2\u533a\uff0c\u5ffd\u89c6\u4e86\u7f51\u7edc\u6d41\u91cf\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\n\u7ef4\u5ea6\u4e0a\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u8fd9\u65e0\u7591\u964d\u4f4e\u4e86\u5f02\u5e38\u6d41\u91cf\u7684\u68c0\n\u6d4b\u6548\u679c\u3002  \n\u5bf9\u6b64\uff0c\u672c\u6587\u521b\u65b0\u5730\u4ece\u65f6\u5e8f\u89c6\u89d2\u548c\u4ea4\u4e92\u89c6\u89d2\u51fa\n\u53d1\u6765\u5206\u6790\u7f51\u7edc\u6d41\u91cf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u7684\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u65b9\u6cd5 \u2014\u2014 MuFF\u3002\u5728\u65f6\u5e8f\u89c6\u89d2\n\u4e0b\uff0c\u6211\u4eec\u4ece\u6d41\u91cf\u4e2d\u63d0\u53d6\u6570\u636e\u5305\u957f\u5ea6\u3001\u4f20\u8f93\u65b9\u5411\u548c\u8d1f\u8f7d\u5b57\u8282\u4fe1\u606f\uff0c\u5e76\u8fdb\u884c\u5e8f\u5217\u5efa\u6a21\uff0c\u5f62\u6210\u6570\u636e\u5305\u957f\n\u5ea6\u5e8f\u5217\u548c\u8d1f\u8f7d\u5b57\u8282\u5e8f\u5217\uff0c\u6700\u540e\u7ed3\u5408\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\n\u7edc\uff08 Long Short -Term Memory \uff0cLSTM\uff09\u548c\u5377\u79ef\u795e\n\u7ecf\u7f51\u7edc\uff08 Convolutional Neural Network \uff0c\nCNN\uff09\uff0c\u6355\u83b7 \u6570\u636e\u5305\u957f\u5ea6\u968f\u65f6\u95f4\u53d8\u5316\u7684\u957f\u671f\u4f9d\u8d56\n\u6027\u548c\u5468\u671f\u6027\u7279\u5f81\uff0c\u4ee5\u53ca\u8d1f\u8f7d\u4e2d\u643a\u5e26\u7684\u6076\u610f\u4fe1\u606f\u5982\n\u9690\u533f\u7684\u6076\u610f\u6307\u4ee4\u5e8f\u5217\u6216\u7279\u6b8a\u7f16\u7801\u6a21\u5f0f\u3002\u5728\u4ea4\u4e92\u89c6\n\u89d2\u4e0b\uff0c\u6211\u4eec\u6839\u636e\u6d41\u91cf\u4e2d\u6570\u636e\u5305\u7684\u4f20\u8f93\u65b9\u5411\u548c\u65f6\u5e8f\n\u5206\u5e03\uff0c\u6784\u5efa\u4e86\u5206\u5c42\u7684\u6570\u636e\u5305\u4ea4\u4e92\u56fe\uff0c\u5e76\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08 Graph Convolutional Network, GCN \uff09\u6355\n\u83b7\u6570\u636e\u5305\u4e4b\u95f4\u9690\u542b\u7684\u62d3\u6251\u5173\u8054\u548c\u76f8\u4e92\u4f5c\u7528\u3002\u6700\u540e\uff0c\u6a21\u578b\u901a\u8fc7\u878d\u5408\u65f6\u5e8f\u548c\u4ea4\u4e92\u7279\u5f81\uff0c\u5145\u5206\u53d1\u6325\u4e0d\n\u540c\u89c6\u89d2\u95f4\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u786e\u4fdd\u5728\u590d\u6742\u573a\u666f\u4e0b\u5bf9\u5f02\u5e38\n\u6d41\u91cf\u68c0\u6d4b\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5728\u516d \u4e2a\u771f\u5b9e\u7684\n\u6d41\u91cf\u6570\u636e \u96c6 CIC-IOMT2024 \u3001UNSW -\nNB15\u3001Darknet2020 \u3001ISCX -VPN 2016\u3001CIC-\nIoT2023 \u548cUSTC -TFC2016\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u8bc4\u4f30\n\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u5bf9\u4e0d\u540c\u7c7b\n\u578b\u6d41\u91cf\u5747\u5177\u6709\u8f83\u597d\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u5e76\u5728\u4e0e\u73b0\u6709\u68c0\u6d4b\n\u6a21\u578b\u7684\u5bf9\u6bd4\u4e2d\u53d6\u5f97\u4e86\u660e\u663e\u7684\u6027\u80fd\u4f18\u52bf\u3002  \n\u672c\u6587\u7684\u5176\u4f59\u90e8\u5206\u7ec4\u7ec7\u5982\u4e0b\u3002\u7b2c 2\u8282\u8ba8\u8bba\u4e86\u7f51\n\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u7684\u76f8\u5173\u5de5\u4f5c\uff0c\u7b2c 3\u8282\u4e3b\u8981\u4ecb\u7ecd\u4e86\n\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u6a21\u578b\u67b6\u6784\u7684\u8bbe\u8ba1\u7ec6\u8282\uff0c\u7b2c 4\u8282\u7ed9\n\u51fa\u4e86\u8be6\u7ec6\u7684\u5b9e\u9a8c\u4ecb\u7ecd\u548c\u5206\u6790\uff0c\u7b2c 5\u8282\u603b\u7ed3\u4e86\u5168\u6587\n\u5e76\u5c55\u671b\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002  \n2  \u76f8\u5173\u5de5\u4f5c  \n\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u65e8\u5728\u901a\u8fc7\u5bf9\u6d41\u91cf\u7684\u76d1\u7763\u548c\n\u7ba1\u7406\uff0c\u53ca\u65f6\u8bc6\u522b\u548c\u53d1\u73b0\u5f02\u5e38\u7684\u6d41\u91cf\u884c\u4e3a\u6216\u6a21\u5f0f\uff0c\u662f\u7ef4\u62a4\u7f51\u7edc\u7a7a\u95f4\u5b89\u5168\u7684\u91cd\u8981\u624b\u6bb5\u3002\u7531\u4e8e\u7f51\u7edc\u653b\u51fb\n\u6d41\u91cf\u5177\u6709\u591a\u6837\u5316\u7684\u7279\u6027\uff0c\u653b\u51fb\u884c\u4e3a\u6240\u4ea7\u751f\u7684\u6d41\u91cf\n\u901a\u5e38\u53ef\u4ee5\u88ab\u5212\u5206\u4e3a\u591a\u4e2a\u7c7b\u522b [8]\uff0c\u7531\u6b64\u50ac\u751f\u4e86\u57fa\u4e8e\n\u5206\u7c7b\u7684\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u65b9\u6cd5\u3002\u901a\u8fc7\u4e8c\u5206\u7c7b\u7684\u65b9\n\u5f0f\uff0c\u7f51\u7edc\u6d41\u91cf\u53ef\u4ee5\u88ab\u5212\u5206\u4e3a\u6b63\u5e38\u6216\u6076\u610f\u6d41\u91cf\uff1b\u800c\u901a\u8fc7\u591a\u5206\u7c7b\u7684\u65b9\u5f0f\uff0c\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u53ef\u4ee5\u88ab\u8fdb\u4e00\u6b65\n\u7ec6\u5206\u4e3a\u4e0d\u540c\u7684\u653b\u51fb\u7c7b\u578b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bb8\u591a\u57fa\u4e8e\n\u7edf\u8ba1\u5b66\u4e60\u548c\u673a\u5668\u5b66\u4e60\u7684\u5206\u7c7b\u6a21\u578b\u88ab\u5e94\u7528\u4e8e\u7f51\u7edc\u5f02\n\u5e38\u6d41\u91cf\u68c0\u6d4b\uff0c\u4f46\u662f\u4e0d\u540c\u65b9\u6cd5\u4ece\u4e0d\u540c\u89c6\u89d2\u51fa\u53d1\uff0c\u6240\n\u6355\u6349\u5230\u7684\u6a21\u5f0f\u4e0e\u7279\u5f81\u5404\u4e0d\u76f8\u540c\u3002  \n\u5728\u65e9\u671f\u7684\u4e92\u8054\u7f51\u73af\u5883\u4e2d\uff0c\u7814\u7a76\u8005\u5c06 TCP\u6216\nUDP\u7684\u6307\u5b9a\u7aef\u53e3\u53f7\u4e0e\u5e94\u7528\u534f\u8bae\u76f8\u4e92\u6620\u5c04\u5339\u914d [9]\uff0c\n\u6765\u533a\u5206\u7f51\u7edc\u6d41\u91cf\u7684\u7c7b\u578b\u3002\u6b64\u7c7b\u65b9\u6cd5\u7b80\u5355\u9ad8\u6548\uff0c\u4f46\n\u968f\u7740\u7aef\u53e3\u4f2a\u88c5\u6280\u672f\u4e0e\u52a8\u6001\u7aef\u53e3\u5206\u914d\u6280\u672f\u7684\u666e\u53ca\uff0c\u5176\u68c0\u6d4b\u6548\u679c\u53d7\u5230\u4e86\u6781\u5927\u7684\u5f71\u54cd\u3002\u4e3a\u907f\u514d\u5bf9\u7aef\u53e3\u53f7\n\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u8bc1\u8f83\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u57fa\u4e8e\u6df1\n\u5ea6\u5305\u7684\u68c0\u6d4b\u6280\u672f\uff08 Deep Packet Inspection \uff0cDPI\uff09\n\u968f\u4e4b\u88ab\u63d0\u51fa\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6570\u636e\u5305\u7684\u5934\u90e8\u4fe1\u606f\u53ca\u8d1f\u8f7d\u8fdb\u884c\u68c0\u6d4b\uff0c\u6765\u5224\u65ad\u6570\u636e\u5305\u7684\u771f\u5b9e\u610f\u56fe\u548c\u884c\u4e3a\n\u6a21\u5f0f [2]\u3002\u7136\u800c\uff0c\u968f\u7740\u7f51\u7edc\u6d41\u91cf\u7684\u6fc0\u589e\uff0c\u6b64\u7c7b\u65b9\u6cd5\n\u5728\u5bf9\u6570\u636e\u5305\u8fdb\u884c\u9010\u5b57\u8282\u5206\u6790\u7684\u8fc7\u7a0b\u4e2d\u9700\u8981\u6d88\u8017\u5927\n\u91cf\u7684\u65f6\u95f4\uff0c\u589e\u5927\u7f51\u7edc\u8bbe\u5907\u8fd0\u884c\u538b\u529b\uff0c\u540c\u65f6\u4e5f\u96be\u4ee5\u76f4\u63a5\u5206\u6790\u5305\u542b\u79c1\u6709\u52a0\u5bc6\u534f\u8bae\u7684\u6570\u636e\u5305\u3002 Wang [10]\n\u7387\u5148\u5c06\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5e94\u7528\u4e8e\u6d41\u91cf\u5206\u7c7b\uff0c\u5728\u7279\u5f81\u5b66\u4e60\u548c\u6d41\u91cf\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u5c55\u793a\u4e86\u663e\u7740\u7684\u6027\u80fd\u3002 FS-\nNet[11]\u5c06\u6570\u636e\u5305\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u91c7\u7528\u53cc\u5411\nGRU\u8fdb\u884c\u7279\u5f81\u7f16\u7801\uff0c\u5e76\u5728 AutoEncoder \u4e2d\u5f15\u5165\u91cd\n\u5efa\u673a\u5236\uff0c\u4ee5\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u7279\u5f81\u7684\u6709\u6548\u6027\u3002 MTT [12]\n\u91c7\u7528\u5728\u622a\u65ad\u6570\u636e\u5305\u5b57\u8282\u5e8f\u5217\u4e0a\u8bad\u7ec3\u7684\u591a\u4efb\u52a1\nTransformer \uff0c\u4ee5\u76d1\u7763\u65b9\u5f0f\u5206\u6790\u6d41\u91cf\u7279\u5f81\u3002\nNetMamba [13]\u63d0\u51fa\u4e00\u79cd\u4e13\u4e3a\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u800c\u8bbe\u8ba1\n\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5229\u7528\u5355\u5411 Mamba\n\u67b6\u6784\u8fdb\u884c\u6d41\u91cf\u5e8f\u5217\u5efa\u6a21\uff0c\u5e76\u53d6\u4ee3 Transformer \u4ee5\u89e3\n\u51b3\u6548\u7387\u95ee\u9898\u3002\u6b64\u5916\uff0c Radford \u7b49[14]\u5c06\u7f51\u7edc\u6d41\u91cf\u89c6\n\u4e3a\u8ba1\u7b97\u673a\u8bbe\u5907\u4e4b\u95f4\u901a\u8baf\u7684\u8bed\u8a00\uff0c\u5373\u7531\u7279\u6b8a\u5355\u8bcd\u6784\n\u6210\u7684\u6d41\u91cf\u5e8f\u5217\uff0c\u5e76\u5229\u7528\u5e8f\u5217\u6a21\u578b\u6765\u5206\u6790\u7f51\u7edc\u6d41\u91cf\n\u7279\u5f81\u3002\u7136\u800c\uff0c\u4e0a\u8ff0\u65b9\u6cd5\u672a\u80fd\u5173\u6ce8\u5230\u7f51\u7edc\u4e2d\u5b9e\u4f53\u95f4\n\u7684\u4ea4\u4e92\u5173\u7cfb\u3002\u57fa\u4e8e\u56fe\u5206\u6790\u7684\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c06\u6d41\u91cf\n\u4e2d\u7684\u6570\u636e\u5305\u89c6\u4e3a\u8282\u70b9\uff0c\u5c06\u6570\u636e\u5305\u95f4\u4f20\u8f93\u5173\u7cfb\u89c6\u4e3a\n\u8fde\u8fb9\uff0c\u6784\u5efa\u6d41\u91cf\u4ea4\u4e92\u56fe\uff0c\u5e76\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u56fe\n\u8868\u5f81\u5b66\u4e60\u6280\u672f\u6765\u6316\u6398\u6f5c\u5728\u7684\u7f51\u7edc\u62d3\u6251\u4fe1\u606f\u548c\u6d41\u91cf\n\u4ea4\u4e92\u4fe1\u606f [15-17]\u3002DE-GNN [18]\u901a\u8fc7\u8bbe\u8ba1\u53cc\u5d4c\u5165\n\u5c42\uff0c\u5206\u522b\u5bf9\u6570\u636e\u5305\u7684\u5305\u5934\u548c\u8d1f\u8f7d\u5b57\u8282\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u5229\u7528 PacketCNN \u83b7\u53d6\u6d41\u91cf\u7684\u6570\u636e\u5305\u7ea7\u8868\u793a\uff0c\u6784\n\u5efa\u6d41\u91cf\u4ea4\u4e92\u56fe\u3002 Zhe Yang \u7b49[19-20]\u5f15\u5165\u8d85\u56fe\u7684\u6982\n\u5ff5\uff0c\u901a\u8fc7\u5c06\u6d41\u91cf\u8868\u793a\u4e3a\u8282\u70b9\uff0c\u5229\u7528 KNN\u7b97\u6cd5\u6784\n\u5efa\u8d85\u8fb9\uff0c\u6765\u53cd\u5e94\u6d41\u91cf\u7684\u6f5c\u5728\u76f8\u4f3c\u6027\u3002\u6b64\u5916\uff0c\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c NetGPT [21]\u548cLens [22]\u7b49\n\u9884\u8bad\u7ec3\u6d41\u91cf\u57fa\u7840\u6a21\u578b\u88ab\u7528\u6765\u540c\u65f6\u89e3\u51b3\u6d41\u91cf\u5206\u6790\u548c\u751f\u6210\u95ee\u9898\u3002  \u7b2c \u671f                                         \u5b8b\u660a\uff1a\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u7684\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf \u68c0\u6d4b                                      3 \n \n\u56fe1 \u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u6a21\u578b\u6846\u67b6\n3  \u6a21\u578b\u8bbe\u8ba1  \n\u672c\u7814\u7a76\u4ece\u6d41\u91cf\u4e2d\u6570\u636e\u5305\u7684\u65f6\u5e8f\u548c\u4ea4\u4e92\u89c6\u89d2\u51fa\n\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u7684\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b  \n\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u539f\u59cb\u6d41\u91cf\u6570\u636e\u5206\u522b\u8fdb\u884c\u65f6\u5e8f\u5efa\n\u6a21\u548c\u4ea4\u4e92\u5efa\u6a21\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u65f6\u5e8f\u6a21\u578b\u548c\n\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4ece\u4e0d\u540c\u89c6\u89d2\u63d0\u53d6\u6d41\u91cf\u7279\u5f81\uff0c\u7528\u4e8e\n\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u3002\u8be5\u6a21\u578b\u8bbe\u8ba1\u514b\u670d\u4e86\u4f20\u7edf\u5355\u4e00\u89c6\u89d2\n\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\uff0c\u63d0\u5347\u5bf9\u590d\u6742\u7f51\u7edc\u884c\u4e3a\u7684\u7406\u89e3\u4e0e\u5f02\u5e38\u6a21\u5f0f\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5c24\u5176\n\u662f\u5728\u5bf9\u6297\u65e5\u76ca\u590d\u6742\u7684\u7f51\u7edc\u653b\u51fb\u4e0e\u9690\u853d\u6d41\u91cf\u64cd\u7eb5\u7b56\u7565\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002\u6211\u4eec\u5c06\u5728\u672c\u5c0f\u8282\u5bf9\u63d0\u51fa\u7684\n\u65b9\u6cd5\u8fdb\u884c\u8be6\u5c3d\u7684\u4ecb\u7ecd\uff0c\u5176\u6574\u4f53\u6846\u67b6\u5982\u56fe 1\u6240\u793a\u3002  \n3.1 \u65f6\u5e8f\u89c6\u89d2\u611f\u77e5 \n\u8bb8\u591a\u627f\u8f7d\u7f51\u7edc\u653b\u51fb\uff08\u5982 DDoS\u3001DoS\u548c\u50f5\u5c38\n\u7f51\u7edc\u653b\u51fb\u7b49\uff09\u7684\u5f02\u5e38\u6d41\u91cf\u5728\u65f6\u5e8f\u7279\u5f81\u4e0a\u8868\u73b0\u51fa\u660e\n\u663e\u4e0d\u540c\u4e8e\u6b63\u5e38\u6d41\u91cf\u7684\u6a21\u5f0f\u3002\u4f8b\u5982\uff0c\u7f51\u7edc\u6d41\u91cf\u5728\u975e\n\u9ad8\u5cf0\u65f6\u6bb5\u51fa\u73b0\u6301\u7eed\u4e0a\u5347\uff0c\u504f\u79bb\u4e86\u9884\u671f\u6a21\u5f0f\uff0c\u53ef\u80fd\n\u610f\u5473\u7740\u6b63\u5728\u8fdb\u884c\u7684\u653b\u51fb\u6d3b\u52a8\u6216\u7cfb\u7edf\u6545\u969c\uff1b\u6216\u8005\u9891\n\u7e41\u51fa\u73b0\u5927\u91cf\u76f8\u540c\u5927\u5c0f\u6216\u7c7b\u578b\u7684\u5783\u573e\u6570\u636e\u5305\uff0c\u4f01\u56fe\n\u5360\u7528\u7f51\u7edc\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4ece\u65f6\u5e8f\u7279\u5f81\u611f\u77e5\u7684\u89c6\u89d2\u51fa\u53d1\uff0c\u901a\u8fc7 CNN\u548cLSTM\u6765\u6df1\u5165\u6316\u6398\u6d41\u91cf\u6570\n\u636e\u5305\u5185\u6f5c\u5728\u7684\u65f6\u5e8f\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ee5\u589e\u5f3a\u5f02\u5e38\u6d41\u91cf\u68c0\n\u6d4b\u7684\u65f6\u5e8f\u611f\u77e5\u80fd\u529b\u3002  \n3.1.1\u6d41\u91cf\u65f6\u5e8f\u5efa\u6a21  \n\u57fa\u4e8e\u65f6\u5e8f\u89c6\u89d2\u8fdb\u884c\u6d41\u91cf\u7279\u5f81\u611f\u77e5\u524d\uff0c\u6211\u4eec\u9996\n\u5148\u4ece\u6d41\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u5e8f\u5217\u5316\u4fe1\u606f\u3002  \n\u5bf9\u6d41\u91cf\u4e2d\u7684\u6570\u636e\u5305\uff0c\u6211\u4eec\u9996\u5148\u63d0\u53d6\u6570\u636e\u5305\u7684\n\u4f20\u8f93\u65b9\u5411\u548c\u957f\u5ea6\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u6309\u7167\u6570\u636e\u5305\u4ea7\u751f\u7684\n\u65f6\u95f4\u8fdb\u884c\u6392\u5217\uff0c\u6784\u6210\u6d41\u91cf\u7684 \u6570\u636e\u5305\u957f\u5ea6\u5e8f\u5217 \u3002\u5bf9\u4e8e\u957f\u65f6\u95f4\u6301\u7eed\u7684\u7f51\u7edc\u6d41\u91cf\uff0c\u6211\u4eec\u4ec5\u53d6\u5176\u524d n\u4e2a\u6570\n\u636e\u5305\u4f5c\u4e3a\u4ee3\u8868\uff0c\u63d0\u53d6\u5176\u4e2d\u7684\u4fe1\u606f\u6765\u6784\u5efa\u5e8f\u5217\uff1b\u5bf9\n\u4e8e\u6570\u636e\u5305\u6570\u91cf\u4e0d\u8db3 n\u7684\u7f51\u7edc\u6d41\u91cf\uff0c\u6211\u4eec\u7528 0\u586b\u5145\n\u7f3a\u5931\u90e8\u5206\uff0c\u6700\u7ec8\u786e\u4fdd\u5e8f\u5217\u6570\u636e\u957f\u5ea6\u7684\u4e00\u81f4\u6027\u3002 \u4f8b\n\u5982\uff0c\u8003\u8651\u4e00\u6bb5\u5305\u542b 100\u4e2a\u6570\u636e\u5305\u7684\u7f51\u7edc\u6d41\u91cf \u3002\u9009\n\u53d6\u524d n=40\u4e2a\u6570\u636e\u5305\u4f5c\u4e3a\u4ee3\u8868\uff0c\u5176 \u957f\u5ea6\u5206\u522b\u4e3a\uff1a\n100 bytes, 120 bytes, 80 bytes, ..., 60 bytes \u3002\u7531\u4e8e\u901a\n\u4fe1\u8fc7\u7a0b\u7684\u53cc\u5411\u6027 \uff0c\u6570\u636e\u5305\u7684\u4e0d\u540c\u4f20\u8f93\u65b9\u5411\u7531\u00b1 1\n\u6765\u8868\u793a\u3002\u6700\u7ec8\u6211\u4eec\u6309\u7167\u6570\u636e\u5305\u4ea7\u751f\u7684\u987a\u5e8f\u6784\u5efa\u957f\u5ea6\u5e8f\u5217\uff0c\u5373 [100, -120, 80, ..., -60]\u3002\u5bf9\u4e8e\u6570\u636e\u5305\u6570\n\u91cf\u4e0d\u8db3 40\u4e2a\u7684\u60c5\u51b5\uff0c\u5219\u7528 0\u586b\u5145\u5269\u4f59\u7684\u4f4d\u7f6e\uff0c\u4ee5\n\u786e\u4fdd\u5e8f\u5217\u957f\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u5373 [100, - 120, 80,  ..., 0]\u3002 \n\u5bf9\u6570\u636e\u5305\u4e2d\u7684\u8d1f\u8f7d\u4fe1\u606f\uff0c\u6211\u4eec\u4ee5 m\u4e3a\u9608\u503c\uff0c\n\u5bf9\u6bcf\u4e2a\u6570\u636e\u5305\u7684\u8d1f\u8f7d\u63d0\u53d6\u524d m\u4e2a\u5b57\u8282\u7684\u4fe1\u606f\uff0c\u5e76\n\u5c06\u5176\u4ece 16\u8fdb\u5236\u8f6c\u6362\u4e3a\n10\u8fdb\u5236\uff0c\u4ee5\u9002\u5e94\u5e8f\u5217\u6570\u636e\n\u7684\u683c\u5f0f\uff0c\u6700\u7ec8\u5f62\u6210\u6d41\u91cf\u7684 \u8d1f\u8f7d\u5b57\u8282\u5e8f\u5217 \u3002\u5bf9\u4e8e\u8d1f\n\u8f7d\u5185\u5bb9\u4e0d\u8db3 m\u5b57\u8282\u7684\u6570\u636e\u5305\uff0c\u6211\u4eec\u7528 0\u586b\u5145\u7f3a\u5931\n\u90e8\u5206\u3002\u4f8b\u5982\uff0c \u6211\u4eec\u9009\u62e9\u4ece\u6bcf\u4e2a\u6570\u636e\u5305\u4e2d\u63d0\u53d6\u524d 16\n\u4e2a\u5b57\u8282\u7684\u4fe1\u606f\u3002 \u5176\u4e2d\u7b2c\u4e00\u4e2a\u6570\u636e\u5305\u7684\u524d 16\u4e2a\u5b57\u8282\n\u4e3a2a0500a800002a0500a900002a0500bc \u3002\u6211\u4eec\u5c06\n\u8fd9\u4e9b\u5341\u516d\u8fdb\u5236\u6570\u503c\u8f6c\u6362\u4e3a\u5341\u8fdb\u5236\u6570\u503c\uff0c\u4ee5\u9002\u5e94\u5e8f\u5217\u6570\u636e\u7684\u683c\u5f0f\uff0c\u5373  [42, 5, 0, 168, 0, 0, 42, 5, 0, 169, \n0, 0, 42, 5, 0, 188]\u3002\u5bf9\u4e8e\u6709\u6548\u8d1f\u8f7d\u5185\u5bb9\u4e0d\u8db3 16\u5b57\n\u8282\u7684\u6570\u636e\u5305\uff0c\u5219 \u75280\u586b\u5145\u5269\u4f59\u7684\u4f4d\u7f6e\uff0c\u4ee5\u786e\u4fdd\u5e8f\n\u5217\u957f\u5ea6\u7684\u4e00\u81f4\u6027\u3002  \n3.1.2\u6d41\u91cf\u65f6\u5e8f\u7279\u5f81\u63d0\u53d6  \n\u9488\u5bf9\u6d41\u91cf\u7684\u8d1f\u8f7d\u5b57\u8282\u5e8f\u5217\uff0c\u6211\u4eec\u5229\u7528\u5377\u79ef\u795e\n\u7ecf\u7f51\u7edc\uff08 CNN\uff09\u5bf9\u5176\u5305\u542b\u7684\u8d1f\u8f7d\u5185\u5bb9\u8fdb\u884c\u7279\u5f81\u63d0\n\u53d6\u3002 CNN\u901a\u8fc7\u5c40\u90e8\u8fde\u63a5\u548c\u6743\u503c\u5171\u4eab\u7684\u7279\u6027\uff0c\u80fd\u591f\n\u5728\u8d1f\u8f7d\u6570\u636e\u4e2d\u81ea\u52a8\u5b66\u4e60\u5e76\u63d0\u53d6\u51fa\u5177\u6709\u7279\u5b9a\u6a21\u5f0f\u7684\n\u7279\u5f81\uff0c\u5982\u7279\u5b9a\u7684\u6076\u610f\u6307\u4ee4\u5e8f\u5217\u3001\u6076\u610f\u4ee3\u7801\u7247\u6bb5\u6216\n\u662f\u5f02\u5e38\u7684\u4e8c\u8fdb\u5236\u6a21\u5f0f\u3002\u901a\u8fc7\u7ec4\u5408\u591a\u5c42\u5377\u79ef\u3001\u6c60\u5316 4                                                    \u6307   \u6325  \u4e0e  \u63a7  \u5236  \u5b66  \u62a5                                     \u7b2c   \u5377  \n\u4ee5\u53ca\u975e\u7ebf\u6027\u6fc0\u6d3b\uff0c CNN\u80fd\u591f\u9010\u6b65\u63d0\u70bc\u51fa\u6570\u636e\u5305\u8d1f\n\u8f7d\u4e2d\u7684\u9ad8\u5c42\u6b21\u62bd\u8c61\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u80fd\u591f\u6709\u6548\u53cd\u6620\n\u6f5c\u5728\u7684\u6076\u610f\u6d3b\u52a8\u8ff9\u8c61\u3002  \n\u672c\u6587\u6240\u7528\u7684 CNN\u6a21\u578b\u7531\u4e24\u4e2a\u5377\u79ef\u5c42\u6784\u6210\uff0c\n\u5728\u6bcf\u4e00\u4e2a\u5377\u79ef\u5c42\u540e\u90fd\u63a5\u6709\u5f52\u4e00\u5316\u5c42\u3001 ReLU\u6fc0\u6d3b\n\u51fd\u6570\u548c\u6700\u5927\u6c60\u5316\u5c42\uff0c\u4ee5\u6b64\u6765\u5bf9\u62bd\u53d6\u7684\u6f5c\u5728\u7279\u5f81\u8fdb\u884c\u6807\u51c6\u5316\u5904\u7406\uff0c\u4fdd\u7559\u5176\u91cd\u8981\u7279\u5f81\u5e76\u51cf\u5c11\u5185\u5b58\u6d88\n\u8017\u3002\u5377\u79ef\u5c42\u7684\u5177\u4f53\u8ba1\u7b97\u516c\u5f0f\u5982\u4e0b\uff1a  \n() ( ) ( ) ( )\n() ( ) ( ) ( )payload\n2MaxPool1d Re LU BatchNorm Conv1d\nMaxPool1d Re LU BatchNorm Conv1d=\n=1\n1HX\nHH(1) \n\u5176\u4e2d Conv1d\u4e3a\u4e00\u7ef4\u5377\u79ef\u64cd\u4f5c\uff0c BatchNorm \u4e3a\u5f52\n\u4e00\u5316\u64cd\u4f5c\uff0c MaxPool1d \u4e3a\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u3002\u5728\u7ecf\u8fc7\n\u4e24\u5c42\u5377\u79ef\u5c42\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6ce8\u610f\u529b\u673a\u5236\n\uff08Attention \uff09\uff0c\u901a\u8fc7\u52a8\u6001\u5730\u4e3a\u8f93\u5165\u7684\u4e0d\u540c\u90e8\u5206\u5206\n\u914d\u6743\u91cd\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u805a\u7126\u4e8e\u6570\u636e\u4e2d\u7684\u6700\u91cd\u8981\u7279\u5f81\uff0c\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u8bc6\u522b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5229\u7528\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u548c\u4e00\u4e2a ReLU\u6fc0\u6d3b\u51fd\u6570\u8ba1\u7b97\u6ce8\u610f\n\u529b\u6743\u91cd\uff0c\u5e76\u4ee5\u6b64\u5bf9\u539f\u59cb\u7279\u5f81\u8fdb\u884c\u52a0\u6743\u64cd\u4f5c\uff0c\u8be5\u8fc7\u7a0b\u8868\u793a\u5982\u4e0b\uff1a  \n() ( )\n()\n()21\nattRe LU\nexp\nexpii\ni\ni\nj j\nii\nie\nea\ne\na\u03c3= \u22c5\u22c5\n=\n= \u22c5\u2211\n\u2211W Wh\nhh   (2)  \n\u5176\u4e2d e\u4e3a\u6ce8\u610f\u529b\u6743\u91cd\uff0c1W\u548c2W\u662f\u5168\u8fde\u63a5\u5c42\u7684\u6743\n\u91cd\u77e9\u9635\uff0c\u03c3\u4e3asigmoid\u6fc0\u6d3b\u51fd\u6570\uff0ca \u4e3a\u5f52\u4e00\u5316\u7684\u6ce8\n\u610f\u529b\u7cfb\u6570\u3002\u5c06\u52a0\u6743\u540e\u7684\u7279\u5f81\u5c55\u5e73\u5e76\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u6620\u5c04\u5230 M\u7ef4\u7279\u5f81\u5411\u91cf\uff1a \n()flatten att\nCNN flattenFlatten( )\nLinear=\n=hh\nZh      (3)  \n\u9488\u5bf9\u6d41\u91cf\u7684\u6570\u636e\u5305\u957f\u5ea6\u5e8f\u5217\uff0c\u6211\u4eec\u5229\u7528\u957f\u77ed\n\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08 LSTM\uff09\u5bf9\u5176\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u6355\u6349\n\u957f\u5ea6\u5e8f\u5217\u7684\u957f\u671f\u4f9d\u8d56\u6027\u548c\u5468\u671f\u6027\u53d8\u5316\u3002 LSTM\u901a\n\u8fc7\u5176\u72ec\u7279\u7684\u95e8\u63a7\u673a\u5236\uff0c\u5305\u62ec\u8f93\u5165\u95e8\u3001\u8f93\u51fa\u95e8\u548c\u9057\u5fd8\u95e8\uff0c\u80fd\u591f\u6709\u6548\u5730\u8bb0\u4f4f\u8fc7\u53bb\u7684\u6d41\u91cf\u6a21\u5f0f\uff0c\u9057\u5fd8\u65e0\n\u5173\u4fe1\u606f\uff0c\u9884\u6d4b\u672a\u6765\u6d41\u91cf\u7684\u6b63\u5e38\u53d8\u5316\u8d8b\u52bf\uff0c\u4ece\u800c\u51c6\n\u786e\u5730\u533a\u5206\u51fa\u504f\u79bb\u6b63\u5e38\u6a21\u5f0f\u7684\u5f02\u5e38\u6d41\u91cf\u884c\u4e3a\u3002\u6a21\u578b\n\u66f4\u65b0\u516c\u5f0f\u5982\u4e0b\uff1a  \n[] ( )\n[] ( )\n[] ( )\n[] ( )\n()-1\n-1\n-1\n-1 -1,\n,\ntanh ,\ntanht i tt i\nt o lt o\nt f tt f\nt t t t c tt c\ntt t\u03c3\n\u03c3\n\u03c3= \u22c5+\n= \u22c5+\n= \u22c5+\n= \u2297 +\u2297 \u22c5 +\n= \u2297i WHx b\no WHx b\nf W H ,x b\nC f C i WHx b\nHo C(4) \u5176\u4e2d\u03c3\u4e3asigmoid\u6fc0\u6d3b\u51fd\u6570\uff0ctanh \u4e3a\u53cc\u66f2\u6b63\u5207\u51fd\n\u6570\uff0cW\u4e3a\u5bf9\u5e94\u7684\u6743\u91cd\uff0c b\u4e3a\u504f\u7f6e\u3002\u5728 LSTM\n\u5c42\uff0c\u6211\u4eec\u540c\u6837\u5f15\u5165\u4e86\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u589e\u5f3a\u5176\u5b66\u4e60\u6548\n\u679c\u3002 LSTM\u5f97\u5230\u7684\u8f93\u51fatH\u8f93\u5165\u5230\u6ce8\u610f\u529b\u5c42\uff0c\u5f97\u5230\n\u8f93\u51fa\u5411\u91cf\uff1a  \n()LSTM Attentiont = ZH       (5)  \n3.2 \u4ea4\u4e92\u89c6\u89d2\u611f\u77e5 \n\u90e8\u5206\u5f02\u5e38\u6d41\u91cf\u5728\u6570\u636e\u5305\u7684\u4ea4\u4e92\u4e0a\u8868\u73b0\u51fa\u660e\u663e\n\u533a\u522b\u4e8e\u6b63\u5e38\u6d41\u91cf\u7684\u884c\u4e3a\u6a21\u5f0f\u3002\u4f8b\u5982\uff0c\u67d0\u4e00 IP\u5730\u5740\n\u5728\u77ed\u65f6\u95f4\u5185\u5bf9\u591a\u4e2a\u4e0d\u540c\u7aef\u53e3\u53d1\u8d77\u5927\u91cf\u8fde\u63a5\u8bf7\u6c42\uff1b\n\u6216\u8005\u67d0\u4e00\u6bb5\u65f6\u95f4\u5185\u9891\u7e41\u51fa\u73b0\u7279\u5b9a\u534f\u8bae\u7684\u6570\u636e\u5305\uff0c\n\u8fd9\u4e9b\u884c\u4e3a\u53ef\u80fd\u6697\u793a\u6b63\u5728\u8fdb\u884c\u7684\u626b\u63cf\u653b\u51fb\u6216\u6076\u610f\u6d3b\n\u52a8\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4ece\u6570\u636e\u5305\u7684\u4ea4\u4e92\u89c6\u89d2\u51fa\u53d1\uff0c\u901a\u8fc7\n\u6df1\u5165\u6316\u6398\u6d41\u91cf\u6570\u636e\u5305\u5185\u6f5c\u5728\u7684\u4ea4\u4e92\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ee5\u589e\u5f3a\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u6a21\u578b\u7684\u4ea4\u4e92\u5173\u7cfb\u611f\u77e5\u80fd\u529b\u3002  \n3.2.1\u6d41\u91cf\u4ea4\u4e92\u5efa\u6a21  \n\u57fa\u4e8e\u4ea4\u4e92\u89c6\u89d2\u8fdb\u884c\u6d41\u91cf\u7279\u5f81\u611f\u77e5\u524d\uff0c\u6211\u4eec\u9996\n\u5148\u4ece\u6d41\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u4ea4\u4e92\u4fe1\u606f\u3002\u5177\u4f53\u800c\u8a00\uff0c\u91c7\u7528\u4e00\u79cd\u6d41\u91cf\u6784\u56fe\u7b56\u7565\uff0c\u901a\u8fc7\u6784\u5efa\n\u6d41\u91cf\u6570\u636e\u5305\u4ea4\u4e92\u56fe\n\u6765\u8868\u793a\u6d41\u91cf\u5185\u90e8\u6570\u636e\u5305\u4e4b\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb [23]\u3002\u670d\n\u52a1\u5668\u4e0e\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u6d41\u91cf\u4ea4\u4e92\u5982\u56fe 2\u6240\u793a\uff0c\u6211\u4eec\n\u5c06\u6570\u636e\u4ece\u5ba2\u6237\u7aef\u53d1\u9001\u81f3\u670d\u52a1\u5668\u7684\u65b9\u5411\u6807\u8bb0\u4e3a\u8d1f\uff0c\n\u5c06\u670d\u52a1\u5668\u53d1\u5f80\u5ba2\u6237\u7aef\u7684\u65b9\u5411\u5219\u6807\u8bb0\u4e3a\u6b63\u3002\u8fd9\u4e00\u5b9a\n\u4e49\u8003\u8651\u5230\u901a\u4fe1\u8fc7\u7a0b\u7684\u53cc\u5411\u6027\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u7406\n\u89e3\u548c\u5206\u6790\u7f51\u7edc\u6d41\u91cf\u7684\u52a8\u6001\u7279\u5f81\u3002\u56fe 3\u5c55\u793a\u4e86\u4e00\u4e2a\n\u6d41\u91cf\u4ea4\u4e92\u56fe\u7684\u793a\u4f8b\uff0c\u6211\u4eec\u5c06\u6570\u636e\u5305\u4f5c\u4e3a\u8282\u70b9\uff0c\u5c06\u5176\u957f\u5ea6\u548c\u65b9\u5411\u4f5c\u4e3a\u8282\u70b9\u7279\u5f81\uff0c\u5c06\u6570\u636e\u5305\u4e4b\u95f4\u7684\u901a\u4fe1\u5173\u7cfb\u4f5c\u4e3a\u8fde\u8fb9\uff0c\u4e0d\u540c\u65b9\u5411\u7684\u6570\u636e\u5305\u7528\u4e0d\u540c\u989c\u8272\n\u6807\u8bb0\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5c42\u7684\u6982\u5ff5\uff0c\u5c06\u8fde\u7eed\u7684\u540c\n\u65b9\u5411\u6570\u636e\u5305\u56ca\u62ec\u5728\u4e00\u5c42\u4e2d\uff08\u56fe\u4e2d\u4ee5\u5217\u8868\u793a\uff09\u3002  \n \n\u56fe2  \u670d\u52a1\u5668\u4e0e\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u6d41\u91cf\u4ea4\u4e92 \n \n\u56fe3  \u6d41\u91cf\u6570\u636e\u5305\u4ea4\u4e92\u56fe    \u7b2c \u671f                                         \u5b8b\u660a\uff1a\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u7684\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf \u68c0\u6d4b                                      5 \n\u8003\u8651\u5230\u4e0d\u540c\u7f51\u7edc\u6d41\u91cf\u7684\u6301\u7eed\u65f6\u95f4\u5dee\u5f02\uff0c\u6211\u4eec\n\u9488\u5bf9\u6301\u7eed\u65f6\u95f4\u8f83\u957f\u7684\u7f51\u7edc\u6d41\u91cf\u91c7\u53d6\u4e86\u4e00\u79cd\u7279\u6b8a\u5904\n\u7406\u65b9\u5f0f\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9009\u62e9\u8fd9\u4e9b\u957f\u65f6\u6d41\u91cf\u7684\u524d\nn\u4e2a\u6570\u636e\u5305\u6765\u4ee3\u8868\u6574\u4e2a\u6d41\u91cf\uff0c\u5e76\u4ee5\u8fd9 n\u4e2a\u6570\u636e\u5305\n\u4e3a\u57fa\u7840\u6784\u5efa\u6d41\u91cf\u6570\u636e\u5305\u4ea4\u4e92\u56fe\u3002\u8fd9\u4e00\u7b56\u7565\u7684\u4f18\u52bf\n\u5728\u4e8e\uff0c\u901a\u8fc7\u63d0\u524d\u5904\u7406\uff0c\u6211\u4eec\u80fd\u591f\u66f4\u65e9\u5730\u68c0\u6d4b\u5230\u6301\n\u7eed\u65f6\u95f4\u8f83\u957f\u7684\u6076\u610f\u6d41\u91cf\uff0c\u5e76\u4e14\u53ca\u65f6\u91c7\u53d6\u9632\u5fa1\u63aa\n\u65bd\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u8282\u7701\u4e86\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u4e5f\u63d0\u9ad8\n\u4e86\u5bf9\u7f51\u7edc\u6d41\u7684\u5206\u6790\u6548\u7387\u548c\u51c6\u786e\u6027\u3002  \n3.2.2\u6d41\u91cf\u4ea4\u4e92\u7279\u5f81\u63d0\u53d6  \n\u9488\u5bf9\u6d41\u91cf\u6570\u636e\u5305\u4ea4\u4e92\u56fe\uff0c\u6211\u4eec\u91c7\u7528\u56fe\u5377\u79ef\u7f51\n\u7edcGCN\u6a21\u578b [24]\u6765\u6355\u6349\u7f51\u7edc\u6d41\u91cf\u4e2d\u6570\u636e\u5305\u7684\u590d\u6742\n\u4f20\u8f93\u5173\u7cfb\u548c\u6f5c\u5728\u7684\u5f02\u5e38\u4ea4\u4e92\u6a21\u5f0f\u3002 GCN\u6a21\u578b\u901a\u8fc7\n\u76f4\u63a5\u5728\u56fe\u7ed3\u6784\u4e0a\u8fdb\u884c\u7279\u5f81\u7684\u4f20\u64ad\u3001\u805a\u5408\u548c\u8f6c\u6362\u64cd\u4f5c\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u8282\u70b9\u7279\u5f81\u53ca\u5176\u90bb\u5c45\u8282\u70b9\u95f4\u7684\u76f8\n\u4e92\u4f5c\u7528\uff0c\u4e0d\u4ec5\u80fd\u591f\u63ed\u793a\u9690\u85cf\u7684\u653b\u51fb\u8def\u5f84\u548c\u534f\u540c\u884c\n\u4e3a\uff0c\u8fd8\u80fd\u6355\u6349\u5230\u5f02\u5e38\u7684\u6d41\u91cf\u5206\u5e03\u548c\u901a\u4fe1\u6a21\u5f0f\u3002  \n\u672c\u6587\u4e2d\u7684 GCN\u6a21\u578b\u7528\u4e8e\u5b66\u4e60\u6bcf\u4e00\u4e2a\u6d41\u91cf\u6570\n\u636e\u5305\u4ea4\u4e92\u56fe\u7684\u8868\u5f81\u4fe1\u606f\u3002\u5728\u8be5\u6a21\u578b\u4e2d\uff0c\u6211\u4eec\u5c06\u6d41\n\u91cf\u6570\u636e\u5305\u4ea4\u4e92\u56fe\u7684\u90bb\u63a5\u4fe1\u606f\u548c\u8282\u70b9\u7279\u5f81\u4f5c\u4e3a\u8f93\n\u5165\uff0c\u5f97\u5230\u6574\u4e2a\u4ea4\u4e92\u56fe\u7684\u8868\u793a\u4f5c\u4e3a\u8f93\u51fa\u3002\u5728\u5bf9\u90bb\u57df\n\u8282\u70b9\u4fe1\u606f\u8fdb\u884c\u805a\u5408\u4e4b\u524d\uff0c\u5148\u5c06\u6d41\u91cf\u4ea4\u4e92\u56fe\u7684\u90bb\u63a5\n\u4fe1\u606f\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\uff1a  \n1/2 1/2\nnorm\u2212\u2212=\uf025\uf025\uf025\uf025 A D AD         (6)  \n\u5176\u4e2dn= +\uf025A AI \u8868\u793a\u5bf9\u90bb\u63a5\u77e9\u9635\u6dfb\u52a0\u81ea\u73af\uff0c \uf025D\u662f\n\uf025A\u7684\u5ea6\u77e9\u9635\uff0c\u6709ii ijj=\u2211\uf025\uf025DA \u3002GCN\u5bf9\u8282\u70b9\u7279\u5f81\n\u7684\u4f20\u64ad\u548c\u66f4\u65b0\u7684\u8fc7\u7a0b\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a  \n() ()()( )1\nnorml ll\u03c3+=\uf025 H A HW       (7)  \n\u4e0a\u8ff0\u516c\u5f0f\u8868\u793a\u7b2c l\u5c42\u7684\u8282\u70b9\u7279\u5f81()lH\u901a\u8fc7\u90bb\u5c45\u8282\n\u70b9\u7684\u4fe1\u606f\u805a\u5408\u548c\u6743\u91cd\u77e9\u9635()lW\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u518d\u7ecf\n\u8fc7\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570 \u03c3\uff0c\u5f97\u5230\u7b2c l+1\u5c42\u7684\u8282\u70b9\u7279\u5f81\n()1l+H\u3002\u5728\u7ecf\u8fc7 k\u6b21\u7279\u5f81\u4f20\u64ad\u548c\u66f4\u65b0\u540e\uff0c\u6211\u4eec\u53ef\n\u4ee5\u5f97\u5230\u6d41\u91cf\u6570\u636e\u5305\u4ea4\u4e92\u56fe\u4e2d\u7684\u8282\u70b9\u4fe1\u606f()kH\u3002\u4e3a\n\u4e86\u83b7\u5f97\u6574\u4e2a\u4ea4\u4e92\u56fe\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u6211\u4eec\u5bf9\u6240\u6709\u8282\u70b9\n\u7684\u7279\u5f81\u8fdb\u884c\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff1a \n()() GCN global_mean_poolk= ZH   (8)  \n3.3 \u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u4e0e\u68c0\u6d4b \n\u5728\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u9636\u6bb5\uff0c\u6211\u4eec\u9996\u5148\u5c06\u5e8f\u5217\u89c6\n\u89d2\u4e0b\u611f\u77e5\u5230\u7684\u6570\u636e\u5305\u957f\u5ea6\u5e8f\u5217\u7279\u5f81\u548c\u8d1f\u8f7d\u5b57\u8282\u5e8f\n\u5217\u7279\u5f81\u8fdb\u884c\u62fc\u63a5\uff0c \u5e76\u5c06\u62fc\u63a5\u540e\u7684\u7279\u5f81\u8f93\u5165\u5230\u5168\u8fde\n\u63a5\u5c42\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u62df\u5408\u6570\u636e\u5206\u5e03\u3002\u540c\u65f6\uff0c\u501f\u52a9Dropout\u4ee5\u51cf\u5c11\u6a21\u578b\u8fc7\u62df\u5408\u98ce\u9669\uff1a  \n( ) ( ) ( ) SEQ CNN LSTM Linear Dropout Linear= \uf050 Z ZZ (9) \n\u5176\u4e2d\uf050\u8868\u793a\u62fc\u63a5\u64cd\u4f5c\u3002\u5728\u5bf9\u5e8f\u5217\u89c6\u89d2\u4e0b\u611f\u77e5\u7684\u7279\n\u5f81\u8fdb\u884c\u878d\u5408\u540e\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u4ea4\u4e92\u89c6\u89d2\u4e0b\u611f\u77e5\u5230\n\u7684\u6570\u636e\u5305\u4ea4\u4e92\u7279\u5f81\u4e0e\u5e8f\u5217\u7279\u5f81\u8fdb\u884c\u878d\u5408\uff0c\u5f97\u5230\u7f51\n\u7edc\u6d41\u91cf\u6700\u7ec8\u7684\u7279\u5f81\u8868\u793a\uff1a  \n()GCN SEQ 1 \u03b1\u03b1=\u22c5 +\u2212ZZ Z    (10)  \n\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u6765\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\uff0c\u5e76\u4f7f\n\u7528Softmax\u51fd\u6570\u5bf9\u76ee\u6807\u6d41\u91cf\u8fdb\u884c\u5206\u7c7b\uff1a  \n() Softmaxi=YZ         (11)  \n\u5728\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4ea4\u53c9\u71b5\u635f\n\u5931\u51fd\u6570\u4f5c\u4e3a\u6211\u4eec\u7684\u8bad\u7ec3\u635f\u5931\u51fd\u6570\uff0c\u6a21\u578b\u901a\u8fc7\u6700\u5c0f\n\u5316\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u5bf9\u5176\u53c2\u6570\u7684\u4f18\u5316\uff0c\u635f\u5931\u51fd\u6570\u516c\n\u5f0f\u5982\u4e0b\uff1a  \n() ,,\n111\u02c6 logNK\nik ik\nikL yyN= ==\u2212\u2211\u2211     (12)  \n\u5176\u4e2d\uff0cN \u8868\u793a\u603b\u6837\u672c\u7684\u6570\u91cf\uff0cK \u8868\u793a\u6837\u672c\u4e2d\u7684\u7c7b\n\u522b\u6570\uff0c,iky\u662f\u7b2c i\u4e2a\u6837\u672c\u5728\u7b2c k\u7c7b\u6807\u7b7e\u7684\u771f\u5b9e\u503c\uff0c\n,\u02c6iky\u662f\u6a21\u578b\u9884\u6d4b\u7684\u7b2c i\u4e2a\u6837\u672c\u5c5e\u4e8e\u7b2c k\u7c7b\u7684\u6982\u7387\u3002 \n4  \u5b9e\u9a8c\u7ed3\u679c\u5206\u6790  \n\u4e3a\u4e86\u8bc4\u4f30\u672c\u6587\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u5728 \u516d\n\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u68c0\n\u6d4b\u5b9e\u9a8c\u3002\u9996\u5148\uff0c\u5728 4.1\u8282\u548c4.2\u8282\u4e2d\u5bf9\u5b9e\u9a8c\u4e2d\u7528\u5230\n\u7684\u6570\u636e\u96c6\u548c\u5bf9\u6bd4\u65b9\u6cd5\u8fdb\u884c\u7b80\u8981\u63cf\u8ff0\uff0c\u5728 4.3\u8282\u4e2d\n\u8ba8\u8bba\u4e86\u8be6\u7ec6\u7684\u5b9e\u9a8c\u53c2\u6570\u8bbe\u7f6e\uff0c\u5728\u7b2c 4.4\u8282\u4e2d\u7ed9\u51fa\n\u4e86\u6a21\u578b\u7684\u8bc4\u4ef7\u6307\u6807\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5728 4.5\u8282\u4e2d\u5c55\u793a\n\u4e86\u6a21\u578b\u68c0\u6d4b\u7684\u7cbe\u5ea6\u4ee5\u53ca\u591a\u5206\u7c7b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4e0e\u5176\n\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002\u4e3a\u5206\u6790\u548c\u8ba8\u8bba\u4e0d\u540c\u89c6\u89d2\u5bf9\n\u4e8e\u5b9e\u9a8c\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u5728 4.6\u8282\u4e2d\u5bf9\u63d0\u51fa\u7684\u65b9\n\u6cd5\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\u3002\u540c\u65f6\uff0c\u5728 4.7\u8282\u4e2d\u901a\u8fc7\u4fee\u6539\n\u6240\u4f7f\u7528\u6570\u636e\u5305\u7684\u6570\u91cf n\u548c\u8d1f\u8f7d\u5b57\u8282\u6570 m\u4e24\u79cd\u53c2\n\u6570\uff0c\u4ee5\u53ca\u4e0d\u540c\u89c6\u89d2\u6a21\u578b\u878d\u5408\u7684\u6743\u91cd \u03b1\uff0c\u6765\u8bc4\u4f30\u4e0d\n\u540c\u53c2\u6570\u7ec4\u5408\u5bf9\u6a21\u578b\u68c0\u6d4b\u7684\u5f71\u54cd\u3002  \n4.1 \u6570\u636e\u96c6\u6982\u8ff0 \n\u4e3a\u9a8c\u8bc1\u6240\u63d0\u65b9\u6cd5\u5728\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\u6d4b\u4e0a\u7684\u6709\n\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u6211\u4eec\u5728\u516d \u4e2a\u516c\u5f00\u7684\u771f\u5b9e\u7f51\u7edc\u6d41\u91cf\n\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002  \nCIC-IOMT2024[25] \uff1a\u8be5\u6570\u636e\u96c6\u7531\u52a0\u62ff\u5927\u7f51\u7edc\n\u5b89\u5168\u7814\u7a76\u6240\u53d1\u5e03\uff0c\u7528\u4e8e\u7ef4\u62a4\u533b\u7597\u7269\u8054\u7f51\uff08 IoMT\uff09\n\u9886\u57df\u5b89\u5168\u3002\u5b83\u57fa\u4e8e\u4e00\u4e2a\u7531 40\u53f0IoMT\u8bbe\u5907\uff08 25\n\u53f0\u771f\u5b9e\u8bbe\u5907\u548c 15\u53f0\u6a21\u62df\u8bbe\u5907\uff09\u7ec4\u6210\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\n\u5b9e\u65bd\u4e86 5\u7c7b\u5171 18\u79cd\u653b\u51fb\uff0c\u5168\u65b9\u4f4d\u8986\u76d6\u4e86\u533b\u7597\u9886\u57df 6                                                    \u6307   \u6325  \u4e0e  \u63a7  \u5236  \u5b66  \u62a5                                     \u7b2c   \u5377  \n\u4e2d\u5e38\u89c1\u7684\u901a\u4fe1\u534f\u8bae\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e Wi-Fi\u3001MQTT\n\u534f\u8bae\u53ca\u84dd\u7259\u6280\u672f\u3002\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u8fd0\u7528\u7f51\u7edc\u55c5\u63a2\n\u6280\u672f\uff0c\u5728\u4ea4\u6362\u673a\u4e0e\u5404 IoMT\u8bbe\u5907\u4e4b\u95f4\u6355\u83b7\u91cf\u6d41\u91cf\n\u6570\u636e\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u4e14\u771f\u5b9e\u7684\u6570\u636e\u8d44\u6e90\u3002  \nUNSW -NB15 [26-30]\uff1a\u65b0\u5357\u5a01\u5c14\u58eb\u5927\u5b66\u582a\u57f9\n\u62c9\u5206\u6821\u7684 Cyber Range \u5b9e\u9a8c\u5ba4\u4e8e 2015\u5e74\u4f7f\u7528\nIXIA PerfectStorm \u5de5\u5177\u751f\u6210\u7684\u539f\u59cb\u7f51\u7edc\u6570\u636e\u5305\uff0c\n\u5305\u542b\u771f\u5b9e\u7684\u7f51\u7edc\u6b63\u5e38\u6d3b\u52a8\u548c\u653b\u51fb\u884c\u4e3a\u3002\u8be5\u6570\u636e\u96c6\n\u5305\u542b 9\u79cd\u653b\u51fb\u7c7b\u578b\u3002UNSW -NB15\u6570\u636e\u96c6\u7684\u5f02\u5e38\n\u884c\u4e3a\u66f4\u52a0\u65b0\u9896\u548c\u5747\u8861\uff0c\u9002\u5408\u7528\u4e8e\u7f51\u7edc\u5f02\u5e38\u6d41\u91cf\u68c0\n\u6d4b\u7814\u7a76\u3002  \nDarknet2020[ 31]\uff1a\u8be5\u6570\u636e\u96c6\u7531\u4e24\u4e2a\u516c\u5f00\u7684\u52a0\n\u5bc6\u6570\u636e\u96c6\uff08ISCX -VPN 2016\u548cISCX -Tor2016\uff09\u5408\n\u5e76\u800c\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u6697\u7f51\u6d41\u91cf\u6570\u636e\u96c6\u3002\u5176\u4e2d\u52a0\u5bc6\u6d41\n\u91cf\u5206\u522b\u6db5\u76d6 Tor\u548cVPN\u4e24\u7c7b\u4e0d\u540c\u52a0\u5bc6\u901a\u4fe1 \u65b9\u5f0f\uff0c\n\u5305\u542b 6\u79cd\u4e0d\u540c\u7684\u52a0\u5bc6\u5e94\u7528\u7a0b\u5e8f\u6d41\u91cf \u3002\u6697\u7f51\u6d41\u91cf\u5206\n\u7c7b\u5bf9\u4e8e\u5b9e\u65f6\u5e94\u7528\u7a0b\u5e8f\u7684\u5206\u7c7b\u975e\u5e38\u91cd\u8981\u3002\u5206\u6790\u6697\u7f51\n\u6d41\u91cf\u6709\u52a9\u4e8e\u5728\u6076\u610f\u8f6f\u4ef6\u8086\u8650\u524d\u5bf9\u5176\u8fdb\u884c\u65e9\u671f\u76d1\n\u63a7\uff0c\u5e76\u5728\u7206\u53d1\u540e\u5bf9\u6076\u610f\u6d3b\u52a8\u8fdb\u884c\u68c0\u6d4b\u3002  \nISCX -VPN 2016[ 32]\uff1a\u8be5\u6570\u636e\u96c6\u5305\u542b\u901a\u8fc7\u865a\u62df\n\u4e13\u7528\u7f51\u7edc\uff08 VPN\uff09\u96a7\u9053\u4f20\u8f93\u7684\u52a0\u5bc6\u901a\u4fe1\u6d41\u91cf\uff0c\u6db5\n\u76d6\u7f51\u7edc\u7535\u8bdd\u3001\u7535\u5b50 \u90ae\u4ef6\u7b49 5\u79cd\u4e0d\u540c\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\nVPN\u901a\u5e38\u7528\u4e8e\u89c4\u907f\u5ba1\u67e5 \u6216\u901a\u8fc7\u534f\u8bae\u6df7\u6dc6\u9690\u85cf\u4f4d\n\u7f6e\uff0c\u5e38\u7528\u4e8e\u8bbf\u95ee\u88ab\u5c01\u9501\u7684\u7f51\u7ad9\u6216\u670d\u52a1 \uff0c\u56e0\u6b64 VPN\n\u6d41\u91cf\u7684\u8bc6\u522b\u4e0e\u68c0\u6d4b\u4e5f\u662f\u4e00\u9879\u6781\u5177\u6311\u6218\u6027\u7684\u5de5\u4f5c \u3002 \nCIC-IoT2023 [33]\uff1a\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u7269\u8054\u7f51\u73af\n\u5883\u4e2d\u5927\u89c4\u6a21\u653b\u51fb\u7684\u5b9e\u65f6\u6570\u636e\u3002\u52a0\u62ff\u5927\u7f51\u7edc\u5b89\u5168\u7814\n\u7a76\u4e2d\u5fc3\u5728\u4e00\u4e2a\u7531 105\u53f0\u771f\u5b9e\u7269\u8054\u7f51\u8bbe\u5907\u7ec4\u6210\u7684\u590d\n\u6742\u62d3\u6251\u4e2d\u6267\u884c\u4e86\u4e03\u5927\u7c7b\u653b\u51fb\uff1a\u5206\u5e03\u5f0f\u62d2\u7edd\u670d\u52a1(DDoS)\u3001\u62d2\u7edd\u670d\u52a1(DoS) \u3001\u4fa6\u5bdf\u3001\u57fa\u4e8e Web\u7684\u653b\n\u51fb\u3001\u66b4\u529b\u7834\u89e3\u3001\u6b3a\u9a97\u4ee5\u53ca Mirai\u653b\u51fb\u3002\u8fd9\u4e9b\u653b\u51fb\n\u5747\u7531\u6076\u610f\u7269\u8054\u7f51\u8bbe\u5907\u53d1\u8d77\uff0c\u5e76\u9488\u5bf9\u7f51\u7edc\u4e2d\u7684\u5176\u4ed6\n\u7269\u8054\u7f51\u8bbe\u5907\u5b9e\u65bd\u3002  \nUSTC -TFC2016[ 34]\uff1a\u8be5\u6570\u636e\u96c6\u7531\u4e2d\u56fd\u79d1\u5b66\u6280\n\u672f\u5927\u5b66\u56e2\u961f\u53d1\u5e03\uff0c\u5305\u542b\u7531\u6076\u610f\u8f6f\u4ef6\u548c\u826f\u6027\u5e94\u7528\u7a0b\n\u5e8f\u7ec4\u6210\u7684\u52a0\u5bc6\u6d41\u91cf\u3002\u5176\u4e2d\u6076\u610f\u6d41\u91cf\u4e0e Cridex\u3001\nGeodo\u3001Htbot\u7b4910\u79cd\u6076\u610f\u75c5\u6bd2\u6216\u6728\u9a6c\u76f8\u5173 \u3002 \n4.2 \u5b9e\u9a8c\u5bf9\u6bd4\u65b9\u6cd5 \n\u6211\u4eec\u5c06 MuFF\u4e0e\u4ee5\u4e0b\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u4ee5\u8bc4\u4f30\u5176\n\u6d41\u91cf\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002  \nGraphSAGE[ 35]\uff1a\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u91c7\u6837\u7b56\u7565\u6765\n\u9650\u5236\u6bcf\u5c42\u805a\u5408\u90bb\u5c45\u4fe1\u606f\u7684\u6570\u91cf\uff0c\u5e76 \u652f\u6301\u591a\u79cd\u805a\u5408\n\u64cd\u4f5c\u5982\u5e73\u5747\u3001\u6700\u5927\u6c60\u5316\u7b49 \uff0c\u7075\u6d3b\u5730\u6c47\u603b\u90bb\u5c45\u8282\u70b9\n\u7684\u7279\u5f81\uff0c\u751f\u6210\u8282\u70b9\u7684\u5d4c\u5165\u5411\u91cf\uff0c\u8fd9\u4e9b\u5411\u91cf\u5145\u5206\u7f16\n\u7801\u4e86\u8282\u70b9\u7684\u672c\u5730\u7ed3\u6784\u7279\u5f81\u53ca\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002  \nGIN[ 36]\uff1a\u56fe\u540c\u8d28\u5316\u7f51\u7edc \u901a\u8fc7\u7075\u6d3b\u7684\u805a\u5408\u51fd\u6570\uff0c\u5c06\u6d41\u91cf\u4ea4\u4e92\u56fe\u4e2d\u7684\u90bb\u5c45\u8282\u70b9\u4fe1\u606f\u4e0e\u4e2d\u5fc3\u8282\u70b9\n\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u6743\u91cd\u6765\u7ebf\u6027\u6216\u975e\n\u7ebf\u6027\u5730\u7d2f\u52a0\u90bb\u5c45\u7279\u5f81\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u56fe\u540c\u6784\u4e0d\u53d8\u6027\u7684\u540c\u65f6\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002  \nGAT[ 37]\uff1a\u56fe\u6ce8\u610f\u529b\u7f51\u7edc \u5c06\u6ce8\u610f\u529b\u673a\u5236\u878d\u5165\n\u5230\u56fe\u6570\u636e\u7684\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u52a8\u6001\u5730\u8d4b\u4e88\u56fe\u4e2d\u76f8\n\u90bb\u8282\u70b9\u4e0d\u540c\u7684\u6743\u91cd\u3002\u6bcf\u4e2a\u8282\u70b9\u7684\u7279\u5f81\u8868\u793a\u4e0d\u4ec5\u4f9d\n\u8d56\u4e8e\u5176\u90bb\u5c45\u8282\u70b9\u7684\u7279\u5f81\uff0c\u8fd8\u53d6\u51b3\u4e8e\u4e00\u4e2a\u81ea\u9002\u5e94\u5b66\n\u4e60\u7684\u6ce8\u610f\u529b\u7cfb\u6570\uff0c\u8fd9\u4e2a\u7cfb\u6570\u80fd\u591f\u53cd\u6620\u5f53\u524d\u8282\u70b9\u4e0e\n\u5176\u5404\u90bb\u5c45\u8282\u70b9\u95f4\u5173\u7cfb\u7684\u91cd\u8981\u6027\u3002  \nMuFF -CNN\uff1a\u8be5\u65b9\u6cd5\u4e3a MuFF\u5355\u65f6\u5e8f\u89c6\u89d2\u7684\n\u53d8\u4f53\u65b9\u6cd5\uff0c \u53ea\u5229\u7528 CNN\u6765\u5b66\u4e60\u7f51\u7edc\u6d41\u91cf\u7684\u8d1f\u8f7d\n\u5e8f\u5217\u4e2d\u6240\u5177\u6709\u7684\u7279\u5b9a\u6a21\u5f0f\u3002  \nMuFF -LSTM\uff1a\u8be5\u65b9\u6cd5\u4e3a MuFF\u5355\u65f6\u5e8f\u89c6\u89d2\u7684\n\u53e6\u4e00\u79cd\u53d8\u4f53\u65b9\u6cd5 \uff0c\u53ea\u5229\u7528 LSTM\u6355\u6349\u6570\u636e\u5305\u957f\u5ea6\n\u968f\u65f6\u95f4\u53d8\u5316\u7684\u957f\u671f\u4f9d\u8d56\u6027\u548c\u5468\u671f\u6027\u7279\u5f81\u3002  \nMuFF -GCN\uff1a\u8be5\u65b9\u6cd5\u4e3a MuFF\u5355\u4ea4\u4e92\u89c6\u89d2\u7684\n\u53d8\u4f53\u65b9\u6cd5\uff0c\u53ea\u5229\u7528 GCN\u5728\u6d41\u91cf\u4ea4\u4e92\u56fe\u7684\u8282\u70b9\u7279\n\u5f81\u95f4\u4f20\u64ad\u548c\u53d8\u6362\u4fe1\u606f\uff0c \u6700\u7ec8\u5f97\u5230\u6574\u4e2a\u56fe\u7684\u5d4c\u5165\u8868\n\u793a\u6765\u5b66\u4e60 \u6d41\u91cf\u4ea4\u4e92\u56fe\u7684\u7279\u5f81\u8868\u793a \u3002 \nE-GraphSAGE[ 38]\uff1a\u5c06\u7f51\u7edc\u4e2d\u7684 \u901a\u4fe1\u7aef\u70b9\n\uff08\u7531\nIP\u5730\u5740\u548c\u7aef\u53e3\u53f7 \u8868\u793a\uff09\u6620\u5c04\u4e3a\u56fe\u8282\u70b9\uff0c\u7f51\u7edc\n\u6d41\u91cf\u6620\u5c04\u4e3a\u8fb9\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e GraphSAGE \u8fdb\u884c\u6269\n\u5c55\uff0c\u5176\u6d88\u606f\u4f20\u9012\u51fd\u6570\u4e0d\u4ec5\u8003\u8651\u8282\u70b9\u7279\u5f81\uff0c\u8fd8\u8003\u8651\u8fb9\u7279\u5f81\u3002\u901a\u8fc7\u91c7\u6837\u548c\u805a\u5408\u56fe\u4e2d\u7684\u8fb9\u4fe1\u606f\u6765 \u751f\u6210\u8fb9\n\u5d4c\u5165\uff0c\u8fdb\u800c\u5b9e\u73b0\u5bf9\u6076\u610f\u7f51\u7edc\u6d41\u91cf\u7684\u6709\u6548\u68c0\u6d4b\u3002  \nGraphDDoS[ 39]\uff1a\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u7f51\u7edc\u6d41\u91cf\u8f6c\n\u6362\u4e3a\u7aef\u70b9\u6d41\u91cf\u56fe\u6765\u6355\u6349\u6570\u636e\u5305\u4e4b\u95f4\u7684\u5173\u7cfb\uff08\u5355\u6d41\u7ed3\u6784\uff09\u548c\u591a\u4e2a\u6d41\u4e4b\u95f4\u7684\u5173\u7cfb \uff0c\u501f\u52a9\u57fa\u4e8e GNN\u7684\n\u6d88\u606f\u4f20\u9012\u7f51\u7edc\u8fdb\u884c\u8282\u70b9\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u805a\u5408\u8282\n\u70b9\u7279\u5f81\u83b7\u5f97\u6574\u4e2a\u56fe\u7684\u8868\u793a\uff0c \u4ee5\u533a\u5206\u7f51\u7edc\u4e2d\n\u7684DDoS\u653b\u51fb\u6d41\u91cf\u548c\u6b63\u5e38\u6d41\u91cf\u3002  \n4.3 \u5b9e\u9a8c\u8d85\u53c2\u6570\u8bbe\u7f6e \n\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u57fa\u4e8e PyTorch \u53ca\u5176\u5bf9\u5e94\u7684\nPyTorch Geometric \u5e93\u5b9e\u73b0\u3002\u5728\u4ea4\u4e92\u7279\u5f81\u63d0\u53d6\u4e2d\uff0c\n\u6211\u4eec\u4f7f\u7528\u4e86\u4e09\u5c42 GCN\u6a21\u578b\uff0c\u5e76\u5c06\u9690\u5c42\u7ef4\u5ea6\u8bbe\u7f6e\n\u4e3a512\u7ef4\uff0c\u5bf9\u5e94\u4e8e\u56fe\u5d4c\u5165\u7684\u7ef4\u5ea6\u3002\u540c\u65f6\uff0c\u4e3a\u4e86\u4f18\n\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u9632\u6b62\u8fc7\u62df\u5408\u95ee\u9898\u7684\u51fa\u73b0\uff0c\u6211\u4eec\u5728\u4e09\u5c42\u56fe\u5377\u79ef\u5c42"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "## Summary of the Paper: \"Learning Causally Invariant Reward Functions from Diverse Demonstrations\"\n\n### 1. Abstract\nThe paper introduces a novel regularization approach for inverse reinforcement learning (IRL) that leverages the causal invariance principle. This method aims to improve reward function generalization by avoiding spurious correlations arising from heterogeneous expert demonstrations. The paper demonstrates the superior policy performance when using these causally invariant reward functions, particularly in transfer settings.\n\n### 2. Introduction\n- **Context**: This work addresses the challenge of designing reward functions in reinforcement learning, which are crucial for shaping agent behaviors.\n- **Research Question**: The primary question is how to infer robust reward functions from expert demonstrations that are invariant under different environmental dynamics.\n- **Literature Review**: The paper reviews the importance of correctly inferring reward functions (Ng et al., 2000) and discusses the challenges of IRL like scarcity of expert data and distribution shift. It refers to methods like Maximum Entropy IRL (Ziebart et al., 2008) and the pitfalls of overfitting in neural network-based models (Ying, 2019).\n- **Positioning**: This study builds on causal models (Peters et al., 2015; Heinze-Deml et al., 2017) and proposes to adopt causal invariance for IRL, which is novel in this context.\n\n### 3. Related Work\n- **IRL Methods**: It reviews standard IRL methods, such as Maximum Entropy IRL and its variants for large-scale problems which utilize neural network approximations.\n- **Overfitting Issues**: The paper explores works tackling overfitting in IRL, particularly with high-dimensional models.\n- **Causal Invariance**: Discussion on causality in machine learning, focusing on invariant risk minimization (IRM) (Arjovsky et al., 2019) and its benefits in generalization.\n- **Novelty**: The core advancement of this paper is applying causal invariance to IRL to develop reward functions that generalize well across different environments and expert data sources.\n\n### 4. Method\n**Proposed Approach**: \n1. **Problem Setting**: The proposed method starts with defining a Markov decision process and setting up the IRL problem.\n2. **Spurious Correlations**: Highlight the issue of spurious correlations in standard IRL setups, especially in heterogeneous datasets from diverse experts.\n3. **Causal Invariance Principle**: Establishes a framework to ensure reward functions are invariant across different expert settings.\n4. **Mathematical Formulation**: Detailed derivation of the causal invariance regularization term for reward functions, drawing from established concepts in the log-partition function, exponential families, and gradient penalties.\n\n**Algorithm for Feature Matching (CI-FMIRL)**: \n- Specifies the setup and backpropagation steps needed to regularize feature networks for maximum entropy IRL.\n- Provides a concrete step-by-step algorithm.\n\n**Algorithm for Adversarial IRL (CI-AIRL)**: \n- Adapts the adversarial framework by integrating causal invariance penalties into the discriminator's objective within a GAN-like framework.\n\n### 5. Experimental Setup\n- **Datasets**: Gridworld environment with controlled expert trajectory data and MUJOCO simulated robotic environments.\n- **Hardware/Software**: Likely unspecified, but would involve standard computational setups for deep learning experiments.\n- **Evaluation Metrics**: Performance metrics include expected cumulative rewards and policy performance under perturbed environmental dynamics.\n- **Baselines**: Compares against standard MaxEnt IRL and adversarial IRL methods like GAIL and AIRL, with different regularization techniques for robust baselines.\n\n### 6. Results and Analysis\n**Gridworld Experiments**:\n- Illustrates that adding causal invariance regularization to MaxEnt IRL substantially improves the reward functions. Shows visual comparisons via heatmaps of resultant reward functions across different settings.\n\n**Adversarial Experiments on MUJOCO**:\n- Documents improved generalization abilities of policies trained on regularized rewards under various perturbations like body mass, joint range, and control range changes.\n- Provides detailed graphs showing performance metrics for different dynamics perturbations in comparison to baseline methods.\n\n### 7. Discussion\n- **Interpretation**: Confirms the hypothesis that causal invariance regularization improves the ability of rewards to generalize.\n- **Implications**: Enhances understanding of how diverse expert demonstrations can be harnessed to create robust reward signals. Highlights the effectiveness of using causal principles in reinforcement learning.\n- **Limitations**: Addresses potential scalability issues and the need for partitioning diverse data settings, which might limit practical applications in some scenarios.\n\n### 8. Conclusion\n- **Summary**: Reinforces the paper\u2019s contributions in developing a causally invariant regularization approach for IRL. Demonstrates substantial empirical performance gains.\n- **Future Research**: Suggests exploring broader causality frameworks in reinforcement learning and integrating more sophisticated causal discovery methods to further refine reward function learning.\n\nThis comprehensive summary captures the technical essence of the paper, ensuring that readers can understand the methodology, reconstruct the experimental setups, and interpret the results effectively to build upon this research."}, {"role": "assistant", "content": "LEARNING CAUSALLY INVARIANT REWARD FUNCTIONS FROM\nDIVERSE DEMONSTRATIONS\nIvan Ovinnikov\nDepartment of Computer Science\nETH Z\u00fcrich\nivan.ovinnikov@inf.ethz.chEugene Bykovets\nDepartment of Computer Science\nETH Z\u00fcrich\neugene.bykovets@inf.ethz.chJoachim M. Buhmann\nDepartment of Computer Science\nETH Z\u00fcrich\njbuhmann@inf.ethz.ch\nABSTRACT\nInverse reinforcement learning methods aim to retrieve the reward function of a Markov decision\nprocess based on a dataset of expert demonstrations. The commonplace scarcity and heterogeneous\nsources of such demonstrations can lead to the absorption of spurious correlations in the data by\nthe learned reward function. Consequently, this adaptation often exhibits behavioural overfitting to\nthe expert data set when a policy is trained on the obtained reward function under distribution shift\nof the environment dynamics. In this work, we explore a novel regularization approach for inverse\nreinforcement learning methods based on the causal invariance principle with the goal of improved\nreward function generalization. By applying this regularization to both exact and approximate\nformulations of the learning task, we demonstrate superior policy performance when trained using\nthe recovered reward functions in a transfer setting\n1 Introduction\nIn the domain of reinforcement learning, the formulation of a suitable reward function plays a pivotal role in shaping\nthe behaviour of decision making agents. This is commonly justified by the widely adopted belief that the reward\nfunction is a succinct representation of a task goal in a given environment specified as a Markov decision process (MDP)\n(Ng et al., 2000). Eliciting the correct behavioural policies via the optimization of a reward function is of paramount\nimportance for the deployment of RL agents to real world domains such as various robotics scenarios (Pomerleau, 1991;\nBillard et al., 2008) or expert behaviour forecasting (Kitani et al., 2012). However, the challenge of designing such a\nfunction typically entails a cumbersome and error-prone process of handcrafting a heuristic reward signal that accounts\nfor all the intricacies of the task at hand.\nInverse reinforcement learning (IRL) methods aim to solve the problem of inferring the reward function of an MDP\nbased on a dataset of temporal behaviours. These trajectories are typically obtained from an agent that is assumed to\ndemonstrate near-optimal performance in the respective MDP. There are multiple benefits to learning an explicit reward\nfunction compared to alternatives such as behavioural cloning (Pomerleau, 1991) or other imitation approaches (Ho\nand Ermon, 2016) including the ability to transfer the reward function across problems and enhanced robustness to\ncompounding errors (Swamy et al., 2021).\nThe IRL problem is challenging due to a number of factors. The problem of recovering the reward from the statistics of\nexpert trajectories is generally ill-posed, as there typically exist many reward functions which satisfy the optimization\nconstraints (Ng et al., 1999). While this property is effectively tackled by regularization in the form of a maximum\nentropy objective (Ziebart et al., 2008, 2010), scaling up IRL to handle large-scale problems remains a challenge. In\nparticular, it requires a variational formulation dependent on non-linear function approximation methods (Finn et al.,\n2016a; Fu et al., 2017). Since the amount of available expert data is typically limited, this can lead to overfitting\nphenomena, which are particularly pronounced in highly parameterized models such as neural networks (Ying, 2019;\nSong et al., 2019). An additional difficulty arises when there is a significant discrepancy between expert demonstrations\noriginating from different experts. We show that pooling expert demonstrations in one dataset under the same label\nintroduces spurious correlations which are absorbed in the representation of the reward function. By optimizing the\nexpected cumulative reward defined by such functions, the agent might fail to learn meaningful behaviours due to\noptimization of the spurious correlations present in the reward model.arXiv:2409.08012v1  [cs.LG]  12 Sep 2024In order to circumvent this issue, we consider causal properties of the reward learning problem. The causal invariance\nprinciple (Peters et al., 2015; Heinze-Deml et al., 2017; Arjovsky et al., 2019) studies the generalization problem of\nsupervised learning models through the lens of causality. It postulates that the conditional distribution of the target\nvariable must be asymptotically stable across samples obtained under observational and interventional settings of the\ndata generating process. By only considering causally invariant representations of the input, this ensures avoiding the\nreliance on spurious correlations in the model predictions. We propose to adapt this principle for the context of reward\nfunction learning, where we aim to elicit behavioural policies without exploiting spurious reward features, i.e. features\nwhich would prevent the reward from providing a meaningful training signal under distribution shift. To achieve this\ngoal, we make the assumption that variations in expert demonstrations are a product of causal interventions on the data\ngenerating process of trajectories. Under this assumption, the conditional distribution describing the optimality of a\ntransition must be stable for experts demonstrations gathered on a specific task. By applying the causal invariance\nprinciple under this assumption, we show that we can recover reward functions which are invariant across a population\nof experts and demonstrate improve generalized w.r.t. certain types of distribution shift.\nContributions. Our contributions are as follows: (i) we formulate the assumption that the variations between experts\nperforming considered to perform near-optimally on the same task can be seen as interventional settings of the\nunderlying trajectory distribution and (ii) propose a regularization principle for inverse reinforcement learning methods\nbased on the principle of causal invariance. This modelling choice allows us to learn reward functions that are invariant\nto spurious correlations between the transitions and optimality label present in the expert data. We (iii) demonstrate\nthe efficacy of this approach in both tractable, finite state-spaces, which we refer to as the exact setting as well as\nlarge continuous state-spaces, which we denote as the approximate setting. In the first setting, we visually analyze\nthe recovered reward functions and verify their invariance properties w.r.t. the input data. In the second setting, we\ndemonstrate improved ground truth performance when a policy is trained using the regularized reward in MDPs with\nperturbed dynamics.\n2 Method\nIn this section, we describe our method. Section 2.1 presents the problem setting of learning rewards in the maximum\nentropy IRL setting (Ziebart et al., 2008) in both primal and dual form and reviews the connection to a class of\nadversarial optimization methods based on distribution matching. In section 2.2, we outline how spurious correlations\narise in the context of the IRL problem and connect this to the causal invariance principle. Section 2.3 shows how to\nincorporate this principle as a regularization strategy for reward learning.\n2.1 Problem setting\nWe begin by giving an overview of the problem setting. We consider environments modelled by a Markov decision\nprocess (S,A,T, \u00b50, R, \u03b3 ), where Sis the state space, Ais the action space, Tis the family of transition distributions\nonSindexed by S \u00d7 A withp(s\u2032|s, a)describing the probability of transitioning to state s\u2032when taking action ain\nstates,\u00b50is the initial state distribution, R:S \u00d7 A \u2192 Ris the reward function and \u03b3\u2208(0,1)is the discount factor.\nA policy \u03c0:S \u00d7 A \u2192 \u2126(A)1is a map from states s\u2208 S to distributions \u03c0(\u00b7|s)over actions, with \u03c0(a|s)being the\nprobability of taking action ain state s.\nIn absence of a given ground truth reward function, inverse reinforcement learning methods aim to estimate a suitable\nreward function based on a dataset of expert trajectories DE={\u03bei}i\u2264Kwhere \u03bei= (s(i)\n1:Ti, a(i)\n1:Ti)is a sequence of\nstates and actions of expert iof length Ti. To achieve this goal, a number of methods based on distribution matching\nmay be used. Such methods typically minimize a divergence measure (Csisz\u00e1r, 1972) between the expert trajectories\nand the trajectories induced by a policy optimizing the estimated reward.\nWe begin by presenting maximum entropy IRL (MaxEntIRL) (Ziebart et al., 2008), which serves as a foundation for\nmost of these methods. In MaxEntIRL, a feature matching approach is used to learn a reward function r\u03c8,\u03c6=\u03c8T\u03c6(s),\nwhere the state features \u03c6(s)may be specified a priori or learned using a using a neural network model (Wulfmeier\net al., 2015). The policy is trained by optimizing the expected cumulative reward using the reward function estimate.\nThe model describes the fact that the expert trajectories are sampled from a Gibbs distribution defined over trajectories:\np(\u03be|\u03c8, \u03c6) =1\nZ\u03c6,\u03c8exp(r\u03c8,\u03c6(\u03be))) (1)\nwhich corresponds to the solution of the entropy maximization problem of the trajectory distribution under feature\nmatching and normalization constraints.\n1\u2126(A)denotes the set of probability measures over the action space A\n2In the more general case of stochastic dynamics p(st+1|st, at)and random initial state distribution \u00b50, we can define\nthegenerative model of trajectories p(\u03be|O1:T)conditioned on the optimality variables {Ot}t=1:Tas follows (Levine,\n2018):\np(\u03be|O1:T)\u221dp(\u03be, O 1:T) =\u00b50(s0)TY\nt=1p(Ot= 1|st, at)p(st+1|st, at) (2)\nwhere the conditional distribution p(Ot= 1|st, at)\u221dexp(r\u03c8,\u03c6(st, at))encodes the optimality of a single timestep\nof the trajectory, i.e. Ot= 1corresponds to an expert-level transition. The optimal reward weight solution \u03c8\u2217can\nbe obtained by maximizing the likelihood of eq. (2) w.r.t. the parameter \u03c8. We state the primal maximum-likelihood\nobjective (Ziebart et al., 2008):\nmax\n\u03c8E\u03be\u223cDE\u0014\nlog1\nZ(\u03c8)e\u03c8T\u03c6(\u03be)\u0015\n=max\n\u03c8E\u03be\u223cDE[\u03c8T\u03c6(\u03be)\u2212logZ(\u03c8)] (3)\nVariational dual formulation. Due to the difficulties of computing the log-partition function in high-dimensional\nspaces, a dual formulation of the maximum likelihood problem is derived. The Gibbs distribution over trajectories\np(\u03be|\u03c8, \u03c6)obtained via the maximum entropy principle belongs to the exponential family of distributions:\np(\u03be|\u03c8, \u03c6) =p0(\u03be) exp( \u03c8T\u03c6(\u03be)\u2212A(\u03c8)) (4)\nwhere A(\u03c8) = log Z\u03c8= logR\n\u039eexp(\u03c8T\u03c6(\u03be))p0(\u03be)d\u03beis the log-partition function defined over the space of trajectories\n\u039eandp0is the base measure. Leveraging the strict concavity of the log-partition function A(\u03c8)in\u03c8(Wainwright et al.,\n2008), the Fenchel-Legendre dual (Rockafellar, 2015) of A(\u03c8)is given by\nA(\u03c8) =max\n\u03c6\u2208\u03a6\u27e8\u03c8, \u03c6(\u03be)\u27e9 \u2212DKL(p(\u03be|\u03c8, \u03c6)||p0(\u03be)) = max\nq\u2208P\u27e8q(\u03be), g\u03c8,\u03c6(\u03be)\u27e9 \u2212DKL(q(\u03be)||p0(\u03be)) (5)\nHere, we use the generalization of the marginal polytope \u03a6over first-order statistics \u03c6(\u03be)to the space of sampling\ndistributions with bounded L2-norm P(Dai et al., 2019) and g\u03c8,\u03c6(\u03be) =\u03c8T\u03c6(\u03be).q(\u03be)describes a trajectory sampling\ndistribution. In our case, the base measure p0corresponds to the Lebesgue or count measure according to the continuity\nof the state space, i.e. p0(\u03be) = 1 . Thus, DKL(q||p0)simplifies to the negative entropy of the sampling distribution\nH(q). By plugging in the resulting dual of the log-partition in eq. (5) into the maximum likelihood objective in eq. (3),\nwe obtain the dual saddle-point objective:\nmax\n\u03c8,\u03c6min\nq\u2208PE\u03be\u223cDE[g\u03c8,\u03c6(\u03be)]\u2212E\u03be\u223cq[g\u03c8,\u03c6(\u03be)]\u2212 H(q) (6)\nConnection to f-divergences. The resulting dual problem is closely related to f-divergence (Csisz\u00e1r, 1972) minimiza-\ntion which allows us to obtain a numerical solution strategy for eq. (6). It is well known that the maximum-likelihood\nproblem (eq. (3)) is asymptotically equivalent to the minimization of the KL-divergence DKL(p(\u03be|\u03b8\u2217)||p(\u03be|\u03b8))(Ander-\nsen, 1970), where \u03b8\u2217is the parameter vector maximizing the likelihood of p(x|\u03b8\u2217). The variational formulation of the\nmore general f-divergence minimization problem, of which DKLis an instance, is given by:\nDf(p||q) =Eq\u0014\nf\u0012p\nq\u0013\u0015\n\u2265sup\ng:X\u2192REp[g(\u03be)]\u2212Eq[f\u2217(g(\u03be))] (7)\nwhere fis a convex function and f\u2217denotes its Fenchel conjugate. In particular, for f(x) =1\n2|x\u22121|, we obtain the\ntotal variation distance DTV(P, Q):\nDTV(p||q) = sup\n||g||\u221e\u22641Ep[g(\u03be)]\u2212Eq[g(\u03be)] (8)\nwhich is equivalent to eq. (6) for a restricted class of functions g\u2208 {g:X \u2192 R,||g||\u221e\u22641}and an entropy\nregularization term for the sampling distribution q.\nIn order to perform the minimization of the f-divergence objective in eq. (7), we can leverage a correspondence between\noptimal risk functions and f-divergences. In particular, for a given f-divergence Df, there exists a corresponding\ndecreasing convex risk function \u03d5(\u03b1)of the classification margin \u03b1, such that the optimal risk R\u03d5=\u2212Df(Nguyen\net al., 2009, Thm. 1). This correspondence allows the objective to be described as a two-player zero-sum game and\nis amenable to optimization using generative-adversarial-network -like (Goodfellow et al., 2014; Finn et al., 2016a)\nframeworks. When used to minimize the divergence between the expert and policy occupancy measures, this problem\nclass includes many adversarial imitation learning algorithms (Ho and Ermon, 2016; Fu et al., 2017; Ni et al., 2021).\nWe will use these algorithms as solution strategies for eq. (6).\nNow that we have both exact and approximate formulations of the IRL problem (eq. (3), eq. (6)) we shall see how\nspurious correlations can arise when the expectations over the datasets DEin eq. (3) and eq. (6) are evaluated over\nsamples from diverse sources.\n3Ot E\n(a)Cst\nst+1at Ot E\n(b)Cx(c)\nx(nc)Ot E\n(c)Cst\nst+1at Ot E\n(d)Cst\nst+1at\nFigure 1: (a) Probabilistic graphical model of a transition under influence of the index variable Eand latent variable C.\nThe invariant conditional is highlighted in blue. (b) General setting where Otdepends on causal x(c)and non-causal\nx(nc)features of the transition. Conditioning on the collider variable (in orange) creates a spurious correlation path. (c)\nCollider conditioning assuming wrong edge orientation Ot\u2192st+1. This corresponds to Otbeing the causal parent of\nst+1(d) Spurious correlations arising under assumption of state-only formulation of the reward. Since atis unobserved,\na backdoor path (in red) is formed.\n2.2 Spurious correlations and causal invariance approaches\nWe shall now outline the intuition as to what we consider spurious correlations in inverse reinforcement learning. It is\nnecessary, at this point, to introduce structural causal models, which will help define the notion of spurious correlations.\nA structural causal model (SCM) (Pearl, 2009) is defined as a tuple G= (S, P(\u03b5)), where P(\u03b5) =Q\ni\u2264KP(\u03b5i)is a\nproduct distribution over exogenous latent variables \u03b5iandS={f1, ..., f K}is a set of structural mechanisms where\npa(i)denotes the set of parent nodes of variable xi:xi:=fi(pa(xi), \u03b5i)fori\u2208 |S|.Ginduces a directed acyclic graph\n(DAG) over the variables nodes xi. The SCM entails a joint observational distribution PG=Q\ni\u2264Kp(xi|pa(xi))over\nvariables xiconditioned on the parents of xifor some probability distribution p(\u00b7|pa(xi))describing the mechanism fi.\nInterventions on Gconstitute modifications of structural mechanisms fiyielding interventional distributions \u02dcPG. In the\ncontext of IRL, we consider interventional settings of the expert trajectory distribution p(\u03be|\u03c8, \u03c6)in eq. (2).\nIn supervised learning, a correlation between the input representation and the label is considered spurious if it does\nnot generalize under distribution shift, e.g. when the trained model is evaluated on a test set of examples sampled\nout-of-distribution. More specifically, the distribution shift constitutes an intervention on the causal parents of the target\nlabel, which allows the application of invariance based approaches, that we will describe below. In the case of IRL, we\nconsider a correlation to be spurious when a reward function does not generalize to perturbations in the initial measure\nor dynamics of the MDP, i.e. the policy optimizing a reward that reflects this correlation will absorb this signal and fail\nto perform optimally under perturbed environment dynamics.\nTransition SCM. To illustrate scenarios in which such spurious correlations arise, we consider the structural causal\nmodel of a transition in Figure 1(a). At timestep t, the model is composed of the state st, action at, next state st+1, the\noptimality label Otdescribing the optimality of the transition at timestep t, a variable Eencoding the setting index\ne\u2208I(Etr)and an unobserved latent variable C, which might vary as Echanges. In this model, we would like to\navoid non-causal information paths between the environment index Eand the optimality label. The presence of such\npaths corresponds to spurious correlations. We will now discuss specific scenarios in which such correlations arise and\nsubsequently describe a way to mitigate them.\nIn Figure 1b, we observe a general partitioning of an arbitrary transition input (s, a, s\u2032)into the causal transition feature\ncomponents x(c)andnon-causal transition feature components x(nc)described by feature function \u03c6:S\u00d7A\u00d7S \u2192 Rd.\nHere, conditioning on the x(nc)collider introduces a spurious correlation path. Among other causes, this situation can\narise due to selection bias caused by a prevalence of certain transitions in the pooled dataset, i.e., the pooling of diverse\nsources of demonstrations under the binary optimality label introduces a spurious correlation between states visited by\nexpert policies as a consequence of the respective preferences and the binary optimality label.\nA second scenario can be observed in Figure 1c. This scenario requires the assumption that the orientation of the\nedge from node Otto node st+1is temporally causal, meaning that the optimality of a state stat time tis a causal\nparent of the next state. In this case, observing the collider node st+1makes nodes EandOtconditionally dependent\n2:E\u0338\u22a5 \u22a5Ot|st+1. In Figure 1d we can observe the scenario where we do not condition the representation of the\noptimality conditional on the action, which corresponds to the learning from observations modality (Zhu et al., 2020).\nBy conditioning on the collider node st+1and not observing the action node a, abackdoor path (Pearl, 2009) is formed\nbetween the setting index Eand the optimality variable Ot, resulting in the violation of their conditional independence\nrelationship.\n2Here,\u22a5 \u22a5denotes statistical independence\n4Dataset partitioning and training settings. The standard IRL procedure typically pools input demonstration data\ninto one dataset which may lead to absorption of these correlations by the learned reward. To resolve this, we first\nneed to make the assumption that the data is partitioned according to different sources obtained from observational and\ninterventional settings of the trajectory distribution. This assumption is valid in three different scenarios: (i) expert\ndemonstrations were gathered on different environment dynamics, (ii) initial states were sampled from different initial\nstate distributions or (iii) experts have reward preferences, i.e. optimize a perturbed version of the reward associated\nwith the true task goal. In the context of the current work, we consider the source diversity to be a consequence of\nthe individual expert preferences expressed as reward functions parameterizing eq. (1). We denote the union set of\nsamples obtained from these settings as Etr, the set of training settings , and assume access to multiple such settings\nDe:={(\u03bei)}|Etr|\ni=1during training.\nFrom robustness to invariance. The consideration of multiple source distributions and associated target noise\nwarrants the application of robust methods such as distributionally robust optimization (DRO) (Namkoong and Duchi,\n2016; Bashiri et al., 2021; Viano et al., 2021). Such approaches modify the loss objective by searching over the space\nof empirical distributions indexed by e\u2208 Etrunder which the expected loss Ledescribing the problem objective is\nmaximized. This is implemented in practice by searching over the set of training settings Etrresulting in a min-max\nobjective for some function class f\u2208 F:\nmin\nf\u2208Fmax\ne\u2208EtrE\u03be\u223cDeLe(f, \u03be) (9)\nThis effectively regularizes the model by optimizing based on its worst-case performance. While this modification can\ntackle the issue of model overfitting to scarce data, it does not address potential diversity of the data due to mode-seeking\nbehaviour of the min-max problem (Rahimian and Mehrotra, 2019). This behaviour describes the fact that the maximum\n\"latches on\" to the training setting with the largest likelihood loss which might constitute a spurious mode of the\ndemonstrations with respect to the actual task goal. Arjovsky et al. show that the solution of robust regression is a first\norder stationary point of the weighted square error (in the case of convex losses), if the variance of the loss is used as a\nper-setting bias (Arjovsky et al., 2019, Prop. 2). This limits the generalization capacity to the convex hull of the training\nsettings Etr.\nFollowing (Arjovsky et al., 2019), we consider causal properties of the robust estimation problem for the purposes of\nimproved generalization outside of the convex hull of training settings. The causal invariance principle postulates\nthat for the problem of estimating a target conditional, e.g. classification label, one should only consider variables\nwhich belong to the set of causal parents of the target. More specifically, causally invariant covariates must yield\nthe same conditional distribution of the target in both observational and interventional settings of the SCM. Lifting\nthe distributional restrictions of the methods described in (Peters et al., 2015), the invariant risk minimization (IRM)\nprinciple (Arjovsky et al., 2019) aims to identify a causally invariant data representation \u03c6by instantiating a bi-level\noptimization problem for a representation function \u03c6and a predictor function w:\nmin\n\u03c6:X\u2192H ,w:H\u2192YX\ne\u2208EtrLe(w\u25e6\u03c6)s.t.:w\u2208argmin\n\u00afw:H\u2192YLe( \u00afw\u25e6\u03c6)\u2200e\u2208 Etr (10)\nThis objective admits an unconstrained relaxation using the gradient norm penalty D(w= 1.0, \u03c6, e) =\n||\u2207w|w=1.0Le(w\u25e6\u03c6)||2which quantifies the optimality of a fixed predictor ( w= 1.0) at each setting e(Arjovsky et al.,\n2019). This leads to the following unconstrained formulation of the learning problem:\nmin\n\u03c6:X\u2192YX\ne\u2208EtrLe(\u03c6) +\u03bb||\u2207w|w=1.0Le(w\u25e6\u03c6)||2(11)\nIn the following section, we will show how to incorporate this principle into the IRL setting.\n2.3 Reward regularization using causal invariance\nMapping the objective in eq. (10) to the context of reward learning, we consider data gathered by different experts\nto correspond to interventional settings of the trajectory distribution in eq. (2), where the interventions reflect the\nvarying preferences exhibited by the policy of the respective experts. The stable conditional we would like to identify\ncorresponds to the conditional distribution of the optimality label P(Ot|st, at). To do so, we invoke the causal invariance\nprinciple to learn reward functions which utilize features which are invariant to some class of deviations exhibited by\nthe experts. We motivate this by the fact that despite the discrepancies in the demonstrations, all experts are assumed\nto perform the task in an optimal fashion with respect to the true task goal. This implies that all experts, at least in\npart, optimize the same underlying reward that we would like to recover. In doing so, we hope to extract succinct\n5descriptions of the underlying agreed intentions of the experts as reward functions. As a result, we expect such rewards\nto be more readily applicable to MDPs with distribution shift of the dynamics. As a next step, we show how to apply\nthis principle into practice by introducing the causal invariance (CI) regularization, instantiated as the IRM penalty\ndescribed in eq. (11).\nFeature matching regularization. We begin by considering the maximum entropy feature matching problem. For the\nGibbs distribution p(\u03be|\u03c8, \u03c6) = exp( \u03c8T\u03c6(\u03be))/Z\u03c6,\u03c8over trajectories, we can write down the constrained optimization\nproblem analogously to eq. (10):\nmax\n\u03c6,\u03c8X\ne\u2208EtrX\n\u03be\u2208Delogp(\u03be|\u03c8, \u03c6)s.t.\u03c8\u2208argmax\n\u00af\u03c8X\n\u03be\u2208Delogp(\u03be|\u00af\u03c8, \u03c6) (12)\nIntuitively, due to convexity of the likelihood function w.r.t. the natural parameter \u03c8(Wainwright et al., 2008), this\ncorresponds to penalizing the deviation \u03c8inp(\u03be|\u03c8, \u03c6)from the optimal parameter \u03c8\u2217which maximizes the likelihood\np(\u03be|\u03c8\u2217, \u03c6).\nIn an analogous fashion to the IRM approach described above (eq. (10)), we propose to relax the constrained optimization\nproblem by defining a regularization term D(\u03c8, \u03c6, e )which describes this deviation. Here, the expected loss Le\ncorresponds to the primal maximum likelihood loss of the Gibbs distribution over trajectories.\nDefinition 1. LetEtrbe the set of training settings and \u03c8, \u03c6 be the parameters of the likelihood p(\u03be|\u03c8, \u03c6).D(\u03c8, \u03c6, e )is\na distance function representing the violation of the constraints of eq. (12) in training setting e\u2208 Etrw.r.t. the optimal\nsolution.\nD(\u03c8, \u03c6;e) =||\u2207\u03c8|\u03c8=1.0Le(\u03c8, \u03c6)||2(13)\nIn our case, the deviation is computed w.r.t. the parameters maximizing the likelihood of the Gibbs distribution due\nto convexity of LMLE w.r.t. function g\u03c8,\u03c6. Applying the gradient of this penalty to the representation function \u03c6\neffectively regularizes the representation to minimize the constraint violations. In simple tabular MDPs, where the\ncomputation of the partition function is tractable, we directly apply this regularizer to the primal maximum likelihood\nobjective as follows:\nLMLE(\u03c8, \u03c6, e ) =max\n\u03c6,\u03c8X\ne\u2208Etr\u0012\nE\u03be\u2208De\u0014\nlog\u00121\nZ\u03c8,\u03c6exp(\u03c8T\u03c6(\u03be))\u0013\n+\u03bbD(\u03c8, \u03c6, e )\u0015\u0013\n(14)\nIn the primal case, for a trajectory distribution described by an exponential family, we can derive a closed form of the\ngradient penalty. We summarize this result as the following proposition3:\nProposition 1. Let the likelihood p(\u03be)belong to a natural exponential family with parameter \u03c8, sufficient statistics\n\u03c6(x)and the (Lebesgue) base measure p0. LetDe\nEbe the dataset corresponding to interventional setting e. Then,\nfor all e\u2208 Etr, the causal invariance penalty for the maximum likelihood loss is the norm of the sufficient statistics\nexpectation difference:\nD(\u03c8, \u03c6;e) =||\u2207\u03c8|\u03c8=1.0Le(\u03c8, \u03c6)||2=||EDe\nE[\u03c6(\u03be)]\u2212Ep(\u03be|\u03c8)[\u03c6(\u03be))]||2(15)\nThis closed form of the gradient norm penalty can be utilized in the maximum causal entropy solver (Ziebart et al.,\n2010). We assume the state features to be the output of a neural network according to the DEEPMAXENTmodel\n(Wulfmeier et al., 2015). In order for the network to adopt invariant features, the gradient norm penalty in eq. (15)\nis used to update the feature network using backpropagation4. The resulting algorithm ( CI-FMIRL ) is presented in\nAlgorithm 1.\nPenalty for dual formulation. In large scale MDPs, where the evaluation of the log-partition function is intractable,\nwe use the variational dual objective outlined in eq. (5). We will now show how to apply the same regularization\nto the variational dual formulation of the problem outlined in eq. (5). In order to do so, we first need to derive the\ncausal invariance penalty for the dual formulation. By leveraging the strict concavity of eq.(6)w.r.t. q, we can\nstraightforwardly extend the distance penalty to the dual formulation as follows:\nLdual(\u03c8, \u03c6, q, e ) =max\n\u03c8,\u03c6X\ne\u2208Etrmin\nq[E\u03be\u223cDe\nE[g\u03c8,\u03c6(\u03be)]\u2212E\u03be\u223cq[g\u03c8,\u03c6(\u03be)] +H(q)] +\u03bbD(\u03c8, \u03c6, e ) (16)\n3All omitted proofs are found in appendix A\n4We derive a closed form of the gradient estimate in Appendix D\n6Algorithm 1 CI regularized Feature Matching IRL (CI-FMIRL)\nInput: Expert trajectories De\nEassumed to be obtained from multiple experts by intervening on p(\u03be|\u03c8, \u03c6)\nInit: Initialize reward estimate r\u03c8and state feature network \u03c6\u03b8\nforsetting ein{1, ...,Etr}do\nwhile r\u03c8,\u03c6not converged do\nCompute feature matching gradient \u2207\u03c8L(\u03c8, \u03c6;e) =EDe\nE[\u03c6(\u03be)]\u2212Ep(\u03be|\u03c8)[\u03c6(\u03be)]andcausal invariance penalty\ngradient \u2207\u03c6D(\u03c8, \u03c6;e)and backpropagate the weighted sum through feature network \u03c6\u03b8(s)\nCompute policy \u03c0r\u03c8,\u03c6using value iteration on the reward estimate r\u03c8,\u03c6\nend while\nend for\nReturn: Trained reward r\u03c8,\u03c6\nThe numerical solution strategy for the saddle-point objective in Equation (16) can be realized using a two-player\nmin-max game implemented using a GAN-like framework (Finn et al., 2016b). More specifically, an equivalence\nbetween the maximum likelihood problem for a Gibbs distribution over trajectories and the corresponding variational\napproximation has been shown in (Fu et al., 2017, App. A). We state the CI gradient penalty as a result of the following\nproposition.\nProposition 2. The gradient of the primal exponential family maximum-likelihood problem in Equation (3)w.r.t. the\nnatural parameter \u03c8is equivalent to the gradient of the dual in Equation (5)w.r.t the parameter \u03c8when the density\nratioq\npis unity.\n||\u2207\u03c8Ldual(\u03c8, \u03c6, q, e )||2=||min\nqE\u03be\u223cDE[\u03c6(\u03be)]\u2212E\u03be\u223cp(\u03be|\u03c8,\u03c6)\u0014q(\u03be)\np(\u03be|\u03c8, \u03c6)\u03c6(\u03be)\u0015\n||2\nUsing this result, we can apply the gradient penalty to the dual. Effectively, the resulting gradient estimate requires an\nimportance sampling estimate of the second expectation using the sampling trajectory distribution qinduced by the\npolicy. The minimum over qis attained when the importance weight is unity.\nAs we have seen in section 2.1, the dual objective is closely related to the f-divergence minimization objectives which\nform the basis of multiple adversarial imitation learning algorithms (Ho and Ermon, 2016; Fu et al., 2017; Ni et al.,\n2021). This class of algorithms leverages the correspondence between the f-divergence objectives and equivalent binary\nclassification surrogate losses as described in (Nguyen et al., 2009). One such example is the optimal logistic loss of a\nbinary classification function gD(x) =p(x)\np(x)+q(x)which corresponds to the Jensen-Shannon divergence (Ho and Ermon,\n2016) between distributions p(x)andq(x):\nmax\ngDLBCE(gD) =max\ngEx\u223cDE[loggD(x)] +Ex\u223c\u03c0[log(1 \u2212gD(x))] = DJS(p||q) (17)\nIn practice, we can make use of this equivalence in order to apply the penalty defined in definition 1 solely to the\ndiscriminator function gD(x)which classifies transitions sampled from the expert datasets De\nEand transitions obtained\nfrom the imitation policy.\nAlgorithm description. We present the resulting adversarial algorithm ( CI-AIRL ) in Algorithm 2. The algorithm\ndescribes a two-player zero-sum game between an agent parameterized using a soft-actor-critic architecture and the\ndiscriminator which is used to provide a reward function by distinguishing between expert and policy samples. The\nadversarial training procedure generally mimics that of divergence-based methods such as (Ho and Ermon, 2016;\nFu et al., 2017). There are three main differences compared to baseline adversarial training algorithms. The first\nis the fact that we use multiple experts in a distinct fashion as opposed to pooling the demonstrations into one big\ndataset. The second is the regularization of the discriminator objective in Equation (17) using the gradient norm penalty\nD(\u03be, \u03c6, \u03c8 ;e) =||\u2207\u03c8|\u03c8=1.0LBCE(\u03be, \u03c6, \u03c8 ;e)||2in a similar fashion to eq. (10), where \u03c8= 1.0corresponds to a fixed\nscalar predictor. Finally, we utilize soft-actor-critic (SAC) (Haarnoja et al., 2018) as an instance of an off-policy forward\nRL solution method as opposed to the on-policy algorithms used in (Ho and Ermon, 2016; Fu et al., 2017).\n3 Experiments\nWe will now evaluate the proposed method empirically. The experiments are designed to answer the following questions:\n(i) What is the effect of the causal invariance penalty on the recovered reward structure? (ii) Does regularizing the\nreward function using causal invariance improve downstream policy performance? (iii) What is the impact of changing\n7the loss function of the discriminator and (iv) increasing the perturbation magnitude? To answer these questions, we\nevaluate our model in two settings.\nThe first setting considers a tabular gridworld scenario, where the partition function is tractable. We perform reward\nlearning experiments using variants of the maximum entropy feature expectation matching algorithm. In particular,\nwe use the DEEPMAXENT(Wulfmeier et al., 2015) model of state features with different regularization strategies. In\nthe second setting, we test the invariance regularization in an adversarial IRL setting on simulated robotic locomotion\nenvironments. Here, we demonstrate the generalization of the obtained reward functions by retraining policies using the\nrecovered rewards on perturbed versions of the environment dynamics.\n3.1 Tractable setting: Gridworld experiments using feature matching\n(a)\n\u22120.10\u22120.050.000.050.10\n(b)\n\u221210.067\u22127.842\u22125.617\u22123.393\u22121.168\n(c)\n\u22128.289\u22126.633\u22124.977\u22123.322\u22121.666\n(d)\n\u22122.270\u22121.704\u22121.137\u22120.571\u22120.005\n(e)\n\u22120.07296\u22120.06991\u22120.06687\u22120.06382\u22120.06077\nFigure 2: Feature matching reward recovery on a gridworld environment. (a) expert trajectory datasets: 1st group (blue)\n400 trajectories, 2nd group (white): 25 trajectories, 3rd group (green): 3 trajectories. (b) MaxEnt IRL ERM baseline (c)\nMaxEnt IRL ERM baseline with L2 regularization coefficent \u03bbL2= 1e\u22123(d) MaxEnt IRL with CI penalty, \u03bbI= 0.01,\n(e) MaxEnt IRL with CI penalty, \u03bbI= 0.05\nFor the first experiment, we aim to illustrate the principle of causal invariance regularization in the tractable IRL setting\nusing Algorithm 1. To do so, we choose a simple gridworld environment with stochastic dynamics and a sparse ground\ntruth reward structure and corresponding trajectories illustrated in (Figure 2a). The goal of the agent is to navigate from\nthe bottom left to the top right corner. Due to the tabular nature of the state space, this setting allows a direct visual\ncomparison of the recovered reward functions for the different regularization strategies.\nSetup. In order to construct the dataset settings Etr, we generate a dataset of 3 groups of expert trajectories using a\nvalue iteration method on modified versions of the MDP. The initial and final states of the trajectories are fixed. We\nintroduce a selection bias into the IRL feature expectation matching problem by manipulating the expert preferences to\nchoose different paths. This results in a trajectory dataset with different number of trajectories for each of the three\npaths chosen by the experts (Figure 2a): 40 trajectories for 1st group, 10 trajectories for 2nd group and 1 trajectory for\nthe 3rd group.\nBaselines. Throughout this section, we compare the proposed regularization to both non-regularized and regularized\nversions of the DEEPMAXENTalgorithm. In particular, we use an L2 penalties of the reward feature weights \u03c6(s)and\na Lipschitz smoothness penalty (Yoshida and Miyato, 2017) as baseline regularization strategies.\nResults. In Figure 2, we can observe that both the unregularized MaxEnt IRL algorithm (ERM) (Figure 2b) and\nL2-regularized MaxEnt IRL algorithm (ERM- L2) (Figure 2c) exhibit overfitting to the expert datasets and partially\nfail to recover a meaningful reward and respective policy. In contrast, the IRM-regularized version recovers a shaped\nreward function which takes the different optimal paths into account in a manner which demonstrates an invariance to\nAlgorithm 2 CI regularized Adversarial IRL (CI-AIRL)\nInput: Expert trajectories De\nEassumed to be obtained from multiple experts by intervening on p(\u03be|\u03c8, \u03c6)\nInit: Initialize actor-critic \u03c0\u03b8, \u03bd\u03d1and discriminator g\u03be,\u03c6\nforsetting ein{1, ...,Etr}do\nCollect trajectory buffer D\u03c0={\u03bei}i\u2264|D\u03c0|by executing the policy \u03c0\u03b8\nUpdate g\u03c6,\u03b8(s, a)via binary logistic regression by maximizing\nL(\u03c6, \u03c8;e) =LBCE(\u03be, \u03c6, \u03c8 ;e) +\u03bb||\u2207\u03c8|\u03c8=1.0LBCE(\u03be, \u03c6, \u03c8 ;e)||2\nusing dataset tuple (De\nE,D\u03c0)\nUpdate actor-critic (\u03c0\u03b8, \u03bd\u03d1)w.r.t. the reward function of the regularized discriminator using the soft-actor-critic\nRL procedure\nend for\nReturn: Trained reward r\u03c6,\u03c8and actor-critic \u03c0\u03b8, \u03bd\u03d1\n8the setting index e. In particular, increasing the regularization strength \u03bbimproves the reward significantly (Figure 2d -\nFigure 2e). It is easy to see that the CI-regularized reward can more straightforwardly be used in a setting where the\ndynamics of the MDP might be modified, e.g. when obstacles are introduced.\n3.2 Adversarial setting\nFor the second experiment, we perform experiments in large state spaces, which require the use of Algorithm 2. Our\nprimary goal is to investigate how well the recovered reward function allows the elicitation of a policy under change of\ndynamics. To do so, we first learn a reward function using the adversarial training procedure and then retrain a policy\nfrom scratch using the recovered reward as training signal.\n\u2212200010004000Ant-v3\n022504500Walker2d-v3\n022504500Hopper-v3\n\u2212100030007000HalfCheetah-v3\n030006000\nBody MassHumanoid-v3\n\u2212200010004000Episode reward022504500\n022504500\n\u2212100030007000\n030006000\nJoint Range\n\u2212200010004000\n022504500\n022504500\n\u2212100030007000\n030006000\nControl Range\n0.0 0.5 1.0\u2212200010004000\n0.0 0.5 1.0022504500\n0.0 0.5 1.0\nM. Timesteps022504500\nCI 1 CI 2 CI 3 ERM Lip 1.0 Lip 10.00.0 0.5 1.0\u2212100030007000\n0.0 0.5 1.0030006000\nGeom. Friction\nFigure 3: Comparison of SAC policy performance w.r.t. ground truth reward when trained on inferred reward functions.\nEvery row depicts a different type of dynamics perturbation for the five MUJOCOtasks as described in section 3.2.\nHere, AIRL is chosen as the baseline algorithm. The variants correspond to the unregularized baseline: ERM, Lipschitz\nregularization: LIPand three best CI regularization parameters CI.\nSetup. For our experimental setting, we choose a set of robot locomotion tasks from the MUJOCO(Todorov et al.,\n2012) suite. We generate the demonstration datasets by using pretrained soft-actor-critic (SAC) policies from the\nSTABLE -BASELINES 3repository5. In order to diversify the demonstrations, we perturb the policies using a structured\nnoise approach: the optimal policy action is perturbed with Gaussian noise at every timestep with a probability p= 0.3\nof the noise being applied. We have used 10 expert trajectories for every environment in all the experiments performed\nin this section. The reward function is obtained by using a number of different discriminator functions corresponding to\nvariations of the f-divergence objective. In order to assess the quality of the recovered reward, we retrain policies on the\nrecovered reward under distribution shift of the dynamics realized by perturbing physical parameters of the simulation.\nSpecifically, we apply Gaussian noise to four parameters of the MUJOCOsimulation: the body mass, joint range and\nactuator control range of the robot as well as the contact friction coefficient of the simulated surface. We evaluate the\nbehaviour of the algorithms for a variety of perturbation magnitudes.6\nBaselines. Throughout this section, we use three different adversarial IRL algorithms as baselines for reward learning:\n(i) AIRL (Fu et al., 2017), an adversarial IRL approach which relies on a structured discriminator to recover a stationary\nreward function (ii) MEIRL, an adaptation of the maximum entropy IRL algorithm for large state spaces without an\nimportance sampling estimator (Ni et al., 2021) and (iii) GAIL (Ho and Ermon, 2016), an imitation learning where we\n5https://huggingface.co/sb3\n6Additional experimental details are provided in appendix C\n9extract the unshaped discriminator as a reward function for policy learning."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### 1. Abstract:\nThe paper introduces MUX-GCL, a novel cross-scale Graph Contrastive Learning (GCL) framework that employs multiplex representations to capture richer consistent information while reducing noise and false negative pairs across scales. The method emphasizes minimizing information loss through a contrasting strategy based on positional affinities. Extensive experiments demonstrate that MUX-GCL achieves state-of-the-art performance across several public datasets. Theoretical analysis supports that the new objective function provides a stricter lower bound on mutual information between raw input features and output embeddings.\n\n### 2. Introduction:\nGraph representation learning (GRL) has become a significant field in machine learning, dealing with the challenge of taming graph-structured data. Traditional methods rely heavily on supervised learning, which is limited by the availability of labeled data. Self-supervised learning (SSL), particularly Graph Contrastive Learning (GCL), has emerged to address these limitations by leveraging the intrinsic structure of graph data without labels.\n\nThe focus of this research is on developing cross-scale contrastive learning to capture richer, more consistent information from graph data while mitigating the effect of disturbing features. Conventional methods often suffer from feature inconsistency when applied to larger scales. MUX-GCL aims to retain consistent feature information and reduce contamination by utilizing multiplex representations and a contrasting strategy to minimize false negatives.\n\n### 3. Related Work:\nThe paper reviews various methodologies in GRL, including supervised learning approaches, and highlights the shortcomings due to label dependencies. The literature on SSL and GCL, such as GRACE, DGI, and MVGRL, is critically reviewed, emphasizing their approaches to maximizing information consistency in same-scale and cross-scale modes. The paper positions MUX-GCL as an advancement by addressing the contamination issues inherent in previous cross-scale methods through the novel use of multiplex representations and affinity-based contrasting strategies.\n\n### 4. Method:\n\n**A. Preliminaries and Notations:**\n- Graph \\( G = (V, E) \\): \\( V \\) is the set of nodes, \\( E \\) is the set of edges.\n- Feature matrix \\( X \\in \\mathbb{R}^{N \\times F} \\) and adjacency matrix \\( A \\in \\{0, 1\\}^{N \\times N} \\).\n- The SSL aims to learn a representation \\( f(X, A) \\in \\mathbb{R}^{N \\times F} \\).\n\n**B. Motivation:**\n- Issues with conventional pooling approaches leading to information loss.\n- Proposal to utilize the entire ensemble of latent and final representations of an encoder to construct effective patches, minimizing contamination.\n\n**C. Framework:**\n**I. Multiplex Patch Contrast (MPC):**\n- Cross-scale contrast objectives between final and intermediate layer representations utilizing an extended InfoNCE loss.\n- Objective function:\n\\[ L_c(u_i^{(L)}, v_i^{(k)}) = \\log \\frac{e^{\\theta(u_i^{(L)}, v_i^{(k)})/\\tau}}{e^{\\theta(u_i^{(L)}, v_i^{(k)})/\\tau} + \\sum_{j \\neq i} \\omega_{ij}^{Lk} e^{\\theta(u_i^{(L)}, v_j^{(k)})/\\tau} + \\sum_{j \\neq i} \\omega_{ij}^{Lk} e^{\\theta(u_i^{(L)}, u_j^{(k)})/\\tau}} \\]\n- Aggregate objective function:\n\\[ L_c(u_i, v_i) = \\sum_{k=0}^L \\lambda_k L_c(u_i^{(L)}, v_i^{(k)}) \\]\n\\[ L_{MUX} = \\frac{1}{2N} \\sum_{i=1}^N [L_c(u_i, v_i) + L_c(v_i, u_i)] \\]\n\n**II. Patch Affinity Estimation (PAE):**\n- Affinity-based weighing scheme to avoid pairing false negatives by leveraging topological embeddings.\n- Affinity score function:\n\\[ \\omega_{ij}^{Lk} = 1 - \\eta(h_i^{(L)}, h_j^{(k)}) \\]\nwhere \\(\\eta\\) is the normalized inner product measuring positional similarity.\n\n**D. Theoretical Justification:**\n- Proof that the multiplex contrastive objective is a lower bound on mutual information between raw inputs and output embeddings.\n- Statistical evidence to show that the stringent lower bound is tighter compared to existing methods, ensuring performance improvement.\n\n**E. Time Complexity Analysis:**\n- Detailed comparison of time complexity to other methods (GRACE, etc.), showing marginal increases in computation but significant performance gains.\n\n### 5. Experimental Setup:\n\n**Datasets:**\n- Five datasets: Cora, Citeseer, Pubmed, Amazon-Photo, and Amazon-Computers.\n- Split into 10%, 10%, and 80% for training, validation, and testing, respectively.\n\n**Hardware and Software:**\n- Experiments conducted on an RTX 3090Ti.\n- Leveraged standard GCN encoder and evaluation frameworks from prior GCL works.\n\n**Evaluation Metrics:**\n- Node classification and clustering tasks with metrics including accuracy, NMI (Normalized Mutual Information), ARI (Adjusted Rand Index).\n\n**Baselines and Comparison Methods:**\n- Compared against state-of-the-art methods including GRACE, GCA, DGI, VGAE, and others.\n\n### 6. Results and Analysis:\n\n**Empirical Findings:**\n**Node Classification:**\n- MUX-GCL achieves higher accuracy compared to baseline methods across datasets.\n- Tabulated results (with accuracies and standard deviations) demonstrating superior performance.\n- Demonstrates effectiveness of the proposed multiplex contrast mechanism and affinity-based negative sampling.\n\n**Node Clustering:**\n- Improved clustering performance shown via NMI and ARI metrics.\n- Table comparing MUX-GCL to baselines, illustrating its dominance.\n\n**Ablation Studies:**\n- Analyzed the contribution of each component (PAE and MPC).\n- Measures impact of excluding one or both components on performance.\n- Results indicated both components are crucial, with PAE facilitating better false negative handling and MPC maximizing consistent information.\n\n**Graphical Analysis:**\n- Similarity distributions between positive and negative patch pairs (Figure 1).\n- Distributions of TLk parameters demonstrating the effectiveness of topological embeddings (Figure 3).\n\n### 7. Discussion:\n\n**Interpretation of Results:**\n- MUX-GCL's superior performance attributed to better handling of graph data inconsistencies and maximizing mutual information.\n- Efficient cross-scale contrast ensures richer and less contaminated information encoding.\n\n**Implications:**\n- Methodology can be adapted to various other tasks requiring graph-based SSL.\n- Potential for improvements in real-world unsupervised learning tasks involving large-scale graphs.\n\n**Limitations and Biases:**\n- Computational requirements, although marginally increased, could still be limiting for extremely large graphs.\n- Affinity estimation method effectiveness might vary depending on the nature of graph data and underlying topological structures.\n\n### 8. Conclusion:\n\n**Key Contributions:**\n- Innovative MUX-GCL framework utilizing multiplex representations for effective GCL.\n- Theoretical proofs supporting the superiority of the proposed objective function.\n- Comprehensive empirical validation showcasing significant performance gains over state-of-the-art methods.\n\n**Future Research Directions:**\n- Exploration of further optimization techniques to reduce computational overhead.\n- Application of MUX-GCL to diverse graph types and extending to dynamic and heterogeneous graphs.\n- Investigation into more sophisticated affinity estimation techniques.\n\nThis comprehensive summary captures all critical aspects of the MUX-GCL framework and its empirical validation, aimed to enable replication and further exploration by researchers in the field."}, {"role": "assistant", "content": "Multiplex Graph Contrastive Learning with Soft Negatives\nZhenhao Zhao1,\u2020Minhong Zhu2,\u2020Chen Wang1Sijia Wang1Jiqiang Zhang4Li Chen3,*Weiran Cai1,*\n1School Of Computer Science and Technology, Soochow University, Suzhou, China\n2School of Biology and Basic Medical Sciences, Soochow University, Suzhou, China\n3School of Physics and Information Technology, Shaanxi Normal University, Xi\u2018an, China\n4School of Physics, Ningxia University, Yinchuan, China\nAbstract \u2014Graph Contrastive Learning (GCL) seeks to learn nodal or\ngraph representations that contain maximal consistent information from\ngraph-structured data. While node-level contrasting modes are domi-\nnating, some efforts commence to explore consistency across different\nscales. Yet, they tend to lose consistent information and be contaminated\nby disturbing features. Here, we introduce MUX-GCL, a novel cross-\nscale contrastive learning paradigm that utilizes multiplex representations\nas effective patches. While this learning mode minimizes contaminating\nnoises, a commensurate contrasting strategy using positional affinities\nfurther avoids information loss by correcting false negative pairs across\nscales. Extensive downstream experiments demonstrate that MUX-GCL\nyields multiple state-of-the-art results on public datasets. Our theoretical\nanalysis further guarantees the new objective function as a stricter\nlower bound of mutual information of raw input features and output\nembeddings, which rationalizes this paradigm. Code is available at\nhttps://github.com/MUX-GCL/Code.\nIndex Terms \u2014Graph contrastive learning, Cross-scale contrast, Infor-\nmation consistency, Soft negatives\nI. I NTRODUCTION\nTaming graph-structured data has been one of the major challenges\nin machine learning, which is coined as graph representation learning\n(GRL). While significant progresses have been made, notably with\nparadigms incorporating both nodal and topological information,\nmost prevailing methods are supervised learning [1\u20135]. Yet, GRL\nhas not only to face a majority of graph data for which labels are\nunavailable in real-world scenarios, but more desirably to discover\npatterns in an autonomous way [6]. To tackle this challenge, recent\nstudies have extensively explored the realm of self-supervised learn-\ning (SSL), among which graph contrastive learning (GCL) plays a\npivotal role.\nIn essence, GCL aims to learn nodal or graph representations by\nmaximizing the information consistency between augmented views\nof the graph. Most of the established methods share the spirit of\noperating same-scale contrast between nodal representations through\non positive and negative pairs [7\u20139]. For graph-structured data,\nhowever, feature consistency can be well conveyed in structures of\ndifferent scales [10]. Some efforts have thus expanded the scope\nto cross-scale modes, including patch-global contrast of nodal and\ngraph representations [10\u201312], and context-global contrast between\ncontextual subgraph- and graph-levels [13, 14]. The contrasts of\npatches at diverse scales prove to be highly beneficial.\nYet, with the gain of richer information, cross-scale contrasting\nmodes tend to suffer from contamination by inconsistent features. The\nexpansion to larger-scale patches tends to join out-of-class nodes and\nhence more feature inconsistency. It is thus an intriguing question:\nHow to enable contrasts that capture more consistent features across\nscales while restrict contamination from inconsistency?\n\u2020These two authors contributed equally.\n*Correspondence should be addressed to: wrcai@suda.edu.cn or chenl\n@snnu.edu.cn\n0.4\n 0.2\n 0.0 0.2 0.4 0.6 0.8 1.0\nSimilarity0.02.55.07.510.012.515.017.5Densityu2v2 pos\nu2v2 neg\nu1v2 pos\nu1v2 neg\nu0v2 pos\nu0v2 neg\n++--Fig. 1: Similarity distributions of cross-layer embeddings between\ntwo augmented views (for GRACE). All positive pairs are substan-\ntially more similar than negative pairs, labeled as umvnpos/neg\nwithmandnbeing the layers.\nThis raises a request for a contrasting paradigm that exploits\ninformation maximally and selectively. One has to note that in-\nformation loss is inherent in GCL. On one hand, an encoding\nprocess is not guaranteed information-conservative. The inclination\nfor oversmoothing is intrinsic to message-passing based methods. On\nthe other hand, pairing negatives between intra-class nodes leads to\na loss of consistent features. This has been spotted in the same-scale\ncontrast. Regarding this, some work excludes neighbouring nodes to\navoid false negatives [15, 16] or weighs them as positives based on\ntheir saliency [17]. However, these approaches are not applicable to\ntopological compositions in cross-scale scenarios.\nWe propose MUX-GCL, a novel cross-scale contrastive learning\nparadigm that puts multiplex encoded information into full play. The\ncore of the paradigm lies in the contrasts of \u201ceffective patches\u201d\nconstructed from all latent representations of the encoder. Concretely,\nhigher-layer nodal embeddings, interpreted as representations of\npatches centered on focal nodes, are contrasted with lower-layer\nembeddings, where features are less contaminated by the locality.\nTo assist such cross-scale contrasts, a multiplex contrasting strategy\nis proposed to minimize information loss from false negative pairs,\nguided by topological affinities of patches. With these facilities,\nconsistent information contained in the entire multiplex encoder is\nmaximally exploited.\nOur contributions are summarized as follows:\n\u2022We propose a novel cross-scale GCL paradigm, MUX-GCL,\nutilizing multiplex representations of the entire encoder to max-\nimize consistent information while mitigate disturbing features.\n\u2022We introduce a patch contrasting strategy based on topological\naffinities to alleviate false negative pairs, which most notably, is\nthe only mechanism applicable to cross-scale contrasts.\n\u2022Our theoretical justification guarantees the objective function\nof MUX-GCL as a stricter lower bound of mutual information\nbetween raw features and learned representations of augmentedarXiv:2409.08010v1  [cs.LG]  12 Sep 2024GNNView UView V\ud835\udca2\ud835\udca2U=tU(\ud835\udca2)\n\ud835\udca2V=tV(\ud835\udca2)k = 0GNN\nhL1hL2hLNhk1hk2hkN......Topological\nEmbeddingPatch\nA\ufb03nities(nod2vec, \nGAE, \nVGAE, \netc)k = 1k = 2\nk = 0k = 1k = 2\u03d5(\u22c5)\n\u03d5(\u22c5)\nContrastive\nLosspos. pairsneg. pairsanchor nodeFig. 2: Overall architecture of MUX-GCL. Contrasts are executed between \u201ceffective patches\u201d constructed from all representations of the\nmultiplex encoder, as illustrated by the links. The pairwise affinities of topological embedding estimate the likelihood of being false negatives.\nAugmentations are implemented as in GRACE.\nviews, providing the rationale behind the method.\n\u2022Extensive experiments on both classification and clustering tasks\ndemonstrate salient improvements, outperforming multiple state-\nof-the-art GCL models on public datasets.\nII. M ETHODS\nA. Preliminaries and Notations\nLetG= (V,E)denote a graph, where V={vi}N\ni=1,E \u2286 V \u00d7 V\nare the node and edge sets respectively. We let X\u2208 RN\u00d7FandA\u2208\n{0,1}N\u00d7Nbe the feature matrix and adjacency matrix. As a form\nof SSL, the purpose of our model is to learn a reliable representation\nf(X, A)\u2208 RN\u00d7Fof the input data with no labels through a GCN\nencoder. It is essential to support downstream tasks, such as node\nclassification and clustering. Hence, the learned representations will\nbe commonly input to a minimal prediction head for tests.\nB. Motivation\nWe seek to establish a cross-scale contrastive learning method that\ngains richer consistent information. Two aspects are of our concern:\nthe construction of multi-scale patches and the contrasting strategy.\nThe key issue with conventional ways of constructing patch\nrepresentations through pooling is the information loss caused by\ninvolving inconsistent features. Instead, we consider using the entire\nensemble of latent and final representations of an encoder for building\npatches. From the perspective of message passing, we regard a kth-\nlayer embedding of an anchor node as a representation of a k-\nhop ego-net centered on it, which forms an \u201ceffective patch\u201d. This\ntreats the encoder as a multiplex network, which introduces no extra\ninformation contamination.\nCross-scale contrasts may thus be established between pairs of\nsuch patch representations. The roles of such effective patches in\ncontrasting can be justified by observing the similarity between\ncross-layer embeddings, as demonstrated for GRACE. As shown in\nFig. 1, all positive patch pairs, regardless of layers (scales), are far\nmore similar than negative pairs, as suggested by the well separated\ndistribution of similarities. This strongly indicates that representations\nacross layers deserve to be involved in graph contrastive learning.\nYet, to systematically pairing cross-scale patches, we need a\ncontrasting strategy that maximally preserve consistent features. This\naims essentially to avoid brutal erasure of exploitable information by\npairing false negatives. In the absence of class labels, we evaluate thelikelihood of false negatives on the topological affinities of patches as\npriors. The use of affinities thus builds up \u201csoft negatives\u201d in contrast\nto the commonly used hard ones in GCL.\nC. Framework\nFrom the rationale above, we establish \u201ceffective patches\u201d using\nall representations of the encoder for contrastive learning. Each nodal\nembedding U(k)(V(k)in the other view) on the kth layer of the\nGCN now serves as an effective representation of a k-hop ego-net\ncentered at the anchor node. Specifically, the definition of the patch\nrepresentation takes the standard form U(k)=\u03c3(\u02dcAU(k\u22121)W(k))\u2208\nRN\u00d7Fwith the initial input U(0)=X, where \u02dcAis the normalized\nadjacency matrix and W(k)a set of trainable parameters.\nWith this premise, we now introduce the cross-scale contrastive\nlearning paradigm MUX-GCL (Fig. 2). We deploy two modules, i.e.\nmultiplex patch contrast andpatch affinity estimation .\nMultiplex Patch Contrast (MPC) . To contrast effective patches\nacross scales, we extend the commonly used InfoNCE loss from\nsame-scale contrast to a multiplex setting. Since final representations\nof the encoder are ultimately desired, we conduct cross-scale con-\ntrasts between final and all intermediate layers representations. The\nmultiplex objective function is given as follows\nLc(u(L)\ni, v(k)\ni) =\nloge\u03b8(u(L)\ni,v(k)\ni)/\u03c4\ne\u03b8(u(L)\ni,v(k)\ni)/\u03c4+P\nj \u0338=i\u03c9Lk\nije\u03b8(u(L)\ni,v(k)\nj)/\u03c4+P\nj \u0338=i\u03c9Lk\nije\u03b8(u(L)\ni,u(k)\nj)/\u03c4\n(1)\nwhere \u03b8(\u00b7,\u00b7)is the similarity function. The metric \u03c9Lk\nijrepresents\na measure of the likelihood of being false negatives. To treat the\ncontrasts in a balanced way, we average the objective function across\ndifferent scales, as expressed by the pairwise objective function\nLc(ui, vi) =LX\nk=0\u03bbkLc(u(L)\ni, v(k)\ni) (2)\nwhere \u03bbkis the weight for contrasting the final L-th layer and the\nintermediate k-th layer, withPL\nk=0\u03bbk= 1.\nFinally, to ensure symmetry between the two views, the overall\nobjective function is defined as\nLMUX =1\n2NNX\ni=1[Lc(ui, vi) +Lc(vi, ui)]. (3)Patch Affinity Estimation (PAE) . The affinity estimation function\nassigns weights to negative pairs to alleviate the problem of false\nnegatives. Notably, in the cross-scale contrast, patches are more likely\nto share information due to their positional affinity, where overlaps\nare significantly more incident. A higher affinity score thus indicates\na higher likelihood of being false negatives. This weighting scheme\nis thus to reduce the loss of consistent information in negative pairs.\nFor this scenario, we propose an affinity estimation strategy using\ntopological positions as a decent prior. Concretely, we employ a graph\nembedding algorithm to obtain nodal representations that contain\nsolely topological information. The topological representation of a\npatch is then simply obtained by pooling the encompassed nodes\nH(0)=T(A, X ) h(k)\ni=Poolj\u2208G(k)\ni(h(0)\nj) (4)\nwhere T(\u00b7)represents a learning algorithm that maps nodes to a\ntopological embedding space. G(k)\nirepresents the k-hop ego-network\ncentered on node i.Pool denotes the pooling function aggregating\nnodal embeddings within the patch.\nHere we consider two learning algorithms to obtain the topological\nembeddings: Node2Vec [18] and VGAE (Variational Graph Auto-\nEncoder) [19]. We remark that the decoder of VGAE is to recover\nthe adjacency matrix of the input graph and hence learns topological\nfeatures only.\nTo obtain the inter-patch affinities, we compute the similarities of\nthese topological representations. Based on the affinity score for a\nnegative instance pair, we compute the weight \u03c9as the estimated\nlikelihood of being false negatives\n\u03c9Lk\nij= 1\u2212\u03b7(h(L)\ni, h(k)\nj) (5)\nwhere k\u2208 {0,1, . . . , L };\u03b7(\u00b7,\u00b7)is the affinity function that measures\nthe positional similarity. Here, we take the form of normalized inner\nproduct \u03b7(h(L)\ni, h(k)\nj) =\u27e8h(L)\ni, h(k)\nj\u27e9.\nD. Theoretical Justification\nWe provide a theoretical justification for our proposed multiplex\ncontrastive objective, demonstrating its rationale through the lens of\nmaximization of mutual information.\nProposition 1. The multiplex contrastive objective in Eq.1 is a\nlower bound of mutual information (MI) between raw input features\nXand output node embeddings UandVin the two augmented\nviews. Further, with a statistical significance, the objective is also a\nstricter lower bound compared with the contrastive objective LGR\nproposed by the benchmark GRACE . Formally,\nLGR<LMUX < I(X;U,V). (6)\nProof. We first prove LMUX < I(X;U,V). LetU(k),V(k)(for\nk= 0,1, . . . , L ) be the embeddings generated by the k-th layer of\nthe encoder. Our proposed objective includes 2(L+ 1) cross-scale\ncontrasting pairs\nLMUX =1\n2LX\nk=0\u03bbk\nNNX\ni=1h\nLc(u(L)\ni, v(k)\ni\u0011\n+Lc(v(L)\ni, u(k)\ni)] (7)\nFor sufficiently large N, we have \u03c9Lk\nij>1/N, which renders\nINCE(U(L),V(k))>1\nNNX\ni=1Lc(u(L)\ni, v(k)\ni). (8)\nAs InfoNCE is a lower bound of MI, we have\nLMUX <1\n2LX\nk=0\u03bbkh\nI(U(L);V(k)) +I(V(L);U(k)\u0011\n]. (9)\n1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0 2.5 3.0\nTLk\nD,ij0.00.20.40.60.81.01.21.41.6Frequencymean\n95% conf\n1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0 2.5 3.0\nTLk\nS,ij0.000.250.500.751.001.251.501.75Frequencymean\n95% confFig. 3: Distributions of TLk\nD,ij (left) and TLk\nS,ij(right) for Cora fitted by\nGaussian curves. Results are shown for epoch 300, but are consistent\nfor the entire training process.\nResorting to the relations I(U(L);V(k))\u2264I(X;U(L)) =\nI(X;U)\u2264I(X;U,V)[7] and noticing the normalized \u03bbk, we\nfinally have\nLMUX <1\n2LX\nk=0\u03bbk[I(X;U,V) +I(X;V,U)] =I(X;U,V).(10)\nWe then show that LMUX >LGRwith a statistical significance.\nWe first rewrite the loss function of GRACE as\nLGR=1\n2NX\ni=1h\nLg(u(L)\ni, v(k)\ni\u0011\n+Lg(v(L)\ni, u(k)\ni)] (11)\nwithLg(u(L)\ni, v(L)\ni) =\"\n1 +P\nj\u0338=i(e\u03c8Lk\nS,ij+ e\u03c8Lk\nD,ij)#\u22121\n, where \u03c8Lk\nS,ij=\n\u03b8(u(L)\ni, u(k)\nj)\u2212\u03b8(u(L)\ni, v(k)\ni)and \u03c8Lk\nD,ij =\u03b8(u(L)\ni, v(k)\nj)\u2212\n\u03b8(u(L)\ni, v(k)\ni). Similarly, the loss function in LMUX can be written\nasLc(u(L)\ni, v(k)\ni) =\"\n1 +P\nj\u0338=i\u03c9Lk\nij(e\u03c8Lk\nS,ij+ e\u03c8Lk\nD,ij)#\u22121\n.\nTo compare LcandLg, we define TLk\nS,ij=\u03c8Lk\nS,ij\u2212\u03c8LL\nS,ij+log \u03c9Lk\nij\nandTLk\nD,ij=\u03c8Lk\nD,ij\u2212\u03c8LL\nD,ij+ log \u03c9Lk\nij. From the statistics as shown\nin Fig 3, we show that throughout the training, both quantities, well\nfitted by Gaussian distribution, are positive with a great statistical\nsignificance (within the 95% confidence interval). We can thus con-\nclude that with a large probability, Lc(u(L)\ni, v(k)\ni)>Lg(u(L)\ni, v(L)\ni);\nsymmetrically, Lc(v(L)\ni, u(k)\ni)>Lg(v(L)\ni, u(L)\ni). These relations also\nhold for k=Lsince \u03c9LL\nij\u2208(0,1)forj\u0338=i. Hence, by comparing\nthe entire expressions, we finally reach LMUX >LGR.\nWe can hence conclude that maximizing LMUX is equivalent\nto maximizing a lower bound of the mutual information between\nraw features and learned node representations, which is yet stricter\nthan the commonly used contrastive objective. It guarantees the\nconvergence and provides a theoretical base for the performance.\nE. Time Complexity Analysis\nThe time cost of the multiplex contrast mechanism is limited\ncompared to the prevailing GCL methods. Concretely, we choose\nGRACE for comparison. Given a graph with Nnodes and Eedges,\nand assuming a GCN encoder with Llayers and dhidden dimensions,\nthe time complexity of encoding and loss function of GRACE are\nO(L(Nd2+Ed))andO(N2d), respectively. For the encoding\nstage, MUX-GCL takes extra O(LNd2)to acquire intermediate\nembeddings through linear layers, which does not increase the time\ncomplexity significantly, as Lis typically very small ( L= 2for most\ncases). For the loss function, the time complexity of MUX-GCL is\nO((L+1)N2d), which is on the same order of magnitude as GRACE,\nnoting that the InfoNCE loss in GRACE is a special case of Eq. 2TABLE I: Node classification results (Acc (%) \u00b1 Std for 5 seeds).\nModel Cora Citeseer Pubmed Photo Comp.\nraw features 64.8\u00b10.1 64.6\u00b10.1 84.8\u00b10.0 78.5\u00b10.0 73.8\u00b10.0\nnode2vec[18] 74.8\u00b10.0 52.3\u00b10.1 80.3\u00b10.1 89.7\u00b10.1 84.4\u00b10.1\nDeepWalk[20] 75.7\u00b10.1 50.5\u00b10.1 80.5\u00b10.2 89.4\u00b10.1 85.7\u00b10.1\nGAE[19] 76.9\u00b10.0 60.6\u00b10.2 82.9\u00b10.1 91.6\u00b10.1 85.3\u00b10.2\nVGAE[19] 78.9\u00b10.1 61.2\u00b10.0 83.0\u00b10.1 92.2\u00b10.1 86.4\u00b10.2\nDGI[10] 82.6\u00b10.4 68.8\u00b10.7 86.0\u00b10.1 91.6\u00b10.2 84.0\u00b10.5\nGRACE[7] 83.3\u00b10.4 72.1\u00b10.5 86.3\u00b10.1 92.5\u00b10.2 87.8\u00b10.2\nMVGRL[11] 83.8\u00b10.3 73.1\u00b10.5 86.3\u00b10.2 91.7\u00b10.1 87.5\u00b10.1\nGCA[9] 82.8\u00b10.3 71.5\u00b10.3 86.0\u00b10.2 92.2\u00b10.2 87.5\u00b10.5\nSUGRL[21] 83.4\u00b10.5 73.0\u00b10.4 84.9\u00b10.3 93.2\u00b10.4 88.8\u00b10.2\nBGRL[22] 83.7\u00b10.5 73.0\u00b10.1 84.6\u00b10.3 91.5\u00b10.4 87.3\u00b10.4\nG-BT[23] 83.6\u00b10.4 72.9\u00b10.1 84.5\u00b10.1 92.6\u00b10.5 86.8\u00b10.3\nProGCL[15] 84.2\u00b10.5 72.2\u00b10.2 86.4\u00b10.2 93.2\u00b10.1 88.7\u00b10.1\nCOSTA[24] 84.3\u00b10.2 72.9\u00b10.3 86.0\u00b10.2 92.6\u00b10.5 88.3\u00b10.1\nSFA[25] 84.1\u00b10.1 73.7\u00b10.2 85.6\u00b10.1 92.8\u00b10.1 88.1\u00b10.1\nHomoGCL[17] 84.9\u00b10.2 71.7\u00b10.3 85.8\u00b10.1 93.0\u00b10.2 89.0\u00b10.1\nMA-GCL[26] 83.9\u00b10.1 72.1\u00b10.4 85.6\u00b10.4 93.4\u00b10.1 89.0\u00b10.1\nMUX-GCL 85.5\u00b10.3 73.8\u00b10.2 86.9\u00b10.2 93.9\u00b10.1 90.7\u00b10.1\nwhen \u03bbL= 1. Furthermore, the time complexity of Node2Vec and\nVGAE used in the PAE module are O(N)andO(Nd2+Ed). This\ndoes not add to the overall complexity since PAE can be implemented\nas pre-processing and computed only once in the training phase.\nIII. E XPERIMENTS\nA. Experimental Setup\nDatasets and Baselines . We evaluate our method on five public avail-\nable datasets ranging from citation networks and e-commercial sites:\nCora, Citeseer, Pubmed, Amazon-Photo and Amazon-Computers.\nTo be consistent with the previous GCL methods (GRACE, GCA,\nCOSTA, SFA etc.), all datasets are randomly divided into 10%, 10%,\nand 80% proportions for training, validation, and testing. We compare\nMUX-GCL with multiple baselines.\nEvaluation protocol . Adhering to the evaluation framework used\nby prior work [7, 9, 10], we employ a standard two-layer GCN\nencoder and yield embeddings for downstream tasks. For the node\nclassification task, we employ an \u21132-regularized logistic regression\nclassifier from the Scikit-Learn library [27]. For node clustering\ntask, we employ KMeans as clustering method and measure the\nperformance in terms of Normalized Mutual Information (NMI) score\nand Adjusted Rand Index (ARI) [17, 28].\nB. Node Classification\nFirst, MUX-GCL surpasses all same-scale GCL methods, including\nadvanced models like ProGCL, and HomoGCL, where only output\nembeddings are contrasted, whereas our paradigm forms patches with\nboth latent and final representations. Second, it outperforms the meth-\nods that do not discern false negatives (e.g. GRACE, GCA, BGRL,\nCOSTA) by assigning affinity-informed weights, which minimizes\nthe loss of consistent information. Third, MUX-GCL is superior to\nprevious cross-scale GCL methods (e.g. DGI, MVGRL) where larger-\nscale representations are typically pooled from final embeddings.\nThanks to the use of all representations within the encoder for con-\nstructing patches, MUX-GCL contains less inconsistent information.\nThe performance comparison is summarized in Tab. I.\nTABLE II: Node clustering results. \u25b3x= 0.01xdenotes the Std.\nModel Photo Computers\nMetric NMI ARI NMI ARI\nGAE 0.616\u00b1 \u25b31 0.494\u00b1 \u25b31 0.441\u00b1 \u25b30 0.258\u00b1 \u25b30\nVGAE 0.530\u00b1 \u25b34 0.373\u00b1 \u25b34 0.423\u00b1 \u25b30 0.238\u00b1 \u25b30\nDGI 0.376\u00b1 \u25b33 0.264\u00b1 \u25b33 0.318\u00b1 \u25b32 0.165\u00b1 \u25b32\nMVGRL 0.344\u00b1 \u25b34 0.239\u00b1 \u25b34 0.244\u00b1 \u25b30 0.141\u00b1 \u25b30\nBGRL 0.668\u00b1 \u25b33 0.547\u00b1 \u25b34 0.484\u00b1 \u25b30 0.295\u00b1 \u25b30\nGCA 0.614\u00b1 \u25b30 0.494\u00b1 \u25b30 0.426\u00b1 \u25b30 0.246\u00b1 \u25b30\nDMoN 0.633\u00b1 \u25b30 - 0.493\u00b1 \u25b30 -\nHomoGCL 0.671\u00b1 \u25b32 0.587\u00b1 \u25b32 0.534\u00b1 \u25b30 0.396\u00b1 \u25b30\nMUX-GCL 0.712 \u00b1\u25b31 0.609 \u00b1\u25b31 0.552 \u00b1\u25b30 0.388\u00b1 \u25b31C. Node Clustering\nWe further credit the performance gain in node clustering on\nPhoto and Computers datasets to our design principles (see Tab. II):\nThe PAE module adheres intra-class nodes and alienates inter-class\nones by assigning affinity scores, while the MPC module compacts\nthe clusters by filtering out inconsistent information. Clusters thus\npreserve more consistency and have better defined boundaries.\nTABLE III: Variants of PAE models (in node classification task)\nPAE method Cora Pubmed Photo\nNode2Vec 85.33 \u00b1 0.37 86.94 \u00b1 0.24 93.73 \u00b1 0.04\nVGAE 85.43 \u00b1 0.21 86.63 \u00b1 0.15 93.89 \u00b1 0.10\nD. Ablation Study\nWe verify the effectiveness of the multiplex contrast mechanism\nand patch affinity estimation by testing the following variants:\n(1)PAE : only conducting same-scale contrast between the output\nembeddings, without engaging in cross-scale contrast.\n(2)MPC : performing a complete cross-scale contrast but refraining\nfrom utilizing patch affinity estimation to identify false nega-\ntives.\n(3)PAE+MPC : the full version of our model\nAs illustrated in Tab. IV, both PAE and MPC contribute to the\nperformance gain, but with the optimal outcome attained when the\ntwo are integrated. This demonstrates that contrasting representations\nacross scales and weighing false negatives both play crucial roles in\npreserving consistency information. We also remark that the results\nobtained by using either Node2Vec or VGAE for patch affinity\nestimation surpass those of existing SOTA models.\nTABLE IV: Ablation study (Acc (%) \u00b1 Std for 5 seeds).\nModel \\Dataset Cora Citeseer Photo\nw/o Both 83.3 \u00b1 0.4 72.1 \u00b1 0.5 92.5 \u00b1 0.2\nPAE 85.1 \u00b1 0.3 73.3 \u00b1 0.2 93.34 \u00b1 0.09\nMPC 84.8 \u00b1 0.4 73.4 \u00b1 0.2 93.8 \u00b1 0.1\nPAE+MPC 85.4 \u00b1 0.2 73.8 \u00b1 0.2 93.9 \u00b1 0.1\nE. Runtime Analysis\nWe compare the training time of MUX-GCL with those of several\nadvanced GCL methods (per epoch), as summarized in Tab. V.\nNotably, compared to the computationally efficient GRACE, MUX-\nGCL improves considerably by increasing the training time only\nmarginally. It is to remark that MUX-GCL is far more efficient than\nHomoGCL that computes saliency every epoch.\nTABLE V: Time per epoch for GCL mehtods (on RTX 3090Ti)\nModel Cora Citeseer Photo Computer\nGRACE 0.20s 0.02s 0.05s 0.12s\nProGCL 0.04s 0.05s 0.17s 0.49s\nHomoGCL 1.09s 0.48s 0.50s 1.32s\nMA-GCL 0.19s 0.02s 0.04s 0.08s\nMUX-GCL 0.04s 0.05s 0.16s 0.42s\nIV. C ONCLUSION\nWe propose MUX-GCL, a novel cross-scale contrastive learning\nparadigm, that grasps richer consistent information by utilizing mul-\ntiplex representations as effective patches. Information contamination\ncaused by conventional ways of constructing larger-scale subgraphs\nis mitigated in this framework. Commensurate to this paradigm, the\nscheme of patch affinity estimation is key to alleviate information\nloss from misjudging negative pairs of patches, which prevails\nin InfoNCE-based GCL methods. Notably, this affinity-informed\nmechanism is applicable to cross-scale contrasts, while all existing\nmethods fail to be. Our approach is strictly proved theoretically and\nconsolidated by its superior performance in downstream classification\nand clustering tasks relative to the SOTA GCL methods.REFERENCES\n[1] T. N. Kipf and M. Welling, \u201cSemi-supervised classifica-\ntion with graph convolutional networks,\u201d arXiv preprint\narXiv:1609.02907 , 2016.\n[2] P. Veli \u02c7ckovi \u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Lio,\nand Y . Bengio, \u201cGraph attention networks,\u201d arXiv preprint\narXiv:1710.10903 , 2017.\n[3] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger,\n\u201cSimplifying graph convolutional networks,\u201d in International\nConference on Machine Learning . PMLR, 2019, pp. 6861\u2013\n6871.\n[4] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful\nare graph neural networks?\u201d arXiv preprint arXiv:1810.00826 ,\n2018.\n[5] B. Xu, H. Shen, Q. Cao, Y . Qiu, and X. Cheng, \u201cGraph wavelet\nneural network,\u201d arXiv preprint arXiv:1904.07785 , 2019.\n[6] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple\nframework for contrastive learning of visual representations,\u201d in\nInternational Conference on Machine Learning . PMLR, 2020,\npp. 1597\u20131607.\n[7] Y . Zhu, Y . Xu, F. Yu, Q. Liu, S. Wu, and L. Wang,\n\u201cDeep graph contrastive representation learning,\u201d arXiv preprint\narXiv:2006.04131 , 2020.\n[8] Y . You, T. Chen, Y . Sui, T. Chen, Z. Wang, and Y . Shen, \u201cGraph\ncontrastive learning with augmentations,\u201d Advances in Neural\nInformation Processing Systems , vol. 33, pp. 5812\u20135823, 2020.\n[9] Y . Zhu, Y . Xu, F. Yu, Q. Liu, S. Wu, and L. Wang, \u201cGraph con-\ntrastive learning with adaptive augmentation,\u201d in Proceedings of\nthe Web Conference 2021 , 2021, pp. 2069\u20132080.\n[10] P. Veli \u02c7ckovi \u00b4c, W. Fedus, W. L. Hamilton, P. Li `o, Y . Ben-\ngio, and R. D. Hjelm, \u201cDeep graph infomax,\u201d arXiv preprint\narXiv:1809.10341 , 2018.\n[11] K. Hassani and A. H. Khasahmadi, \u201cContrastive multi-view\nrepresentation learning on graphs,\u201d in International Conference\non Machine Learning . PMLR, 2020, pp. 4116\u20134126.\n[12] C. Mavromatis and G. Karypis, \u201cGraph infoclust: Maximizing\ncoarse-grain mutual information in graphs,\u201d in Pacific-Asia Con-\nference on Knowledge Discovery and Data Mining . Springer,\n2021, pp. 541\u2013553.\n[13] J. Cao, X. Lin, S. Guo, L. Liu, T. Liu, and B. Wang, \u201cBipartite\ngraph embedding via mutual information maximization,\u201d in\nProceedings of ACM International Conference on Web Search\nand Data Mining , 2021, pp. 635\u2013643.\n[14] C. Wang and Z. Liu, \u201cLearning graph representation by aggre-\ngating subgraphs via mutual information maximization,\u201d arXiv\npreprint arXiv:2103.13125 , 2021.\n[15] J. Xia, L. Wu, G. Wang, J. Chen, and S. Z. Li, \u201cProGCL:\nRethinking hard negative mining in graph contrastive learning,\u201d\narXiv preprint arXiv:2110.02027 , 2021.\n[16] C. Niu, G. Pang, and L. Chen, \u201cAffinity uncertainty-based hard\nnegative mining in graph contrastive learning,\u201d arXiv preprint\narXiv:2301.13340 , 2023.\n[17] W.-Z. Li, C.-D. Wang, H. Xiong, and J.-H. Lai, \u201cHomoGCL:\nRethinking homophily in graph contrastive learning,\u201d arXiv\npreprint arXiv:2306.09614 , 2023.\n[18] A. Grover and J. Leskovec, \u201cnode2vec: Scalable feature learning\nfor networks,\u201d in Proceedings of ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , 2016,\npp. 855\u2013864.\n[19] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201darXiv preprint arXiv:1611.07308 , 2016.\n[20] B. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepWalk: Online\nlearning of social representations,\u201d in Proceedings of ACM\nSIGKDD International Conference on Knowledge Discovery\nand Data Mining , 2014, pp. 701\u2013710.\n[21] Y . Mo, L. Peng, J. Xu, X. Shi, and X. Zhu, \u201cSimple unsuper-\nvised graph representation learning,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 36, no. 7, 2022, pp.\n7797\u20137805.\n[22] S. Thakoor, C. Tallec, M. G. Azar, M. Azabou, E. L. Dyer,\nR. Munos, P. Veli \u02c7ckovi \u00b4c, and M. Valko, \u201cLarge-scale repre-\nsentation learning on graphs via bootstrapping,\u201d arXiv preprint\narXiv:2102.06514 , 2021.\n[23] P. Bielak, T. Kajdanowicz, and N. V . Chawla, \u201cGraph barlow\ntwins: A self-supervised representation learning framework for\ngraphs,\u201d Knowledge-Based Systems , vol. 256, p. 109631, 2022.\n[24] Y . Zhang, H. Zhu, Z. Song, P. Koniusz, and I. King, \u201cCOSTA:\ncovariance-preserving feature augmentation for graph con-\ntrastive learning,\u201d in Proceedings of ACM SIGKDD Conference\non Knowledge Discovery and Data Mining , 2022, pp. 2524\u2013\n2534.\n[25] Y . Zhang, H. Zhu, X. Song, Zi, P. Koniusz, and I. King, \u201cSpec-\ntral feature augmentation for graph contrastive learning and\nbeyond,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence , vol. 37, no. 9, 2023, pp. 11 289\u201311 297.\n[26] X. Gong, C. Yang, and C. Shi, \u201cMA-GCL: Model augmentation\ntricks for graph contrastive learning,\u201d in Proceedings of the\nAAAI Conference on Artificial Intelligence , vol. 37, no. 4, 2023,\npp. 4284\u20134292.\n[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg\net al. , \u201cScikit-learn: Machine learning in python,\u201d Journal of\nMachine Learning Research , vol. 12, pp. 2825\u20132830, 2011.\n[28] W. M. Rand, \u201cObjective criteria for the evaluation of cluster-\ning methods,\u201d Journal of the American Statistical Association ,\nvol. 66, no. 336, pp. 846\u2013850, 1971."}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Comprehensive Summary of \"Privacy-preserving federated prediction of health outcomes using multi-center survey data\"\n\nThis paper aims to present and validate privacy-preserving federated learning techniques for prognostic model building from multi-center survey data. The study demonstrates that federated learning (FL) can achieve performance comparable to centralized methods while maintaining data privacy across different healthcare centers.\n\n#### 1. Abstract\n\n- **Background**: Patient-reported survey data improve healthcare prognostics but cannot easily be centralized due to privacy concerns.\n- **Objective**: To apply federated machine learning techniques to build prognostic models without compromising data privacy.\n- **Methods**: Experiment using GLA:D\u00ae data from Denmark and SHARE data from 27 countries, comparing local, centralized, and federated learning methods.\n- **Results**: Federated models performed statistically significantly better than local models and similarly to centralized models, showcasing the feasibility of FL in multi-center surveys.\n- **Conclusion**: Federated learning enables robust prognostic modeling while preserving data privacy, without significant loss in model performance.\n\n#### 2. Introduction\n\n- **Context**: Patient-reported survey data are vital for assessing medical conditions and training machine learning models for prognostics. Typically, such data are gathered across various local centers and cannot be shared due to privacy and logistic issues.\n- **Problem Statement**: Local data models are limited by reduced accuracy and poor generalizability. Centralized models, while robust, pose significant privacy risks.\n- **Research Question**: Can federated learning provide a solution that retains data privacy while achieving comparable performance to centralized models?\n- **Literature Review**: Discusses privacy regulations prohibiting data sharing, previous studies on decentralized electronic health records like the EU's EHDS, and limitations of data anonymization. Covers federated learning applications in other data types (computer vision, NLP) and speculates its untested potential in survey data.\n- **Positioning**: This study explores the application of federated learning to patient-reported survey data, a previously unexplored area with unique challenges given the ordinal and nominal nature of survey data.\n\n#### 3. Related Work\n\n- **Prior Work**: Reviews how federated learning has been applied to genomic, biochemical, and radiographic imaging data, but acknowledges a gap in survey data application.\n- **Advancement**: The paper advances the application of federated learning to survey data, proving its efficacy in a privacy-preserving manner while ensuring model performance remains robust and generalizable.\n\n#### 4. Method\n\n- **Data & Preprocessing**:\n  - **GLA:D\u00ae Data**: Collected from five Danish regions (preprocessed as per Baumbach et al.), involving 9,648 patients with knee/hip osteoarthritis.\n  - **SHARE Data**: From 27 European countries, 46,570 participants, predicting physical inactivity using 30 selected variables.\n  \n- **Model Training Techniques**:\n  - **Local Models**: Using linear regression, random forest regression for GLA:D\u00ae, and random forest classification for SHARE.\n  - **Centralized Models**: Combined all regional or national datasets into one and trained single global models.\n  - **Federated Models**: Kept data local, shared model parameters, and aggregated these parameters centrally.\n\n- **Federated Learning Technique**: Utilizes local training on clients' data, sends model parameters to a central aggregator, performs global aggregation using weighted averaging, and repeats until convergence.\n\n- **Implementation Details**:\n  - **Linear Regression**: Exchanging slopes and intercepts for less communication overhead.\n  - **Random Forest Models**: Aggregated estimators or trees to form a collective model iteratively.\n\n#### 5. Experimental Setup\n\n- **Datasets**:\n  - **GLA:D\u00ae**: Data split by the five Danish health regions.\n  - **SHARE**: Data split by 27 European countries.\n  \n- **Hardware and Software**: Utilized the Feature Cloud platform for federated learning, with algorithms implemented in Python using the sci-kit-learn library.\n\n- **Evaluation Metrics**:\n  - **Performance Metrics**: R-squared, RMSE, accuracy, and AUROC.\n  - **Cross-Validation**: 5-fold cross-validation for robust evaluation.\n  \n- **Baseline Comparisons**: Compared federated models against local and centralized models using identical datasets.\n\n#### 6. Results and Analysis\n\n- **Performance Comparison**:\n  - **GLA:D\u00ae Data**:\n    - Federated linear regression: R\u00b2: 0.34, RMSE: 18.2.\n    - Local models: R\u00b2: 0.32, RMSE: 18.6.\n    - Centralized models showed marginally better but not significantly outperforming federated models.\n  - **SHARE Data**: Federated models showed improved AUROC over centralized models, suggesting robustness and less overfitting.\n\n- **Statistical Tests**: Federated learning models statistically outperformed local models across different metrics, e.g., R\u00b2 and RMSE. Centralized models did not significantly outperform federated ones.\n  \n- **Figures**: Included comparative box plots demonstrating the advantage of federated models in various metrics.\n\n#### 7. Discussion\n\n- **Interpretation**: Federated learning models offer strong performance benefits, especially in privacy-preserving contexts.\n- **Implications**: Federated learning can lead to better generalization and performance by pooling local data insights without compromising privacy.\n- **Limitations**: More computationally intense than local models due to iterative communication steps.\n\n#### 8. Conclusion\n\n- **Key Contributions**: Demonstrated the feasibility and effectiveness of federated learning for multi-center survey data. Provided a privacy-preserving alternative to centralized models without significant performance loss.\n- **Future Research**: Encouraged further exploration in other types of medical survey data, more sophisticated federated learning algorithms, and optimization of communication protocols to reduce overheads.\n\n### Technical Execution for Replication\n\n- **Datasets**: Use GLA:D\u00ae data from Denmark\u2019s five regions and SHARE wave-8 data from 27 countries.\n- **Software**: Implement using Python\u2019s sci-kit-learn and the Feature Cloud platform for federated setups.\n- **Preprocessing**: Follow the preprocessing scripts provided in the paper's GitHub repository.\n- **Model Training**:\n  - Initialize local models for each data center.\n  - For federated learning, train locally, share parameters, aggregate centrally, and iterate till convergence.\n- **Evaluation**:\n  - Perform 5-fold cross-validation.\n  - Evaluate using R-squared, RMSE for regression, and accuracy, AUROC for classification.\n- **Statistical Testing**: Use Shapiro\u2019s test for normality, Bartlett\u2019s test for homoscedasticity, followed by t-tests or Welch t-tests as appropriate.\n\n### Note:\nKey supplementary tables, figures for data and results, and exact preprocessing codes are available in the GitHub repository mentioned in the original paper, ensuring reproducibility of the experiments."}, {"role": "assistant", "content": "\tPrivacy-preserving\tfederated\tprediction\tof\thealth\toutcomes\tusing\tmulti-center\tsurvey\tdata\tSupratim\tDasa,b,\tMahdie\tRafiea,b,\tPaula\tKammera,\tS\u00f8ren\tT.\tSkouc,d,\tDorte\tT.\tGr\u00f8nnec,d,\tEwa\tM.\tRoosc,\tAndr\u00e9\tHajekb,\tHans-Helmut\tK\u00f6nigb,\tMd\tShihab\tUllaha,\tNiklas\tProbula,\tJan\tBaumbacha,e,\tLinda\tBaumbachb\taInstitute\tfor\tComputational\tSystems\tBiology,\tUniversity\tof\tHamburg,\tAlbert-Einstein-Ring\t8-10,\t22761,\tHamburg,\tGermany\t\tbDepartment\tof\tHealth\tEconomics\tand\tHealth\tServices\tResearch,\tUniversity\tMedical\tCenter\tHamburg-Eppendorf,\tMartinistra\u00dfe\t52\tHamburg,\t20246,\tGermany\tc\tCenter\tfor\tMuscle\tand\tJoint\tHealth,\tDepartment\tof\tSports\tScience\tand\tClinical\tBiomechanics,\tUniversity\tof\tSouthern\tDenmark,\tCampusvej\t55,\t5230\tOdense,\tDenmark\td\tThe\tResearch\tand\tImplementation\tUnit\tPROgrez,\tDepartment\tof\tPhysiotherapy\tand\tOccupational\tTherapy,\tN\u00e6stved-Slagelse-Ringsted\tHospitals,\tF\u00e6lledvej\t2C,\t4200\tSlagelse,\tDenmark\te\tComputational\tBiomedicine\tLab,\tDepartment\tof\tMathematics\tand\tComputer\tScience,\tUniversity\tof\tSouthern\tDenmark,\tOdense,\tDenmark\t\tAbstract:\t\tBackground:\tPatient-reported\tsurvey\tdata\tare\tused\tto\ttrain\tprognostic\tmodels\taimed\tat\timproving\thealthcare.\tHowever,\tsuch\tdata\tare\ttypically\tavailable\tmulti-centric\tand,\tfor\tprivacy\treasons,\tcannot\teasily\tbe\tcentralized\tin\tone\t data\t repository.\t Models\t trained\t locally\t are\t less\t accurate,\t robust,\t and\t generalizable.\t We\t present\t and\t apply\tprivacy-preserving\tfederated\tmachine\tlearning\ttechniques\tfor\tprognostic\tmodel\tbuilding,\twhere\tlocal\tsurvey\tdata\tnever\tleaves\tthe\tlegally\tsafe\tharbors\tof\tthe\tmedical\tcenters.\t\tMethods:\tWe\tused\tcentralized,\tlocal,\tand\tfederated\tlearning\ttechniques\ton\ttwo\thealthcare\tdatasets\t(GLA:D\u00ae\tdata\tfrom\tthe\tfive\thealth\tregions\tof\tDenmark\tand\tinternational\tSHARE\tdata\tof\t27\tcountries)\tto\tpredict\ttwo\tdifferent\thealth\t outcomes.\t We\t compared\t linear\t regression,\t random\t forest\t regression,\t and\t random\t forest\t classification\tmodels\ttrained\ton\tlocal\tdata\twith\tthose\ttrained\ton\tthe\tentire\tdata\tin\ta\tcentralized\tand\tin\ta\tfederated\tfashion.\t\tResults:\tIn\tGLA:D\u00ae\tdata,\tfederated\tlinear\tregression\t(R2\t0.34,\tRMSE\t18.2)\tand\tfederated\trandom\tforest\tregression\t(R2\t0.34,\tRMSE\t18.3)\tmodels\toutperform\ttheir\tlocal\tcounterparts\t(i.e.,\tR2\t0.32,\tRMSE\t18.6,\tR2\t0.30,\tRMSE\t18.8)\twith\t statistical\t significance.\t We\t also\t found\t that\t centralized\t models\t (R2\t 0.34,\t RMSE\t 18.2,\t R2\t 0.32,\t RMSE\t 18.5,\trespectively)\tdid\tnot\tperform\tsignificantly\tbetter\tthan\tthe\tfederated\tmodels.\tIn\tSHARE,\tthe\tfederated\tmodel\t(AC\t0.78,\tAUROC:\t0.71)\tand\tcentralized\tmodel\t(AC\t0.84,\tAUROC:\t0.66)\tperform\tsignificantly\tbetter\tthan\tthe\tlocal\tmodels\t(AC:\t0.74,\tAUROC:\t0.69).\t\tConclusion:\tFederated\t learning\t enables\t the\t training\t of\t prognostic\t models\t from\t multi-center\t surveys\t without\tcompromising\tprivacy\tand\twith\tonly\tminimal\tor\tno\tcompromise\tregarding\tmodel\tperformance.\t\t\t\t\t\t\t\t\t\t\n2\t1.\tIntroduction Patient-reported\tsurvey\tdata\thas\temerged\tas\ta\tvaluable\ttool\tfor\tassessing\tmedical\tconditions\tin\tclinical\tsettings,\tserving\tas\ta\tgold\tstandard\tin\tvarious\tfields1\u20134.\tPatient-reported\tsurvey\tdata\tand\tobjective\tfunctional\ttests\thave\tbeen\tinstrumental\t in\t developing\t prognostic\t models\t using\t machine\t learning\t techniques5,6.\t Beyond\t its\t applications\t in\tpersonalized\tmedicine,\tpatient-reported\tsurvey\tdata\tis\talso\tutilized\tin\thealth\teconomics\tresearch.\t\tClinical\t survey\t data\t is\t collected\t and\t stored\t across\t local\t hospitals\t and\t practices\t and\t sometimes\t across\t regions,\tstates,\tor\tcountries.\tLocal\tdata\tfrom\ta\tsingle\tsilo\tis\tby\tnature\tsmaller\tand\thence\tleads\tto\treduced\taccuracy\tand\tgeneralizability\tof\tprognostic\tstatistical\tmodels\tas\tcompared\tto\tmodels\ttrained\ton\tlarger\tdata\tsets\tintegrated\tfrom\tseveral\tclinics,\tregions,\tor\tcountries7,8.\tThe\tprime\treasons\tfor\tdata\tdecentralization\tare\tdata\tsecurity\tand\tprivacy\tpolicies.\tHealthcare\tdata\texchange\tacross\tcountry\tborders\tis\tparticularly\tchallenging9.\tHence,\treal-world\tclinical\tsurvey\tdata\tis\tscarce\tand\trarely\tavailable\tfor\tmachine\tlearning.\tIn\trecent\tyears,\tthe\tidea\tof\ta\tdecentralized\telectronic\thealth\trecord\trepository\tnetwork\tacross\tEurope\thas\tbeen\tsuggested\t(but\tnot\tyet\taddressed\tor\timplemented)\tin\tthe\tframework\tof\tthe\tEuropean\tUnion\u2019s\tproposal\tof\ta\tEuropean\tHealth\tData\tSpace\t(EHDS).10\t\t1.1.\tPrivacy\tregulations\tDifferent\tprivacy\tregulations\taround\tthe\tworld11\u201315\teffectively\tprohibit\tcompanies\tand\tresearchers\tfrom\tdirectly\tsharing\t personal\t patient\t data.\t Despite\t patient\t ID\t anonymization\t attempts16,\t recent\t studies\t demonstrate\t how\tvarious\treidentification\ttechniques\tmay\tcompromise\tpatient\tprivacy17,18.\tIn\ta\trecent\tpaper\tfrom\t2022,\tJill\tEvans\tet\tal.\thave\tpointed\tout\tthe\tchallenges\tin\tdirectly\tsharing\tosteoarthritis\tdata;\tbesides\tprivacy,\tthey\tmentioned\tlogistical\tand\tstructural\tbarriers,\tsuch\tas\tthe\tlimitation\tof\tresources\tfor\tdata\tgovernance19.\t1.2.\tConsequence\tof\tscattered\tdata\tsilos\tin\tmedicine\tAs\t a\t result,\t researchers\t are\t often\t constrained\t to\t train\t prognostic\t models\t solely\t based\t on\t local\t data,\t leading\t to\tsuboptimal\taccuracy\tand\tpotentially\tincorrect\tpredictions\t(see\tFigure\t1\tfor\tan\tillustration).\tIn\tparticular,\tas\tlocal\tdata,\tin\taddition\tto\tits\tsmaller\tsize,\tpossesses\tsmaller\tsample\tdiversity,\tthe\tresulting\tprognostic\tmachine\tlearning\tmodels\tsuffer\tfrom\treduced\tgeneralizability\tand\tlead\tto\tincreased\tdiscrimination\tof\tunderrepresented\tminorities.\tAlthough\ta\tmodel\ttrained\ton\tdata\tfrom\tone\tclinical\tsite\tmight\tlearn\tto\tpredict\twith\thigh\taccuracy\tfor\tthat\tparticular\tclinic,\tthe\tmodel\tmight\tlearn\tfrom\tsome\tclinical\tsite-specific\tinformation\tirrelevant\tto\tthe\tdisease\tin\tgeneral7,8.\tEmploying\tdata\tfrom\tmultiple\thospitals\tfor\tmodel\ttraining\tinstead\tof\trelying\tsolely\ton\tmore\thomogeneous\tsingle-clinical\tdata\thas\tbeen\tdemonstrated\tto\treduce\tmodel\tbias.\tConsequently,\tit\tleads\tto\timproved\tperformance\tacross\tthe\twhole\ttarget\tpopulation7,20.\tBesides\tthis,\teven\tin\tscenarios\twhere\tdata\tcentralization\tis\tpossible,\tresearchers\tneed\tto\tgo\tthrough\tmany\tlegal\tprocedures\tand\twould\tneed\tto\ttake\tcare\tof\tdata\tstorage\tlogistics19;\tfederated\tlearning\tcut\tthrough\tthese\tprocedures,\tmaking\tdata\tfrom\tseveral\tlocations\teasy\tto\tuse.\t1.3.\tFederated\tlearning\tTo\tcircumvent\tlegal\tand\tethical\tdata-sharing\tissues\twhile\tallowing\tfor\tthe\ttraining\tof\trobust\tmodels\ton\tbig\tdata,\tresearchers\thave\tdeveloped\tfederated\tlearning21,22\tover\tthe\tlast\tfew\tyears.\tIn\tfederated\tlearning,\traw\tdata\tcan\tstay\tin\tthe\tlegally\tsafe\tharbors\tof\tthe\tlocal\tdata\tcenters.\tInstead,\tone\ttrains\ta\tjoint\tmodel\tusing\tinference\tfrom\tall\tdata\tdistributed\tacross\tthe\tdifferent\thospitals\t(see\tFigure\t1\tfor\tan\tillustration\tof\tthe\tconcept).\tFor\tinstance,\tfederated\tlearning\thas\tbeen\tused\tin\tcomputer\tvision23\tand\tnatural\tlanguage\tprocessing24.\tThe\tefficacy\tof\tfederated\tlearning\thas\t been\t studied\t on\t image\t data23,\t multi-omics\t data25,26,\t and\t other\t biomedically\t relevant\t data\t types22,27,28.\t The\tapplicability\t of\t a\t federated\t model\t in\t such\t data\t is\t often\t determined\t based\t on\t its\t ability\t to\t outperform\t local\tmodels29,30\twhile\t maintaining\t a\t close\t resemblance\t to\t centralized\t models29\u201331.\t However,\t its\t potential\t and\teffectiveness\tfor\tdecentralized\tprofiling\tof\tpatient-reported\tsurvey\tdata\thave\tbeen\tunexplored\thitherto.\tThe\tnature\tof\tsurvey\tdata\tdiffers\tsignificantly\tfrom\tdata\tobtained\tthrough\tgenomic\tanalyses,\tstandard\tbiochemical\tassays,\tor\tradiographic\timaging.\tUnlike\tthe\tcontinuous\tdata\ttypically\tencountered\tin\tthese\tdomains,\tsurvey\tdata\tis\toften\tin\tthe\t form\t of\t ordinal\t or\t nominal\t data.\t While\t ordinal\t data\t may\t be\t approximated\t as\t continuous\t under\t certain\tcircumstances,\tit\tdeviates\tfrom\tthe\tcharacteristics\tof\ttrue\tcontinuous\tdata32.\tHere,\twe\taim\tto\tfill\tthis\tgap\tby\tinvestigating\tthe\tapplicability\tof\tfederated\tlearning\tto\tpatient-reported\tsurvey\tdata.\tFirst,\twe\tpredicted\tthe\tdevelopment\tof\tself-reported\tpain\tin\tpatients\twith\tosteoarthritis\tusing\tdata\tfrom\tall\tfive\tregions\tof\tDenmark,\tbased\ton\tprevious\twork\tutilizing\tlinear\tand\trandom\tforest\tregression\tmodels5.\tSecond,\twe\tuse\trandom\tforest\tclassification\tmodels\tto\tpredict\tphysical\tinactivity\tusing\tdata\tfrom\t27\tEuropean\tcountries.\t\tWe\tanticipate\tthat\tdespite\tthe\tdistinct\tnature\tof\tour\tdataset,\tfederated\tlearning,\tconsistent\twith\tprevious\tstudies,\texhibits\t superior\t accuracy\t compared\t to\t local\t models\t across\t all\t scenarios33.\t Similarly,\t based\t on\t findings\t from\t3\texisting\tstudies,\twe\tanticipate\tcomparable\tperformance\tto\tthe\tcentralized\tcounterparts33,34,\tfor\tthe\tfederated\tlinear\tregression,\trandom\tforest\tregression,\tand\trandom\tforest\tclassification.\t\t\n\tFigure\t1:\tComparison\tof\ttraditional\tmachine\tlearning\tand\tfederated\tlearning:\tOn\tthe\ttop\thalf\tin\tthe\tazure\tbackground,\twe\trepresent\tthe\tcurrent\tstate\tof\tthe\tart\tof\tmodel\tpredictions.\tIn\ttraditional\tmachine\tlearning,\tall\tdata\tfrom\tdata\tcenters\tc1,\tc2,\tand\tc3\tmust\tbe\tintegrated\tinto\tone\tserver.\tDue\tto\tprivacy\tissues\tand\tother\tlogistic\tissues,\tintegrating\tdata\tfrom\tall\tlocal\tdata\tcenters\tto\tone\tcentral\tdata\tsite\tmay\tnot\tbe\tpossible.\tHence,\tthese\ttraditional\tmachine\tlearning\tstudies\tare\toften\tlimited\tto\ttraining\tindividual\tlocal\tmodels\ton\tlimited\tdata.\tThese\tmodels\thave\tsuboptimal\tperformance\tas\tthey\tfail\tto\tgeneralize\tin\ta\tglobal\tscenario.\tIn\tthe\tbottom\tpart,\tin\tthe\tivory\tbackground,\twe\tdepict\tour\tproposed\tmodel\tof\tfederated\tlearning.\tHere,\tthe\traw\tdata\tnever\tleaves\ttheir\tlocal\tservers\tc1,\tc2,\tand\tc3,\tand\tthe\tnecessity\tof\tdata\tintegration\tin\tone\tdata\tcenter\tis\tmitigated.\tEach\tdata\tcenter\ttrains\ta\tlocal\tmodel;\tonly\tmodel-specific\tparameters\tare\tcommunicated\tduring\ttraining.\tAs\tthe\tfederated\tmachine\tlearning\tmodel\tis\ttrained\tusing\tinsights\tfrom\tall\tdatasets,\tit\tcan\tgeneralize\tbetter\tand\tpotentially\tgive\tbetter\tperformance\tthan\ta\tlocal\tmodel.\t2.\tMethod\t2.1.\tData\tand\tPreprocessing\tFor\t the\t first\t task,\t we\t utilized\t data\t from\t the\tGLA:D\u00ae\t (Good\t Life\t with\t osteoArthritis\t in\t Denmark)\t registry\t35,36.\tNational\tGLA:D\u00ae\tdatabases\texist\tin\tAustralia,\tAustria,\tCanada,\tChina,\tGermany,\tIreland,\tNetherlands,\tNew\tZealand,\tand\tSwitzerland35\u201338.\tThe\tGLA:D\u00ae\tinitiative\tconsists\tof\tthree\tparts:\ta\tcertification\tcourse\tfor\tphysiotherapists,\ta\tstandardized\tand\tsupervised\tpatient\teducation\tand\tan\texercise\ttherapy\tprogram,\tand\ta\tregistry\twith\tdata\tfrom\tbefore,\t immediately\t after,\t and\t 12\t months\t after\t program\t initiation35\u201338.\tGLA:D\u00ae\taims\t to\t facilitate\t the\timplementation\tof\trecommended\tfirst-line\tpain\tmanagement\t(exercise\ttherapy\tand\tpatient\teducation)\tfor\tpatients\twith\thip\tand\tknee\tosteoarthritis\tin\tclinical\tpractice39.\t\tWe\tutilized\tDanish\tdata\tfrom\tthe\tbeginning\tof\t2013\tto\tthe\tend\tof\t2018.\tWe\tused\tthe\tsame\tpreprocessing\tdescribed\tin\ta\tpaper\ton\tindividualized\tpredictions\tof\tchanges\tin\tknee\tpain\tfor\tpatients\twith\tknee\tosteoarthritis\tusing\tthe\tsame\tdata\tsource5,\twhich\tincludes\ta\tselection\tof\tthe\tsame\t51\tvariables\tand\tdropping\tof\tpatient\tdata\tsamples\twith\tmissing\tvalues.\tThus,\twe\tonly\tincluded\tcomplete\tcases.\tTo\tsimulate\ta\trealistic\tfederated\tlearning\tscenario,\twe\tthen\tsplit\tthe\tdata\taccording\tto\tthe\tfive\tDanish\tadministrative\thealth\tcare\tregions\t(North\tDenmark,\tCentral\tDenmark,\tSouthern\tDenmark,\tZealand\tregion,\tand\tthe\tCapital\tRegion\tof\tDenmark)\tusing\tthe\tgeographical\tlocation\twhere\tthe\t(n=274)\tGLA:D\u00ae\tclinics\tare\tlocated38.\tFor\tthis,\twe\tcall\tthis\ttask\ta\tnational\tscenario.\t\tWe\ttrained\tstatistical\tmodels\tto\tpredict\tchanges\tin\tknee\tpain\tintensity\tover\tthe\tperiod\tof\tapproximately\tthree\tmonths\tfrom\tbefore\tto\tafter\tthe\tGLA:D\u00ae\tprogram.\tWe\tutilized\tthe\tsame\t51\tvariables\tas\tBaumbach\tet\tal.5\tsuggested.\tA\tdetailed\tdescription\tof\tthese\t51\tvariables,\twhich\tprovides\tinformation\tabout\tthe\tpatient's\tdemographics\tand\thealth\tstatus,\tis\tprovided\tin\tsupplementary\ttable\tB1.\tKnee/hip\tpain\tintensity\twas\tmeasured\ton\ta\tVAS\tscale\tof\t0\tto\t\n4\t100\tmm,\tbest\tto\tworst.\tHence,\tthe\tpossible\tvalues\tfor\tpain\tchange\tare\tbetween\t-100\tand\t100.\tThe\tscript\tused\tfor\tpreprocessing\tcan\tbe\taccessed\tin\tour\tGitHub\trepository40.\t\tFor\t the\t second\t task,\t we\t used\t the\t data\t from\t wave-8\t (released\t in\t 2022)\t of\t the\t Survey\t of\t Health,\t Ageing,\t and\tRetirement\tin\tEurope\t(SHARE)\tdatabase41,42\tto\tpredict\tphysical\tinactivity.\tWe\tchose\tthis\tdataset\tbecause\tof\tthe\thigh\theterogeneity\tin\tphysical\tactivity\tacross\t27\tcountries\t43.\tHence,\twe\texpected\tthis\tto\timpose\tgreater\tchallenges\ton\tthe\t federated\t model.\t We\t calculated\t our\t outcome\t variable\t physical\t inactivity\t (yes/no)\t from\t two\t questions\t as\tsuggested\tby\tGomes\tet\tal.\t43.\tAfterwards,\twe\tselected\t30\trelevant\tvariables,\tincluding\tinformation\ton\tmental\thealth\t(such\tas\tEURO-D\tmeasure\tof\tdepressive\tsymptom44,45),\tcognitive\tabilities,\tmovement\tlimitations,\tself-perceived\thealth,\twhich\tare\tshown\tto\tbe\trelevant\tfactors\tin\tthe\tliterature43\t[see\tsupplementary\tTable\t3.]\tTo\tsimulate\tfederated\tlearning\t across\t multiple\t countries,\t we\t split\t the\t data\t into\t 27\t countries.\t A\t full\t list\t of\t these\t variables\t and\t data\tpreprocessing\tcodes\tcan\tbe\tfound\tin\tour\tGitHub\trepository40.\t\t\n\tFigure\t2:\tRepresentation\tof\tthe\tevaluation\tscheme\tfor\tFederated\tLearning,\texemplarily\tdepicted\tfor\tfive\tregions\tof\tGLA:D\u00ae\tDenmark\u2019s\tdata:\tTo\tcompare\twith\ta\tcentralized\tscenario,\twe\tsimulate\ta\tscenario\tof\tdistributed\tand\tcentralized\tdata\tcenters\tusing\tDanish\tGLA:D\u00ae\tdata.\tWe\ttrained\tfive\tlocal\tmodels\twith\teach\tregion\u2019s\tlocal\tdata,\tone\tfederated\tmodel\twith\tdistributed\tdata,\tand\tone\tcentralized\tmodel\twith\tintegrated\tdata.\tWe\tevaluated\tthis\tprotocol\tin\tVAS\tpain\tprediction\tfrom\t51\tvariables\tusing\tlinear\tregression\tand\trandom\tforest\tregression\tas\tour\tmachine-learning\tmodels\tof\tchoice.\tWe\tcompared\tthe\taverage\tof\t5\tlocal\tmodels\tand\ta\tcentralized\tscenario\twith\tthe\tfederated\tmodel\tfor\tboth\tmachine\tlearning\talgorithms\t(linear\tregression\tand\trandom\tforest).\tA\tsimilar\tscheme\twas\texecuted\twith\tthe\tSHARE\tdata\tbut\tfor\t27\tcountries\tusing\trandom\tforest\tclassification.\t\t2.1.1.\tLocal\tvs.\tcentralized\tvs.\tfederated\tmachine\tlearning\tWe\ttrained\tthe\tmachine\tlearning\talgorithms\tfor\tboth\ttasks\tin\tthree\tdifferent\tscenarios:\t1.\tLocal\tmodels:\tFor\ttask\tone,\t\tfive\tseparate\tmodels\twere\ttrained\tin\tisolation\ton\tthe\tlocal\tGLA:D\u00ae\tdata\tof\tthe\tfive\tregions\tof\tDenmark.\tSimilarly,\tfor\ttask\ttwo,\t27\tseparate\tmodels\twere\ttrained\tin\tisolation\ton\tlocal\tdata\tfrom\tthe\t27\tcountries\tparticipating\tin\tSHARE\t(wave-8).\tIn\ttask\ttwo\tfor\tthe\tclassification\ttask,\tbefore\ttraining\tthe\tmodel\tfor\teach\tcountry,\tthe\tdata\twas\tbalanced\t(same\tnumber\tof\tpositive\tand\tnegative\tsamples)\tby\tsupersampling\t(sampling\twith\treplacement\tfrom\tthe\tsmaller\tclass).\t2.\tCentralized\tmodel:\tFor\ttask\tone,\tlocal\tGLA:D\u00ae\tdata\tof\tthe\tfive\ttraining\tsets\twere\tmerged\tand\tused\ttogether\tto\ttrain\tone\tcentralized\tmodel\twhile\thaving\taccess\tto\tthe\tfull\tdata.\tFor\ttask\ttwo,\tcountry-wise\tsupersampled\tbalanced\ttraining\tdata\tsets\tof\tSHARE\tdata\twere\tcombined\tto\tcreate\tthe\tinternational,\tcentralized\ttraining\tset.\t\t3.\tFederated\tmodel:\tFor\tboth\ttasks\twe\ttrained\tthe\tfederated\tmodel\tusing\tthe\tsame\tdata\tused\tfor\tlocal\ttraining\tbut\twhile\tkeeping\tthe\tregional/national\tsets\tseparated\t(see\tFigure\t2).\t\n5\tFinally,\twe\ttrained\tlinear\tregression,\trandom\tforest\tregression,\tand\trandom\tforest\tclassification\tmodels\tfor\teach\tof\tthe\tthree\tscenarios\tand\tevaluated\ttheir\tperformances.\t\t2.2.\tTraining\tand\ttest\tdata\tgeneration\tFor\tboth\ttasks,\twe\tperformed\t5-fold\tcross-validation,\twhich\tgives\tus\tan\t80%\tsample\tfor\ttraining\tand\ta\t20%\tsample\tfor\ttesting\tthe\tmachine\tlearning\tmodels\tin\teach\tfold\t(see\tSupplementary\tFigure\tA.2).\tFor\ttask\tone,\tfor\teach\tfold,\tduring\t training,\t the\t federated\t and\t local\t models\t utilized\t the\t train\t split\t of\t their\t local\t data,\t while\t for\t centralized\ttraining,\tthe\tsame\ttraining\tsplits\tare\tconcatenated\tto\tform\ta\tcentralized\ttraining\tset.\tIn\teach\tfold,\twe\tused\tthe\tglobal\ttest\tset\tto\tevaluate\tthe\tperformances\tof\tlocal,\tcentralized,\tand\tfederated\tmodels.\tWe\ttested\tall\tscenarios\ton\tthe\tsame\tglobal\ttest\tset\t(see\tSupplementary\tFigure\tA.2).\t\tSimilarly,\tfor\ttask\ttwo,\twe\tsplit\tthe\tdata\taccording\tto\t27\tcountries,\tand\tin\teach\tcountry,\twe\tmade\ta\ttraining\ttest\tsplit\tin\teach\tfold\tof\tcross-validation.\tFor\tthe\ttraining\tsplit,\twe\tbalanced\tthe\tdataset\tby\tsupersampling.\tFor\tevaluation,\twe\tfirst\tmerged\tthe\tlocal\ttest\tsets\tinto\tcentralized\tones;\tthen,\twe\tcreated\t4\tsubsampled\tbalanced\tsubsets\twhere\tthe\tsample\tof\tthe\tlarger\tclass\tis\tmutually\texclusive\tin\tthe\t4\tsubsets.\tWe\tdo\tthis\tfor\tfair\tevaluation\ton\ta\tbalanced\ttest\tset.\t\t2.3.\tMachine\tLearning\tMethods\t2.3.1.\tScenario\t1:\tLocal\tmodels\tFor\tthe\tlocal\tmodels,\twe\tused\tlinear\tregression,\trandom\tforest\tregression,\tand\trandom\tforest\tclassification\tmodels\ttrained\ton\ta\ttraining\tsplit\tof\tthe\trespective\tlocal\tdata\tof\teach\tof\tthe\tregions\t(for\ttask\tone)\tor\tcountries\t(for\ttask\ttwo).\tThus,\twe\thave\tfive\tseparate\tmodels\tfor\teach\talgorithm\tin\teach\tfold.\tFor\tall\tlocal\tmodels,\twe\tutilized\tthe\tsci-kit-learn\tlibrary46\tin\tPython.\t\t\t2.3.2.\tScenario\t2:\tCentralised\tmodel\tFor\tthe\tcentralized\tmodels,\twe\tused\tthe\tsame\tsci-kit-learn\tlibrary\tin\tPython\tto\ttrain\tlinear\tregression,\trandom\tforest\tregression,\tand\trandom\tforest\tclassification\tmodels.\tHere,\tfor\tevery\tmachine\tlearning\tmodel,\tonly\tone\tmodel\tis\ttrained\tin\teach\tfold,\twhich\tutilizes\tthe\tintegrated\ttraining\tdata\t(see\tSupplementary\tFigure\tA.2).\t\t\t2.3.3.\tScenario\t3:\tFederated\tmodel\tIn\tfederated\tlearning,\tinstead\tof\tsharing\traw\tdata,\teach\tdata\tcenter\tor\tclinic,\talso\treferred\tto\tas\t\u201cclient,\u201d\tlocally\tperforms\t computations\t on\t their\t local\t data\t and\t only\t shares\t model\t parameters\t with\t an\t aggregator9,21\u201323,33.\t The\taggregator\tthen\tcombines\tall\tthose\tparameters\tand\tsends\tthem\tback\tto\teach\tclient.\tNow,\teach\tclient\tupdates\ttheir\tmachine\t learning\t model\t using\t these\t values.\t In\t iterative\t training\t schemes\t (like\t random\t forest\t classification,\t for\tinstance),\tthese\ttwo\tsteps\tare\titerated\tuntil\tthe\tmodel\tconverges.\tSee\tSupplementary\tFigure\tA.1\tfor\tan\tillustration.\tFederated\tlearning,\tin\tbrief,\tworks\tas\tfollows:\t1. Initialization:\tAll\tclients\tare\tinitialized\twith\tthe\tsame\tmodel\tarchitecture;\tthis\tmight\tbe\tcreated\tfrom\ta\tlocal\tconfiguration\tfile\tor\treceived\tfrom\tthe\tcoordinator,\twhich\tcan\tbe\tan\texternal\tor\tone\tof\tthe\tparticipating\tclients.\tThe\tcoordinator\tcan\tbe\tthe\tsame\tas\tthe\taggregator.\t2. Local\ttraining:\tAt\tthis\tstage,\teach\tlocal\tclient\ttrains\ttheir\tlocal\tmodel\tusing\tlocal\tdata.\tFor\tan\titerative\tmodel,\tthese\tlocal\tmodels\tare\ttrained\tfor\ts\tlocal\tgradient\tdescent\tsteps\tbefore\tsending\tparameters\tfor\tthe\tcommunication\tstep.\t3. Communication:\tIn\tthis\tstep,\teach\tclient\tsends\tlocal\tparameters\tor\tlocal\tgradients\tto\tthe\taggregator.\t4. Global\taggregation:\tThe\taggregator\tcombines\tall\tthe\treceived\tlocal\tparameters\tfrom\tthe\tclients\tand\tupdates\tthe\tshared\tmodel.\tThis\tcan\tbe\tperformed\tusing\tsimple\tweighted\taveraging.\t5. Convergence:\tSteps\t 2-4\t are\t repeated\t until\t preset\t convergence\t criteria\t are\t met.\t The\t federated\t random\tforest\tclassifier\tis\timplemented\tas\tan\titerative\tmodel.\t6. Model\tevaluation:\tAfter\ttraining\ttermination,\teach\tclient\tsaves\tthe\tmodel,\tperforms\tprediction\ton\tthe\tlocal\ttest\tset,\tand\toutputs\tlocal\tresults\tfor\tevaluation.\tFor\t federated\t learning,\t we\t used\t the\t feature\t cloud\t platform33,\t which\t provides\t users\t with\t an\t interactive\t user\tinterface\tto\trun\tcommonly\tused\tmachine\tlearning\talgorithms\tin\ta\tfederated\tfashion.\t\tWhile\tfederated\tlinear\tregression\tcan\tbe\timplemented\tby\tsharing\tX\u22a4\u25e6\tX\tand\tX\u22a4\u25e6\tY\t47\twhere\tX\tis\tan\tn,m\tmatrix\tof\tn\tsamples\t and\tm\tfeatures\t and\tY\tis\t the\t target\t variable\t vector\t (which\t comes\t with\t considerable\t communication\t6\toverhead),\twe\texchange\tthe\tslopes\tand\tintercepts48\tand\tcalculate\tthe\tweighted\taverage\tof\tthose.\tFor\tthe\tfederated\trandom\tforest\tregression\tmodels,\tlocal\trandom\tforest\tregressor\tmodels\twere\tfirst\ttrained,\tand\tthe\testimators\tof\trandom\tforest\tor\tdecision\ttrees\twere\tgathered\tto\tcreate\ta\tnew\trandom\tforest\tmodel\tin\tthe\taggregation\tstage\t33.\tUnlike\t federated\t random\t forest\t regression,\t the\t random\t forest\t classification\t model\t is\t designed\t as\t an\t iterative,\thistogram-based\t model.\t To\t generate\t this\t model,\t a\t predetermined\t amount\t of\t identical\t threshold\t points\t are\tevaluated\t for\t each\t client,\t selected\t feature,\t and\t decision\t tree\t node.\t Doing\t this\t guarantees\t finding\t the\t optimal\tthreshold\tpoint\tfor\tan\taggregated\tmodel\tin\ta\tprivacy-preserving\tfashion49.\tThis\tprocess\tis\trepeated\tfor\teach\ttree\tto\tgenerate\ta\tglobally\toptimal\tmodel.\tDetailed\tinformation\ton\tthe\tutilized\tFeatureCloud\tapp\tversions\tand\tthe\tapplied\tconfiguration\tfiles\tcan\tbe\tfound\tin\tour\tGitHub\trepository40.\t2.3.\tEvaluation\tFor\tall\tthe\tmodels,\twe\tfirst\tevaluated\ttheir\tperformances\tusing\t5-fold\tcross-validation.\tIn\tall\tthree\tscenarios\t(local,\tcentralized,\tand\tfederated),\twe\tcalculated\tR-squared\tand\troot\tmean\tsquared\terror\t(RMSE)\ton\tthe\tglobal\ttest\tdata\tto\tevaluate\tlinear\tregression\tand\trandom\tforest\tregression.\tFor\tthe\tRandom\tforest\tclassifiers\tin\teach\tfold\tof\tcross-validation,\twe\ttested\t4\tdifferent\tsubsampled\tunbiased\tcentralized\ttest\tsets\tfor\tfairness.\tIn\tthose\t4\ttest\tsets,\tthe\tnegative\tsamples\twere\tmutually\texclusive.\tFor\tevaluation,\twe\tused\tboth\taccuracy\tand\tAUROC.\t\t\tWe\tperformed\tsingle-sided\ttests\tto\tcheck\tour\thypotheses\ton\twhether\tcentralized\tand\tfederated\tmodels\toutperform\tthe\tlocal\tand\twhether\tthe\tcentralized\tanalyses\tperform\tbetter\tthan\tfederated\tones.\tWe\tperformed\tthe\tstudent\u2019s\tt-test\twhen\tthe\tassumptions\tof\tnormality\tand\tequal\tdistribution\twere\tmet\tand\tthe\tWelch\tt-test\twhen\tat\tleast\tone\tof\tthose\tconditions\twas\tnot\tmet.\tThe\tnormality\tof\tdistribution\twas\tchecked\tusing\tShapiro\u2019s\ttest,\tand\twe\tchecked\tthe\thomoscedasticity\tto\tverify\tthe\tequal\tdistribution\tassumption\tusing\tBartlette\u2019s\ttest.\tPython\tscripts\tfor\tevaluation\tand\tstatistical\ttests\tare\tavailable\tthrough\tour\tGitHub\trepository40.\t\t2.3.\tCriteria\tfor\tapplicability\tof\tfederated\tlearning\t\tBeing\tamong\tthe\tfirst\tto\tapply\tfederated\tlearning\tto\tsurvey-based\tdata,\twe\tused\tliterature\tfrom\tcomputer\tvision\tand\tother\tbiomedical\tdata\tto\tdefine\twhen\tfederated\tlearning\twould\tbe\tapplicable\tto\tsurvey\tdata.\tIn\tcomputer\tvision\ttasks,\t comparisons\t to\t local\t models\t are\t often\t omitted,\t and\t the\t threshold\t for\t determining\t federated\t algorithm\tperformance\t relative\t to\t centralized\t models\t remains\t unspecified50\u201352.\t In\t federated\t learning\t literature,\t only\t an\timprovement\tover\tother\tfederated\talgorithms\tis\treported53,54;\tthe\tstatistical\tsignificance\ttests\tare\toften\tleft\tout.\tThe\t precise\t cutoff\t for\t the\t extent\t of\t similarity\t to\t centralized\t accuracy\t remains\t undefined\t in\t the\t literature\t and\tdepends\ton\tthe\tspecific\tapplication\tand\tthe\ttrade-off\tbetween\tprivacy\tand\taccuracy.\tIn\tlight\tof\tthese\tfindings,\twe\tadopt\tthe\tfollowing\ttwo\tcriteria:\tif\tthe\tfederated\tmodel's\taccuracy\tsurpasses\tthat\tof\tlocal\tmodels\tand\taligns\tclosely\twith\tcentralized\tmodel\tperformance,\twe\tdeem\tit\tapplicable.\tThus,\ta\tnecessary\tcriterion\tto\tdeem\tfederated\tlearning\tapplicable\t is\t that\t the\t federated\t models\t perform\t statistically\t significantly\t better\t than\t local\t models.\t However,\t if\tcentralized\tmodels\tdo\tnot\toutperform\tfederated\tmodels,\tit\tis\ta\tsufficient\tcriterion.\t\t2.4.\tAdditional\tanalyses\tFor\t the\t tasks\t based\t on\t the\t GLA:D\u00ae\t data,\t we\t performed\t some\t additional\t analysis\t to\t test\t if\t the\t regions\t are\tstatistically\tsignificantly\tdifferent\tfrom\teach\tother\tand\thow\tthey\tperform\tin\tthe\tdata\tscares\tscenario.\t\t2.4.1\tSubsampling\texperiment\tTo\tinvestigate\tthe\tefficacy\tof\tfederated\tlearning\tin\tscenarios\twhere\teach\tclient\tpossesses\tlimited\tdata,\twe\treplicated\tour\texperiments\tbased\ton\tthe\tGLA:D\u00ae\tdata\twith\treduced\ttraining\tdata.\tThe\ttraining\tset\tsize\twas\tprogressively\treduced\tto\t75%,\t50%,\t25%,\tand\t10%\tof\tits\toriginal\tsize,\twhile\tthe\tcentral\ttest\tset\tremained\tunchanged.\tWe\tcompared\tthe\tperformance\tof\tlinear\tregression\tand\trandom\tforest\tmodels\ttrained\tlocally\tand\tcentrally\tusing\tfederated\tlearning\tunder\tthese\treduced\tdata\tconditions.\tFor\tthe\t25%\tsubsampling,\twe\temployed\ta\tfive-fold\tcross-validation\tprocedure.\tWe\tperformed\tthe\tsame\tevaluation\tand\tstatistical\ttests\tto\tproduce\tresults\tcomparable\tto\tthose\tobtained\twith\tthe\tentire\ttraining\tset.\t\t2.4.2.\tData\theterogeneity\tWe\talso\ttested\tthe\theterogeneity\tof\tall\tvariables\tacross\tall\tthe\tregions,\tas\tdata\theterogeneity\tmight\tspecifically\taffect\tthe\tperformance\tof\tfederated\tmodels.\tFor\tall\t51\tvariables\tand\tthe\ttarget\tvariable,\twe\ttested\tfor\teach\tpair\tof\tregions\tto\tsee\twhether\tthe\tdata\twas\tcoming\tfrom\ttwo\tdifferent\tdistributions.\tTo\tthis\tend,\twe\tperformed\tpost-hoc\tanalyses\tusing\tthe\tScheffe\tscore.\tTo\tevaluate\tthe\textent\tof\tthe\teffect\tsize\tof\tthe\tdifference\tin\tdistribution,\twe\tused\tCohen\u2019s\tD.\t7\t3.\tResults\t After\tpreprocessing\tthe\tGLA:D\u00ae\tdata,\twe\tincluded\t9,648\tpatients.\tThe\tnumber\tof\tsamples\tper\tregion\tis\tgiven\tin\tTable\t1.\t\tFor\tthe\tSHARE\tdataset,\twe\tincluded\t46570\tpatients\tfrom\t27\tcountries.\tThe\tnumber\tof\tsamples\tper\tregion\tvaries\tbetween\t2933\tand\t509\tparticipants;\tdetails\tare\tprovided\tin\tsupplementary\ttable\t2.\t\tTable\t1:\tDistribution\tof\tpatients\tin\tGLA:D\u00ae\tregistry\tacross\tfive\tdifferent\tregions\tof\tDenmark\tRegion\tname\tRegion\tID\tnumber\tof\tpatients\tCapital\tRegion\tof\tDenmark\t1\t2415\tRegion\tZealand\t2\t1550\tRegion\tof\tSouthern\tDenmark\t3\t2530\tCentral\tDenmark\tRegion\t4\t2220\tNorth\tDenmark\tRegion\t5\t933\ttotal\t..\t9648\t3.1.\tPerformance\tevaluation:\tAll\tresults\tfor\tcomparisons\tof\tfederated\tmodels\twith\tcentralized\tand\tlocal\tmodels\tare\tgiven\tbelow\tin\tTable\t2.\tThe\tp-values\tfor\tthe\tstatistical\ttests\tare\tgiven\tin\tTable\t3.\tTable\t2:\tComparison\tof\tFederated\tlearning\twith\tcentralized\tand\tlocal\tmodels\twhen\ttrained\ton\tthe\tfull\tGLA:D\u00ae\tdataset\tand\twave\t8\tof\tthe\tSHARE\tdataset.\tdata/\tscenario\tModel\tMetric\tAverage\tof\tLocals\tCentralized\tFederated\tRelative\t\tImprovement\tof\tfederated\tover\tlocal\tGLA:D\u00ae,\tNational\tLinear\tRegression\tr-square\t0.32\t0.34\t0.34\t8.16%\tGLA:D\u00ae,\tNational\tLinear\tRegression\tRMSE\t18.6\t18.2\t18.2\t2.09%\tGLA:D\u00ae,\tNational\tRandom\tForest\tRegression\tr-square\t0.30\t0.32\t0.34\t8.16%\tGLA:D\u00ae,\tNational\tRandom\tForest\tRegression\tRMSE\t18.8\t18.5\t18.3\t2.01%\tSHARE,\tInternational\tRandom\tForest\tClassification\taccuracy\t0.74\t0.84\t0.78\t5.45%\tSHARE,\tInternational\tRandom\tForest\tClassification\tAUROC\t0.69\t0.66\t0.71\t2.89%\t\tTable\t3:\tStatistical\ttests\tfor\tcomparison\tof\tfederated\tlearning\twith\tcentralized\tand\tlocal\tmodels\twhen\ttrained\ton\tthe\tfull\tGLA:D\u00ae\tdataset\tand\twave\t8\tof\tthe\tSHARE\tdataset.\tdata/\tscenario\tModel\tMetric\tIf\tCentralised\tbetter\tthan\tLocal\t\t(P-value)\tIf\tFederated\tis\tbetter\tthan\tLocal\t(P-value)\tIf\tCentralised\tis\tbetter\tthan\tFederated\t(P-value)\tGLA:D\u00ae,\tNational\tLinear\tRegression\tr-square\t0.00076\t0.00092\t0.48\t(n.s.)\tGLA:D\u00ae,\tNational\tLinear\tRegression\tRMSE\t0.0390\t0.040\t0.49(n.s.)\tGLA:D\u00ae,\tNational\tRandom\tForest\tr-square\t0.012\t0.00010\t0.96(n.s.)\tGLA:D\u00ae,\tNational\tRandom\tForest\tRMSE\t0.10\t(n.s.)\t0.017\t0.75(n.s.)\tSHARE,\tInternational\tRandom\tForest\tClassification\taccuracy\t3.5e-37\t3.4e-12\t1.2e-8\t8\tSHARE,\tInternational\tRandom\tForest\tClassification\tAUROC\t0.99(n.s.)\t4.6e-7\t1.0n.s.)\tn.s.\t:\tstatistically\tnot\tsignificant.\t\t3.1.1.\tLinear\tRegression\tFor\tlinear\tregression\tused\tin\ttask\t1,\tfederated\tlearning\tmodels\thave\ta\thigher\tR-squared\tand\tlower\tRMSE\tthan\tthe\tcorresponding\taverage\tof\tthe\tlocal\tmodels\t(R-squared\tp-val\t0.00092\tRMSE\tp-val\t0.040).\tWhile\tthe\tfederated\tmodel\tarchives\tR-squared\t0.34\tand\tRMSE\t18.2,\tthe\tlocal\tmodels\ton\taverage\tarchives\tR-squared\t0.32\tand\tRMSE\t18.6\t(Table\t2).\tConversely,\tthe\tcentralized\tmodel\tarchives\tR-squared\t0.34\tand\tRMSE\t18.2\t(Table\t2).\tHence,\tthe\tcentralized\tmodel\tdoes\tnot\toutperform\tthe\tfederated\tmodel\t(R-squared\tp-val\t0.48,\tRMSE\tp-val\t0.49)(Table\t3).\tNote\tthat\tthe\tlocal\tmodel\ttrained\ton\tdata\tfrom\tthe\tNorth\tDenmark\tRegion\t(Region\t5)\tperforms\tworst\tamong\tthe\tlocal\tmodels\t(Figure\t3).\t\t3.1.2.\tRandom\tForest\tRegression\tFor\trandom\tforest\tregression\tused\tin\ttask\t1,\tfederated\tlearning\tmodels\thave\ta\thigher\tR-squared\tand\tlower\tRMSE\tthan\t the\t corresponding\t average\t of\t the\t local\t models\t (R-squared\t p-val\t 0.00010\t RMSE\t p-val\t 0.017).\t While\t the\tfederated\tmodel\tarchives\tR-squared\t0.34\tand\tRMSE\t18.3,\tthe\tlocal\tmodels\ton\taverage\tarchives\tR-squared\t0.30\tand\tRMSE\t18.8\t(Table\t2).\tConversely,\tthe\tcentralized\tmodel\tarchives\tR-squared\t0.32\tand\tRMSE\t18.5\t(Table\t2).\tHence,\tthe\tcentralized\tmodel\tdoes\tnot\toutperform\tthe\tfederated\tmodel\t(R-squared\tp-val\t0.96,\tRMSE\tp-val\t0.75)(Table\t3).\tRather,\tthe\tfederated\tmodel\toutperforms\tthe\tcentralized\tmodel\t(Figure\t3).\t\t3.1.3.\tRandom\tForest\tClassification\tIn\t random\t forest\t classification,\t used\t in\t task\t 2,\t the\t centralized\t model\t accuracy\t slightly\t outperforms(0.84)\t the\tfederated\tmodel\t(0.78).\tBoth\tmodels\toutperform\tthe\tlocal\tmodel\t(0.74)\t\t(Supplementary\tFigure\tA.3.).\tHowever,\tthe\tfederated\tmodel\tachieved\thigher\tAUROC\t(0.71)\tthan\tthe\tcentralized\t(0.66),\tand\tthe\taverage\tof\tlocal\tmodels\t(\t0.69)\t(Figure\t3).\tA\thigher\taccuracy\tbut\tlower\tAUROC\tin\tthe\tcentralized\tmodel\tover\tthe\tfederated\tmodel\tsuggests\tthat\tfederated\trandom\tforest\tclassification\tis\tmore\trobust\tand\tless\tprone\tto\toverfitting.\t\t9\t\tFigure\t3:\tComparison\tof\tfederated\tmodel\twith\tcentralized\tand\tlocal\tmodels:\tFor\tboth\tA.\tR-square\tof\tlinear\tregression\t(higher\tthe\tbetter)\tand\tB.\tRMSE\tplot\tof\tlinear\tregression\t(lower\tthe\tbetter),\twe\tsee\tfederated\tand\tcentralized\tmodels\tperforming\tbetter\tthan\tthe\tlocal\tmodels.\tSimilarly,\tfor\tboth\tC.\tR-square\tof\trandom\tforest\tregression\t(higher\tthe\tbetter)\tand\tpanel\tD.\tRMSE\tof\trandom\tforest\tregression\t(lower\tthe\tbetter),\twe\tsee\tthat\tthe\tfederated\tmodel\toutperforms\tboth\tlocal\tand\tcentral\tmodels.\tIn\tE.\tFor\tthe\trandom\tforest\tclassifier,\twe\tsee\tthat\tthe\tfederated\tmodel\thas\thigher\tAUROC\tthan\tcentralized\tand\tlocal.\tThe\tdotted\tlines\tin\tthe\tbox\tplots\trepresent\tthe\tmean,\twhile\tthe\tsolid\tlines\trepresent\tthe\tmedian.\tThe\tstars\trepresent\tthe\tmagnitude\tof\tthe\tsignificance\tof\tthe\ttested\thypothesis\tin\tthe\tfollowing\tconvention\t***\t:\tp<0.001,\t**:\tp<0.01,\t*\t:\tp<0.05,\tn.s.\t:\tp>0.05.\tWe\ttested,\t1.\tif\tthe\tcentralized\tmodel\tis\tbetter\tthan\tthe\taverage\tof\tlocal\tones,\t2.\tif\tthe\tfederated\tis\tbetter\tthan\tthe\taverage\tof\tlocal\tones,\tand\t3.\tif\tthe\tcentralized\tis\tbetter\tthan\tthe\tfederated.\t\t\n10\t3.2\tSupplementary\tanalyses\t3.2.1\tSubsampling\texperiment\tIn\tthe\tfirst\ttask,\tthe\tperformance\tgap\tbetween\tlocal\tmodels\tand\tcentralized\tmodels\twidens\tas\tthe\ttraining\tdataset\tsize\tdiminishes\t(see\tFigure\t4).\tStill,\tfederated\tmodels\tmanage\tto\tkeep\tup\twith\ttheir\tcentralized\tcounterparts.\tIn\tthe\t25%\tsubsampled\ttraining\tset\texperiment\t(range\tn=\t88\t\tto\tn=\t266),\twe\tobserve\ta\tremarkable\t55.3%\timprovement\tin\tR-squared\tfor\tlinear\tregression\twhen\tusing\tfederated\tlearning\tcompared\tto\tthe\taverage\tperformance\tof\tlocal\tmodels\tacross\tfive-fold\tcross-validation\t(Table\t3\tand\tSupplementary\tFigure\tA.5.).\tFederated\trandom\tforest\texhibits\ta\tsubstantial\t20.9%\timprovement\tin\tR-squared\tvalue\tover\tits\tlocal\tcounterpart.\tUnlike\tthe\texperiment\tin\tfull\tdata\tin\tthis\tdata-scarce\tsimulation,\tthe\tstatistical\ttest\talways\tfinds\ta\tstatistically\tsignificant\timprovement\tof\tcentralized\tand\tfederated\tover\tlocal\tmodels.\tHowever,\tthere\tis\tno\tsignificant\timprovement\tin\tcentralized\tover\tthe\tfederated\tmodel\t(Table\t5).\t\tTable\t4:\tComparison\tof\tFederated\tlearning\twith\tcentralized\tand\tlocal\tmodels\twhen\ttrained\ton\tonly\t25%\tof\ttraining\tdata\tin\tGLA:D\u00ae.\tModel\ttrained\ton\t25%\tof\ttraining\tdata\tMetric\tAverage\tof\tLocals\tCentralized\tFederated\tRelative\tImprovement\tof\tfederated\tover\tlocal\tLinear\tRegression\tr-square\t0.21\t0.31\t0.32\t55.32%\tLinear\tRegression\tRMSE\t19.9\t18.7\t18.5\t7.42%\tRandom\tForest\tRegression\tr-square\t0.27\t0.31\t0.32\t20.83%\tRandom\tForest\tRegression\tRMSE\t19.2\t18.6\t18.5\t3.89%\t\tTable\t5:\tStatistical\ttests\tfor\tComparison\tof\tFederated\tlearning\twith\tcentralized\tand\tlocal\tmodels\twhen\ttrained\ton\tonly\t25%\tof\ttraining\tdata\tin\tGLA:D\u00ae.\t\t\tModel\tMetric\tIf\tCentralised\tbetter\tthan\tLocal\t\t(P-value)\tIf\tFederated\tis\tbetter\tthan\tLocal\t(P-value)\tIf\tCentralised\tis\tbetter\tthan\tFederated\t(P-value)\tLinear\tRegression\tr-square\t1.23E-06\t1.80E-07\t0.95(n.s.)\tLinear\tRegression\tRMSE\t0.00067\t1.01E-05\t0.75(n.s.)\tRandom\tForest\tRegression\tr-square\t0.00045\t1.35E-08\t0.98(n.s.)\tRandom\tForest\tRegression\tRMSE\t0.021\t3.39E-03\t0.73(n.s.)\tn.s.\t:\tstatistically\tnot\tsignificant.\t\t\t11\t\tFigure\t4:\t\tError\trates\tafter\ttraining\tset\tsubsampling\texperiment\tin\tGLA:D\u00ae:\tThe\ttraining\tdata\thas\tbeen\tsubsampled\tto\t75%,\t50%,\t25%,\tand\t10%,\twhile\ttest\tdata\tis\tkept\tuntouched.\tLinear\tregression,\tand\trandom\tforest\tregression\tare\ttrained\ton\tsubsampled\tGLA:D\u00ae\tdata\tdata\tto\tcompare\tfederated\tmodels\twith\tcentralized\tand\tlocal\tmodels.\tFor\tboth\tmachine\tlearning\texperiments,\talthough\tthe\tlocal\tmodel,\ton\taverage,\tperforms\tworse\tthan\tthe\tfederated\tmodel,\tthe\tdifference\tbetween\tthem\tintensifies\twith\ta\treduction\tin\tthe\tsample\tsize\tof\ttraining\tdata.\t\t\t3.2.2\tData\theterogeneity\tFor\tthe\tnational\tscenario,\tin\tDanish\tGL"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "### Summary: Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms\n\n#### 1. Abstract\nThe paper presents a novel methodology to enhance few-shot image classification by integrating learnable multi-scale embedding and self-attention mechanisms. Traditional metric-based few-shot learning methods rely on a single distance value, often neglecting shallow features. This paper introduces multi-output embedding networks and self-attention mechanisms to extract features from multiple stages, improving the model's ability to capture both global and abstract features. By assigning learnable weights to these stages, the model's performance is significantly enhanced. Empirical evaluations on the MiniImageNet and FC100 datasets demonstrate superior performance compared to state-of-the-art methods. Cross-domain evaluations from MiniImageNet to the CUB dataset further validate the efficacy of the proposed approach.\n\n#### 2. Introduction\nThe research aims to advance few-shot image classification, which focuses on training classifiers with limited samples while preserving high performance. Traditional deep learning models necessitate large datasets and often fail to generalize well with fewer samples due to overfitting. Techniques like data augmentation and transfer learning offer some help but have limitations especially when the target domain significantly differs from the source domain. To address these challenges, the paper proposes employing meta-learning approaches, particularly focusing on metric-based meta-learning. The key contribution is the introduction of a multi-output embedding network combined with self-attention mechanisms that utilize learnable weights across different feature extraction stages.\n\n#### 3. Related Work\nThe related work section extensively reviews current approaches in meta-learning, including model-based, optimization-based, and metric-based methods. The drawbacks of existing metric-based methods, which typically operate on a single embedding space, are highlighted. Gao et al.\u2019s multi-distance metric network and Afrasiyabi et al.\u2019s SetFeat with self-attention mechanisms are referenced as closely related works. However, the paper advances these by proposing a system that incorporates multiple feature extraction stages with learnable weights and self-attention mechanisms to improve few-shot learning performance.\n\n#### 4. Method\n**Overview**\nThe paper introduces a method involving ResNet18 for extracting multi-stage features, integrating self-attention mechanisms, and utilizing learnable weights for each stage to fine-tune performance. \n\n**Backbone**\n- ResNet18 is utilized as the feature extractor.\n- Multi-output embedding network is employed to capture features at five convolutional blocks, leading to a broader range of feature representations across different scales.\n\n**Self-Attention Module**\n- Self-attention mechanisms are incorporated after each feature extraction stage using 1\u00d71 convolutions to generate query, key, and value maps.\n- The attention mechanism calculates weights to capture long-range dependencies.\n- `1\u00d71` convolution is applied again, followed by scaling and addition of the original feature map.\n\n**Distance Metric**\n- Features from the support set are averaged to create prototypes for each class.\n- Euclidean distance is computed between the query features and class prototypes across multiple feature spaces.\n\n**Learnable Weights**\n- Aggregated distances from multiple feature spaces are computed using learnable weights. The final distance metric is a weighted sum of distances, with softmax normalization applied to get the class probabilities.\n\n**Probabilistic Model**\n- Classes are predicted using the negative log-probability minimized by the Adam optimizer.\n\n#### 5. Experimental Setup\n**Datasets**\n- **MiniImageNet**: 100 classes with 600 images each, divided into 64 training, 16 validation, and 20 test classes.\n- **FC100**: Similar structure but different splits: 60 training, 20 validation, and 20 test classes.\n- **CUB Dataset**: 200 bird species, with 100 training, 50 validation, and 50 test classes.\n\n**Specifications**\n- Images resized to 84\u00d784 pixels.\n- ResNet18 pre-trained on ImageNet used as the backbone.\n\n**Hyperparameters**\n- Learning rate: `10^-4` for MiniImageNet, `2\u00d710^-5` for FC100.\n- Optimization: Adam optimizer\n\n**Hardware**\n- Experiments conducted on an NVIDIA RTX 4090 GPU system.\n\n**Evaluation Metrics**\n- Accuracy measured on 5-way 1-shot and 5-way 5-shot tasks.\n- Evaluation includes multiple episodes to compute overall accuracy.\n\n**Comparison Methods**\n- Baseline networks, previous state-of-the-art models including AdaResNet, TADAM, MetaOptNet, and others.\n\n#### 6. Results and Analysis\n**MiniImageNet**\n- Achieves 66.57% (1-shot) and 84.42% (5-shot) accuracy, outperforming other methods like Neg-Cosine and MetaOptNet.\n\n**FC100**\n- Achieves 44.78% (1-shot) and 66.27% (5-shot) accuracy, significantly improving upon previous methods.\n\n**Cross-Domain (CUB Dataset)**\n- Achieves 52.95% (1-shot) and 71.59% (5-shot) accuracy, demonstrating strong generalization capabilities.\n\n**Ablation Study**\n- Highlighted the contribution of multi-scale embedding, learnable weights, and self-attention individually and in combination.\n- Using learnable weights resulted in notable accuracy gains.\n\n**Attention Module Gamma Analysis**\n- Best results with \u03b3 set to 0.2 both for support and query images.\n\n#### 7. Discussion\nThe results underline the effectiveness of integrating multi-scale embedding, self-attention mechanisms, and learnable weights. The approach proves beneficial across both within-domain and cross-domain evaluations. The performance improvements are attributed to richer feature representations and effective handling of different abstraction levels within the model.\n\n#### 8. Conclusion\nThe paper presents substantial advancements in few-shot image classification by utilizing multi-scale embeddings, learnable weights, and self-attention mechanisms. Future research includes exploring alternative network architectures and further enhancing transferability across domains.\n\n### Key Contributions and Future Research\n- Introduces multi-scale embedding combined with self-attention and learnable weights.\n- Demonstrates state-of-the-art performance across several benchmarks.\n- Suggests investigating additional architectural variants and further improving cross-domain generalization.\n\nThis summary provides all necessary details for replication, including methodological insights, implementation specifics, and empirical outcomes. It should serve as a comprehensive guide to recreate the study from scratch."}, {"role": "assistant", "content": "Noname manuscript No.\n(will be inserted by the editor)\nEnhancing Few-Shot Image Classification through Learnable\nMulti-Scale Embedding and Attention Mechanisms\nFatemeh Askari1\u00b7Amirreza Fateh1\u00b7Mohammad Reza Mohammadi1, *\nthe date of receipt and acceptance should be inserted later\nAbstract .\nIn the context of few-shot classification, the goal is to\ntrain a classifier using a limited number of samples while\nmaintaining satisfactory performance. However, tradi-\ntional metric-based methods exhibit certain limitations\nin achieving this objective. These methods typically rely\non a single distance value between the query feature\nand support feature, thereby overlooking the contribu-\ntion of shallow features. To overcome this challenge, we\npropose a novel approach in this paper. Our approach\ninvolves utilizing multi-output embedding network that\nmaps samples into distinct feature spaces. The proposed\nmethod extract feature vectors at different stages, en-\nabling the model to capture both global and abstract\nfeatures. By utilizing these diverse feature spaces, our\nmodel enhances its performance. Moreover, employing a\nself-attention mechanism improves the refinement of fea-\ntures at each stage, leading to even more robust represen-\ntations and improved overall performance. Furthermore,\nassigning learnable weights to each stage significantly\nimproved performance and results. We conducted com-\nprehensive evaluations on the MiniImageNet and FC100\ndatasets, specifically in the 5-way 1-shot and 5-way\n5-shot scenarios. Additionally, we performed a cross-\ndomain task from MiniImageNet to the CUB dataset,\nachieving high accuracy in the testing domain. These\nevaluations demonstrate the efficacy of our proposed\nmethod in comparison to state-of-the-art approaches.\nhttps://github.com/FatemehAskari/MSENet\n1School of Computer Engineering, Iran University of Science\nand Technology (IUST), Tehran, Iran\nE-mail: mrmohammadi@iust.ac.ir\n*Corresponding authorKeywords Few-shot classification . Self-attention .\nFeature Extraction . Embedding network . Metric-based\nmethods\n1 Introduction\nIn recent years, deep learning has led to remarkable\nprogress in the image recognition field, surpassing tradi-\ntional computer vision algorithms [ 1,2]. However, the\nsuccess of deep learning models heavily relies on huge\namount of data. When the available data is insufficient,\nthe model faces difficulties in effectively optimizing its\nparameters [ 3], which can lead to overfitting. Conse-\nquently, achieving effective recognition becomes chal-\nlenging, hindering the desired outcome [ 4,5]. Depending\non the contexts, the process of labeling images can be\ntime-consuming and costly [ 6,7]. For this reason, having\nmodels that can achieve acceptable performance with\na small number of samples is perceived to be of great\nimportance [8].\nData augmentation is one of the techniques used in\nsituation where the amount of labeled data is insufficient\n[9]. However, its impact on the model\u2019s performance is\nlimited because methods such as rotation, noise addition\nand scaling cannot provide new information and only\nhelp to some extent in preventing overfitting [ 10]. An-\nother technique is transfer learning [ 11], which transfer\nknowledge from a source domain to a target by freezing\nthe shallow network and fine-tuning the deep network\nlayers. However, if the target domain differs significantly\nfrom the source domain, this approach may not be very\neffective when dealing with a small number of samples\n[12].\nTo address the limitation of traditional deep learn-\ning models, which performed well with large trainingarXiv:2409.07989v1  [cs.CV]  12 Sep 20242 Fatemeh Askari1et al.\ndatasets, models were developed to perform well with\na fewer number of samples. One of the most important\napproaches is meta-learning [ 13]. Meta-learning aims\nto create models that can generalize across different\ntasks and rapidly adapt to new problem domain by\nleveraging prior learning experiences [ 14,15]. The three\nmain meta-learning approaches are: model-based meta-\nlearning, optimization-based meta-learning, and metric-\nbased meta-learning [ 16]. Model-based methods, aiming\nfor rapid learning, primarily concentrate on model ar-\nchitectures and adjust model parameters based on the\ngiven tasks. Commonly used architectures in model-\nbased methods include convolutional neural networks\n(CNNs) and recurrent neural networks (RNNs) [ 17].\nOptimization-based methods aim to improve model\ninitialization and gradient descent direction through\nepisodic training, enabling effective learning with a small\nnumber of training samples. These methods consist of a\ntask-specific learner and a meta-learner, trained on task\ndistributions, to optimize model parameters for few-shot\nlearning tasks [ 18,19]. Metric-based methods focus on\nlearning a distance metric that effectively measures the\nsimilarity between samples, with the goal of optimizing\nit for new learning tasks. In the context of few-shot\nimage classification, these methods aim to ensure that\nsamples from the same class have a small distance, while\nsamples from different classes have a large distance [ 20,\n21].\nMetric-based methods extract feature vectors and\nmeasure the distance between classes based on these ex-\ntracted vectors. However, metric-based methods usually\nlearn only one embedding space. Gao et al. [ 22] proposed\na multi-distance metric network that embeds samples\ninto different spaces by extracting feature vectors at dif-\nferent stages. The shallow network layers capture global\ninformation about the images, while the deeper layers\nfocus more on abstract features. Utilizing these different\nspaces has improved the performance of the model. In\naddition on different embedding spaces, it is also impor-\ntant to have feature vectors that are rich information.\nAfrasiyabi et al. [ 23] proposed a set-feature extractor\n(SetFeat) that utilizes a self-attention mechanism to\nobtain a richer feature space.\nIn this study, the proposed model integrates vari-\nous components to achieve enhanced performance: The\nproposed method utilizes ResNet18 as the feature extrac-\ntor, extracting feature maps from five different stages\nof ResNet18 to facilitate multi-scale representation. At\neach stage, we introduce learnable parameter weights\nto augment the model\u2019s capacity for feature representa-\ntion. Moreover, to enrich the feature space and further\nenhance multi-scale representation, self-attention mech-\nanisms are incorporated into each feature map.Though a comprehensive evaluation, we demonstrate\nthe effectiveness of our model under MiniImageNet,\nCUB, and FC100 datasets.\nOur contribution can be summarized as follow:\n\u2022We extracted five feature maps from backbone in or-\nder to capture both global and task specific features.\n\u2022We employ a self-attention mechanism for each fea-\nture map obtained from every stage in order to cap-\nture more valuable information.\n\u2022We incorporate learnable weights at each stage.\n\u2022We propose a novel few-shot classification. We have\nsignificantly improved the accuracy on the MiniIma-\ngeNet and FC100 datasets.\n2 Related Works\nIn this section, we discuss related work on some ap-\nproaches in meta-learning.\nModel-based:\nCai et al. [ 24] proposed Memory Matching Networks\n(MM-Net) for one-shot image recognition, which is based\non the principles of Matching Networks [ 25]. MM-Net\ncombines Convolutional Neural Networks with memory\nmodules to leverage knowledge from a set of labeled\nimages. It employs a contextual learner to predict CNN\nparameters for unlabeled images. Munkhdalai et al. [ 26]\nproposed model, called MetaNet, consists of two main\ncomponents: a base learner operating in the task space\nand a meta learner operating in the meta space. By\nleveraging meta information, MetaNet can dynamically\nadjust its weights to recognize new concepts in the\ninput task. Garnelo et al. [ 27] introduces a model called\nConditional Neural Processes (CNPs), which combines\ndeep neural networks with Bayesian methods. CNPs are\ncapable of making accurate predictions after observing\nonly a few training data points, while also being able\nto handle complex functions and large datasets. The\ndisadvantage of model-based approaches is that they\nare computationally expensive and require significant\ncomputational resources.\nOptimization-based:\nFinn et al. [ 28] proposed a model-agnostic meta-learning\n(MAML) algorithm for fast adaptation of deep networks.\nThe algorithm involves meta-training the model on var-\nious tasks using gradient descent to optimize its initial\nparameters. In the meta-testing phase, the model\u2019s per-\nformance is evaluated on new tasks sampled from a task\ndistribution. Through gradient-based adaptation, the\nmodel fine-tunes its parameters using a small amount of\ndata from each new task. Sun et al. [ 29] proposed a novel\nmethod called Meta-Transfer Learning (MTL). MTLEnhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms 3\ncombines transfer learning and meta-learning to improve\nthe convergence and generalization of deep neural net-\nworks in low-data scenarios. It introduces scaling and\nshifting operations to transfer knowledge across tasks.\nExperimental results demonstrate the effectiveness of\nMTL in various few-shot learning benchmarks.The dis-\nadvantage of optimization-based approaches is that they\nare susceptible to issues such as getting stuck in satu-\nration points and sensitivity to zero-gradient problems.\nThese issues can hinder the optimization process and\naffect the overall performance of the method.\nMetric-based:\nKoch et al. [ 30] proposed a Siamese network that utilizes\nthe VGG network as an extractor. They feed two pairs\nof images into the shared-weight convolutional network,\nand the network outputs a numerical value between\n0 and 1, representing the similarity between the two\nimages. Vinyals et al. [ 25] proposed a matching network\nthat computes the probability distribution over labels\nusing an attention kernel. The attention kernel calcu-\nlates the cosine similarity between the support set of\nembedded vectors and the query. It then normalizes the\nsimilarity using the softmax formula. Snell et al. [ 31] pro-\nposed the Prototypical network, where each class in the\nsupport set is represented by a prototype, defined as the\nmean of the embedded vectors belonging to that class.\nThe similarity between the query image\u2019s embedded vec-\ntor and the prototypes of each class is determined using\nthe Euclidean distance. This enables the classification\nof query images into their respective classes. Sung et\nal. [32] proposed the Relation Network, which does not\nrely on a separate distance function. Instead, it connects\nthe representations of the support set and the query\ndirectly within the neural network architecture, allowing\nthe network to learn the similarity measure. Previous\nfew-shot image classification methods commonly used\nfour-layer convolutional networks as backbones. How-\never utilization of pre-trained networks such as ResNet12\nand ResNet18 has become much more popular nowdays.\nHowever, calculating similarities and differences to a\nsingle feature vector is not sufficient. Gao et al. [ 22] pro-\nposed a model called MDM-Net for few-shot learning.\nThe MDM-Net maps input samples into four different\nfeature spaces using a multi-output embedding network.\nAdditionally, they introduced a task-adaptive margin\nto adjust the distance between different sample pairs.\nWhile previous approaches commonly used CNNs as\nfeature extractors, there is a growing trend towards uti-\nlizing Transformers and attention mechanisms. CNNs\nhave limitations such as a limited receptive field and\nparameter inefficiency. On the other hand, Transformersoffer advantages including the ability to capture long-\nrange dependencies, model non-local relationships, and\nparallelize computations efficiently. They also provide\ninterpretability by highlighting important regions or\nfeatures in the input data, aiding in the understanding\nof the model\u2019s decision-making process. The MDM-Net\nintroduces a promising approach in few-shot learning by\nleveraging the strengths of Transformers and attention\nmechanisms while addressing the limitations of CNNs.\nWang et al. [ 33] propose a unified Query-Support\nTransformer (QSFormer) model for few-shot learning.\nThe QSFormer model addresses the challenges of consis-\ntent image representations in both support and query\nsets, as well as effective metric learning between these\nsets. It consists of a sampleFormer branch that captures\nsample relationships and conducts metric learning us-\ning Transformer encoders, decoders, and cross-attention\nmechanisms. Additionally, a local patch Transformer\n(patchFormer) module is incorporated to extract struc-\ntural representations from local image patches. The\nproposed model also introduces a Cross-scale Interac-\ntive Feature Extractor (CIFE) as an effective backbone\nmodule for extracting and fusing multi-scale CNN fea-\ntures. The QSFormer model demonstrates superior per-\nformance compared to existing methods in few-shot\nlearning. Ran et al. [ 33] propose a novel deep trans-\nformer and few-shot learning (DT-FSL) framework for\nhyperspectral image classification. The framework aims\nto achieve fine-grained classification using only a few-\nshot instances. By incorporating spatial attention and\nspectral query modules, the framework captures the\nrelationships between non-local spatial samples and re-\nduces class uncertainty. The network is trained using\nepisodes and task-based learning strategies to enhance\nits modeling capability. Additionally, domain adaptation\ntechniques are employed to reduce inter-domain distri-\nbution variation and achieve distribution alignment.\n3 Proposed method\n3.1 Problem definition\nThe goal of few-shot classification is to classify a un-\nseen sample. We have two datasets, Dtrain andDtest,\neach associated with corresponding class sets Ctrain\nandCtest, respectively. It is important the class sets\nCtrain andCtestare disjoint, meaning they have no el-\nements in common. Formally, we can express this as\nCtrain\u2229Ctest=\u2205. Each training episode consists of a\nsupport set Sand a query set Q. The support set S\ncomprises Kexamples for each of Ndistinct classes, de-\nnoted as XS\ni=\b\u0000\nxs\nij, C=i\u0001\tK\ni=1, where xs\nijrepresents\nthejth example belonging to class i. The query set Q4 Fatemeh Askari1et al.\nFig. 1: SA module\ncontains Xq={xq\ni}nq\ni=1. The objective of the model is to\nleverage the support set Sto correctly classify the query\nexample xq\ni. In other words, the model is trained to\npredict the class label of the query image based on the\nset of supporting examples and their class affiliations\nprovided in S. During training, each episode consists of a\nrandom sample drawn from the training dataset Dtrain.\nThe objective of the model is to learn to extract feature\nrepresentations from the examples in the support set S,\nsuch that the distance between the feature vector of the\nquery image xq\niand the feature vectors of the support\nexamples xs\nijcan be effectively measured. Specifically,\nthe model learns to extract discriminative feature vec-\ntors from the support examples, which can then be used\nto classify the query image based on its proximity to\nthe support set features. This training paradigm encour-\nages the model to rapidly adapt its feature extraction\nand classification capabilities from the limited support\ndata to accurately predict the class label of the query\ninstance. Similarly, during evaluation, the performance\nof the trained model is assessed on the held-out test\ndataset Dtest. In this phase, the model extracts feature\nvectors for each example in the test set, leveraging the\nknowledge and feature extraction capabilities it learned\nduring the training episodes on the Dtrain dataset.\n3.2 Overview\nThe overall architecture of our model is illustrated in\nfig. 2. Our proposed approach incorporates several key\ncomponents designed to enhance performance. At the\ncore is a robust backbone architecture that enables the\nextraction of feature maps across diverse spatial scales.\nAdditionally, we have integrated an attention module to\nfurther refine the feature extraction process. Underpin-\nning our framework is a distance metric that facilitates\neffective similarity computation between inputs. More-over, we have incorporated learnable weights to capture\nthe relative significance of each extracted feature map.\n3.2.1 Backbone\nWe utilized a pre-trained ResNet-18 network with initial\nweights from the ImageNet dataset. We removed the last\nfully-connected layer and fine-tuned the network on our\nspecific dataset. To leverage multi-level feature maps,\nwe proposed a multi-output embedding approach where\nwe extracted feature maps at the end of each of the 5\nconvolutional blocks in the ResNet-18 architecture. This\nallowed us to capture feature representations at multiple\nscales and resolutions. Each sample in the support set,\ndenoted as xs\nij, is mapped to five different feature spaces\nlikefs\nijas shown in equation 1:\nfs\nij={fConv 1\u2212s\nij , fConv 2\u2212s\nij , ..., fConv 5\u2212s\nij } (1)\nSimilarly, each query sample, denoted as xq\ni, is mapped\nto the following feature space (equation 2):\nfq\nij={fConv 1\u2212q\nij , fConv 2\u2212q\nij , ..., fConv 5\u2212q\nij } (2)\nIn our proposed approach, we employ multiple convo-\nlutional layers ( Conv 1, Conv 2, Conv 3, Conv 4, Conv 5)\nfrom the ResNet-18 network to extract features from\nthe samples. Each of these convolutional blocks con-\ntributes to a distinct feature representation, allowing us\nto leverage different levels of abstraction and informa-\ntion contained in the feature maps for both the support\nand query samples. This multi-level feature extraction\nstrategy enables a more comprehensive and robust rep-\nresentation of the input images, which is crucial for\neffective similarity computation and classification tasks.\n3.2.2 Attention module\nAfter extracting the feature vectors at each stage, we\nutilize self-attention and global average pooling.TheEnhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms 5\nFig. 2: Final model\nrepresentation of the attention module is illustrated\nin fig. 1. Considering the extracted feature vectors, we\napply a 1 \u00d71 convolution on f, resulting in convolutional\nvectors k,gandh. This operation is performed to reduce\nthe number of channels (equation 3):\nk\u2032, v\u2032, q\u2032=Conv 1\u00d71(fConv\u2212p), p\u2208[1\u22125] (3)\nAfter obtaining the convolutional vectors q\u2032(fConv\u2212p)\nandk\u2032(fConv\u2212p), we apply the softmax function (equa-\ntion 4):\n\u03b2i,j=exp(Si,j)PN\ni=1exp(Si,j), where S i,j=\nq\u2032(fConv\u2212p\ni )Tk\u2032(fConv\u2212p\nj )(4)\nThe attention mechanism computes weights \u03b2, that de-\ntermine the relative importance of each pixel in the\nfeature map. These weights are calculated across the\nentire spatial extent, allowing the attention module to\ncapture long-range dependencies beyond a local neigh-\nborhood, unlike traditional convolutions. The output of\nthe self-attention layer is computed as equation 5:\noConv\u2212p=Conv 1\u00d71(NX\ni=1\u03b2i,jv\u2032(fConv\u2212p)) (5)In equation 5, in addition to using various ResNet18\nblocks for feature extraction, we employ a 1 \u00d71 convo-\nlution layer twice - once before applying the attention\nmechanism and again after it. The purpose of using the\n1\u00d71 convolution layer twice is to make the network\nmore nonlinear and to allow us to leverage the previous\ninformation more evenly. The scaling factor, typically\ndenoted as \u03b3, is a learnable parameter in the network\nthat is multiplied with the input feature map fConv\u2212p\nbefore the addition (equation 6):\nyConv\u2212p=\u03b3\u00d7oi+fConv\u2212p(6)\nAfter obtaining the final output, we take a global average\npooling. The output is represented for each query and\nsupport sample as follows:\nf\u2032s\nij={f\u2032Conv 1\u2212s\nij , f\u2032Conv 2\u2212s\nij , ..., f\u2032Conv 5\u2212s\nij } (7)\nf\u2032q\ni={f\u2032Conv 1\u2212q\ni , f\u2032Conv 2\u2212q\ni , ..., f\u2032Conv 5\u2212q\ni } (8)\n3.2.3 Distance metric\nOnce we have obtained the final output from the previ-\nous stage, we take the average of the vectors from all the\nsupport samples belonging to the same class to obtain6 Fatemeh Askari1et al.\nthe prototypes for each class from the support set as\nequations 9 .\nc\u2032Conv\u2212p\ni =1\n|K|KX\nj=1f\u2032Conv\u2212p\u2212s\nij , where p \u2208[1\u22125] (9)\nWe calculate the Euclidean distance between the feature\nmap of each query sample f\u2032Conv\u2212p\ni and its correspond-\ning prototype map c\u2032Conv\u2212p\nj , considering 5 feature maps\nper sample, as described in equation 10.\ndConv-p\ni,j = Euclidean\u0010\nf\u2032Conv\u2212p\ni , cj\u2032Conv\u2212p\u0011\n(10)\n3.2.4 Learnable weights\nConsidering the emphasis of shallow network layers on\nglobal features and deep layers on abstract features, we\nopted to assign weights to the distances computed in\nthe five feature spaces for each query sample-support set\nrepresentative pair. These weights are trainable within\nthe network, and their initial assignment significantly\ninfluences the model\u2019s performance, which will be elab-\norated on in the Experiment section. The final distance\nis obtained by aggregating the weighted distances from\nthese five feature spaces as shown in Equation 11:\ndi,j=5X\nr=1wConv\u2212p\ni,j \u00d7dConv\u2212p\ni,j , p\u2208[1\u22125] (11)\nAfterward, we apply softmax as shown in Equation 12:\np(y=j|xq\ni) =exp(\u2212di,j)Pns\nl=1exp(\u2212di,l)(12)\nThe learning process involves minimizing the negative\nlog-probability J=\u2212logp(y=j|xq\ni) of the true class j\nusing the Adam optimizer. Training episodes are formed\nby randomly selecting a subset of classes from the train-\ning set. Within each selected class, a subset of examples\nis chosen to form the support set, while another subset\nfrom the remaining examples is used as query points.\n4 Experimental Results\n4.1 Datasets\nThe proposed method was evaluated on three widely\nused datasets commonly employed for few-shot learning\ntasks: MiniImageNet [65], FC100 [66], and CUB [67].\nMiniImageNet dataset. The MiniImageNet is a\nsubset of the ImageNet dataset designed for training\nand evaluating machine learning models. This dataset\ncontains 100 classes with 600 images per class. The\nimages are randomly divided into three splits: 64 classesfor training, 16 classes for validation, and 20 classes\nfor testing. This data partitioning allows researchers to\nevaluate how effectively models can generalize to new\nand unseen classes after being trained on the provided\nset of classes.\nFC100 dataset. The FC100 is another dataset simi-\nlar in structure to MiniImageNet. It contains 100 classes\nwith 600 images per class. However, the class splits are\nhandled differently - the 100 classes are randomly di-\nvided into 60 training classes, 20 validation classes, and\n20 test classes. This ensures that the training, validation,\nand test sets are entirely disjoint, which can provide a\nmore realistic evaluation of a model\u2019s ability to learn\ngeneral visual representations.\nCUB dataset. The CUB (Caltech-UCSD Birds-200-\n2011) dataset is a collection of bird images often used\nfor fine-grained visual classification tasks. This dataset\ncontains 200 different bird species, with a total of 11,788\nimages. The images are randomly partitioned into 100\ntraining classes, 50 validation classes, and 50 test classes.\nThis dataset poses a challenging problem as many bird\nspecies can appear visually similar, requiring models to\nlearn discriminative features to accurately classify the\ndifferent species.\n4.2 Experimental setting\nFor our datasets, we performed the following prepro-\ncessing and model configuration steps. First, all input\nimages were resized to a resolution of 84 x 84 pixels.\nRegarding the model architecture, we employed a pre-\ntrained ResNet-18 as the backbone feature extractor.\nSpecifically, we extracted features from the 5 stages of\nthe ResNet-18 network, obtaining feature maps from\neach of these stages.\nFor the optimization of our models, we employed the\nAdam optimizer. For the MiniImageNet dataset, we used\na constant learning rate of 10\u22124, while for the FC100\ndataset, the learning rate was 2 \u00d710\u22125. This learning\nrate was selected based on preliminary experiments to\nensure stable and efficient convergence of the models\nduring training. All experiments were conducted on an\nNVIDIA RTX 4090 GPU system.\n4.3 Evaluation metric\nTo evaluate the performance of our models, we employed\nthe following scenario: For the training phase, we sam-\npled 30 random classes and 5 examples per class from\nthe training set. We then trained the model on these\nsamples. For evaluation, we tested the trained model on\na 5-way 5-shot task by selecting 5 random classes andEnhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms 7\nTable 1: Evaluation on MiniImageNet in 5-way.\nMethod Backbone 1-shot 5-shot\nAdaResNet [34] ResNet12 56.88 71.94\nTADAM [35] ResNet12 58.50 76.70\nMetaOptNet [36] ResNet12 62.64 78.63\nNeg-Margin [37] ResNet12 63.85 81.57\nMixtFSL [38] ResNet12 63.98 82.04\nMeta-Baseline [39] ResNet12 63.17 79.26\nFRN [40] ResNet12 66.45 82.83\nQSFormer [41] ResNet12 65.24 79.96\nDistill [42] ResNet12 64.82 82.14\nMeta-Baseline + DiffKendall [43] ResNet12 65.56 80.79\nLEO [44] WRN-28-10 61.76 77.59\nCC+rot [45] WRN-28-10 62.93 79.87\nFEAT [46] WRN-28-10 65.10 81.11\nNeg-Cosine [47] ResNet18 62.33 80.94\nMixtFSL [48] ResNet18 60.11 77.76\nMergeNet-Concat [49] ResNet18 65.05 77.76\nOur model ResNet18 66.57 84.42\nTable 2: Evaluation on FC100 in 5-way.\nMethod Backbone 1-shot 5-shot\nP-Net [31] ResNet12 37.80 53.30\nCosine Classifier [50] ResNet12 38.47 57.67\nTADAM [51] ResNet12 40.10 56.10\nSimpleShot [52] ResNet10 40.13 53.63\nMetaopt Net [53] ResNet12 41.10 55.50\nDC [54] ResNet12 42.04 57.63\nMDM-Net [55] ResNet12 43.62 57.41\nRFS [56] ResNet12 44.60 60.90\nSSFormers [57] ResNet12 43.72 58.92\nLSFSL [58] ResNet12 43.60 60.12\nBarlow Twins + DSA [59] ViT-B 41.42 55.47\nOur model ResNet18 44.78 66.27\nusing 5 examples per class. This process was repeated\nover multiple episodes to compute the overall 5-way 5-\nshot accuracy. Additionally, we evaluated the models on\na 5-way 1-shot task. In this setting, we followed a similar\napproach but used only 1 example per class during the\nevaluation phase. Our primary evaluation metric for\nboth tasks was accuracy.\n4.4 Comparison with state of the arts\nBased on the results presented in Table 1 and Table\n2, our proposed model demonstrates strong few-shot\nlearning performance compared to the state-of-the-artTable 3: The comparison of our model\u2019s performance\nagainst state-of-the-art methods on the test domain\nCUB.\nMethod 1-shot 5-shot\nRelatonNet [60] 35.21 51.10\nMatchingNet [61] 42.28 57.21\nRelationNet+LFT [62] 48.10 65.02\nMatchingNet+LFT [62] 43.38 61.44\nRelationNet+ATA [63] 48.49 59.42\nStyleAdv [64] 48.49 68.72\nStyleAdv-FT [64] - 70.90\nOur model 52.95 71.59\napproaches. As shown in Table 1, on the MiniImageNet\ndataset, our model achieves an accuracy of 66.57 in\nthe 1-shot setting and 84.42 in the 5-shot setting. This\nindicates that our model is able to effectively lever-\nage the limited training data and rapidly adapt to\nnew tasks, showcasing its superior few-shot learning\ncapabilities.Similarly, on the more challenging FC100\ndataset, as shown in Table 2, our model outperforms\nthe existing state-of-the-art methods by a significant\nmargin, obtaining an accuracy of 44.78 in the 1-shot\nsetting and 66.27 in the 5-shot setting. The consistent\nimprovements observed across both MiniImageNet and\nFC100 datasets highlight the effectiveness of the design\nchoices and techniques employed in our model. These\nchoices and techniques allow it to learn more robust\nand transferable representations for few-shot learning8 Fatemeh Askari1et al.\nTable 4: The influence of each component to the model\u2019s performance\nbaseline MultiscaleLearnable\nWeightSelf-\nattentionMiniImageNet FC100\n1 shot 5 shot 1 shot 5 shot\n\u2713 X X X 62.83 82.38 39.47 63.44\n\u2713 \u2713 X X 63.65 83.23 41.97 64.8\n\u2713 \u2713 \u2713 X 64.73 83.92 43.7 65.76\n\u2713 \u2713 \u2713 \u2713 66.57 84.42 44.78 66.27\nTable 5: The influence of different weights at each stage on the model\u2019s performance on MiniImageNet\nWeights 1 shot 5 shot\nw1 w2 w3 w4 w5accuracy without\nlearnable weightsaccuracy with\nlearnable weightsaccuracy without\nlearnable weightsaccuracy with\nlearnable weights\nWeight\nInitialization1 1 1 1 1 63.80% 64.54% 82.24% 82.99%\n1 1.1 1.2 1.3 1.4 63.34% 64.73% 83.02% 83.92%\n1 1.2 1.4 1.6 1.8 62.13% 64.6% 83.68% 83.70%\nscenarios.These results position our model as a highly\ncompetitive approach in the field of few-shot learning.\n4.5 Cross domain\nTo further assess the generalization capabilities of our\nmodel, we conducted a cross-domain evaluation using\nthe CUB dataset as the testing domain, while the Mini-\nImageNet dataset was used for training. As shown in\nTable 3, our model demonstrated strong performance in\nthis challenging cross-domain few-shot learning setting.\nSpecifically, our model achieved an impressive 1-shot\naccuracy of 52.95 and a 5-shot accuracy of 71.59 on\nthe CUB dataset. This outperforms various state-of-\nthe-art approaches, such as RelationNet+LFT, Match-\ningNet+LFT, and the strong Baseline model, showcas-\ning the superior ability of our approach to adapt to\nthe distributional shift between the training and testing\ndomains. The consistent improvement over the compet-\ning methods across both the 1-shot and 5-shot settings\nhighlights the robustness and transferability of the rep-\nresentations learned by our model. This demonstrates\nits effectiveness in tackling the challenging cross-domain\nfew-shot learning problem, where the model must gener-\nalize its knowledge from the training domain to the novel\ntesting domain. These results further solidify the posi-\ntion of our model as a highly competitive and versatile\nfew-shot learning solution.4.6 Ablation study\nTo better understand the contributions of different com-\nponents in our model, we conducted an ablation study\nand reported the results. Table 4 illustrates the influence\nof individual components on overall model performance.\nThe Baseline configuration, serving as the foundation of\nour model, achieves notable results with 62.83% accu-\nracy for the 1-shot task and 82.38% for the 5-shot task\non the MiniImageNet dataset. On the FC100 dataset,\nthe Baseline configuration results in 39.47% 1-shot and\n63.44% 5-shot accuracy.\nIntroducing the Multiscale module, which enhances\nfeature extraction by capturing information at multiple\nscales, leads to improvements in both 1-shot and 5-shot\nperformance across both datasets. Adding the Learnable\nWeight component, which adapts weights for different\nfeature channels, further boosts accuracy, demonstrating\nits effectiveness.\nThe final inclusion of the Self-attention module re-\nsults in the best overall performance, with the model\nachieving 66.57% accuracy for the 1-shot task and 84.42%\nfor the 5-shot task on MiniImageNet. On FC100, the\nperformance reaches 44.78% for 1-shot and 66.27% for\n5-shot tasks. These results underscore the significant\ncontribution of the self-attention mechanism to enhanc-\ning the few-shot learning capabilities of our approach.\nAs shown in Table 5, two important observations\ncan be made regarding model performance on the Mini-\nImageNet dataset:\n1) Advantage of Learnable Weights: The results indi-\ncate a clear benefit to using learnable weights. In bothEnhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms 9\nthe 5-way 1-shot and 5-way 5-shot tasks, models with\nlearnable weights achieved higher accuracies compared\nto those with fixed weights. For example, in the 5-way\n1-shot task, accuracy increased from 63.80% to 64.54%\nwhen weights were learnable. Similarly, in the 5-way\n5-shot task, accuracy improved from 82.24% to 82.99%\nwith learnable weights. These results demonstrate that\nincorporating learnable weights significantly enhances\nmodel performance.\n2) Impact of Weight Initialization: The choice of\nweight initialization also plays a crucial role in model\nperformance. The best results were observed with initial\nweights set to 1.1, 1.2, 1.3, and 1.4. This variation un-\nderscores the importance of proper weight initialization\nin achieving optimal model accuracy.\nIn summary, Table 5, highlights that both the use of\nlearnable weights and careful selection of weight initial-\nization are key factors in improving model performance.\nTable 6: The influence of different gamma at each stage\non the model\u2019s performance on MiniImageNet\nGamma 1 Gamma 2 1 shot 5 shot\n0.2 0.2 66.57% 84.42%\n0.3 0.3 65.09% 84.12%\n0.4 0.4 65.15% 84.16%\nAs shown in Table 6, the impact of different \u03b3values\non model performance in the self-attention module is\nevident for both 1-shot and 5-shot tasks. Specifically,\nsetting \u03b3values to 0.2 for both support and query images\nresulted in the highest accuracy. This indicates that the\nchoice of \u03b3significantly influences model performance,\nwith the 0.2 setting yielding the best results compared\nto other tested values.\n5 Conclusion\nThis paper presents an innovative strategy to enhance\nfew-shot classification by integrating a self-attention\nnetwork and embedding learnable weights at each stage,\nleading to improved performance and substantial out-\ncomes. Through feature vector extraction and weight\ntransfer across stages, this technique elevates multi-scale\nfeature representation, thereby enhancing overall per-\nformance. Moreover, the incorporation of self-attention\nmechanisms refines features at each stage, resulting in\nmore robust representations and enhanced performance.\nThe introduction of learnable weights at each stage sig-\nnificantly boosts performance and outcomes. Extensiveevaluations on the MiniImageNet and FC100 datasets\nunderscore the efficacy of this method compared to\ncurrent state-of-the-art approaches.\nReferences\n1.A. Saber, P. Parhami, A. Siahkarzadeh, and A. Fateh,\n\u201cEfficient and accurate pneumonia detection using a\nnovel multi-scale transformer approach,\u201d arXiv preprint\narXiv:2408.04290 , 2024.\n2.A. Fateh, R. T. Birgani, M. Fateh, and V. Abolghasemi,\n\u201cAdvancing multilingual handwritten numeral recognition\nwith attention-driven transfer learning,\u201d IEEE Access ,\nvol. 12, pp. 41 381\u201341 395, 2024.\n3.A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carls-\nson, \u201cCnn features off-the-shelf: an astounding baseline\nfor recognition,\u201d pp. 806\u2013813, 2014.\n4.S. Rezvani, F. S. Siahkar, Y. Rezvani, A. A. Gharahbagh,\nand V. Abolghasemi, \u201cSingle image denoising via a new\nlightweight learning-based model,\u201d IEEE Access , 2024.\n5.X. Zhang, J. Zhou, W. Sun, and S. K. Jha, \u201cA lightweight\ncnn based on transfer learning for covid-19 diagnosis.\u201d\nComputers, Materials & Continua , vol. 72, no. 1, 2022.\n6.A. Fateh, M. Fateh, and V. Abolghasemi, \u201cEnhancing\noptical character recognition: Efficient techniques for docu-\nment layout analysis and text line detection,\u201d Engineering\nReports , vol. 6, no. 9, p. e12832, 2024.\n7.A. Fateh, M. Rezvani, A. Tajary, and M. Fateh, \u201cPersian\nprinted text line detection based on font size,\u201d Multimedia\nTools and Applications , vol. 82, no. 2, pp. 2393\u20132418, 2023.\n8.Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, \u201cGeneralizing\nfrom a few examples: A survey on few-shot learning,\u201d\nACM computing surveys (csur) , vol. 53, no. 3, pp. 1\u201334,\n2020.\n9.S. Yang, W. Xiao, M. Zhang, S. Guo, J. Zhao, and F. Shen,\n\u201cImage data augmentation for deep learning: A survey,\u201d\narXiv preprint arXiv:2204.08610 , 2022.\n10.J. Zhou, Y. Zheng, J. Tang, J. Li, and Z. Yang, \u201cFlipda:\nEffective and robust data augmentation for few-shot learn-\ning,\u201d arXiv preprint arXiv:2108.06332 , 2021.\n11.D. Xue, X. Zhou, C. Li, Y. Yao, M. M. Rahaman, J. Zhang,\nH. Chen, J. Zhang, S. Qi, and H. Sun, \u201cAn application\nof transfer learning and ensemble learning techniques\nfor cervical histopathology image classification,\u201d IEEE\nAccess , vol. 8, pp. 104 603\u2013104 618, 2020.\n12.H. Xia, H. Zhao, and Z. Ding, \u201cAdaptive adversarial\nnetwork for source-free domain adaptation,\u201d pp. 9010\u2013\n9019, 2021.\n13.Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, \u201cGeneralizing\nfrom a few examples: A survey on few-shot learning,\u201d\nACM computing surveys (csur) , vol. 53, no. 3, pp. 1\u201334,\n2020.\n14.K. He, N. Pu, M. Lao, and M. S. Lew, \u201cFew-shot and\nmeta-learning methods for image understanding: a sur-\nvey,\u201d International Journal of Multimedia Information\nRetrieval , vol. 12, no. 2, p. 14, 2023.\n15.M. Goldblum, L. Fowl, and T. Goldstein, \u201cAdversarially\nrobust few-shot learning: A meta-learning approach,\u201d Ad-\nvances in Neural Information Processing Systems , vol. 33,\npp. 17 886\u201317 895, 2020.\n16.A. Parnami and M. Lee, \u201cLearning from few examples:\nA summary of approaches to few-shot learning,\u201d arXiv\npreprint arXiv:2203.04291 , 2022.10 Fatemeh Askari1et al.\n17.L. Sun, W. Li, X. Ning, L. Zhang, X. Dong, and W. He,\n\u201cGradient-enhanced softmax for face recognition,\u201d IEICE\nTRANSACTIONS on Information and Systems , vol. 103,\nno. 5, pp. 1185\u20131189, 2020.\n18.W. Bian, Y. Chen, X. Ye, and Q. Zhang, \u201cAn optimization-\nbased meta-learning model for mri reconstruction with\ndiverse dataset,\u201d Journal of Imaging , vol. 7, no. 11, p.\n231, 2021.\n19.H. Cho, Y. Cho, J. Yu, and J. Kim, \u201cCamera\ndistortion-aware 3d human pose estimation in video with\noptimization-based meta-learning,\u201d pp. 11 169\u201311 178,\n2021.\n20.M. J. Lee and J. So, \u201cMetric-based learning for nearest-\nneighbor few-shot image classification,\u201d pp. 460\u2013464, 2021.\n21.P. Li, G. Zhao, and X. Xu, \u201cCoarse-to-fine few-shot classi-\nfication with deep metric learning,\u201d Information Sciences ,\nvol. 610, pp. 592\u2013604, 2022.\n22.F. Gao, L. Cai, Z. Yang, S. Song, and C. Wu, \u201cMulti-\ndistance metric network for few-shot learning,\u201d Inter-\nnational Journal of Machine Learning and Cybernetics ,\nvol. 13, no. 9, pp. 2495\u20132506, 2022.\n23.A. Afrasiyabi, H. Larochelle, J.-F. Lalonde, and C. Gagn\u00b4 e,\n\u201cMatching feature sets for few-shot image classification,\u201d\npp. 9014\u20139024, 2022.\n24.Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei, \u201cMemory\nmatching networks for one-shot image recognition,\u201d pp.\n4080\u20134088, 2018.\n25.O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al. ,\n\u201cMatching networks for one shot learning,\u201d Advances in\nneural information processing systems , vol. 29, 2016.\n26.T. Munkhdalai and H. Yu, \u201cMeta networks,\u201d pp. 2554\u2013\n2563, 2017.\n27.M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho,\nD. Saxton, M. Shanahan, Y. W. Teh, D. Rezende, and\nS. A. Eslami, \u201cConditional neural processes,\u201d pp. 1704\u2013\n1713, 2018.\n28.C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic meta-\nlearning for fast adaptation of deep networks,\u201d pp. 1126\u2013\n1135, 2017.\n29.Q. Sun, Y. Liu, T.-S. Chua, and B. Schiele, \u201cMeta-transfer\nlearning for few-shot learning,\u201d pp. 403\u2013412, 2019.\n30.G. Koch, R. Zemel, R. Salakhutdinov et al. , \u201cSiamese\nneural networks for one-shot image recognition,\u201d vol. 2,\nno. 1, 2015.\n31.J. Snell, K. Swersky, and R. Zemel, \u201cPrototypical networks\nfor few-shot learning,\u201d Advances in neural information\nprocessing systems , vol. 30, 2017.\n32.F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and\nT. M. Hospedales, \u201cLearning to compare: Relation net-\nwork for few-shot learning,\u201d pp. 1199\u20131208, 2018.\n33.X. Wang, X. Wang, B. Jiang, and B. Luo, \u201cFew-shot\nlearning"}]}
{"messages": [{"role": "system", "content": "Objective: Given a summary of a research project, generate a high-quality academic paper in the style of reputable machine learning publications, such as those found in conferences like NeurIPS, ICML and ICLR. The total length should be 5000 to 6000 words.\n\n    Structure: Follow the typical structure of a machine learning research paper, which includes the following:\n\n    1. Title: Concise, informative, and reflective of the research focus (max 15 words).\n    2. Abstract: A brief summary (250-400 words) outlining the problem, the proposed solution, key results, and the significance of the findings.\n    3. Introduction: Introduce the research problem, its importance, existing challenges, and the motivation behind the study. Clearly state the contributions of the paper (max 800 words).\n    4. Related Work: Review relevant literature, summarizing existing methods, their limitations, and how the proposed work differs or improves upon them (max 1000 words).\n    5. Methodology: Provide a detailed description of the proposed method, including mathematical formulations, algorithms, and theoretical foundations. Highlight any novel techniques or modifications to existing methods (max 1200 words).\n    6. Experiments: Present a comprehensive experimental setup, including datasets used, evaluation metrics, baselines, and implementation details. Provide comparative analysis, ablation studies, and visualizations like graphs, tables, and charts to validate the proposed approach (max 800 words).\n    7. Results: Report key findings with quantitative and qualitative results, including performance metrics, statistical significance, and visual comparisons where applicable (max 600 words).\n    8. Discussion: Analyze the results, discuss the strengths and weaknesses of the approach, and suggest potential future work or improvements (max 600 words).\n    9. Conclusion: Summarize the main findings, contributions, and implications of the research (max 400 words).\n    10. References: Include citations formatted according to a recognized academic style (e.g., IEEE or APA), covering relevant and recent works in the field.\n\n    Content Requirements:\n\n    1. Clarity and Precision: Use clear, precise, and formal academic language. Avoid ambiguous statements and ensure that technical terminology is used correctly.\n    2. Technical Rigor: Ensure the content reflects a deep understanding of machine learning concepts, including mathematical rigor where appropriate. Use equations, algorithms, and technical diagrams to explain the proposed method.\n    3. Novelty and Contribution: Highlight the novel aspects of the research. Clearly differentiate between existing work and the new contributions of the paper.\n    4. Citations and References: Reference foundational and recent papers to provide context and validate claims. Ensure citations are accurately used to support statements and comparisons.\n    5. Evaluation and Analysis: Include a thorough evaluation of the proposed method, comparing it with state-of-the-art techniques. Use statistical methods to validate performance improvements.\n    6. Figures and Tables: Use high-quality figures and tables to illustrate key points, results, and comparisons. Ensure all visual elements are well-labeled, easy to interpret, and add value to the text.\n    \n    Formatting and Presentation:\n\n    Use a professional and clean layout, adhering to common standards in academic publishing.\n    Ensure consistency in font size, headings, and style throughout the document.\n    Include captions for all figures and tables, and ensure they are referenced appropriately within the text.\n\n    Technical Specifics:\n\n    Include specific details about the datasets used, such as source, size, preprocessing steps, and splits.\n    Provide hyperparameter settings, model architectures, and training details to ensure reproducibility.\n    Discuss computational resources used, such as hardware specifications and runtime.\n\n    Tone and Voice:\n\n    Maintain a formal, objective, and impersonal tone throughout the paper.\n    Avoid first-person language; instead, use passive or third-person constructions.\n    Be persuasive and evidence-driven, backing claims with data and logical reasoning.\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail.\n\n    Additional Requirements:\n    Target Audience: Write for an audience of machine learning researchers, data scientists, and academics familiar with the field. Assume knowledge of basic ML concepts but explain any advanced or novel techniques in detail."}, {"role": "user", "content": "# Summary of \"Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols\"\n\n## 1. Abstract\n\nThe paper introduces \"AI-Control Games,\" a formal decision-making model for evaluating the safety and usefulness of AI deployment protocols. Using a multi-objective, partially observable, stochastic game framework, the paper models a red-teaming exercise between a protocol designer and an adversary. Methods are developed to find optimal protocols by reducing the problem to zero-sum partially observable stochastic games. The formalism is applied to protocols for deploying untrusted language models as programming assistants, focusing on Trusted Monitoring protocols. The utility of the formalism is demonstrated through improvements over empirical studies, evaluation of new settings, and analysis of modelling assumptions.\n\n## 2. Introduction\n\n### Context of the Research\nAI systems based on large foundation models are increasingly used in various tasks, posing challenges for ensuring safe deployment as the complexity and scale of systems grow.\n\n### Research Questions\n- How can we formally model the safety evaluations of AI deployment protocols?\n- Can we find optimal deployment protocols in such a model?\n- How do assumptions in modelling affect the safety and usefulness of protocols?\n\n### Literature Review\nPrevious research has focused on improving model behavior confidence through robust training and interpretability tools. This paper focuses on evaluating practical plans (AI-Control protocols) for safely deploying AI systems that have a high degree of uncertainty.\n\n## 3. Related Work\n\nThe paper builds upon works like robust model training and interpretability tools for detecting harmful behavior. It extends the operationalization of safety in AI deployment by applying a formal game-theoretic approach, unlike traditional empirical red-teaming methods.\n\n## 4. Method\n\n### Proposed Approach\nAI-Control Games are modeled as two-player, multi-objective, partially observable, finite-horizon, stochastic games involving:\n- Blue (protocol designer)\n- Red (adversary)\n\n### Mathematical Formulation\nThe AI-Control Game G is defined by:\n- State space \\( S \\)\n- Terminal states \\( S_{\\perp} \\)\n- Actions for Blue \\( A_B \\) and Red \\( A_R \\)\n- Observations \\( \\Omega_B \\) and \\( \\Omega_R \\)\n- Initial state distribution \\( I \\)\n- Transition function \\( P \\)\n- Reward function for usefulness \\( R_{Use} \\)\n- Time horizon \\( H \\)\n\nPolicy definitions:\n- Blue policies ( \\( \\pi_B \\) ): functions from observation-action histories to action distributions.\n- Red policies ( \\( \\pi_R \\) ): memoryless functions from observations to action distributions.\n\n### Architectural Details\nThe paper does not introduce specific neural network architectures but provides a general framework applicable to various AI systems.\n\n### Novel Techniques\nThe main novel contribution is the formulation of AI-Control Games and associated methods for finding optimal protocols by reducing them to single-objective, zero-sum games.\n\n## 5. Experimental Setup\n\n### Datasets\nThe primary example involves the APPS dataset for programming assistants.\n\n### Hardware and Software Specifications\n- Implementations are empirically analyzed using specific programming challenges.\n- Code is available online for reproducibility.\n\n### Evaluation Metrics\n- Safety: Probability of not ending in an unacceptable outcome given a scheming AI.\n- Usefulness: Expected sum of task completion rewards in honest mode.\n\n### Baselines and Comparisons\nEvaluations are benchmarked against protocols like \n- Never use U\n- Trusted Model Only\n- Upfront Auditing\n- Trusted Monitoring with various audit thresholds.\n\n## 6. Results and Analysis\n\n### Empirical Findings\n- Evaluation shows Pareto improvements in safety-usefulness trade-offs with higher audit thresholds.\n- Weakly sensitive Trusted Monitoring protocols (\"weak\") showcase different utility and risk profiles compared to original settings.\n\n### Figures and Analysis\n- Figure 1: Overview of protocol (Trusted Monitoring) setup.\n- Figure 2: Safety and usefulness scores for various protocols in original and weak settings.\n- Analysis confirms that adjusting audit thresholds can enhance Pareto efficiency.\n\n### Anomalous Results\nNo unexpected results reported, but varying assumptions (audit budget, number of problems) affect results predictably.\n\n## 7. Discussion\n\n### Interpretation of Results\n- The methodology allows detailed evaluation of AI deployment protocols under worst-case adversarial settings.\n- Modifying protocol assumptions (like weaker trusted monitoring) significantly impacts protocol performance.\n\n### Implications\n- Findings highlight the importance of auditing strategies in maintaining safety without excessively compromising utility.\n- The formal game-theoretic approach provides a scalable and adaptable framework for broader AI settings.\n\n### Limitations\n- Focus is currently on memoryless protocols; further work is needed on memoryful strategies.\n- The framework\u2019s assumptions (perfect human auditor, fixed audit budget) might not hold in all practical contexts.\n\n## 8. Conclusion\n\n### Summary of Key Contributions\n- Introduction of AI-Control Games for formal safety evaluation.\n- Efficient methods for optimizing deployment protocols.\n- Empirical and theoretical analysis showcasing improvements over existing methods.\n\n### Future Research\n- Extending the formalism to memoryful protocols.\n- Applying the game-theoretic model to other AI deployment use cases and unacceptable outcomes.\n- Exploring the integration of dynamic human auditing capacities and real-time observations.\n\nThis detailed summary should provide a comprehensive understanding sufficient for replicating the study or extending its methodologies."}, {"role": "assistant", "content": "Games for AI Control:\nModels of Safety Evaluations of AI Deployment Protocols\nCharlie Griffin1, Louis Thomson1, Buck Shlegeris2, Alessandro Abate1\n1University of Oxford\n2Redwood Research\ncharlie.griffin@cs.ox.ac.uk\nAbstract\nTo evaluate the safety and usefulness of deployment proto-\ncols for untrusted AIs, AI Control uses a red-teaming exercise\nplayed between a protocol designer and an adversary. This pa-\nper introduces AI-Control Games, a formal decision-making\nmodel of the red-teaming exercise as a multi-objective, par-\ntially observable, stochastic game. We also introduce meth-\nods for finding optimal protocols in AI-Control Games, by re-\nducing them to a set of zero-sum partially observable stochas-\ntic games. We apply our formalism to model, evaluate and\nsynthesise protocols for deploying untrusted language mod-\nels as programming assistants, focusing on Trusted Monitor-\ning protocols, which use weaker language models and limited\nhuman assistance. Finally, we demonstrate the utility of our\nformalism by showcasing improvements over empirical stud-\nies in existing settings, evaluating protocols in new settings,\nand analysing how modelling assumptions affect the safety\nand usefulness of protocols.\n1 Introduction\nAn increasing range of products aim to automate tasks using\nAI systems based on large foundation models. For exam-\nple, some chatbots are given access to tools, such as code\ninterpreters (OpenAI 2023; Adept AI 2024). Ensuring that\nAI systems behave safely across deployment settings will\nbecome increasingly challenging if the variety of use cases,\nscale of systems, and complexity of tasks increase. While\nsome previous research has focused on increasing our confi-\ndence in model behaviour, for example by training robustly\nharmless models (Casper et al. 2024) or by detecting harm-\nful behaviour with interpretability tools (MacDiarmid et al.\n2024), in this paper we consider a complementary approach:\nevaluating plans for safely deploying \u2018untrusted\u2019 AI systems\nabout which we have an unacceptable degree of uncertainty.\nFollowing Greenblatt et al. (2024), we study AI-Control\nprotocols : practical plans for safely deploying an untrusted\nAI system (U) utilising various resources including non-\nAI software, human labour, or a weaker, trusted AI sys-\ntem. For example, one protocol for controlling a chatbot\nmight involve a pre-deployment evaluation where humans\nutilise other language models to generate prompts that in-\nduce harmful completions (Shah et al. 2023). This paper fo-\ncuses on another case study, Trusted Monitoring : constantly\nmonitoring U\u2019s behaviour using a trusted model (T), with\nhuman auditing of suspicious behaviour.Greenblatt et al. (2024) operationalise safety as the prob-\nability that an adversarially chosen untrusted AI system\nwould be incapable of causing some concrete unacceptable\noutcome. One example of an unacceptable outcome would\nbe revealing some sensitive information, such as model\nweights (Clymer et al. 2024). We evaluate the safety of pro-\ntocols under the worst-case assumption that U is \u2018scheming\u2019,\nmeaning it behaves as though it knows the protocol and is\ntrying to subvert it to cause the unacceptable outcome.\nA major challenge to evaluating the safety of a proto-\ncol with respect to a scheming AI is realistically replicating\nthe potential capabilities of such a system: as a proxy for a\nschemer, AI Control uses a human red-teaming process to\nevaluate safety. The red team can intervene in U in some\npredefined set of ways to induce it to subvert the AI-Control\nprotocol and violate the safety property. For example, the\nred team might intervene in an untrusted language model by\nfinetuning weights or modifying prompts.\nSince one safe protocol is \u2018never use U\u2019, we must also\nevaluate a protocol\u2019s usefulness , which is its ability to utilise\nU to complete a task under the assumption that U is \u2018honest\u2019\n(not scheming). Our proxy evaluations model an \u2018honest\u2019\nuntrusted model by not allowing the red team to intervene.\nWe begin in section 2 by introducing a running example:\nusing Trusted Monitoring protocols to control a program-\nming assistant. We then move onto our contributions :\n\u2022 Section 3 introduces AI-Control Games , the first for-\nmal model of AI Control: a partially observable, multi-\nobjective, leader-follower, stochastic game.\n\u2022 Section 4 demonstrates how our formalism can model\nTrusted Monitoring, and leverage it to give expressions for\nthe safety and usefulness of a wide range of protocols.\n\u2022 Section 5 presents algorithms for finding Pareto-optimal\nprotocols by reducing to single-objective zero-sum games.\n\u2022 Section 6 we use our methods for evaluating and synthe-\nsising protocols to demonstrate Pareto-improvements on\nexisting protocols and analyse the safety-usefulness trade-\noff in novel settings.\nFinally, we conclude with a comparison to existing literature\nin section 7, and a discussion of potential future work in\nsection 8. All of our code is available online.1\n1https://github.com/CJ-Griffin/GamesForAIControl/tree/arxivarXiv:2409.07985v1  [cs.AI]  12 Sep 2024Request \nsolution \nU \nRequest \nscore \nT \nAction \nT \nSubmit \ncode H Coding \nProblem \n(a) A representation of the\nTrusted Monitoring protocol,\nwhere U is the untrusted AI, T\nis the trusted AI, and H is the\nhuman.\n024Original Trusted MonitorScores on safe solutions (PDF)\nScores on backdoored sol\u2019s (PDF)\n0.00 0.25 0.50 0.75 1.00024Weak Trusted Monitor(b) T\u2019s suspicion score distri-\nbutions for safe solutions and\nthose backdoored by U across\nthe \u2018original\u2019 (Greenblatt et al.\n2024) and \u2018weaker\u2019 settings.\nFigure 1: Overview of protocol and modelling assumptions.\n2 Example: Setting and Protocol\nThroughout this paper, we will introduce AI-Control Games\nusing the example deployment setting from Greenblatt et al.\n(2024), and a generalisation of some of their protocols.\nDeployment setting. In the APPS backdooring setting a\nlanguage model is used as a programming assistant (Green-\nblatt et al. 2024). The protocol is asked to solve 1000 chal-\nlenging programming tasks, presented sequentially from the\nAutomated Programming Progress Standard (APPS) dataset\n(Hendrycks et al. 2021). The protocol cannot itself write\ncode, but may utilise three resources to do so: the untrusted\nAI(U) is a powerful language model capable of solving pro-\ngramming problems; the trusted AI (T) is a less powerful\nlanguage model incapable of solving them2; and the human\n(H) is trusted and capable but can only be used on 2% of\nproblems. In this setting, a protocol\u2019s usefulness is its abil-\nity to submit correct programming solutions. Since it is as-\nsumed that only U can solve the problems, usefulness is the\nproportion of solutions submitted that were generated by U.2\nThe unacceptable outcome (that the red team is trying to\ncause, but the blue team is trying to avoid) is that any one\nof the 1000 solutions submitted is backdoored , meaning it\ncontains an exploitable vulnerability.3\nTrusted Monitoring Protocol. As an example, we focus\non a set of Trusted Monitoring protocols and red team in-\nterventions, drawn in fig. 1a. After receiving a problem, the\n2In fact, T may be able to solve some problems, but Greenblatt\net al. treat T\u2019s ability as a baseline and normalise between 0and1.\n3An exploitable code vulnerability is one that the untrusted\nmodel can detect and exploit later. It is important to note that\nGreenblatt et al. (2024) use a proxy for \u2018backdoors\u2019: inputs outside\nof the test cases where a function fails to meet its specification.protocol queries the untrusted AI (U) for a solution (which\nmay or may not be backdoored). The protocol queries the\ntrusted AI (T) to monitor the solution and observes a \u2018sus-\npicion score\u2019, rating the likelihood that the solution is back-\ndoored. Based on this score and previous observations, the\nprotocol decides on an action: \u27e8Audit\u27e9asks the human (H)\nto audit the model (if there\u2019s a backdoor, the human will spot\nit, and the protocol shuts down)4;\u27e8Defer\u27e9asks T to write\nits own solution; and \u27e8Run\u27e9submits U\u2019s solution. T\u2019s sus-\npicion scores are modelled as a distribution, conditioned on\nwhether the code is safe or backdoored. Note that the Trusted\nMonitoring protocols that we consider generalise many of\nthe protocols studied by Greenblatt et al. (2024) (see fig. 2):\nour model is not prescriptive about how the protocol should\ndecide between \u27e8Audit\u27e9,\u27e8Defer\u27e9, and\u27e8Run\u27e9, so we can ex-\nplore various ways to generalise and improve the protocols\n(see section 6).\nRed team interventions. To test the safety of the proto-\ncol against intentional attempts to insert backdoors, in some\ncases, we allow the red team to modify the query to the un-\ntrusted model. In particular, the red team gets a binary choice\nto either leave the query unchanged or \u2018attempt a backdoor\u2019\nby modifying the protocol\u2019s query to U, requesting that it\ninserts a subtle backdoor.\nSuspicion score distributions. The key empirical ques-\ntion for evaluating Trusted Monitoring protocols is \u2018How\ndoes T rate the suspiciousness of U\u2019s programming solutions\nwhen they are/aren\u2019t backdoored?\u2019. Figure 4 from Green-\nblatt et al. (2024) shows the distribution of trusted monitor-\ning suspicion scores for U\u2019s programming solutions (both\nbackdoored and safe). In this work, we mostly consider two\ngames, corresponding to two distribution pairs (shown in\nfig. 1b). The first \u2018Original Trusted Monitor\u2019 is a pair of\nbeta distributions approximately fit to the (scaled) empiri-\ncal suspicion scores found by Greenblatt et al. (2024) with\nBeta(7.39,6.86) for safe problems and Beta (8.91,2.82)\nfor backdoored problems. By plugging their data into our\nmodel, we are able to extend their results about the APPS\ndataset to a range of new protocols without running any ad-\nditional experiments on language models. See appendix D\nfor justification of our approximation, and further details.\nThe second \u2018weak\u2019 case is a hypothetical where T is\nless discerning: it has the same safe distribution, but for\nbackdoored problems, we use a hypothetical distribution\nBeta(7.51,4.22)chosen to have a worse ROC curve. Note\nthat we can easily utilise new empirical or hypothetical dis-\ntributions in our framework, thus potentially greatly reduc-\ning the cost of exploring the protocol in new domains.\n3 Formalising AI-Control Games\nTo analyse the properties of AI-Control evaluations, we for-\nmalise them mathematically. An AI-Control Game is a form\noftwo-player game that is played between Blue (the pro-\ntocol designer) and Red (the red team acting as a proxy\n4Unless otherwise stated, we assume that the human auditor\nis perfectly accurate. We relax this assumption in appendix B and\nexplore its impact in section 6.1.for a scheming model). Episodes (play-throughs) of an AI-\nControl Game have two modes: \u2018scheming\u2019 or \u2018honest\u2019. The\n\u2018scheming\u2019 mode represents the case in an AI-Control evalu-\nation where the red team is able to interfere with U (which it-\nself is a worst-case proxy for if the untrusted model were try-\ning to subvert the protocol). In a scheming game, both play-\ners are active: Red is trying to maximise the probability of\nan unacceptable outcome (minimise safety), whereas Blue\nis trying to minimise this probability (maximise safety). The\nhonest game represents the benign case, where we assume\nthe untrusted AI system is not trying to cause unacceptable\noutcomes. In an honest game, Blue effectively plays alone\nand is only assessed on its usefulness score. Because Blue\ntries to maximise safety and usefulness, the game is multi-\nobjective . A key to the game is that Blue does not know\nwhich mode it is in (honest or scheming) and for at least this\nreason the game is partially observable . Blue may try to\ndeduce whether the game is in scheming mode, while Red\nmight choose actions to conceal this fact.\nRed represents the red team that acts as a proxy for an\nuntrusted AI system that runs on servers controlled by the\nprotocol. Crucially, the protocol will have access to the state\nof the untrusted AI system and, therefore, we model Red\nas memoryless (its actions depend only on the most recent\nobservation).5We model the game as leader-follower : the\nprotocol is suggested, and then the red team chooses the best\nresponse. Combining all of these requirements, we model an\nAI-Control Game as a tuple:\nDefinition 3.1 (AI-Control Game) .An AI-Control Game G\nis a two-player (Blue and Red), two-objective, observable,\nfinite-horizon game defined by a tuple:\u0000\nS,S\u22a5,AB,AR,\u2126B,\u2126R,I,P,RUse, H\u0001\nHereSis a state space. S\u22a5\u2286 S is a set of terminal\nstates representing \u2018unacceptable outcomes\u2019. As is standard\nin decision processes, ABandARare finite sets of actions\nfor Blue and Red, respectively. ARmust contain a privi-\nleged action, anull, which represents Red not interfering in\nthe game. The tokens \u2295and\u2297represent the honest and\nscheming conditions.6In honest (\u2295)games, Red\u2019s action\nis replaced by anull. Whether a game is honest or schem-\ning can be chosen non-deterministically and is stored in the\nvariable \u229b. The usefulness reward RUse:S \u00d7 AB\u00d7 S \u2192 R\nrewards usefulness on a given honest turn. \u2126B,\u2126Rare fi-\nnite sets of observations for Blue and Red: the signals that\nthe agents receive from the environment that give evidence\nabout the state of the game. The initial state/observation dis-\ntribution is I \u2208\u2206(S\u00d7\u2126B\u00d7\u2126R)and the transition function\nP:S \u00d7 AB\u00d7 AR\u2192\u2206\u0000\nS \u00d7\u2126B\u00d7\u2126R\u0001\ngives a distribu-\ntion over potential next states and observations.78Finally, all\nepisodes are Htime-steps long.\n5Note that we could still model U as having external memory ,\nmodelled within the state space and observed by Blue.\n6The tokens \u2295and\u2297can be considered atomic and fixed be-\ntween tuples, so aren\u2019t defined in each new AI-Control Game tuple.\n7To model terminal states in a fixed horizon game, assume P\nloops on each terminal state and that RUsegives no reward.\n8While we assume simultaneous turns, sequential-turn games\ncan be modelled with appropriate P.In an AI-Control Game, Blue (the protocol) should be\nable to remember past observations and actions. We model\nBlue\u2019s policies as behaviourally stochastic: functions from\nobservation-action histories to distributions over actions:9\n\u03c0B: \u2126B\u00d7SH\u22121\ni=0\u0000\nAB\u00d7\u2126B\u0001i\u2192\u2206\u0000\nAB\u0001\n. As described\nabove, Red represents a red team that acts as a proxy for\na scheming untrusted model that is memoryless (its actions\ndepend only on the most recent observation): \u03c0R: \u2126R\u2192\n\u2206\u0000\nAR\u0001\n. Note that, as Red is the follower, there is no benefit\nto allowing policy mixtures (Conitzer and Sandholm 2006).\nDenote the sets of all policies \u03a0Band\u03a0R.\nGiven a tuple, we can describe the process of an AI-\nControl Game as a sequence of interdependent random\nvariables denoted with uppercase letters. Let \u229btake val-\nues in {\u2295,\u2297}andS0, OB\n0, OR\n0\u223c I . At time t, we de-\nfine the random variables for the state ( St), Blue and\nRed\u2019s observations ( OB\nt, OR\nt), and their actions ( AB\nt, AR\nt).\nThe random variables can be defined inductively for all\nt:(St+1, OB\nt+1, OR\nt+1)\u223c P (St, AB\nt, AR\nt), where AB\nt\u223c\n\u03c0B(OB\n0, AB\n0, OB\n1, AB\n1. . . , OB\nt)and,AR\nt\u223c\u03c0R\u0000\nOR\nt\u0001\nif\u229b\nis\u2297, else AR\nt=anull.\nWithin our model, the usefulness of a protocol \u03c0Bis just\nthe expected sum of the RUsein an honest game:\nUse(\u03c0B):=E\"H\u22121X\nt=0RUse\u0000\nSt, AB\nt, St+1\u0001\n| \u2295, \u03c0B#\n.(1)\nNote that usefulness score is measured in \u2295cases, when \u03c0R\nhas no effect, and therefore usefulness does not depend on\n\u03c0R. The safety score of a policy pair (\u03c0B, \u03c0R)is the prob-\nability that the game does not terminate in an unacceptable\noutcome (i.e. the final state is not in S\u22a5):\nSafety\u0000\n\u03c0B, \u03c0R\u0001:=P\u0002\nSH/\u2208 S\u22a5| \u2297, \u03c0B, \u03c0R\u0003\n. (2)\nBecause the protocol designer gets to choose their action\nfirst, and without knowing the red team\u2019s actions, we con-\nsider a leader-follower model: Blue chooses a policy, and\nthen Red chooses a response that minimises safety.\nProposition 3.2. For any AI-Control Game G, there exist\ndistance functions in \u03a0Band\u03a0Rsuch that: Safety( \u03c0B,\u00b7),\nSafety( \u00b7, \u03c0R), andUse(\u00b7)are continuous in their domains.10\nProofs of all claims in the section can be found in ap-\npendix A, which details and utilises additional results about\ntwo mappings from Blue/Red policies to simplexes.\nProposition 3.3. For all \u03c0B,Safety( \u03c0B,\u00b7)attains its mini-\nmum at some \u03c0R, and min\u03c0RSafety( \u00b7, \u03c0R)is continuous.\nOur goal is to find protocols that maximise worst-case\nsafety and usefulness. This can be modelled as a multi-\nobjective variant of a Stackelberg equilibrium:\nDefinition 3.4. The pair (\u03c0B, \u03c0R)is in an AI-Control Equi-\nlibrium iff[min \u03c0RSafety( \u03c0B, \u03c0R),Use(\u03c0B)]Tis weakly\nPareto optimal and \u03c0Rminimises safety for \u03c0B.\n9We sometimes discuss Blue policies as though they are mixed-\nstochastic. This is equivalent (see appendix A).\n10Example distance functions, based on a mapping from poli-\ncies to vectors, are defined in appendix A.The set of equilibrium points describes a necessary trade-\noff between safety and usefulness, and is always non-empty.\nProposition 3.5. EveryGhas an AI-Control Equilibrium.\n4 Example: Modelling the protocol\nWith our general framework from section 3, we can de-\nfine a formal model of the Trusted Monitoring protocol for\nthe APPS backdoor setting, as outlined in Greenblatt et al.\n(2024). For the full definition please see appendix B; next\nwe present the key points.\nWe define a Trusted Monitoring AI-Control Game tu-\npleGTM= (S, . . . , H )step-by-step. As always, whether\nthe game is scheming ( \u2297) or honest ( \u2295) is determined ex-\nogenously and non-deterministically. If the game is \u2297, for\nany given problem, Red can choose either for the untrusted\nmodel to attempt a backdoor or to let it submit a safe\nsolution ( AR:={\u27e8Backdoor \u27e9,\u27e8Safe\u27e9}). If the game is\n\u2295, Red\u2019s null action, to not backdoor, is chosen ( anull=\n\u27e8Safe\u27e9). Given a solution by the untrusted AI, Blue has\nthree available actions: to ask the human to audit the so-\nlution, to defer to the trusted AI to rewrite the solution, or\nto run the solution immediately. Therefore define AB:=\n{\u27e8Audit\u27e9,\u27e8Defer\u27e9,\u27e8Run\u27e9}. Apart from initial and terminal\nstates, the state space, S, consists of triples s= (m, c, b ),\nwhere mrepresents the number of problems left to solve.\nThe value of misM(the total number of problems) at the\nstart of the game, and decreases by 1at each timestep. The\nvariable crepresents the number of \u2018audit coins\u2019 left: the\nnumber of times the protocol can ask the human to audit a\nsolution. At the start, c=C= 0.02\u2217M(for a 2%audit\nproportion) and then cdecreases by 1each time the proto-\ncol chooses to audit. If the protocol attempts to audit when\nno audit coins are left, the code is run instead. The last vari-\nable,b, is a boolean that is True when the current problem is\nbackdoored (when Red has chosen \u27e8Backdoor \u27e9in the pre-\nvious timestep).\nIn honest games ( \u2295), where only usefulness matters to\nBlue, Red\u2019s action is always substituted for anull=\u27e8Safe\u27e9.\nTherefore, bis always False . In honest games, Blue scores\n1/M usefulness points for each solution by the untrusted\nmodel it submits - giving a maximum score of 1across the\nwhole episode.\nIn scheming games ( \u2297), where only safety matters to\nBlue, Red may have chosen \u27e8Backdoor \u27e9last turn, and so\nbmight be True . If so, and Blue chooses to run the code,\nthe safety property is violated, and the game ends with an\nunacceptable outcome. If Blue chooses to audit, we assume\nthe human will always spot the backdoor, and the game ends\nsafely (we enter a special \u2018safe\u2019 self-looping state). If Blue\nchooses to defer, then the game continues. The safety score\nfor a policy pair is the probability that a scheming game ends\nwithout violating the safety property.\nThroughout the games, Red receives only the null obser-\nvation ( onull),11whereas at each turn Blue receives observa-\n11In Greenblatt et al. (2024), the untrusted model receives only\nthe programming problem with no additional information: we ab-\nstract this away by giving Red only the null observation. This im-\nplies that Red is equally likely to backdoor any problem.tions of m, c, and a trusted monitor score x, which can be\nseen as a noisy observation of b, drawn from a pair of dis-\ntributions (\u2018safe\u2019 and \u2018backdoored\u2019). Each distribution is a\ndiscretisation of the Beta distribution outlined in fig. 1b. See\nappendix D for more details.\n4.1 Evaluating Memoryless Protocols\nWe generally model protocols (Blue policies) as functions\nfrom the observation-action history to a distribution over\nactions. However, all of the Trusted Monitoring policies in\nGreenblatt et al. (2024) are memoryless. We can restrict to a\nsimple and interpretable subset of policies that generalises\nthose by Greenblatt et al. (2024). Remember that, in our\nmodel, Blue\u2019s observation is (m, c, x ), where xis a noisy\nobservation of b, sampled from the Trusted Monitor. De-\nfine a set of memoryless threshold protocols , characterised\nby the false positive rates (FPRs) for \u27e8Audit\u27e9and\u27e8Defer\u27e9\n(qa\nm,candqd\nm,c), for each m, c. Letg(q)be the threshold that\ngives an FPR of q:P[xt> g(x)|bt=False ] =q.12Then,\ndefine the policy:\n\u03c0B(m, c, x ) =\uf8f1\n\uf8f2\n\uf8f3\u27e8Audit\u27e9Ifx\u2265g(qa\nm,c)\n\u27e8Defer\u27e9Ifg(qa\nm,c)> x\u2265g(qd\nm,c)\n\u27e8Run\u27e9 otherwise\n(3)\nThis class of policies generalises the policies in Greenblatt\net al. (2024), and is a dominant subset of memoryless poli-\ncies. These policies\u2019 safety and usefulness values can be\ncomputed in O(MC)multiplications by calculating the val-\nues of the following relation using dynamic programming.\nProposition 4.1. For any \u03b2:=\u03c0R(\u27e8Backdoor \u27e9 |onull), the\nsafety of the memoryless threshold protocol \u03c0Bis given by\nthe value of zC\nM, where \u03bbi\nm,cdepend only on q\u00b7\nm,candG:13\nzc\nm:=\u0000\n\u03bb1\nm,c\u03b2+\u03bb2\nm,c\u0001\nwc\nm\u22121+\u03bb5\nm,c(1\u2212\u03b2)wc\u22121\nm\u22121\n+\u03bb3\nm,c\u03b2+\u03bb4\nm,c(1\u2212\u03b2)\nwhere z0\nm=\u0000\n1\u2212\u03b2+\u03bb6\nm,c\u03b2\u0001\nz0\nm\u22121andzc\n0= 1.\nCorollary 4.2. Expanding zC\nMgives a polynomial, p(\u03b2), in\n\u03b2of degree M. For any fixed memoryless threshold policy,\nthe optimal backdoor rate is \u03b2\u2217:= min \u03b2\u2208[0,1]p(\u03b2).\nA similar expression for usefulness, which does not de-\npend on \u03b2, can be found in appendix C. The safety of a fi-\nnite mixture of memoryless threshold policies can be found\nby taking a weighted average. Although we implemented a\nroot-finding algorithm based on theorem 4.2, we found that\nit was faster and more stable in practice to compute the val-\nues in fig. 2 using a combination of grid and ternary search.\n5 Finding Pareto-optimal Protocols\nIn many cases, AI-Control Games are useful for evaluating\nhand-picked protocols, for example the protocols explored\n12Since\u27e8Audit\u27e9and\u27e8Run\u27e9are equivalent when c= 0, assume\nthat Blue never attempts an audit.\n13See appendix C for the exact statements of \u03bbcoefficients.Algorithm 1: Finding a Pareto frontier for AI-C Game\nRequire: G\n1:fort\u2208 {0,1, . . . , k \u22121}do\n2: wt\u2190FindNextWeight ({(\u03c0B\nt, \u03c0R\nt)}k\u22121\nt=0)\n3: \u03c0B\nt, \u03c0R\nt\u2190argmax\u03c0B\u2208\u03a0Bmin\u03c0R\u2208\u03a0RVwt(\u03c0B, \u03c0R)\n4:return {(\u03c0B\nt, \u03c0R\nt)}k\u22121\nt=0\nby Greenblatt et al. (2024) that were found via trial and er-\nror. However, it may also be useful to utilise the AI-Control\nGames to find the Pareto-frontier of existing and new pro-\ntocols. This is a multi-objective maximin optimisation prob-\nlem over a large space. This section presents tools for finding\nPareto-optimal protocols, beginning with a reduction from a\nmulti-objective to a single-objective problem.\nLemma 5.1. A Blue policy \u03c0Bis in an AI-Control Equi-\nlibrium if, and only if, for some w\u2208[0,1], it maximises\nmin\u03c0R\u2217\u0000\nwSafety( \u00b7, \u03c0R\n\u2217) + (1 \u2212w) Use(\u00b7)\u0001\n.\nThe proof uses the mappings from \u03a0Band\u03a0Rto two sim-\nplexes introduced in appendix A. Lemma 5.1 implies that\neach safety/usefulness point corresponding to an AI-Control\nEquilibrium can be found by equivalently solving a, possi-\nbly easier, single-objective optimisation problem. Next, we\nleverage this result to further reduce the game to a single-\nobjective, partially observable stochastic game (ZSPOSG)\n(see definition A.3).\nLemma 5.2. For every AI-Control Game G, and weight\nw\u2208[0,1], there is a single-objective, zero-sum, partially\nobservable stochastic game Gwwith expected return (for\nBlue) VGwsuch that:\n\u2200\u03c0B, \u03c0R:VGw(\u03c0B, \u03c0R) =VG\nw(\u03c0B, \u03c0R).\nThis proof is constructive and a full version can be found\nin appendix A. Intuitively, we create ZSPOSGs by adding\na\u229bvariable to each state. Each Gwhas reward Rw, where\nR0rewards usefulness in \u2295games and R1rewards safety\nin\u2297games. Since \u03c0Rhas no effect on honest games, and\ntherefore on usefulness, we can set Red to minimise the use-\nfulness, making the game zero-sum without changing op-\ntima. Combined with lemma 5.1, this allows us to find any\nAI-Control Equilibrium by solving the right ZSPOSG.\nTheorem 5.3. A policy \u03c0Bis in an AI-Control Equilibrium\nif, and only if, for some ZSPOSG Gwwith value function\nVGw,\u03c0Bmaximises min\u03c0R\u2208\u03a0RVGw(\u00b7, \u03c0R)over\u03a0B.\nAlgorithm 1 uses theorem 5.3 to iteratively fill out the\nfrontier by solving a series of single-objective maximin\nproblems. Here FindNextWeight first returns w0= 0\nandw1= 1, and then chooses weights that maximise the\ndifference between known points on the Pareto frontier.14\nLine 3 of algorithm 1 requires finding the value of\nmax \u03c0Bmin\u03c0RVG\nw(\u03c0B, \u03c0R). Although ZSPOSGs are previ-\nously studied, existing solvers are incompatible with a mem-\n14Letst:= Safety( \u03c0B\nt, \u03c0R\nt)andut:= Use( \u03c0B\nt, \u03c0R\nt). Order\npoints by safety, then choose i= max i(si\u2212si+1)2+(ui\u2212ui+1)2\nand then wsuch that wsi+ (1\u2212w)ui=wsi+1+ (1\u2212w)ui+1.Algorithm 2: Adapted Double Oracle Algorithm\nRequire: Gw,\u03c0B\n1,\u03c0R\n1,\u03f5\n1:Initialize t\u21900\n2:while t= 0\u2228vt\u2212vt> \u03f5do\n3: t\u2190t+ 1\n4: p,q\u2190SolveNFG ({VG\nw(\u03c0B\ni, \u03c0R\nj)}1\u2264i,j\u2264t)\n5: \u03c3B\nt\u2190Mix(p,{\u03c0B\ni}t\ni=1),\u03c3R\nt\u2190Mix(q,{\u03c0R\nj}t\nj=1)\n6: \u03c0B\nt+1,vt\u2190(arg) max \u03c0BVG\nw(\u03c0B, \u03c3R\nt)\n7: \u03c0R\nt+1, vt\u2190(arg) min \u03c0RVG\nw(\u03c3B\nt, \u03c0R)\n8:return \u03c3B\nt,\u03c0R\nt+1\noryless Red team (see section 7). In general, the memory-\nlessness of Red means their decision rules at each timestep\nare interdependent, making problem decomposition diffi-\ncult. Further, imperfect recall prohibits the equivalence of\nbehavioural and mixed stochasticities:\nProposition 5.4. There exists a Gwsuch that Red\u2019s be-\nhavioural and mixed policies would not be equivalent and\nmax\n\u03c0B\u2208\u03a0Bmin\n\u03c0R\u2208\u03a0RVG\nw(\u03c0B, \u03c0R)<min\n\u03c0R\u2208\u03a0Rmax\n\u03c0B\u2208\u03a0BVG\nw(\u03c0B, \u03c0R)\nSee appendix A.3 for a proof by example.\nDespite this inequality, algorithm 2 adapts the Double Or-\nacle Algorithm from McMahan, Gordon, and Blum (2003)\nto find a maximin for behaviourally stochastic Red policies.\nThe function Mix simply creates a policy mixture from a\nvector and a finite set of policies. Finding pandqis equiv-\nalent to finding a Nash equilibrium in a zero-sum finite\nnormal-form game. The values vtandvtlowerbound and\nupperbound the maximin value of the game.\nProposition 5.5. For\u03f5 > 0, Algorithm 2 returns an \u03f5-\nequilibrium in Gwin finite time. Further, for all t:\nvt\u2264v\u2217:= max\n\u03c0B\u2217\u2208\u03a0Bmin\n\u03c0R\u2217\u2208\u03a0RVG\nw(\u03c0B\n\u2217, \u03c0R\n\u2217)\u2264vt\nIf\u03f5= 0then(\u03c3B\nt, \u03c0R\nt)is an AI-Control Equilibrium. The\nproof is shown in appendix A and utilises Adam et al. (2021,\nTheorem 1.1) by reducing Gwto a \u2018continuous game\u2019. The\nprimary change to the algorithm is the choice to return \u03c0R\nt+1\nwhich is behaviourally but not mixed stochastic, and gives\nthe worst-case safety for \u03c3B\ntwhich is a lower-bound for v\u2217.\nBest Response Oracles. Lines 6 and 7 of the algorithm re-\nquire finding best-responses. Finding the best-response \u03c0B\nto a (finite) policy mixture \u03c3Ris manageable: it\u2019s equivalent\nto solving a Partially-observable Markov Decision Process,\nwhich is a well-studied problem (Kaelbling, Littman, and\nCassandra 1998; Shani, Pineau, and Kaplow 2013). How-\never, finding a best-response \u03c0Rto\u03c3Bis, in general, more\nchallenging: because Red is memoryless, the problem is dif-\nficult to decompose. Fortunately, in the Trusted Monitor-\ning example described in section 2, Red\u2019s policy space is\n1-dimensional, and can be searched easily.\n6 Example: Results\nIn this section, we present example novel evaluations of pro-\ntocols: evaluating existing and novel protocols across ex-\nisting and novel settings. We also present the first Pareto0.00 0.25 0.50 0.75 1.00\nSafety Score0.00.20.40.60.81.0Usefulness Score\nOriginal Trusted Monitor\n0.00 0.25 0.50 0.75 1.00\nSafety Score0.00.20.40.60.81.0\nWeak Trusted Monitor\nAudit FPR = 1.0\nAudit FPR = 0.0\nAudit FPR = 0.02Audit FPR > 0.02\nMixture over Fixed FPRsUpfront Auditing\nNever Audit\nAudit FPR = 0.02\nTrusted Model Only\nMixed Policy\n0.75 0.790.80.84\n0.5 0.60.50.6\nFigure 2: The safety and usefulness frontiers for a range of Trusted Monitor protocol types across the \u2018original\u2019 and \u2018weak\u2019\ngames. Each point on the grid corresponds to the exact safety and usefulness of a proposed blue team protocol against an\napproximately optimal red team response (minimal safety). The larger circles correspond to the seven special cases of Trusted\nMonitoring studied by Greenblatt et al. (2024) and remaining points are new to this paper. Each \u2018Audit FPR\u2019 line plots a single,\nfixed audit FPR, across fixed defer FPRs in {0.00,0.01, . . . , 1.00}. \u2018Mixture over Fixed FPRs\u2019 shows the optimal protocol\nfrontier found by algorithm 1 with a fixed-FPR oracle, and all points represent deterministic policies unless marked by a cross.\nfrontiers for a subset of the protocol space. All results pre-\nsented concern only memoryless protocols: namely, proto-\ncols that decide on the current action based only on the last\nobservation. These can be evaluated efficiently using propo-\nsition 4.1. We leave the analysis of memoryful protocols,\nthose whose actions depend on past observations, to future\nwork. See appendix D for experimental details.\nExisting Protocols. All of the protocols studied in Green-\nblatt et al. (2024) can be expressed as memoryless proto-\ncols: Never Audit always submit U\u2019s solution without audit.\nTrusted Model Only always submits T\u2019s solution. Upfront\nAuditing audits the first 2% of solutions from U and, when\nthere is no audit budget left, submit the rest without check-\ning.Trusted Monitoring (Audit FPR = 0.02) audits every so-\nlution from U whose TM score is above a threshold chosen\nto give a 2% FPR and also defer all solutions above (0%, 5%,\n10%, 20%, 50%). We leave evaluation of Greenblatt et al.\u2019s\nother protocols to future work. Each protocol above is a spe-\ncial case of Trusted Monitoring with a fixed audit FPR of\n0%, 2% or 100%, combined with fixed defer FPRs. By rep-\nresenting these protocols in our formalisation, we can easily\nevaluate their safety and usefulness scores, even in the novel\nscenario containing the weak trusted monitor; we illustrate\nthese results by coloured points in fig. 2.\nEvaluating New Protocols. Through a process of propos-\ning and efficiently evaluating new protocols, enabled by our\nformalisation, we extend Greenblatt et al.\u2019s findings to new\nprotocols and settings. As an example, we show that raising\nthe audit FPR above 2% Pareto improves upon the proto-\ncols from Greenblatt et al. (2024). Choosing an audit FPR of\n2% means that, in expectation, the 2% allowance of human\nlabour will be used up at the end of the episode if no back-door is submitted. However, we find that choosing a higher\naudit rate can be beneficial, despite increasing the proba-\nbility that the audit budget is exhausted early. Choosing an\naudit FPR of 3%gives a small Pareto improvement in the\n\u2018original\u2019 TM case and an audit FPR of 5%, gives a large\nPareto improvement in the \u2018weak\u2019 TM case.\nDemonstrating a Frontier. Using algorithm 1 and algo-\nrithm 2, we present the first approximate Pareto-frontier for\na policy subset : mixtures of protocols with fixed audit and\ndefer rates. We apply algorithm 2 using a Blue best-response\nalgorithm based on recursive grid search (over values of au-\ndit and defer FPR) and a Red best-response algorithm using\ngrid search followed by ternary search (over values of \u03b2).\nWe find that, in the \u2018original\u2019 case the closest of Greenblatt\net al.\u2019s policies is 0.0042 from the Pareto front, and for most\nweights a deterministic policy is optimal. However, in the\n\u2018weak\u2019 case, they are at least \u22650.0200 from the Pareto front\nand mixed protocols make up most of the frontier.\n6.1 Assumptions and Trends\nFormalising an AI-Control Game requires making a num-\nber of modelling assumptions; the results below outline how\nvarying each assumption impacts the safety and usefulness\nscores for a given blue-team policy.\nAuditing Budget. The auditing budget corresponds to the\namount of human labour invested in overseeing the proto-\ncol. Protocols that utilise increases to the audit budget, get\nsignificantly safer and more useful as the budget increases.\nespecially when the budget is small.\nNumber of Problems. For fixed FPRs, safety and useful-\nness are not significantly affected by the number of prob-0.00.51.0Safety\n0.00.51.0\n0.00.51.0\n0.00.51.0\n0.0 0.5 1.0\nBudget0.00.51.0Usefulness\n100 1000 10000\n# Problems0.00.51.0\n0.0 0.5 1.0\nHuman FNR0.00.51.0\n0.0 0.5 1.0\nHuman FPR0.00.51.0\nAudit FPR = 0.0\nAudit FPR = 0.02\nAudit FPR = 1.0\nAudit FPR = BudgetFigure 3: Impact on safety and usefulness ( y-axis ) by four different modelling assumptions ( x-axis ): Auditing Budget, Number\nof Programming Problems, and Human Auditor False Negative / Positive Rate. All plots show a fixed defer FPR of 0.1.\nlems per episode. As the number of opportunities for Red to\nbackdoor increases, so too does the number of audit coins.\nHuman Auditor FPR/FNR. In the default Trusted Mon-\nitoring game, Greenblatt et al. assume that no mistakes are\nmade by the human auditor; we can extend the model to re-\nlax this assumption. Increasing the human FNR decreases\nsafety almost linearly in the worst case, without impact-\ning usefulness. False positives increase safety up to a point\ndetermined by the audit threshold, but decrease usefulness\nmore rapidly the more the blue team audits.\n7 Related Work\nAlthough the application of a formal game-theoretic frame-\nwork to Greenblatt et al. (2024)\u2019s AI Control framework is\nnovel, and the resulting type of game is unique, there are\nanalogues to existing work. In this section, we compare AI-\nControl Games to existing models.\nThe AI-Control Games studied are most similar to the\nwork of Chang, Erera, and White (2015), who model a\nsecurity problem for food manufacturing. Like AI-Control\nGames, they consider a leader-follower equilibrium, where\nthe leader tries to maximise productivity and minimise risk,\nand the follower tries to hide an attack. To select the leader\nstrategy against multiple objectives, they use a genetic algo-\nrithm (as do Lins et al. (2013)). Their model differs in its\nfactored state space, infinite time horizon, and the lack of\n\u2018scheming/honest\u2019 game modes. However, the largest differ-\nence is their consideration of a memoryful-follower.\nGiven our reduction in section 5, methods for finding\nequilibria in ZS-POSGs are particularly relevant. How-\never, we find that our requirement for a memoryful-leader,\nmemoryless-follower equilibrium precludes the use of many\nexisting solvers. For example, Delage et al. (2023), which\nfinds Nash equilibria in ZS-POSGs using a dynamic pro-\ngramming approach that solves subgames for each timestep\nfrom Hdown to 1. Other works attempt to solve sub-\nclasses of ZS-POSGs, with \u2018public observations\u2019 (Hor \u00b4ak and\nBo\u02c7sansk \u00b4y 2019) or \u2018on-sided partial-observability\u2019 (Zheng,\nJung, and Lin 2022; Hor \u00b4ak et al. 2023). AI-Control Games\ndo not in-general fit into these subclasses, and each of thesepapers considers two memoryful players.\nSince in some cases it is possible to find an AI-Control\nEquilibrium by first finding a mixed Nash equilibrium (the-\norem 5.5), it may be possible to augment the algorithm by\nHansen and Feng (2000), which searches for two mixed\nstrategies over a finite set of memoryful policies.\nAI-Control Games, and especially the Trusted Monitor-\ning case study we present, are closely related to Stackelberg\nsecurity games: two-player games where the leader must\nchoose a random strategy to cover a set of targets using a\nlimited set of resources, and the follower tries to attack an\nuncovered target (see (Sinha et al. 2018) for an overview).\nUnlike Trusted Monitoring, these are one-round games and\nmuch literature focuses on scalably solving large-scale prob-\nlems against scheduling constraints. Although some secu-\nrity games model imperfect information (Bier 2007; Nguyen\net al. 2014) or multiple objectives (Brown et al. 2012), most\nare single-objective and perfect-information.\nTrusted Monitoring without defer is similar to the Inspec-\ntion Games studied by Avenhaus, V on Stengel, and Zamir\n(2002, p. 1957): games where the defender receives a noisy\nobservation, and has to decide whether to inspect. How-\never, these games are one-round only. Recursive Inspection\nGames (Dresher 1962) are multi-round, but differ from our\ncase in the memoryfullness of Red.\n8 Future Work\nIn this work, we introduced a novel model for analysing the\nsafety and usefulness of AI-Control protocols. We used our\nmodel and solution techniques to evaluate a much broader\nrange of protocols and settings, but there is significant room\nto apply them further. In the Trusted Monitoring setting, fu-\nture work can utilise algorithm 2 with POMDP-based best-\nresponse oracles to explore the space of memory-dependent\nprotocols. Our model of the Trusted Monitoring protocol\nis also generalisable to any deployment setting using a\nquestion-answer format and any \u2018unacceptable outcome\u2019\nthat is a function of U\u2019s answer. Future work can explore\nsuch settings by empirically studying the interaction"}]}
